# Comparing `tmp/surfalize-0.8.2-cp39-cp39-win_amd64.whl.zip` & `tmp/surfalize-0.9.0-cp39-cp39-win_amd64.whl.zip`

## zipinfo {}

```diff
@@ -1,32 +1,37 @@
-Zip file size: 152466 bytes, number of entries: 30
--rw-rw-rw-  2.0 fat       86 b- defN 24-Apr-22 12:45 surfalize/__init__.py
--rw-rw-rw-  2.0 fat     8210 b- defN 24-Apr-22 12:45 surfalize/abbottfirestone.py
--rw-rw-rw-  2.0 fat     5372 b- defN 24-Apr-22 12:45 surfalize/autocorrelation.py
--rw-rw-rw-  2.0 fat    23053 b- defN 24-Apr-22 12:45 surfalize/batch.py
--rw-rw-rw-  2.0 fat     2551 b- defN 24-Apr-22 12:45 surfalize/cache.py
--rw-rw-rw-  2.0 fat   198144 b- defN 24-Apr-22 12:46 surfalize/calculations.cp39-win_amd64.pyd
--rw-rw-rw-  2.0 fat      578 b- defN 24-Apr-22 12:45 surfalize/exceptions.py
--rw-rw-rw-  2.0 fat     3994 b- defN 24-Apr-22 12:45 surfalize/filter.py
--rw-rw-rw-  2.0 fat     7424 b- defN 24-Apr-22 12:45 surfalize/mathutils.py
--rw-rw-rw-  2.0 fat     2282 b- defN 24-Apr-22 12:45 surfalize/profile.py
--rw-rw-rw-  2.0 fat    73225 b- defN 24-Apr-22 12:45 surfalize/surface.py
--rw-rw-rw-  2.0 fat      930 b- defN 24-Apr-22 12:45 surfalize/utils.py
--rw-rw-rw-  2.0 fat       60 b- defN 24-Apr-22 12:45 surfalize/file/__init__.py
--rw-rw-rw-  2.0 fat     2851 b- defN 24-Apr-22 12:45 surfalize/file/al3d.py
--rw-rw-rw-  2.0 fat     4210 b- defN 24-Apr-22 12:45 surfalize/file/common.py
--rw-rw-rw-  2.0 fat     1904 b- defN 24-Apr-22 12:45 surfalize/file/loader.py
--rw-rw-rw-  2.0 fat      893 b- defN 24-Apr-22 12:45 surfalize/file/nms.py
--rw-rw-rw-  2.0 fat     2880 b- defN 24-Apr-22 12:45 surfalize/file/opd.py
--rw-rw-rw-  2.0 fat     1798 b- defN 24-Apr-22 12:45 surfalize/file/plu.py
--rw-rw-rw-  2.0 fat      743 b- defN 24-Apr-22 12:45 surfalize/file/plux.py
--rw-rw-rw-  2.0 fat     1972 b- defN 24-Apr-22 12:45 surfalize/file/sdf.py
--rw-rw-rw-  2.0 fat     6971 b- defN 24-Apr-22 12:45 surfalize/file/sur.py
--rw-rw-rw-  2.0 fat     5472 b- defN 24-Apr-22 12:45 surfalize/file/vk.py
--rw-rw-rw-  2.0 fat      931 b- defN 24-Apr-22 12:45 surfalize/file/xyz.py
--rw-rw-rw-  2.0 fat      888 b- defN 24-Apr-22 12:45 surfalize/file/zmg.py
--rw-rw-rw-  2.0 fat    35817 b- defN 24-Apr-22 12:46 surfalize-0.8.2.dist-info/LICENSE.txt
--rw-rw-rw-  2.0 fat    17416 b- defN 24-Apr-22 12:46 surfalize-0.8.2.dist-info/METADATA
--rw-rw-rw-  2.0 fat      100 b- defN 24-Apr-22 12:46 surfalize-0.8.2.dist-info/WHEEL
--rw-rw-rw-  2.0 fat       10 b- defN 24-Apr-22 12:46 surfalize-0.8.2.dist-info/top_level.txt
--rw-rw-r--  2.0 fat     2383 b- defN 24-Apr-22 12:46 surfalize-0.8.2.dist-info/RECORD
-30 files, 413148 bytes uncompressed, 148696 bytes compressed:  64.0%
+Zip file size: 243039 bytes, number of entries: 35
+-rw-rw-rw-  2.0 fat      121 b- defN 24-May-01 19:37 surfalize/__init__.py
+-rw-rw-rw-  2.0 fat       21 b- defN 24-May-01 19:37 surfalize/_version.py
+-rw-rw-rw-  2.0 fat     9112 b- defN 24-May-01 19:37 surfalize/abbottfirestone.py
+-rw-rw-rw-  2.0 fat     5372 b- defN 24-May-01 19:37 surfalize/autocorrelation.py
+-rw-rw-rw-  2.0 fat    23053 b- defN 24-May-01 19:37 surfalize/batch.py
+-rw-rw-rw-  2.0 fat     2551 b- defN 24-May-01 19:37 surfalize/cache.py
+-rw-rw-rw-  2.0 fat      578 b- defN 24-May-01 19:37 surfalize/exceptions.py
+-rw-rw-rw-  2.0 fat     3994 b- defN 24-May-01 19:37 surfalize/filter.py
+-rw-rw-rw-  2.0 fat     1529 b- defN 24-May-01 19:37 surfalize/image.py
+-rw-rw-rw-  2.0 fat     7424 b- defN 24-May-01 19:37 surfalize/mathutils.py
+-rw-rw-rw-  2.0 fat     2282 b- defN 24-May-01 19:37 surfalize/profile.py
+-rw-rw-rw-  2.0 fat    73274 b- defN 24-May-01 19:37 surfalize/surface.py
+-rw-rw-rw-  2.0 fat      930 b- defN 24-May-01 19:37 surfalize/utils.py
+-rw-rw-rw-  2.0 fat       60 b- defN 24-May-01 19:37 surfalize/file/__init__.py
+-rw-rw-rw-  2.0 fat     2934 b- defN 24-May-01 19:37 surfalize/file/al3d.py
+-rw-rw-rw-  2.0 fat     5767 b- defN 24-May-01 19:37 surfalize/file/common.py
+-rw-rw-rw-  2.0 fat    10703 b- defN 24-May-01 19:37 surfalize/file/gwy.py
+-rw-rw-rw-  2.0 fat     1937 b- defN 24-May-01 19:37 surfalize/file/loader.py
+-rw-rw-rw-  2.0 fat     1402 b- defN 24-May-01 19:37 surfalize/file/nms.py
+-rw-rw-rw-  2.0 fat     4884 b- defN 24-May-01 19:37 surfalize/file/opd.py
+-rw-rw-rw-  2.0 fat     2546 b- defN 24-May-01 19:37 surfalize/file/plu.py
+-rw-rw-rw-  2.0 fat     2233 b- defN 24-May-01 19:37 surfalize/file/plux.py
+-rw-rw-rw-  2.0 fat     2019 b- defN 24-May-01 19:37 surfalize/file/sdf.py
+-rw-rw-rw-  2.0 fat    15815 b- defN 24-May-01 19:37 surfalize/file/sur.py
+-rw-rw-rw-  2.0 fat     8106 b- defN 24-May-01 19:37 surfalize/file/vk.py
+-rw-rw-rw-  2.0 fat     1038 b- defN 24-May-01 19:37 surfalize/file/xyz.py
+-rw-rw-rw-  2.0 fat      935 b- defN 24-May-01 19:37 surfalize/file/zmg.py
+-rw-rw-rw-  2.0 fat       71 b- defN 24-May-01 19:37 surfalize/roughness/__init__.py
+-rw-rw-rw-  2.0 fat   177664 b- defN 24-May-01 19:38 surfalize/roughness/height.cp39-win_amd64.pyd
+-rw-rw-rw-  2.0 fat   205312 b- defN 24-May-01 19:38 surfalize/roughness/hybrid.cp39-win_amd64.pyd
+-rw-rw-rw-  2.0 fat    35817 b- defN 24-May-01 19:38 surfalize-0.9.0.dist-info/LICENSE.txt
+-rw-rw-rw-  2.0 fat     9092 b- defN 24-May-01 19:38 surfalize-0.9.0.dist-info/METADATA
+-rw-rw-rw-  2.0 fat      100 b- defN 24-May-01 19:38 surfalize-0.9.0.dist-info/WHEEL
+-rw-rw-rw-  2.0 fat       10 b- defN 24-May-01 19:38 surfalize-0.9.0.dist-info/top_level.txt
+-rw-rw-r--  2.0 fat     2811 b- defN 24-May-01 19:38 surfalize-0.9.0.dist-info/RECORD
+35 files, 621497 bytes uncompressed, 238609 bytes compressed:  61.6%
```

## zipnote {}

```diff
@@ -1,31 +1,34 @@
 Filename: surfalize/__init__.py
 Comment: 
 
+Filename: surfalize/_version.py
+Comment: 
+
 Filename: surfalize/abbottfirestone.py
 Comment: 
 
 Filename: surfalize/autocorrelation.py
 Comment: 
 
 Filename: surfalize/batch.py
 Comment: 
 
 Filename: surfalize/cache.py
 Comment: 
 
-Filename: surfalize/calculations.cp39-win_amd64.pyd
-Comment: 
-
 Filename: surfalize/exceptions.py
 Comment: 
 
 Filename: surfalize/filter.py
 Comment: 
 
+Filename: surfalize/image.py
+Comment: 
+
 Filename: surfalize/mathutils.py
 Comment: 
 
 Filename: surfalize/profile.py
 Comment: 
 
 Filename: surfalize/surface.py
@@ -39,14 +42,17 @@
 
 Filename: surfalize/file/al3d.py
 Comment: 
 
 Filename: surfalize/file/common.py
 Comment: 
 
+Filename: surfalize/file/gwy.py
+Comment: 
+
 Filename: surfalize/file/loader.py
 Comment: 
 
 Filename: surfalize/file/nms.py
 Comment: 
 
 Filename: surfalize/file/opd.py
@@ -69,23 +75,32 @@
 
 Filename: surfalize/file/xyz.py
 Comment: 
 
 Filename: surfalize/file/zmg.py
 Comment: 
 
-Filename: surfalize-0.8.2.dist-info/LICENSE.txt
+Filename: surfalize/roughness/__init__.py
+Comment: 
+
+Filename: surfalize/roughness/height.cp39-win_amd64.pyd
+Comment: 
+
+Filename: surfalize/roughness/hybrid.cp39-win_amd64.pyd
+Comment: 
+
+Filename: surfalize-0.9.0.dist-info/LICENSE.txt
 Comment: 
 
-Filename: surfalize-0.8.2.dist-info/METADATA
+Filename: surfalize-0.9.0.dist-info/METADATA
 Comment: 
 
-Filename: surfalize-0.8.2.dist-info/WHEEL
+Filename: surfalize-0.9.0.dist-info/WHEEL
 Comment: 
 
-Filename: surfalize-0.8.2.dist-info/top_level.txt
+Filename: surfalize-0.9.0.dist-info/top_level.txt
 Comment: 
 
-Filename: surfalize-0.8.2.dist-info/RECORD
+Filename: surfalize-0.9.0.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## surfalize/__init__.py

```diff
@@ -1,3 +1,4 @@
+from ._version import __version__
 from .surface import Surface
 from .profile import Profile
 from .batch import Batch
```

## surfalize/abbottfirestone.py

```diff
@@ -96,44 +96,103 @@
 
         self._smr_fit = interp1d(height, material_ratio)
         self._height = height
         self._material_ratio = material_ratio
 
     @cache
     def Sk(self):
+        """
+        Calculates Sk.
+
+        Returns
+        -------
+        float
+        """
         return self._yupper - self._ylower
 
     def Smr(self, c):
+        """
+        Calculates Smr(c).
+
+        Parameters
+        ----------
+        c: float
+            Material height.
+
+        Returns
+        -------
+        float
+        """
         return float(self._smr_fit(c))
 
     def Smc(self, mr):
+        """
+        Calculates Smc(mr).
+
+        Parameters
+        ----------
+        mr: float
+            Material ratio.
+
+        Returns
+        -------
+        float
+        """
         return float(self._smc_fit(mr))
 
     @cache
     def Smr1(self):
+        """
+        Calculates Smr1.
+
+        Returns
+        -------
+        float
+        """
         return self.Smr(self._yupper)
 
     @cache
     def Smr2(self):
+        """
+        Calculates Smr2.
+
+        Returns
+        -------
+        float
+        """
         return self.Smr(self._ylower)
 
     @cache
     def Spk(self):
+        """
+        Calculates Spk.
+
+        Returns
+        -------
+        float
+        """
         # For now we are using the closest value in the array to ylower
         # This way, we are losing or gaining a bit of area. In the future we might use some
         # additional interpolation. For now this is sufficient.
 
         # Area enclosed above yupper between y-axis (at x=0) and abbott-firestone curve
         idx = argclosest(self._yupper, self._height)
         A1 = np.abs(np.trapz(self._material_ratio[:idx], x=self._height[:idx]))
         Spk = 2 * A1 / self.Smr1()
         return Spk
 
     @cache
     def Svk(self):
+        """
+        Calculates Svk.
+
+        Returns
+        -------
+        float
+        """
         # Area enclosed below ylower between y-axis (at x=100) and abbott-firestone curve
         idx = argclosest(self._ylower, self._height)
         A2 = np.abs(np.trapz(100 - self._material_ratio[idx:], x=self._height[idx:]))
         Svk = 2 * A2 / (100 - self.Smr2())
         return Svk
 
     @cache
```

## surfalize/surface.py

```diff
@@ -21,20 +21,17 @@
 from .utils import is_list_like, register_returnlabels
 from .cache import CachedInstance, cache
 from .mathutils import Sinusoid, argclosest, interp1d
 from .autocorrelation import AutocorrelationFunction
 from .abbottfirestone import AbbottFirestoneCurve
 from .profile import Profile
 from .filter import GaussianFilter
-try:
-    from .calculations import surface_area
-    CYTHON_DEFINED = True
-except ImportError:
-    logger.warning('Could not import cythonized code. Surface area calculation unavailable.')
-    CYTHON_DEFINED = False
+from .roughness import height_parameters, surface_area
+from .image import Image
+
 
 size = namedtuple('Size', ['y', 'x'])
            
 def no_nonmeasured_points(function):
     """
     Decorator that raises an Exception if the method is called on a surface object that contains non-measured points.
     This decorator should be used for any method that does not compute correctly if nan values are present in the array.
@@ -126,19 +123,22 @@
     >>> filepath = r'path\\to\\surface.plu'
     >>> surface = Surface.load(filepath)
     """
     AVAILABLE_PARAMETERS = ('Sa', 'Sq', 'Sp', 'Sv', 'Sz', 'Ssk', 'Sku', 'Sdr', 'Sdq', 'Sal', 'Str', 'Sk', 'Spk', 'Svk',
                             'Smr1', 'Smr2', 'Sxp', 'Vmp', 'Vmc', 'Vvv', 'Vvc', 'period', 'depth', 'aspect_ratio',
                             'homogeneity', 'stepheight', 'cavity_volume')
     
-    def __init__(self, height_data, step_x, step_y):
+    def __init__(self, height_data, step_x, step_y, metadata=None, image_layers=None):
         super().__init__() # Initialize cached instance
         self.data = height_data
         self.step_x = step_x
         self.step_y = step_y
+        self.metadata = metadata if metadata is not None else {}
+        self.image_layers = image_layers if image_layers is not None else {}
+
         self.width_um = (height_data.shape[1] - 1) * step_x
         self.height_um = (height_data.shape[0] - 1) * step_y
         # True if non-measured points exist on the surface
         self._nonmeasured_points_exist = np.any(np.isnan(self.data))
 
     @property
     def size(self):
@@ -251,29 +251,35 @@
             return False
         return True
 
     def __hash__(self):
         return hash((self.step_x, self.step_y, self.size.x, self.size.y, self.data.mean(), self.data.std()))
 
     @classmethod
-    def load(cls, filepath, encoding='utf-8'):
+    def load(cls, filepath, encoding='utf-8', read_image_layers=False):
         """
         Classmethod to load a topography from a file.
 
         Parameters
         ----------
         filepath: str | pathlib.Path
             Filepath pointing to the topography file.
         encoding: str, Default utf-8
             Encoding of characters in the file. Defaults to utf-8.
+        read_image_layers: bool, Default False
+            If true, reads all available image layers in the file and saves them in Surface.image_layers dict
+
         Returns
         -------
         surface: surfalize.Surface
         """
-        return cls(*load_file(filepath, encoding=encoding))
+        raw_surface = load_file(filepath, encoding=encoding, read_image_layers=read_image_layers)
+        image_layers = {k: Image(v) for k, v in raw_surface.image_layers.items()}
+        return cls(raw_surface.data, raw_surface.step_x, raw_surface.step_y, metadata=raw_surface.metadata,
+                   image_layers=image_layers)
 
     def save(self, filepath, encoding='utf-8'):
         """
         Saves the surface to a supported file format.
 
         Parameters
         ----------
@@ -282,14 +288,24 @@
         encoding: str, Default utf-8
             Encoding of characters in the file. Defaults to utf-8.
         Returns
         -------
         None
         """
         write_file(filepath, self, encoding=encoding)
+
+    def get_image_layer_names(self):
+        """
+        Returns a list of the names of available image layers.
+
+        Returns
+        -------
+        List[str]
+        """
+        return list(self.image_layers.keys())
         
     def get_horizontal_profile(self, y, average=1, average_step=None):
         """
         Extracts a horizontal profile from the surface with optional averaging over parallel profiles.
         Profiles on the edge might be averaged over fewer profiles.
 
         Parameters
@@ -958,100 +974,90 @@
         mask_volume = self.data < upper_median - threshold * (stepheight)
         volume = (upper_median - self.data[mask_volume]).sum() * self.step_x * self.step_y
         return volume
 
     # Characterization #################################################################################################
    
     # Height parameters ################################################################################################
-    
-    @no_nonmeasured_points
+
     @cache
+    def height_parameters(self):
+        return height_parameters(self.data, self.data.mean())
+
     def Sa(self):
         """
         Calcualtes the arithmetic mean height Sa according to ISO 25178-2.
 
         Returns
         -------
         Sa: float
         """
-        return (np.abs(self.data - self.data.mean()).sum() / self.data.size)
-    
-    @no_nonmeasured_points
-    @cache
+        return self.height_parameters()['Sa']
+
     def Sq(self):
         """
         Calcualtes the root mean square height Sq according to ISO 25178-2.
 
         Returns
         -------
         Sq: float
         """
-        return np.sqrt(((self.data - self.data.mean()) ** 2).sum() / self.data.size).round(8)
-    
-    @no_nonmeasured_points
-    @cache
+        return self.height_parameters()['Sq']
+
     def Sp(self):
         """
         Calcualtes the maximum peak height Sp according to ISO 25178-2.
 
         Returns
         -------
         Sp: float
         """
-        return (self.data - self.data.mean()).max()
-    
-    @no_nonmeasured_points
-    @cache
+        return self.height_parameters()['Sp']
+
     def Sv(self):
         """
         Calcualtes the maximum pit height Sv according to ISO 25178-2.
 
         Returns
         -------
         Sv: float
         """
-        return np.abs((self.data - self.data.mean()).min())
-    
-    @no_nonmeasured_points
-    @cache
+        return self.height_parameters()['Sv']
+
     def Sz(self):
         """
         Calcualtes the skewness Ssk according to ISO 25178-2.
 
         Returns
         -------
         Ssk: float
         """
-        return self.Sp() + self.Sv()
-    
-    @no_nonmeasured_points
-    @cache
+        return self.height_parameters()['Sz']
+
     def Ssk(self):
         """
         Calcualtes the skewness Ssk according to ISO 25178-2. It is the quotient of the mean cube value of the ordinate
         values and the cube of Sq within a definition area.
 
         Returns
         -------
         Ssk: float
         """
-        return ((self.data - self.data.mean()) ** 3).sum() / self.data.size / self.Sq()**3
-    
-    @no_nonmeasured_points
-    @cache
+        return self.height_parameters()['Ssk']
+
     def Sku(self):
         """
         Calcualtes the kurtosis Sku  according to ISO 25178-2. It is the quotient of the mean quartic value of the
         ordinate values and the fourth power of Sq within a definition area.
 
         Returns
         -------
         Sku: float
         """
-        return ((self.data - self.data.mean()) ** 4).sum() / self.data.size / self.Sq()**4
+        return self.height_parameters()['Sku']
     
     # Hybrid parameters ################################################################################################
 
     @cache
     def projected_area(self):
         """
         Calculates the projected surface area.
@@ -1060,54 +1066,34 @@
         -------
         projected area: float
         """
         return (self.width_um - self.step_x) * (self.height_um - self.step_y)
     
     @no_nonmeasured_points
     @cache
-    def surface_area(self, method='iso'):
+    def surface_area(self):
         """
-        Calculates the surface area of the surface. The method parameter can be either 'iso' or 'gwyddion'. The default
-        method is the 'iso' method proposed by ISO 25178 and used by MountainsMap, whereby two triangles are
-        spanned between four corner points. The 'gwyddion' method implements the approach used by the open-source
-        software Gwyddion, whereby four triangles are spanned between four corner points and their calculated center
-        point. The method is detailed here: http://gwyddion.net/documentation/user-guide-en/statistical-analysis.html.
+        Calculates the surface area of the surface according to the method proposed by ISO 25178 and used by
+        MountainsMap, whereby two triangles are spanned between four corner points.
 
-        Parameters
-        ----------
-        method: str, Default 'iso'
-            The method by which to calculate the surface area.
         Returns
         -------
         area: float
         """
-        if not CYTHON_DEFINED:
-            raise NotImplementedError("Surface area calculation is based on cython code. Compile cython code to run this"
-                                      "method")
-        return surface_area(self.data, self.step_x, self.step_y, method=method)
-    
-    @no_nonmeasured_points
-    @cache
-    def Sdr(self, method='iso'):
+        return surface_area(self.data, self.step_x, self.step_y)
+
+    def Sdr(self):
         """
-        Calculates Sdr. The method parameter can be either 'iso' or 'gwyddion'. The default method is the 'iso' method
-        proposed by ISO 25178 and used by MountainsMap, whereby two triangles are spanned between four corner points.
-        The 'gwyddion' method implements the approach used by the open-source software Gwyddion, whereby four triangles
-        are spanned between four corner points and their calculated center point. The method is detailed here:
-        http://gwyddion.net/documentation/user-guide-en/statistical-analysis.html.
+        Calculates the developed interfacial area ratio according to ISO 25178-2.
 
-        Parameters
-        ----------
-        method: str, Default 'iso'
-            The method by which to calculate the surface area.
         Returns
         -------
         area: float
         """
-        return (self.surface_area(method=method) / self.projected_area() -1) * 100
+        return (self.surface_area() / self.projected_area() -1) * 100
     
     @no_nonmeasured_points
     @cache
     def Sdq(self):
         """
         Calculates the root mean square gradient Sdq according to ISO 25178-2.
 
@@ -1715,18 +1701,19 @@
         Returns
         -------
         None
         """
         abbott_curve = self._get_abbott_firestone_curve()
         abbott_curve.plot(nbars=nbars)
         
-    def plot_fourier_transform(self, log=True, hanning=False, subtract_mean=True, fxmax=None, fymax=None, cmap='inferno', adjust_colormap=True):
+    def plot_fourier_transform(self, log=True, hanning=False, subtract_mean=True, fxmax=None, fymax=None,
+                               cmap='inferno', adjust_colormap=True):
         """
-        Plots the 2d Fourier transform of the surface. Optionally, a Hanning window can be applied to reduce to spectral leakage effects 
-        that occur when analyzing a signal of finite sample length.
+        Plots the 2d Fourier transform of the surface. Optionally, a Hanning window can be applied to reduce to spectral
+        leakage effects that occur when analyzing a signal of finite sample length.
 
         Parameters
         ----------
         log: bool, Default True
             Shows the logarithm of the Fourier Transform to increase peak visibility.
         hanning: bool, Default False
             Applys a Hanning window to the data before the transform.
@@ -1791,24 +1778,27 @@
         ax.set_xlabel('Frequency [µm$^{-1}$]')
         ax.set_ylabel('Frequency [µm$^{-1}$]')
         extent = (freq_x[ixmin], freq_x[ixmax], freq_y[iymax], freq_y[iymin])
 
         ax.imshow(fft, cmap=cmap, vmin=vmin, vmax=vmax, extent=extent)
         return ax
 
-    def plot_2d(self, cmap='jet', maskcolor='black', ax=None):
+    def plot_2d(self, cmap='jet', maskcolor='black', layer='Topography', ax=None):
         """
         Creates a 2D-plot of the surface using matplotlib.
 
         Parameters
         ----------
         cmap: str | mpl.cmap, default 'jet'
-            Colormap to apply on the data.
+            Colormap to apply on the topography layer. Argument has no effect if an image layer is selected.
         maskcolor: str, default 'Black'
             Color for masked values.
+        layer: str, default Topography
+            Indicate the layer to plot, by default the topography layer is shown. Alternatively, the label of an image
+            layer can be indicated.
         ax: matplotlib axis, default None
             If specified, the plot will be drawn the specified axis.
 
         Returns
         -------
         ax.
         """
@@ -1816,35 +1806,53 @@
         cmap.set_bad(maskcolor)
         if ax is None:
             fig, ax = plt.subplots(dpi=150)
         else:
             fig = ax.figure
         divider = make_axes_locatable(ax)
         cax = divider.append_axes("right", size="5%", pad=0.05)
-        im = ax.imshow(self.data, cmap=cmap, extent=(0, self.width_um, 0, self.height_um))
-        fig.colorbar(im, cax=cax, label='z [µm]')
+        if layer == 'Topography':
+            data = self.data
+            show_cbar = True
+        elif layer in self.image_layers.keys():
+            data = self.image_layers[layer].data
+            show_cbar = False
+            if data.ndim == 3:
+                cmap = None
+            elif data.ndim == 2:
+                cmap = 'gray'
+        else:
+            raise ValueError(f'Layer {layer} does not exist.')
+        im = ax.imshow(data, cmap=cmap, extent=(0, self.width_um, 0, self.height_um))
+        if show_cbar:
+            fig.colorbar(im, cax=cax, label='z [µm]')
+        else:
+            cax.axis('off')
         ax.set_xlabel('x [µm]')
         ax.set_ylabel('y [µm]')
-        if self._nonmeasured_points_exist:
+        if layer == 'Topography' and self._nonmeasured_points_exist:
             handles = [plt.plot([], [], marker='s', c=maskcolor, ls='')[0]]
             ax.legend(handles, ['non-measured points'], loc='lower right', fancybox=False, framealpha=1, fontsize=6)
         return ax
     
-    def show(self, cmap='jet', maskcolor='black', ax=None):
+    def show(self, cmap='jet', maskcolor='black', layer='Topography', ax=None):
         """
         Shows a 2D-plot of the surface using matplotlib.
 
         Parameters
         ----------
         cmap: str | mpl.cmap, default 'jet'
-            Colormap to apply on the data.
+            Colormap to apply on the topography layer. Argument has no effect if an image layer is selected.
         maskcolor: str, default 'Black'
             Color for masked values.
+        layer: str, default Topography
+            Indicate the layer to plot, by default the topography layer is shown. Alternatively, the label of an image
+            layer can be indicated.
         ax: matplotlib axis, default None
             If specified, the plot will be drawn the specified axis.
 
         Returns
         -------
         None.
         """
-        self.plot_2d(cmap=cmap, maskcolor=maskcolor, ax=ax)
+        self.plot_2d(cmap=cmap, maskcolor=maskcolor, layer=layer, ax=ax)
         plt.show()
```

## surfalize/file/al3d.py

```diff
@@ -1,11 +1,11 @@
 import struct
 import numpy as np
 from ..exceptions import CorruptedFileError
-
+from .common import RawSurface
 
 MAGIC = b'AliconaImaging\x00\r\n'
 TAG_LAYOUT = '20s30s2s'
 DTYPE = 'float32'
 
 def read_tag(filehandle):
     key, value, lf = [val.decode().rstrip('\x00') for val in struct.unpack(TAG_LAYOUT, filehandle.read(52))]
@@ -40,15 +40,15 @@
             write_tag(file, key, value, encoding=encoding)
         pos = file.tell()
         n_padding = header['DepthImageOffset'] - pos - 2
         file.write(b'\x00' * n_padding + b'\r\n')
         data = surface.data.astype(DTYPE) * 1e-6
         data.tofile(file)
 
-def read_al3d(filepath):
+def read_al3d(filepath, read_image_layers=False, encoding='utf-8'):
     with open(filepath, 'rb') as file:
         magic = file.read(17)
         if magic != MAGIC:
             raise CorruptedFileError('Incompatible file magic detected.')
         header = dict()
         key, value = read_tag(file)
         if key != 'Version':
@@ -73,8 +73,8 @@
         data = np.fromfile(file, dtype=np.float32, count=nx * ny, offset=0).reshape(ny, nx)
 
     invalidValue = float(header['InvalidPixelValue'])
     data[data == invalidValue] = np.nan
 
     data *= 1e6 # Conversion from m to um
 
-    return (data, step_x, step_y)
+    return RawSurface(data, step_x, step_y)
```

## surfalize/file/common.py

```diff
@@ -1,16 +1,20 @@
 import struct
+import numpy as np
 
 MU_ALIASES = {
     chr(181): 'u',
     chr(956): 'u',
     chr(13211): 'um'
 }
 
 UNIT_EXPONENT = {
+    'm':   0,
+    'dm': -1,
+    'cm': -2,
     'mm': -3,
     'um': -6,
     'nm': -9,
     'pm': -12
 }
 
 def _sanitize_mu(string):
@@ -128,7 +132,48 @@
             continue
         unpacked_data = struct.unpack(f'{format}', filehandle.read(size))[0]
         if isinstance(unpacked_data, bytes):
             unpacked_data = unpacked_data.decode(encoding).rstrip(' \x00')
         result[name] = unpacked_data
     return result
 
+def np_fromany(fileobject, dtype, count=-1, offset=0):
+    """
+    Function that invokes either np.frombuffer or np.fromfile depending on whether the object is a file-like object
+    or a buffer.
+
+    Parameters
+    ----------
+    fileobject: buffer_like or file-like or str or Path
+        An object that exposes the buffer interface or a file-like object or a str or Path representing a filepath.
+    dtype: data-type
+        Data-type of the returned array.
+    count: int, Default -1.
+        Number of items to read. -1 means all data in the buffer or file.
+    offset: int
+        Start reading the buffer from this offset (in bytes); default: 0.
+
+    Returns
+    -------
+    np.ndarray
+    """
+    try:
+        return np.frombuffer(fileobject, dtype, count=count, offset=offset)
+    except TypeError:
+        if offset > 0:
+            fileobject.seek(offset, 1)
+        if count == -1:
+            buffer = fileobject.read()
+        else:
+            buffer = fileobject.read(count * np.dtype(dtype).itemsize)
+        return np.frombuffer(buffer, dtype)
+
+
+class RawSurface:
+
+    def __init__(self, data: np.ndarray, step_x: float, step_y: float, metadata: dict | None = None,
+                 image_layers: dict | None = None):
+        self.data = data
+        self.step_x = step_x
+        self.step_y = step_y
+        self.metadata = {} if metadata is None else metadata
+        self.image_layers = {} if image_layers is None else image_layers
```

## surfalize/file/loader.py

```diff
@@ -8,41 +8,41 @@
 from .sur import read_sur, write_sur
 from .zmg import read_zmg
 from .opd import read_opd
 from .xyz import read_xyz
 from .nms import read_nms
 from .al3d import read_al3d, write_al3d
 from .sdf import read_binary_sdf
+from .gwy import read_gwy
 
 dispatcher = {
     '.sur':     {'read': read_sur, 'write': write_sur},
     '.vk4':     {'read': read_vk4},
     '.vk6':     {'read': read_vk6_vk7},
     '.vk7':     {'read': read_vk6_vk7},
     '.plu':     {'read': read_plu},
     '.plux':    {'read': read_plux},
     '.zmg':     {'read': read_zmg},
     '.opd':     {'read': read_opd},
     '.xyz':     {'read': read_xyz},
     '.nms':     {'read': read_nms},
     '.al3d':    {'read': read_al3d, 'write': write_al3d},
-    '.sdf':     {'read': read_binary_sdf}
+    '.sdf':     {'read': read_binary_sdf},
+    '.gwy':     {'read': read_gwy}
 }
 
 supported_formats = list(dispatcher.keys())
 
-def load_file(filepath, encoding="utf-8"):
+def load_file(filepath, read_image_layers=False, encoding="utf-8"):
     filepath = Path(filepath)
     try:
         loader = dispatcher[filepath.suffix]['read']
-        if 'encoding' in loader.__code__.co_varnames:
-            loader = partial(loader, encoding=encoding)
     except KeyError:
         raise UnsupportedFileFormatError(f"File format {filepath.suffix} is currently not supported.") from None
-    return loader(filepath)
+    return loader(filepath, read_image_layers=read_image_layers, encoding=encoding)
 
 def write_file(filepath, surface, encoding='utf-8'):
     filepath = Path(filepath)
     ext = filepath.suffix
     if not ext:
         raise ValueError('No format for the file specified.') from None
     try:
```

## surfalize/file/nms.py

```diff
@@ -1,31 +1,44 @@
 import struct
 import numpy as np
+import dateutil
+from .common import RawSurface
+from datetime import datetime
 
-from ..exceptions import CorruptedFileError
 
 HEADER_SIZE = 3468
 OFFSET_Z = 16
+OFFSET_DATE = 1076
 OFFSET_POINTS = 1368
 OFFSET_SPACING = 1376
 
+DTYPE_HEIGHT = np.uint16
+DTYPE_IMG = np.uint8
 
-def read_nms(filepath):
+
+def read_nms(filepath, read_image_layers=False, encoding='utf-8'):
     with open(filepath, 'rb') as file:
         file.seek(OFFSET_Z, 0)
         zmin, zmax = struct.unpack('<2d', file.read(16))
+        file.seek(OFFSET_DATE, 0)
+        date = dateutil.parser.parse(file.read(16).decode())
         file.seek(OFFSET_POINTS, 0)
         nx, ny = struct.unpack('<2I', file.read(8))
         file.seek(OFFSET_SPACING, 0)
         dx, dy = struct.unpack('<2d', file.read(16))
         file.seek(HEADER_SIZE, 0)
 
-        data = np.fromfile(file, dtype=np.uint16, count=nx * ny)
+        data = np.fromfile(file, dtype=DTYPE_HEIGHT, count=nx * ny)
         #nonmeasured_points_mask = (data == 0)
         data = data / (2 ** 16 - 2) * (zmax - zmin) + zmax
         #data[nonmeasured_points_mask] = np.nan
         data = data.reshape(ny, nx)
 
         step_x = dx * 1e-3
         step_y = dy * 1e-3
+        image_layers = {}
+        if read_image_layers:
+            image_layers['Grayscale'] = np.fromfile(file, dtype=DTYPE_IMG, count=nx * ny).reshape(ny, ny)
+
+        metadata = dict(date=date)
 
-    return data, step_x, step_y
+    return RawSurface(data, step_x, step_y, metadata=metadata, image_layers=image_layers)
```

## surfalize/file/opd.py

```diff
@@ -1,89 +1,136 @@
-# This code was only tested on .opd files with an itemsize of 2
-
 from dataclasses import dataclass
 import struct
+from enum import IntEnum
+import dateutil
 import numpy as np
 from ..exceptions import CorruptedFileError, CorruptedFileError
+from .common import RawSurface, get_unit_conversion
+
+# This code was only tested on .opd files with an itemsize of 2
+
+FIXED_UNIT_Z = 'nm'
+FIXED_UNIT_XY = 'mm'
 
 BLOCK_SIZE = 24
 BLOCK_NAME_SIZE = 16
 INT16_MAX = 32767
 
-dtype = {1: 'c', 2: '<i2', 4: '4f'}
+
+class BlockType(IntEnum):
+    NONE = 0
+    DIRECTORY = 1
+    ARRAY = 3
+    TEXT = 5
+    SHORT = 6
+    FLOAT = 7
+    DOUBLE = 8
+    LONG = 12
+
+
+dtypes = {
+    BlockType.ARRAY: {1: 'uint8', 2: 'int16', 4: 'float32'},
+    BlockType.SHORT: 'h',
+    BlockType.FLOAT: 'f',
+    BlockType.DOUBLE: 'd',
+    BlockType.LONG: 'l'
+}
+
+invalid_value = {'int16': 32767, 'float32': 1e38}
 
 
 @dataclass
 class Block:
     type: int
     size: int
     flags: int
     offset: int = None
 
+    def _read_array(self, filehandle):
+        nx, ny, itemsize = struct.unpack('<HHH', filehandle.read(6))
+        data_length = nx * ny
+        if data_length * itemsize != self.size - 6:
+            raise CorruptedFileError(f'Size of data ({data_length}) does not match expected size ({self.size - 6}).')
+        data = np.fromfile(filehandle, dtype=dtypes[BlockType.ARRAY][itemsize], count=data_length)
+        data = np.rot90(data.reshape(nx, ny))
+        return data
+
+    def _read_text(self, filehandle, encoding='utf-8'):
+        return filehandle.read(self.size).decode(encoding).rstrip('\x00')
+
+    def _read_number(self, filehandle):
+        dtype = dtypes[self.type]
+        itemsize = struct.calcsize(dtype)
+        return struct.unpack(f'{int(self.size / itemsize)}{dtype}', filehandle.read(self.size))[0]
+
+    def read_contents(self, filehandle, encoding='utf-8'):
+        current_pos = filehandle.tell()
+        filehandle.seek(self.offset, 0)
+        if self.type == BlockType.ARRAY:
+            result = self._read_array(filehandle)
+        elif self.type == BlockType.TEXT:
+            result = self._read_text(filehandle, encoding=encoding)
+        else:
+            result = self._read_number(filehandle)
+        filehandle.seek(current_pos, 0)
+        return result
 
-def read_block(filehandle, encoding='utf-8'):
+
+def read_block_definition(filehandle, encoding='utf-8'):
     name = filehandle.read(16).decode().rstrip('\x00')
     type_, size, flags = struct.unpack('<hlH', filehandle.read(8))
-    return name, Block(type_, size, flags)
+    return name, Block(BlockType(type_), size, flags)
+
 
-def read_opd(filepath, encoding='utf-8'):
-    with open(filepath, 'rb') as file:
-        file.read(2)  # skipping header
-        name, directory_block = read_block(file)
+def read_opd(filepath, read_image_layers=False, encoding='utf-8'):
+    with open(filepath, 'rb') as filehandle:
+        magic = filehandle.read(2)  # skipping header
+        name, directory_block = read_block_definition(filehandle, encoding=encoding)
         if name != 'Directory':
             raise CorruptedFileError('Directory block not found.')
         n_blocks = int(directory_block.size / BLOCK_SIZE)
         blocks = dict()
         for _ in range(n_blocks - 1):
-            name, block = read_block(file)
+            name, block = read_block_definition(filehandle, encoding=encoding)
             blocks[name] = block
-        offset = file.tell()
+        offset = filehandle.tell()
         for block in blocks.values():
             block.offset = offset
             offset += block.size
 
-        file.seek(blocks['RAW_DATA'].offset, 0)
-        nx, ny, itemsize = struct.unpack('<HHH', file.read(6))
-        data_length = nx * ny
-        if data_length * itemsize != blocks['RAW_DATA'].size - 6:
-            raise CorruptedFileError('Size of data does not match expected size.')
-
-        data = np.fromfile(file, dtype=dtype[itemsize], count=data_length)
-        data = np.rot90(data.reshape(nx, ny)).astype('float64')
+        data = blocks['RAW_DATA'].read_contents(filehandle, encoding=encoding)
+        image_layers = {}
+        if read_image_layers and 'Image' in blocks:
+            image_layers['Grayscale'] = blocks['Image'].read_contents(filehandle, encoding=encoding)
+
+        metadata = dict()
+        for name, block in blocks.items():
+            if block.type in [BlockType.TEXT, BlockType.SHORT, BlockType.FLOAT, BlockType.DOUBLE, BlockType.LONG]:
+                contents = block.read_contents(filehandle, encoding=encoding)
+                if block.type == BlockType.TEXT and not contents:
+                    # Skip empty strings
+                    continue
+                metadata[name] = contents
+
+        metadata['timestamp'] = dateutil.parser.parse(metadata['Date'] + ' ' + metadata['Time'])
+        del metadata['Date']
+        del metadata['Time']
+
+        for label in ['Wavelength', 'Mult', 'Aspect', 'Pixel_size']:
+            if label not in metadata:
+                metadata[label] = 1.0
 
+        nan_mask = None
         # Mask invalid datapoints
-        if itemsize == 2:
-            data[data == INT16_MAX] = np.nan
-            data[data == -INT16_MAX - 1] = np.nan
-
-        if 'Wavelength' in blocks:
-            file.seek(blocks['Wavelength'].offset, 0)
-            wavelength = struct.unpack('<f', file.read(blocks['Wavelength'].size))[0]
-        else:
-            wavelength = 1.0
-
-        if 'Mult' in blocks:
-            file.seek(blocks['Mult'].offset, 0)
-            mult = struct.unpack('<H', file.read(blocks['Mult'].size))[0]
-        else:
-            mult = 1.0
-
-        if 'Aspect' in blocks:
-            file.seek(blocks['Aspect'].offset, 0)
-            aspect = struct.unpack('<f', file.read(blocks['Aspect'].size))[0]
-        else:
-            aspect = 1.0
-
-        if 'Pixel_size' in blocks:
-            file.seek(blocks['Pixel_size'].offset, 0)
-            pixel_size = struct.unpack('<f', file.read(blocks['Pixel_size'].size))[0]
-        else:
-            pixel_size = 1.0
-
-        scale_z = wavelength / mult * 1e-6
-
-        data *= scale_z
+        if data.dtype in ['int16', 'float32']:
+            nan_mask = (data == invalid_value[data.dtype.name])
 
-        step_x = pixel_size * 1e-3
-        step_y = pixel_size * aspect * 1e-3
+        metadata['Wavelength'] *= get_unit_conversion(FIXED_UNIT_Z, 'um')
+        scale_z = metadata['Wavelength'] / metadata['Mult']
 
-        return data, step_x, step_y
+        data = data.astype('float64') * scale_z
+        if nan_mask is not None:
+            data[nan_mask] = np.nan
+
+        step_x = metadata['Pixel_size'] * get_unit_conversion(FIXED_UNIT_XY, 'um')
+        step_y = step_x * metadata['Aspect']
+        return RawSurface(data, step_x, step_y, metadata=metadata, image_layers=image_layers)
```

## surfalize/file/plu.py

```diff
@@ -1,15 +1,15 @@
+import dateutil
 import numpy as np
-from .common import read_binary_layout
+from .common import read_binary_layout, RawSurface
 
 NON_MEASURED_VALUE = 1000001
 
 DATE_SIZE = 128
 COMMENT_SIZE = 256
-HEADER_SIZE = DATE_SIZE + COMMENT_SIZE + 4
 
 LAYOUT_CALIBRATION = (
     ('yres', 'I', False),
     ('xres', 'I', False),
     ('N_tall', 'I', True),
     ('dy_multip', 'f', True),
     ('mppx', 'f', False),
@@ -40,22 +40,35 @@
    ('version', 'b', True),
    ('config_hardware', 'b', True),
    ('stack_in_num', 'b', True),
    (None, 3, None),
    ('factorio_delmacio', 'I', True)
 )
 
-def read_plu(filepath, encoding='utf-8'):
+def read_plu(filepath, read_image_layers=False, encoding='utf-8'):
     with open(filepath, 'rb') as filehandle:
-        filehandle.seek(HEADER_SIZE, 1)
+        date_block = filehandle.read(DATE_SIZE)
+        timestamp = dateutil.parser.parse(date_block.decode().rstrip('\x00'))
+        filehandle.seek(COMMENT_SIZE + 4, 1)
         calibration = read_binary_layout(filehandle, LAYOUT_CALIBRATION, encoding=encoding)
         measure_config = read_binary_layout(filehandle, LAYOUT_MEASURE_CONFIG, encoding=encoding)
         data_length = calibration['xres'] * calibration['yres']
         data = np.fromfile(filehandle, dtype=np.float32, count=data_length)
+        image_layers = {}
+        if read_image_layers:
+            filehandle.seek(16, 1) # skip 16 bytes, no idea what they are doing
+            img = np.fromfile(filehandle, dtype=np.uint8, count=data_length * 3)
+            img = img.reshape(calibration['yres'], calibration['xres'], 3)
+            if np.all((img[:, :, 0] == img[:, :, 1]) & (img[:, :, 0] == img[:, :, 2])):
+                image_layers['Grayscale'] = img[:, :, 0]
+            else:
+                image_layers['RGB'] = img
     data = data.reshape((calibration['yres'], calibration['xres']))
     data[data == NON_MEASURED_VALUE] = np.nan
 
     step_x = calibration['mppx']
     step_y = calibration['mppy']
 
-    return (data, step_x, step_y)
+    metadata = {'timestamp': timestamp}
+
+    return RawSurface(data, step_x, step_y, image_layers=image_layers, metadata=metadata)
```

## surfalize/file/plux.py

```diff
@@ -1,21 +1,52 @@
 import zipfile
+import dateutil
 import xml.etree.ElementTree as ET
 import numpy as np
+from .common import RawSurface
 
-def read_plux(filepath, encoding='utf-8'):
-    with zipfile.ZipFile(filepath) as archive:
-        data = archive.read('LAYER_0.raw')
-        metadata = archive.read('index.xml')
+# Names of the files in the zip archive
+TOPOGRAPHY_FILE_NAME = 'LAYER_0.raw'
+IMAGE_FILE_NAME = 'LAYER_0.stack.raw'
+XML_METADATA_FILE_NAME = 'index.xml'
 
-    xml_str = metadata.decode(encoding)
+def read_plux(filepath, read_image_layers=False, encoding='utf-8'):
+    with zipfile.ZipFile(filepath) as archive:
+        contents = archive.namelist()
+        data_raw = archive.read(TOPOGRAPHY_FILE_NAME)
+        if read_image_layers and IMAGE_FILE_NAME in contents:
+            img_raw = archive.read(IMAGE_FILE_NAME)
+        xml_metadata = archive.read(XML_METADATA_FILE_NAME)
 
+    xml_str = xml_metadata.decode(encoding)
+    metadata = {}
     # Parse the XML string
     root = ET.fromstring(xml_str)
+    metadata['timestamp'] = dateutil.parser.parse(root.find('GENERAL/DATE').text)
+
+    # Add selected entires of XML info section to metadata
+    info_section = root.find('INFO')
+    tags = ['Device', 'Objective', 'Technique', 'Measurement Type', 'Algorithm', 'Comment']
+    if info_section is not None:
+        # Iterate over all child elements of INFO
+        for item in info_section:
+            if item.tag.startswith('ITEM_'):
+                # Extract NAME and VALUE from each ITEM
+                name = item.find('NAME').text.strip()
+                if name in tags:
+                    metadata[name.lower()] = item.find('VALUE').text.strip()
+
     shape_x = int(root.find('GENERAL/IMAGE_SIZE_X').text)
     shape_y = int(root.find('GENERAL/IMAGE_SIZE_Y').text)
     step_x = float(root.find('GENERAL/FOV_X').text)
     step_y = float(root.find('GENERAL/FOV_Y').text)
     size = shape_x * shape_y
 
-    data = np.frombuffer(data, dtype=np.float32, count=size).reshape((shape_y, shape_x))
-    return (data, step_x, step_y)
+    data = np.frombuffer(data_raw, dtype=np.float32, count=size).reshape((shape_y, shape_x))
+    image_layers = {}
+    if read_image_layers:
+        img = np.frombuffer(img_raw, dtype=np.uint8).reshape((shape_y, shape_x, 3))
+        if np.all((img[:, :, 0] == img[:, :, 1]) & (img[:, :, 0] == img[:, :, 2])):
+            image_layers['Grayscale'] = img[:, :, 0]
+        else:
+            image_layers['RGB'] = img
+    return RawSurface(data, step_x, step_y, image_layers=image_layers, metadata=metadata)
```

## surfalize/file/sdf.py

```diff
@@ -1,10 +1,10 @@
 import struct
-from .common import read_binary_layout
 import numpy as np
+from .common import read_binary_layout, RawSurface
 
 LAYOUT_HEADER = (
     ("Version", "8s", True),
     ("ManufacturerID", "10s", True),
     ("CreateDate", "12s", True),
     ("ModDate", "12s", True),
     ("NumPoints", "H", False),
@@ -52,13 +52,13 @@
     step_x = header["Xscale"] / 10**-6
     step_y = header["Yscale"] / 10**-6
 
     return (data, step_x, step_y)
 
 
 # Main function to read the SDF file
-def read_binary_sdf(file_path, encoding="utf-8"):
+def read_binary_sdf(file_path, read_image_layers=False, encoding="utf-8"):
     with open(file_path, "rb") as file:
         header = read_binary_layout(file, LAYOUT_HEADER, encoding=encoding)
         data, step_x, step_y = read_data_section(file, header)
 
-    return (data, step_x, step_y)
+    return RawSurface(data, step_x, step_y)
```

## surfalize/file/sur.py

```diff
@@ -1,30 +1,94 @@
 import struct
+import zlib
 from datetime import datetime
-from .common import read_binary_layout, write_binary_layout, get_unit_conversion
-from ..exceptions import CorruptedFileError
+from enum import IntEnum
+from dataclasses import dataclass
+
 import numpy as np
 
+from .common import read_binary_layout, write_binary_layout, get_unit_conversion, RawSurface
+from ..exceptions import CorruptedFileError, UnsupportedFileFormatError
+
 # This is not fully implemented! Won't work with all SUR files.
 
-MAGIC = 'DIGITAL SURF'
+MAGIC_CLASSIC = 'DIGITAL SURF'
+MAGIC_COMPRESSED = 'DSCOMPRESSED'
 HEADER_SIZE = 512
 
+
+@dataclass
+class Directory:
+    """
+    Dataclass that represents a directory block in a compressed surf file data section
+    """
+    len_raw_data: int
+    len_zipped_data: int
+
+
+@dataclass
+class SurObject:
+    """
+    Dataclass that represents a sur object, which contains a full header and a data block.
+    """
+    header: dict
+    data: np.ndarray
+
+
+class StudiableType(IntEnum):
+    PROFILE = 1
+    SURFACE = 2
+    BINARY_IMAGE = 3
+    SERIES_OF_PROFILES = 4
+    SERIES_OF_SURFACES = 5
+    MERIDIAN_DISC = 6
+    MULTILAYER_PROFILE = 7
+    MULTILAYER_SURFACE = 8
+    PARALLEL_DISC = 9
+    INTENSITY_IMAGE = 10
+    INTENSITY_SURFACE = 11
+    RGB_IMAGE = 12
+    RGB_SURFACE = 13
+    FORCE_CURVE = 14
+    SERIES_OF_FORCE_CURVES = 15
+    RGB_INTENSITY_SURFACE = 16
+    PARAMETERIC_PROFILE = 17
+    SERIES_OF_RGB_IMAGES = 18
+    SPECTRUM_STUDIABLE = 19
+
+
+class AcquisitionType(IntEnum):
+    UNKNOWN = 0
+    CONTACT_STYLUS = 1
+    SCANNING_OPTICAL_GAUGE = 2
+    THERMOCOUPLE = 3
+    UNKNOWN_2 = 4
+    CONTACT_STYLUS_WITH_SKID = 5
+    AFM = 6
+    STM = 7
+    VIDEO = 8
+    INTERFEROMETER = 9
+    STRUCTURED_LIGHT_PROJECTION = 10
+
+
 LAYOUT_HEADER = (
-    ('code', '12s', False),
-    ('format', 'h', False),
+    ('code', '12s', False),  # DIGITIAL SURF / DSCOMPRESSED
+    ('format', 'h', False),  # 0 for PC format
     ('n_objects', 'h', False),
     ('version_number', 'h', False),
     ('studiable_type', 'h', True),
     ('name_object', '30s', True),
     ('name_operator', '30s', True),
-    (None, 6, None), # Reserved
+    ('p_size', 'h', True),
+    ('acquisition_type', 'h', True),
+    ('range_type', 'h', True),
     ('non_measured_points', 'h', False),
     ('absolute_z_axis', 'h', False),
-    (None, 8, None), # Reserved
+    ('gauge_resolution', 'f', True),
+    (None, 4, None),  # Reserved
     ('bits_per_point', 'h', False),
     ('min_point', 'i', False),
     ('max_point', 'i', False),
     ('n_points_per_line', 'i', False),
     ('n_lines', 'i', False),
     ('n_total_points', 'i', False),
     ('spacing_x', 'f', False),
@@ -41,63 +105,276 @@
     ('unit_z', '16s', False),
     ('unit_ratio_x', 'f', False),
     ('unit_ratio_y', 'f', False),
     ('unit_ratio_z', 'f', False),
     ('replica', 'h', False),
     ('inverted', 'h', False),
     ('leveled', 'h', False),
-    (None, 12, None), # Reserved
+    (None, 12, None),  # Reserved
     ('seconds', 'h', True),
     ('minutes', 'h', True),
     ('hours', 'h', True),
     ('day', 'h', True),
     ('month', 'h', True),
     ('year', 'h', True),
     ('week_day', 'h', True),
     ('measurement_duration', 'f', True),
-    (None, 10, None), # Reserved
+    ('compressed_data_size', 'I', False),
+    (None, 6, None),  # Reserved
     ('length_comment', 'h', False),
     ('length_private', 'h', False),
     ('client_zone', '128s', True),
     ('offset_x', 'f', True),
     ('offset_y', 'f', True),
     ('offset_z', 'f', True),
     ('spacing_t', 'f', True),
     ('offset_t', 'f', True),
     ('name_t', '13s', True),
     ('unit_step_t', '13s', True)
 )
 
-POINTSIZE = {16: 'h', 32: 'i'}
+DTYPE_MAP = {16: 'int16', 32: 'int32'}
+
+
+def read_sur_header(filehandle, encoding='utf-8'):
+    fp_start = filehandle.tell()
+    header = read_binary_layout(filehandle, LAYOUT_HEADER, encoding=encoding, fast=False)
+
+    if header['code'] not in (MAGIC_CLASSIC, MAGIC_COMPRESSED) or header['version_number'] != 1:
+        raise CorruptedFileError('Unknown header format')
+
+    if header['unit_ratio_x'] != 1 or header['unit_ratio_y'] != 1 or header['unit_ratio_z'] != 1:
+        raise NotImplementedError("This file type cannot be correctly read currently.")
+
+    header['studiable_type'] = StudiableType(header['studiable_type'])
+    header['acquisition_type'] = AcquisitionType(header['acquisition_type'])
+
+    if filehandle.tell() - fp_start != HEADER_SIZE:
+        raise CorruptedFileError("Unknown header size.")
+
+    header['comment'] = filehandle.read(header['length_comment'])
+    header['private'] = filehandle.read(header['length_private'])
+
+    return header
+
+
+def read_directory(filehandle):
+    """
+    Reads a directory block from a compressed surf file's data section. This function expects the file pointer to
+    point to the beginning of a directory block.
+
+    Parameters
+    ----------
+    filehandle
+        Handle to the file object.
+
+    Notes
+    -----
+    The layout of a directory is as follows:
+
+    raw data length - uint32
+    zipped data length - uint32
+
+    Returns
+    -------
+    Directory
+    """
+    return Directory(*struct.unpack('<2I', filehandle.read(8)))
+
+
+def read_uncompressed_data(filehandle, dtype, num_points):
+    return np.fromfile(filehandle, count=num_points, dtype=dtype)
+
+
+def read_compressed_data(filehandle, dtype, expected_compressed_size):
+    """
+    Reads a datablock from a compressed sur file. This function assumes that the filepointer points to the beginning
+    of a datablock.
+
+    Parameters
+    ----------
+    filehandle
+        Handle to the file object.
+    dtype
+        Datatype of the binary data.
+
+    Notes
+    -----
+    The datablock of a compressed sur file is organized as follows:
+
+    directory count - uint32
+    directory item 0
+        raw data length - uint32
+        zipped data length - uint32
+    ...
+    directory item n
+        ...
+    zipped data stream 0
+    ...
+    zipped data stream 1
+
+    The compressed data is organized into an arbitrary amount of binary streams that must be concatenated before
+    decompression. The data block of the file begins a number of directory blocks. The following bytes encode the
+    directory blocks, which holds the raw and zipped data length of the streams. After the nth directory block, the
+    raw streams are encoded. Currently, according to the file format specification, compressed files use only one
+    datastream. However, in the future, this could change.
+
+    Returns
+    -------
+    data: np.ndarray
+        Decompressed 1d array of the data-
+    """
+    # The compressed datablock begins with a uint32 that encodes the number of directories
+    dir_count = struct.unpack('I', filehandle.read(4))[0]
+    # Afterwards, that number of directories is stored consecutively, containing 2 uint32 encoding the length of the
+    # raw data and the length of the zipped data in the stream associated with that directory
+    directories = []
+    total_compressed_size = 4
+    for _ in range(dir_count):
+        directory = read_directory(filehandle)
+        total_compressed_size += 8 + directory.len_zipped_data
+        directories.append(directory)
+    if total_compressed_size != expected_compressed_size:
+        raise CorruptedFileError(
+            f'Compressed data size {total_compressed_size} does not match expected size of {expected_compressed_size}.'
+        )
+    # For each directory, we read data equivalent to the compressed size attribute in the directory and descompress it
+    # using zlib. Then we concatenate the uncompressed data streams and read them with numpy
+    # each datastream into a single
+    decompressed_data = b''
+    for directory in directories:
+        compressed_data_stream = filehandle.read(directory.len_zipped_data)
+        decompressed_data_stream = zlib.decompress(compressed_data_stream)
+        if len(decompressed_data_stream) != directory.len_raw_data:
+            raise CorruptedFileError(
+                f'Decrompressed data size {len(decompressed_data_stream)} does not match expected size \
+                of {directory.len_raw_data}.'
+            )
+        decompressed_data += decompressed_data_stream
+    data = np.frombuffer(decompressed_data, dtype)
+    return data
+
+
+def is_gwyddion_export(sur_obj):
+    """
+    Checks whether .sur file was exported from Gwyddion. Unfortunately, Gwyddion currently seems to indicate the
+    studiable type as PROFILE for exports containing surfaces. If however, the object name and operator name parameters
+    are defined by Gwyddion as SCRATCH and csm, we can assume with reasonable certainty, that it should be infact a
+    surface, not a profile.
+
+    Parameters
+    ----------
+    sur_obj: SurObject
+
+    Returns
+    -------
+    True if the sur object meets the characteristics of a Gwyddion exported surface
+    """
+    return (sur_obj.header['studiable_type'] == StudiableType.PROFILE and sur_obj.header['name_object'] == 'SCRATCH'
+            and sur_obj.header['name_operator'] == 'csm')
+
+
+def read_sur_object(filehandle):
+    """
+    Reads a sur object from a file. The function assumes that the filepointer points to the beginning of a sur object.
+    A sur object consists of a 512-byte long header, followed by a variable length comment zone, private zone and
+    data section. The data section is either compressed or uncompressed, which is determined by the file magic (first
+    few bytes of the header).
+
+    Parameters
+    ----------
+    filehandle
+        Handle to the file object.
+    Returns
+    -------
+    SurObject
+    """
+    header = read_sur_header(filehandle)
+    dtype = DTYPE_MAP[header['bits_per_point']]
+    ny = header['n_lines']
+    nx = header['n_points_per_line']
+
+    # Since 2010 version, there are two formats: compressed and uncompressed.
+    # Which of the versions is used for a sur object is indicated by the file magic
+    if header['code'] == MAGIC_CLASSIC:
+        data = read_uncompressed_data(filehandle, dtype, header['n_total_points']).reshape(ny, nx)
+    elif header['code'] == MAGIC_COMPRESSED:
+        data = read_compressed_data(filehandle, dtype, header['compressed_data_size']).reshape(ny, nx)
+    else:
+        raise CorruptedFileError(f'Unknown file magic found: {header["code"]}.')
+
+    return SurObject(header, data)
+
+
+def get_surface(sur_obj):
+    if sur_obj.header['non_measured_points'] == 1:
+        invalidValue = sur_obj.header['min_point'] - 2
+        nan_mask = (sur_obj.data == invalidValue)
+
+    # The conversion from int to float needs to happen before multiply by the unit conversion factor!
+    # Otherwise, we might overflow the values in the array and end up with white noise
+    data = sur_obj.data * sur_obj.header['spacing_z']
+    data = data * get_unit_conversion(sur_obj.header['unit_step_z'], 'um')
+    step_x = get_unit_conversion(sur_obj.header['unit_step_x'], 'um') * sur_obj.header['spacing_x']
+    step_y = get_unit_conversion(sur_obj.header['unit_step_y'], 'um') * sur_obj.header['spacing_y']
+
+    if sur_obj.header['non_measured_points'] == 1:
+        data[nan_mask] = np.nan
+
+    data += sur_obj.header['offset_z']
+
+    # This can be implemented in the future when metadata support is needed
+    # timestamp = datetime.datetime(year=header['year'], month=header['month'], day=header['day'])
+    return (data, step_x, step_y)
+
+
+def read_sur(filepath, read_image_layers=False, encoding='utf-8'):
+    filesize = filepath.stat().st_size
+    with (open(filepath, 'rb') as filehandle):
+        sur_obj = read_sur_object(filehandle)
+        if sur_obj.header['n_objects'] > 1:
+            raise UnsupportedFileFormatError(f'Multilayer or series studiables are currently not supported.')
+
+        if sur_obj.header['studiable_type'] == StudiableType.SURFACE or is_gwyddion_export(sur_obj):
+            data, step_x, step_y = get_surface(sur_obj)
+        elif sur_obj.header['studiable_type'] == StudiableType.RGB_INTENSITY_SURFACE:
+            # after the surface, the r,g,b channels and the intensity image follow.
+            # These should be read here if necessary in the future.
+            data, step_x, step_y = get_surface(sur_obj)
+        else:
+            raise UnsupportedFileFormatError(
+                f'Studiables of type {sur_obj.header["studiable_type"].name} are not supported.'
+            )
+        return RawSurface(data, step_x, step_y)
+
 
-def write_sur(filepath, surface, encoding='utf-8'):
+def write_sur(filepath, surface, encoding='utf-8', compressed=False):
     INT32_MAX = int(2 ** 32 / 2) - 1
     INT32_MIN = -int(2 ** 32 / 2)
 
-    data = surface.data
     nm_points = int(surface._nonmeasured_points_exist)
 
     INT_DATA_MIN = INT32_MIN + 2
     INT_DATA_MAX = INT32_MAX - 1
     data_max = np.nanmax(surface.data)
     data_min = np.nanmin(surface.data)
     data = ((surface.data - data_min) / (data_max - data_min)) * (INT_DATA_MAX - INT_DATA_MIN) + INT_DATA_MIN
     if nm_points:
         data[np.isnan(data)] = INT_DATA_MIN - 2
     data = data.astype('int32')
     spacing_z = (data_max - data_min) / (INT_DATA_MAX - INT_DATA_MIN)
-    offset_z = offset = data_min + (data_max - data_min)/2
+    offset_z = offset = data_min + (data_max - data_min) / 2
     timestamp = datetime.now()
 
     header = {
-        'code': MAGIC,
+        'code': MAGIC_CLASSIC if not compressed else MAGIC_COMPRESSED,
         'format': 0,  # PC Format
         'n_objects': 1,
         'version_number': 1,
-        'studiable_type': 2,
+        'studiable_type': StudiableType.SURFACE,
         'name_object': '',
         'name_operator': '',
         'non_measured_points': nm_points,
         'absolute_z_axis': 0,
         'bits_per_point': 32,
         'min_point': INT_DATA_MIN,
         'max_point': INT_DATA_MAX,
@@ -147,51 +424,18 @@
         if name is None or not format_.endswith('s'):
             continue
         length = struct.calcsize(format_)
         header[name] = header[name].ljust(length)
 
     with open(filepath, 'wb') as file:
         write_binary_layout(file, LAYOUT_HEADER, header)
-        data.tofile(file)
-
-def read_sur(filepath, encoding='utf-8'):
-    filesize = filepath.stat().st_size
-    with open(filepath, 'rb') as filehandle:
-        header = read_binary_layout(filehandle, LAYOUT_HEADER, encoding=encoding, fast=False)
-
-        if header['code'] != MAGIC or header['version_number'] != 1:
-            raise CorruptedFileError
-
-        if header['unit_ratio_x'] != 1 or header['unit_ratio_y'] != 1 or header['unit_ratio_z'] != 1:
-            raise NotImplementedError("This file type cannot be correctly read currently.")
-
-        filehandle.seek(header['length_comment'], 1)
-        filehandle.seek(header['length_private'], 1)
-        dtype = POINTSIZE[header['bits_per_point']]
-        dsize = struct.calcsize(dtype)
-        data_size = header['n_total_points'] * dsize
-
-        data_size = header['n_total_points'] * dsize
-        total_header_size = HEADER_SIZE + header['length_comment'] + header['length_private']
-        expected_data_size = header['n_total_points'] * dsize
-        if filesize - total_header_size > expected_data_size:
-            filehandle.seek(filesize - data_size, 0)
-        shape = (header['n_lines'], header['n_points_per_line'])
-        data = np.fromfile(filehandle, dtype=np.dtype(dtype)).reshape(shape)
-
-        if header['non_measured_points'] == 1:
-            invalidValue = header['min_point'] - 2
-            nan_mask = (data == invalidValue)
-
-        # The conversion from int to float needs to happen before we multiply by the unit conversion factor!
-        # Otherwise, we might overflow the values in the array and end up with white noise
-        data = data * header['spacing_z']
-        data = data * get_unit_conversion(header['unit_z'], 'um')
-        step_x = get_unit_conversion(header['unit_x'], 'um') * header['spacing_x']
-        step_y = get_unit_conversion(header['unit_y'], 'um') * header['spacing_y']
-
-        data += header['offset_z']
-
-        if header['non_measured_points'] == 1:
-            data[nan_mask] = np.nan
+        if not compressed:
+            data.tofile(file)
+            return
+        else:
+            uncompressed_data = data.tobytes()
+            compressed_data = zlib.compress(uncompressed_data)
+            # Write directory count = 1 and the length of a single data stream containing all the compressed data
+            file.write(struct.pack('<3I', 1, len(uncompressed_data), len(compressed_data)))
+            file.write(compressed_data)
+            return
 
-        return (data, step_x, step_y)
```

## surfalize/file/vk.py

```diff
@@ -1,13 +1,19 @@
 import zipfile
-import io
+import struct
+from datetime import datetime
+
 import numpy as np
-from .common import read_binary_layout, get_unit_conversion
+from .common import read_binary_layout, get_unit_conversion, RawSurface, np_fromany
+from ..exceptions import CorruptedFileError
 
 HEADER_SIZE = 12
+FIXED_UNIT = 'pm'
+
+DTYPE_MAP = {16: 'uint16', 32: 'uint32'}
 
 LAYOUT_OFFSET_TABLE = (
     ('meas_conds', 'I', False),
     ('color_peak', 'I', True),
     ('color_light', 'I', True),
     ('light', 'I', True),
     (None, 8, None),
@@ -103,41 +109,90 @@
     ('compression', 'I', True),
     ('data_byte_size', 'I', True),
     ('palette_range_min', 'I', True),
     ('palette_range_max', 'I', True),
     (None, 768, None)
 )
 
-def read_vk4(filepath, encoding='utf-8'):
+LAYOUT_IMAGE_DATA = (
+    ('width', 'I', False),
+    ('height', 'I', False),
+    ('bit_depth', 'I', False),
+    ('compression', 'I', True),
+    ('data_byte_size', 'I', True)
+)
+
+def read_rgb_layer(filehandle, offset):
+    filehandle.seek(offset, 0)
+    channel_table = read_binary_layout(filehandle, LAYOUT_IMAGE_DATA, fast=False)
+    channel_length = channel_table['width'] * channel_table['height'] * 3
+    if channel_table['data_byte_size'] != channel_length * channel_table['bit_depth'] / (8 * 3):
+        raise CorruptedFileError(f'Size of channel () does not correspond to expected size.')
+    channel_data = np_fromany(filehandle, dtype=np.uint8, count=channel_length)
+    # It seems like vk4 encodes the color channels in the order GRB, therefore we flip the last axis to convert to RGB format
+    channel_data = np.flip(channel_data.reshape(channel_table['height'], channel_table['width'], 3), axis=2)
+    return channel_data
+
+def read_height_layer(filehandle, offset):
+    filehandle.seek(offset, 0)
+    channel_table = read_binary_layout(filehandle, LAYOUT_HEIGHT_DATA, fast=False)
+    channel_length = channel_table['width'] * channel_table['height']
+    if channel_table['data_byte_size'] != channel_length * channel_table['bit_depth'] / 8:
+        raise CorruptedFileError('Size of channel does not correspond to expected size.')
+    dtype = DTYPE_MAP[channel_table['bit_depth']]
+    channel_data = np_fromany(filehandle, dtype=dtype, count=channel_length)
+    channel_data = channel_data.reshape(channel_table['height'], channel_table['width'])
+    return channel_data
+
+def read_string_data(filehandle, offset):
+    filehandle.seek(offset, 0)
+    str_data = dict()
+    size_title = struct.unpack('I', filehandle.read(4))[0] * 2
+    # Every character is followed by a null byte. Therefore, we read twice the since of the expected string length and slice to
+    # remove every second character
+    str_data['title'] = filehandle.read(size_title).decode()[::2]
+    size_lens_name = struct.unpack('I', filehandle.read(4))[0] * 2
+    # Same here
+    str_data['lens_name'] = filehandle.read(size_lens_name).decode()[::2]
+    return str_data
+
+def extract_vk4(filehandle, read_image_layers=False, encoding='utf-8'):
+    metadata = dict()
+    header = filehandle.read(HEADER_SIZE)
+    offset_table = read_binary_layout(filehandle, LAYOUT_OFFSET_TABLE, fast=False)
+    filehandle.seek(offset_table['meas_conds'], 0)
+    measurement_conditions = read_binary_layout(filehandle, LAYOUT_MEASUREMENT_CONDITIONS, fast=False)
+
+    if read_image_layers:
+        image_layers = {}
+        #if measurement_conditions['omit_color_img'] > 0:
+        image_layers['RGB'] = read_rgb_layer(filehandle, offset_table['color_peak'])
+        image_layers['Laser+RGB'] = read_rgb_layer(filehandle, offset_table['color_light'])
+
+        image_layers['Laser'] = read_height_layer(filehandle, offset_table['light'])
+    else:
+        image_layers = None
+    height_layer = read_height_layer(filehandle, offset_table['height'])
+    metadata.update(read_string_data(filehandle, offset_table['string_data']))
+
+    scale_factor = get_unit_conversion(FIXED_UNIT, 'um')
+    scale_factor_height = scale_factor * measurement_conditions['z_length_per_digit']
+    height_layer = height_layer * scale_factor_height
+    step_x = measurement_conditions['x_length_per_pixel'] * scale_factor
+    step_y = measurement_conditions['y_length_per_pixel'] * scale_factor
+
+    metadata['timestamp'] = datetime(year=measurement_conditions['year'], month=measurement_conditions['month'],
+                                     day=measurement_conditions['day'], hour=measurement_conditions['hour'],
+                                     minute=measurement_conditions['minute'], second=measurement_conditions['second'])
+    metadata['optical_zoom'] = measurement_conditions['optical_zoom'] / 10
+    metadata['objective_magnification'] = measurement_conditions['lens_magnification'] / 10
+
+    return RawSurface(height_layer, step_x, step_y, metadata, image_layers)
+
+def read_vk4(filepath, read_image_layers=False, encoding='utf-8'):
     with open(filepath, 'rb') as filehandle:
-        filehandle.seek(HEADER_SIZE, 1)
-        offset_table = read_binary_layout(filehandle, LAYOUT_OFFSET_TABLE, encoding=encoding)
-        filehandle.seek(offset_table['meas_conds'], 0)
-        measurement_conditions = read_binary_layout(filehandle, LAYOUT_MEASUREMENT_CONDITIONS, encoding=encoding)
-        filehandle.seek(offset_table['height'], 0)
-        height_data = read_binary_layout(filehandle, LAYOUT_HEIGHT_DATA, encoding=encoding)
-        data_length = height_data['width'] * height_data['height']
-        data = np.fromfile(filehandle, dtype=np.uint32, count=data_length) / 10_000  # to um
-
-    data = data.reshape(height_data['height'], height_data['width'])
-
-    step_x = measurement_conditions['x_length_per_pixel'] * get_unit_conversion('pm', 'um')
-    step_y = measurement_conditions['y_length_per_pixel'] * get_unit_conversion('pm', 'um')
-    return (data, step_x, step_y)
+        return extract_vk4(filehandle, read_image_layers=read_image_layers, encoding=encoding)
 
-def read_vk6_vk7(filepath, encoding='utf-8'):
+def read_vk6_vk7(filepath, read_image_layers=False, encoding='utf-8'):
     with zipfile.ZipFile(filepath) as archive:
         with archive.open('Vk4File') as filehandle:
-            filehandle.seek(HEADER_SIZE, 1)
-            offset_table = read_binary_layout(filehandle, LAYOUT_OFFSET_TABLE, encoding=encoding)
-            filehandle.seek(offset_table['meas_conds'], 0)
-            measurement_conditions = read_binary_layout(filehandle, LAYOUT_MEASUREMENT_CONDITIONS, encoding=encoding)
-            filehandle.seek(offset_table['height'], 0)
-            height_data = read_binary_layout(filehandle, LAYOUT_HEIGHT_DATA, encoding=encoding)
-            data_length = height_data['width'] * height_data['height']
-            data = np.frombuffer(filehandle.read(data_length * 4), dtype=np.uint32, count=data_length) / 10_000  # to um
-
-    data = data.reshape(height_data['height'], height_data['width'])
-
-    step_x = measurement_conditions['x_length_per_pixel'] * get_unit_conversion('pm', 'um')
-    step_y = measurement_conditions['y_length_per_pixel'] * get_unit_conversion('pm', 'um')
-    return (data, step_x, step_y)
+            return extract_vk4(filehandle, read_image_layers=read_image_layers, encoding=encoding)
```

## surfalize/file/xyz.py

```diff
@@ -1,23 +1,24 @@
 # This code assumes units of meters for xyz data
 import numpy as np
-from ..exceptions import UnsupportedFileFormatError
+from ..exceptions import UnsupportedFileFormatError, CorruptedFileError
+from .common import RawSurface
 
-def read_xyz(filepath):
+def read_xyz(filepath, read_image_layers=False, encoding='utf-8'):
     with open(filepath) as file:
         try:
             raw_data = np.loadtxt(file)
         except UnicodeDecodeError:
             raise UnsupportedFileFormatError('The xyz file contains binary data. Only ASCII xyz files are supported.')
         except ValueError:
             raise UnsupportedFileFormatError('The xyz file format type is not supported.')
     x = np.unique(raw_data[:,0])
     y = np.unique(raw_data[:,1])
     nx = x.size
     ny = y.size
     if nx * ny != raw_data.shape[0]:
-        raise CorrputedFileError('Number of datapoints does not match expected size.')
+        raise CorruptedFileError('Number of datapoints does not match expected size.')
     step_x = (x.max() - x.min()) / (nx) * 10**6
     step_y = (y.max() - y.min()) / (ny) * 10**6
     data = raw_data[:,2].copy().reshape(ny, nx) * 10**6
 
-    return data, step_x, step_y
+    return RawSurface(data, step_x, step_y)
```

## surfalize/file/zmg.py

```diff
@@ -1,28 +1,28 @@
 import numpy as np
-from .common import read_binary_layout
+from .common import read_binary_layout, RawSurface
 
 LAYOUT_HEADER = (
     (None, 85, None),
     ('res_x', 'I', False),
     ('res_y', 'I', False),
     (None, 4, None),
     ('step_x', 'f', False),
     ('step_y', 'f', False),
     ('step_z', 'f', False),
     (None, 8, None),
     ('comment_size', 'I', False),
     (None, 84, None)
 )
 
-def read_zmg(filepath, encoding='utf-8'):
+def read_zmg(filepath, read_image_layers=False, encoding='utf-8'):
     with open(filepath, 'rb') as filehandle:
         header = read_binary_layout(filehandle, LAYOUT_HEADER, encoding=encoding)
         filehandle.seek(header['comment_size'], 1)
         data_length = header['res_x'] * header['res_y']
         data = np.fromfile(filehandle, dtype=np.int16, count=data_length) * header['step_z']
     data = data.reshape((header['res_y'], header['res_x']))
 
     step_x = header['step_x']
     step_y = header['step_y']
 
-    return (data, step_x, step_y)
+    return RawSurface(data, step_x, step_y)
```

## Comparing `surfalize-0.8.2.dist-info/LICENSE.txt` & `surfalize-0.9.0.dist-info/LICENSE.txt`

 * *Files identical despite different names*

