# Comparing `tmp/apache-beam-2.8.0.zip` & `tmp/apache-beam-2.9.0.zip`

## zipinfo {}

```diff
@@ -1,427 +1,452 @@
-Zip file size: 1199980 bytes, number of entries: 425
--rw-r-----  2.0 unx      856 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/MANIFEST.in
--rw-r-----  2.0 unx     6267 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/gen_protos.py
--rw-r-----  2.0 unx      758 b- defN 18-Oct-18 13:43 apache-beam-2.8.0/setup.cfg
--rw-r-----  2.0 unx     7560 b- defN 18-Oct-18 13:41 apache-beam-2.8.0/setup.py
--rw-r-----  2.0 unx     1072 b- defN 18-Oct-18 13:43 apache-beam-2.8.0/PKG-INFO
--rw-r-----  2.0 unx      866 b- defN 18-Oct-18 13:41 apache-beam-2.8.0/apache_beam/version.py
--rw-r-----  2.0 unx     1880 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/pvalue_test.py
--rw-r-----  2.0 unx     3529 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/__init__.py
--rw-r-----  2.0 unx    37059 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/pipeline.py
--rw-r-----  2.0 unx    18363 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/pvalue.py
--rw-r-----  2.0 unx    22418 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/pipeline_test.py
--rw-r-----  2.0 unx     1569 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/error.py
--rw-r-----  2.0 unx     1392 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/runners/__init__.py
--rw-r-----  2.0 unx     1856 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/runners/pipeline_context_test.py
--rw-r-----  2.0 unx     4529 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/runners/pipeline_context.py
--rw-r-----  2.0 unx     5899 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/runners/sdf_common.py
--rw-r-----  2.0 unx     5815 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/runners/runner_test.py
--rw-r-----  2.0 unx     3773 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/runners/common.pxd
--rw-r-----  2.0 unx    34512 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/runners/common.py
--rw-r-----  2.0 unx     1784 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/runners/common_test.py
--rw-r-----  2.0 unx    14033 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/runners/runner.py
--rw-r-----  2.0 unx     1360 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/runners/test/__init__.py
--rw-r-----  2.0 unx     2157 b- defN 18-Oct-18 13:41 apache-beam-2.8.0/apache_beam/runners/direct/test_direct_runner.py
--rw-r-----  2.0 unx     6862 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/runners/direct/bundle_factory.py
--rw-r-----  2.0 unx     2005 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/runners/direct/direct_runner_test.py
--rw-r-----  2.0 unx     1088 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/runners/direct/__init__.py
--rw-r-----  2.0 unx     9689 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/runners/direct/direct_metrics_test.py
--rw-r-----  2.0 unx     3879 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/runners/direct/helper_transforms.py
--rw-r-----  2.0 unx     8377 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/runners/direct/sdf_direct_runner_test.py
--rw-r-----  2.0 unx     4895 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/runners/direct/direct_metrics.py
--rw-r-----  2.0 unx     2388 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/runners/direct/consumer_tracking_pipeline_visitor.py
--rw-r-----  2.0 unx    11225 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/runners/direct/watermark_manager.py
--rw-r-----  2.0 unx    17950 b- defN 18-Oct-18 13:41 apache-beam-2.8.0/apache_beam/runners/direct/direct_runner.py
--rw-r-----  2.0 unx    25292 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/runners/direct/executor.py
--rw-r-----  2.0 unx     1572 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/runners/direct/clock.py
--rw-r-----  2.0 unx     4457 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/runners/direct/consumer_tracking_pipeline_visitor_test.py
--rw-r-----  2.0 unx    37631 b- defN 18-Oct-18 13:41 apache-beam-2.8.0/apache_beam/runners/direct/transform_evaluator.py
--rw-r-----  2.0 unx    15203 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/runners/direct/evaluation_context.py
--rw-r-----  2.0 unx     4659 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/runners/direct/direct_userstate.py
--rw-r-----  2.0 unx     3121 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/runners/direct/util.py
--rw-r-----  2.0 unx    14767 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/runners/direct/sdf_direct_runner.py
--rw-r-----  2.0 unx    17382 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/runners/interactive/pipeline_analyzer.py
--rw-r-----  2.0 unx      784 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/runners/interactive/__init__.py
--rw-r-----  2.0 unx    12073 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/runners/interactive/pipeline_analyzer_test.py
--rw-r-----  2.0 unx     3934 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/runners/interactive/interactive_runner_test.py
--rw-r-----  2.0 unx     8288 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/runners/interactive/interactive_runner.py
--rw-r-----  2.0 unx     7128 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/runners/interactive/cache_manager.py
--rw-r-----  2.0 unx     6674 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/runners/interactive/cache_manager_test.py
--rw-r-----  2.0 unx     6764 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/runners/interactive/display/display_manager.py
--rw-r-----  2.0 unx     5606 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/runners/interactive/display/interactive_pipeline_graph.py
--rw-r-----  2.0 unx     3477 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/runners/interactive/display/pipeline_graph_renderer.py
--rw-r-----  2.0 unx      784 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/runners/interactive/display/__init__.py
--rw-r-----  2.0 unx     8459 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/runners/interactive/display/pipeline_graph.py
--rw-r-----  2.0 unx     1853 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/runners/job/manager.py
--rw-r-----  2.0 unx     1174 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/runners/job/utils.py
--rw-r-----  2.0 unx      823 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/runners/job/__init__.py
--rw-r-----  2.0 unx    10106 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/runners/dataflow/dataflow_metrics_test.py
--rw-r-----  2.0 unx    52921 b- defN 18-Oct-18 13:41 apache-beam-2.8.0/apache_beam/runners/dataflow/dataflow_runner.py
--rw-r-----  2.0 unx     8693 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/runners/dataflow/dataflow_metrics.py
--rw-r-----  2.0 unx     1175 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/runners/dataflow/__init__.py
--rw-r-----  2.0 unx     3891 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/runners/dataflow/test_dataflow_runner.py
--rw-r-----  2.0 unx     3979 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/runners/dataflow/template_runner_test.py
--rw-r-----  2.0 unx    16329 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/runners/dataflow/dataflow_runner_test.py
--rw-r-----  2.0 unx     2049 b- defN 18-Oct-18 13:41 apache-beam-2.8.0/apache_beam/runners/dataflow/ptransform_overrides.py
--rw-r-----  2.0 unx      823 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/runners/dataflow/native_io/__init__.py
--rw-r-----  2.0 unx     6718 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/runners/dataflow/native_io/iobase_test.py
--rw-r-----  2.0 unx     2611 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/runners/dataflow/native_io/streaming_create.py
--rw-r-----  2.0 unx    10660 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/runners/dataflow/native_io/iobase.py
--rw-r-----  2.0 unx     4424 b- defN 18-Oct-18 13:41 apache-beam-2.8.0/apache_beam/runners/dataflow/internal/names.py
--rw-r-----  2.0 unx    15365 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/runners/dataflow/internal/apiclient_test.py
--rw-r-----  2.0 unx      823 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/runners/dataflow/internal/__init__.py
--rw-r-----  2.0 unx    40491 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/runners/dataflow/internal/apiclient.py
--rw-r-----  2.0 unx      823 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/runners/dataflow/internal/clients/__init__.py
--rw-r-----  2.0 unx    37786 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/runners/dataflow/internal/clients/dataflow/dataflow_v1b3_client.py
--rw-r-----  2.0 unx     1429 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/runners/dataflow/internal/clients/dataflow/__init__.py
--rw-r-----  2.0 unx     4365 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/runners/dataflow/internal/clients/dataflow/message_matchers.py
--rw-r-----  2.0 unx   208678 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/runners/dataflow/internal/clients/dataflow/dataflow_v1b3_messages.py
--rw-r-----  2.0 unx     2820 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/runners/dataflow/internal/clients/dataflow/message_matchers_test.py
--rw-r-----  2.0 unx    10643 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/runners/portability/local_job_service.py
--rw-r-----  2.0 unx    24859 b- defN 18-Oct-18 13:41 apache-beam-2.8.0/apache_beam/runners/portability/stager.py
--rw-r-----  2.0 unx     1646 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/runners/portability/local_job_service_main.py
--rw-r-----  2.0 unx     9987 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/runners/portability/portable_runner_test.py
--rw-r-----  2.0 unx      899 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/runners/portability/__init__.py
--rw-r-----  2.0 unx    22345 b- defN 18-Oct-18 13:41 apache-beam-2.8.0/apache_beam/runners/portability/stager_test.py
--rw-r-----  2.0 unx    65088 b- defN 18-Oct-18 13:41 apache-beam-2.8.0/apache_beam/runners/portability/fn_api_runner.py
--rw-r-----  2.0 unx     3135 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/runners/portability/flink_runner_test.py
--rw-r-----  2.0 unx     4141 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/runners/portability/portable_stager.py
--rw-r-----  2.0 unx     9824 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/runners/portability/portable_runner.py
--rw-r-----  2.0 unx     3863 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/runners/portability/job_server.py
--rw-r-----  2.0 unx     6385 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/runners/portability/portable_stager_test.py
--rw-r-----  2.0 unx    21476 b- defN 18-Oct-18 13:41 apache-beam-2.8.0/apache_beam/runners/portability/fn_api_runner_test.py
--rw-r-----  2.0 unx     9134 b- defN 18-Oct-18 13:41 apache-beam-2.8.0/apache_beam/runners/worker/statesampler_fast.pyx
--rw-r-----  2.0 unx     4358 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/runners/worker/sdk_worker_test.py
--rw-r-----  2.0 unx     7461 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/runners/worker/sdk_worker_main.py
--rw-r-----  2.0 unx     4902 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/runners/worker/statesampler.py
--rw-r-----  2.0 unx     2793 b- defN 18-Oct-18 13:41 apache-beam-2.8.0/apache_beam/runners/worker/statesampler_slow.py
--rw-r-----  2.0 unx     5350 b- defN 18-Oct-18 13:41 apache-beam-2.8.0/apache_beam/runners/worker/sdk_worker_main_test.py
--rw-r-----  2.0 unx     4931 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/runners/worker/log_handler.py
--rw-r-----  2.0 unx      892 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/runners/worker/__init__.py
--rw-r-----  2.0 unx     2175 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/runners/worker/statesampler_fast.pxd
--rw-r-----  2.0 unx    29492 b- defN 18-Oct-18 13:41 apache-beam-2.8.0/apache_beam/runners/worker/operations.py
--rw-r-----  2.0 unx    16592 b- defN 18-Oct-18 13:41 apache-beam-2.8.0/apache_beam/runners/worker/sdk_worker.py
--rw-r-----  2.0 unx     7410 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/runners/worker/sideinputs.py
--rw-r-----  2.0 unx     7950 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/runners/worker/opcounters_test.py
--rw-r-----  2.0 unx    17246 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/runners/worker/operation_specs.py
--rw-r-----  2.0 unx    11275 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/runners/worker/opcounters.py
--rw-r-----  2.0 unx     2592 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/runners/worker/worker_id_interceptor_test.py
--rw-r-----  2.0 unx     4633 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/runners/worker/data_plane_test.py
--rw-r-----  2.0 unx     2262 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/runners/worker/opcounters.pxd
--rw-r-----  2.0 unx    11921 b- defN 18-Oct-18 13:41 apache-beam-2.8.0/apache_beam/runners/worker/data_plane.py
--rw-r-----  2.0 unx     2162 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/runners/worker/worker_id_interceptor.py
--rw-r-----  2.0 unx     7941 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/runners/worker/logger_test.py
--rw-r-----  2.0 unx     3856 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/runners/worker/log_handler_test.py
--rw-r-----  2.0 unx    34473 b- defN 18-Oct-18 13:41 apache-beam-2.8.0/apache_beam/runners/worker/bundle_processor.py
--rw-r-----  2.0 unx     4459 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/runners/worker/statesampler_test.py
--rw-r-----  2.0 unx     3002 b- defN 18-Oct-18 13:41 apache-beam-2.8.0/apache_beam/runners/worker/operations.pxd
--rw-r-----  2.0 unx     6745 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/runners/worker/logger.py
--rw-r-----  2.0 unx     5860 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/runners/worker/sideinputs_test.py
--rw-r-----  2.0 unx     6652 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/examples/wordcount_debugging.py
--rw-r-----  2.0 unx     2146 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/examples/wordcount_minimal_test.py
--rw-r-----  2.0 unx     6628 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/examples/streaming_wordcount_debugging.py
--rw-r-----  2.0 unx      824 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/examples/__init__.py
--rw-r-----  2.0 unx     2172 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/examples/wordcount_test.py
--rw-r-----  2.0 unx     3012 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/examples/wordcount_it_test.py
--rw-r-----  2.0 unx     6266 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/examples/fastavro_it_test.py
--rw-r-----  2.0 unx     3354 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/examples/windowed_wordcount.py
--rw-r-----  2.0 unx     5019 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/examples/wordcount.py
--rw-r-----  2.0 unx     4346 b- defN 18-Oct-18 13:41 apache-beam-2.8.0/apache_beam/examples/streaming_wordcount_it_test.py
--rw-r-----  2.0 unx     2101 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/examples/wordcount_debugging_test.py
--rw-r-----  2.0 unx     4954 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/examples/wordcount_minimal.py
--rw-r-----  2.0 unx     5910 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/examples/avro_bitcoin.py
--rw-r-----  2.0 unx     3845 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/examples/streaming_wordcount.py
--rw-r-----  2.0 unx    47921 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/examples/snippets/snippets_test.py
--rw-r-----  2.0 unx      824 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/examples/snippets/__init__.py
--rw-r-----  2.0 unx    47110 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/examples/snippets/snippets.py
--rw-r-----  2.0 unx     1864 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/examples/complete/estimate_pi_test.py
--rw-r-----  2.0 unx     3166 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/examples/complete/autocomplete.py
--rw-r-----  2.0 unx    13825 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/examples/complete/distribopt.py
--rw-r-----  2.0 unx     3460 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/examples/complete/tfidf_test.py
--rw-r-----  2.0 unx      824 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/examples/complete/__init__.py
--rw-r-----  2.0 unx     2947 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/examples/complete/distribopt_test.py
--rw-r-----  2.0 unx     4502 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/examples/complete/estimate_pi.py
--rw-r-----  2.0 unx     6055 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/examples/complete/top_wikipedia_sessions.py
--rw-r-----  2.0 unx     2437 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/examples/complete/top_wikipedia_sessions_test.py
--rw-r-----  2.0 unx     8683 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/examples/complete/tfidf.py
--rw-r-----  2.0 unx     2520 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/examples/complete/autocomplete_test.py
--rw-r-----  2.0 unx      824 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/examples/complete/juliaset/__init__.py
--rw-r-----  2.0 unx     4772 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/examples/complete/juliaset/setup.py
--rw-r-----  2.0 unx     2305 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/examples/complete/juliaset/juliaset_main.py
--rw-r-----  2.0 unx      824 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/examples/complete/juliaset/juliaset/__init__.py
--rw-r-----  2.0 unx     2977 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/examples/complete/juliaset/juliaset/juliaset_test.py
--rw-r-----  2.0 unx     4578 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/examples/complete/juliaset/juliaset/juliaset.py
--rw-r-----  2.0 unx     6138 b- defN 18-Oct-18 13:41 apache-beam-2.8.0/apache_beam/examples/complete/game/game_stats_it_test.py
--rw-r-----  2.0 unx     3320 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/examples/complete/game/game_stats_test.py
--rw-r-----  2.0 unx     2145 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/examples/complete/game/hourly_team_score_test.py
--rw-r-----  2.0 unx     3239 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/examples/complete/game/user_score_it_test.py
--rw-r-----  2.0 unx     6523 b- defN 18-Oct-18 13:41 apache-beam-2.8.0/apache_beam/examples/complete/game/leader_board_it_test.py
--rw-r-----  2.0 unx    14936 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/examples/complete/game/game_stats.py
--rw-r-----  2.0 unx      824 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/examples/complete/game/__init__.py
--rw-r-----  2.0 unx     3871 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/examples/complete/game/hourly_team_score_it_test.py
--rw-r-----  2.0 unx    11676 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/examples/complete/game/hourly_team_score.py
--rw-r-----  2.0 unx    13391 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/examples/complete/game/leader_board.py
--rw-r-----  2.0 unx     5677 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/examples/complete/game/user_score.py
--rw-r-----  2.0 unx     2569 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/examples/complete/game/leader_board_test.py
--rw-r-----  2.0 unx     1994 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/examples/complete/game/user_score_test.py
--rw-r-----  2.0 unx     3520 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/examples/cookbook/group_with_coder_test.py
--rw-r-----  2.0 unx     4017 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/examples/cookbook/filters.py
--rw-r-----  2.0 unx     2581 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/examples/cookbook/datastore_wordcount_it_test.py
--rw-r-----  2.0 unx     2877 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/examples/cookbook/bigquery_tornadoes_it_test.py
--rw-r-----  2.0 unx      824 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/examples/cookbook/__init__.py
--rw-r-----  2.0 unx     2645 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/examples/cookbook/multiple_output_pardo_test.py
--rw-r-----  2.0 unx     3905 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/examples/cookbook/custom_ptransform.py
--rw-r-----  2.0 unx     6949 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/examples/cookbook/multiple_output_pardo.py
--rw-r-----  2.0 unx     4743 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/examples/cookbook/group_with_coder.py
--rw-r-----  2.0 unx     1797 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/examples/cookbook/bigquery_tornadoes_test.py
--rw-r-----  2.0 unx     5374 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/examples/cookbook/mergecontacts_test.py
--rw-r-----  2.0 unx     1860 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/examples/cookbook/custom_ptransform_test.py
--rw-r-----  2.0 unx     2680 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/examples/cookbook/combiners_test.py
--rw-r-----  2.0 unx     1804 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/examples/cookbook/coders_test.py
--rw-r-----  2.0 unx     4484 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/examples/cookbook/bigquery_side_input.py
--rw-r-----  2.0 unx     3583 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/examples/cookbook/bigquery_tornadoes.py
--rw-r-----  2.0 unx     6140 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/examples/cookbook/mergecontacts.py
--rw-r-----  2.0 unx    11008 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/examples/cookbook/datastore_wordcount.py
--rw-r-----  2.0 unx     3379 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/examples/cookbook/coders.py
--rw-r-----  2.0 unx     2307 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/examples/cookbook/bigquery_side_input_test.py
--rw-r-----  2.0 unx     4510 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/examples/cookbook/bigquery_schema.py
--rw-r-----  2.0 unx     2719 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/examples/cookbook/filters_test.py
--rw-r-----  2.0 unx      893 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/internal/__init__.py
--rw-r-----  2.0 unx     2317 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/internal/util_test.py
--rw-r-----  2.0 unx     9281 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/internal/pickler.py
--rw-r-----  2.0 unx     1862 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/internal/module_test.py
--rw-r-----  2.0 unx     3143 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/internal/pickler_test.py
--rw-r-----  2.0 unx     5221 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/internal/util.py
--rw-r-----  2.0 unx      893 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/internal/gcp/__init__.py
--rw-r-----  2.0 unx     4755 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/internal/gcp/auth.py
--rw-r-----  2.0 unx     6009 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/internal/gcp/json_value.py
--rw-r-----  2.0 unx     3671 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/internal/gcp/json_value_test.py
--rw-r-----  2.0 unx     6430 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/io/source_test_utils_test.py
--rw-r-----  2.0 unx    15269 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/io/filebasedsink.py
--rw-r-----  2.0 unx    16092 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/io/localfilesystem_test.py
--rw-r-----  2.0 unx     8510 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/io/filesystems_test.py
--rw-r-----  2.0 unx    10676 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/io/filesystems.py
--rw-r-----  2.0 unx    10482 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/io/concat_source.py
--rw-r-----  2.0 unx    27511 b- defN 18-Oct-18 13:41 apache-beam-2.8.0/apache_beam/io/filebasedsource_test.py
--rw-r-----  2.0 unx     5935 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/io/restriction_trackers_test.py
--rw-r-----  2.0 unx     7178 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/io/filesystemio.py
--rw-r-----  2.0 unx    17891 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/io/range_trackers_test.py
--rw-r-----  2.0 unx    26295 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/io/source_test_utils.py
--rw-r-----  2.0 unx    18083 b- defN 18-Oct-18 13:41 apache-beam-2.8.0/apache_beam/io/tfrecordio_test.py
--rw-r-----  2.0 unx     2388 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/io/utils.py
--rw-r-----  2.0 unx    17352 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/io/filebasedsource.py
--rw-r-----  2.0 unx    16066 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/io/filesystem_test.py
--rw-r-----  2.0 unx    24392 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/io/textio.py
--rw-r-----  2.0 unx     1630 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/io/__init__.py
--rw-r-----  2.0 unx     6349 b- defN 18-Oct-18 13:41 apache-beam-2.8.0/apache_beam/io/filesystemio_test.py
--rw-r-----  2.0 unx     4829 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/io/restriction_trackers.py
--rw-r-----  2.0 unx    10504 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/io/localfilesystem.py
--rw-r-----  2.0 unx    19305 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/io/hadoopfilesystem_test.py
--rw-r-----  2.0 unx    12238 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/io/tfrecordio.py
--rw-r-----  2.0 unx    27994 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/io/filesystem.py
--rw-r-----  2.0 unx    48704 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/io/iobase.py
--rw-r-----  2.0 unx    27132 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/io/vcfio_test.py
--rw-r-----  2.0 unx    37805 b- defN 18-Oct-18 13:41 apache-beam-2.8.0/apache_beam/io/textio_test.py
--rw-r-----  2.0 unx    12068 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/io/hadoopfilesystem.py
--rw-r-----  2.0 unx     9855 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/io/concat_source_test.py
--rw-r-----  2.0 unx    14498 b- defN 18-Oct-18 13:41 apache-beam-2.8.0/apache_beam/io/range_trackers.py
--rw-r-----  2.0 unx    21542 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/io/avroio.py
--rw-r-----  2.0 unx    14701 b- defN 18-Oct-18 13:41 apache-beam-2.8.0/apache_beam/io/filebasedsink_test.py
--rw-r-----  2.0 unx    19439 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/io/vcfio.py
--rw-r-----  2.0 unx     4179 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/io/sources_test.py
--rw-r-----  2.0 unx    16916 b- defN 18-Oct-18 13:41 apache-beam-2.8.0/apache_beam/io/avroio_test.py
--rw-r-----  2.0 unx     2247 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/io/gcp/bigquery_io_read_it_test.py
--rw-r-----  2.0 unx     7090 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/io/gcp/big_query_query_to_table_it_test.py
--rw-r-----  2.0 unx     2656 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/io/gcp/bigquery_io_read_pipeline.py
--rw-r-----  2.0 unx     2405 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/io/gcp/datastore_write_it_test.py
--rw-r-----  2.0 unx     2715 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/io/gcp/big_query_query_to_table_pipeline.py
--rw-r-----  2.0 unx    12160 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/io/gcp/gcsfilesystem_test.py
--rw-r-----  2.0 unx      823 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/io/gcp/__init__.py
--rw-r-----  2.0 unx    25782 b- defN 18-Oct-18 13:41 apache-beam-2.8.0/apache_beam/io/gcp/pubsub_test.py
--rw-r-----  2.0 unx     7220 b- defN 18-Oct-18 13:41 apache-beam-2.8.0/apache_beam/io/gcp/pubsub_integration_test.py
--rw-r-----  2.0 unx     3588 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/io/gcp/pubsub_it_pipeline.py
--rw-r-----  2.0 unx    65383 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/io/gcp/bigquery.py
--rw-r-----  2.0 unx     7522 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/io/gcp/datastore_write_it_pipeline.py
--rw-r-----  2.0 unx    46269 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/io/gcp/bigquery_test.py
--rw-r-----  2.0 unx    24875 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/io/gcp/gcsio_test.py
--rw-r-----  2.0 unx    22037 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/io/gcp/gcsio.py
--rw-r-----  2.0 unx    10907 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/io/gcp/gcsfilesystem.py
--rw-r-----  2.0 unx    15218 b- defN 18-Oct-18 13:41 apache-beam-2.8.0/apache_beam/io/gcp/pubsub.py
--rw-r-----  2.0 unx     2333 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/io/gcp/tests/utils.py
--rw-r-----  2.0 unx      824 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/io/gcp/tests/__init__.py
--rw-r-----  2.0 unx     4299 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/io/gcp/tests/bigquery_matcher_test.py
--rw-r-----  2.0 unx     5494 b- defN 18-Oct-18 13:41 apache-beam-2.8.0/apache_beam/io/gcp/tests/pubsub_matcher.py
--rw-r-----  2.0 unx     3867 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/io/gcp/tests/utils_test.py
--rw-r-----  2.0 unx     3942 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/io/gcp/tests/bigquery_matcher.py
--rw-r-----  2.0 unx     7072 b- defN 18-Oct-18 13:41 apache-beam-2.8.0/apache_beam/io/gcp/tests/pubsub_matcher_test.py
--rw-r-----  2.0 unx      823 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/io/gcp/datastore/__init__.py
--rw-r-----  2.0 unx    10175 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/io/gcp/datastore/v1/helper_test.py
--rw-r-----  2.0 unx     3614 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/io/gcp/datastore/v1/fake_datastore.py
--rw-r-----  2.0 unx     3789 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/io/gcp/datastore/v1/adaptive_throttler.py
--rw-r-----  2.0 unx      823 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/io/gcp/datastore/v1/__init__.py
--rw-r-----  2.0 unx     2394 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/io/gcp/datastore/v1/util_test.py
--rw-r-----  2.0 unx     3916 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/io/gcp/datastore/v1/adaptive_throttler_test.py
--rw-r-----  2.0 unx     7915 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/io/gcp/datastore/v1/query_splitter_test.py
--rw-r-----  2.0 unx     9650 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/io/gcp/datastore/v1/query_splitter.py
--rw-r-----  2.0 unx    10589 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/io/gcp/datastore/v1/helper.py
--rw-r-----  2.0 unx    12817 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/io/gcp/datastore/v1/datastoreio_test.py
--rw-r-----  2.0 unx    20150 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/io/gcp/datastore/v1/datastoreio.py
--rw-r-----  2.0 unx     3789 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/io/gcp/datastore/v1/util.py
--rw-r-----  2.0 unx      823 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/io/gcp/internal/__init__.py
--rw-r-----  2.0 unx      823 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/io/gcp/internal/clients/__init__.py
--rw-r-----  2.0 unx     1405 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/io/gcp/internal/clients/bigquery/__init__.py
--rw-r-----  2.0 unx    28371 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/io/gcp/internal/clients/bigquery/bigquery_v2_client.py
--rw-r-----  2.0 unx    81889 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/io/gcp/internal/clients/bigquery/bigquery_v2_messages.py
--rw-r-----  2.0 unx    44374 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/io/gcp/internal/clients/storage/storage_v1_client.py
--rw-r-----  2.0 unx    78332 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/io/gcp/internal/clients/storage/storage_v1_messages.py
--rw-r-----  2.0 unx     1400 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/io/gcp/internal/clients/storage/__init__.py
--rw-r-----  2.0 unx     6125 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/testing/pipeline_verifiers_test.py
--rw-r-----  2.0 unx    18558 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/testing/synthetic_pipeline.py
--rw-r-----  2.0 unx      823 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/testing/__init__.py
--rw-r-----  2.0 unx     3628 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/testing/util_test.py
--rw-r-----  2.0 unx     6845 b- defN 18-Oct-18 13:41 apache-beam-2.8.0/apache_beam/testing/test_pipeline.py
--rw-r-----  2.0 unx    14604 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/testing/test_stream_test.py
--rw-r-----  2.0 unx     4893 b- defN 18-Oct-18 13:41 apache-beam-2.8.0/apache_beam/testing/test_utils_test.py
--rw-r-----  2.0 unx     6863 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/testing/test_stream.py
--rw-r-----  2.0 unx     5340 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/testing/pipeline_verifiers.py
--rw-r-----  2.0 unx     4249 b- defN 18-Oct-18 13:41 apache-beam-2.8.0/apache_beam/testing/test_pipeline_test.py
--rw-r-----  2.0 unx     5381 b- defN 18-Oct-18 13:41 apache-beam-2.8.0/apache_beam/testing/test_utils.py
--rw-r-----  2.0 unx     5408 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/testing/synthetic_pipeline_test.py
--rw-r-----  2.0 unx     7056 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/testing/util.py
--rw-r-----  2.0 unx     6753 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/testing/data/trigger_transcripts.yaml
--rw-r-----  2.0 unx     6537 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/testing/data/standard_coders.yaml
--rw-r-----  2.0 unx      823 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/testing/benchmarks/__init__.py
--rw-r-----  2.0 unx     9445 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/testing/benchmarks/nexmark/nexmark_launcher.py
--rw-r-----  2.0 unx      823 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/testing/benchmarks/nexmark/__init__.py
--rw-r-----  2.0 unx     3246 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/testing/benchmarks/nexmark/nexmark_util.py
--rw-r-----  2.0 unx     1759 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/testing/benchmarks/nexmark/queries/query2.py
--rw-r-----  2.0 unx     1421 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/testing/benchmarks/nexmark/queries/query0.py
--rw-r-----  2.0 unx     1923 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/testing/benchmarks/nexmark/queries/query1.py
--rw-r-----  2.0 unx      784 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/testing/benchmarks/nexmark/queries/__init__.py
--rw-r-----  2.0 unx      823 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/testing/benchmarks/nexmark/models/__init__.py
--rw-r-----  2.0 unx     2735 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/testing/benchmarks/nexmark/models/nexmark_model.py
--rw-r-----  2.0 unx     8107 b- defN 18-Oct-18 13:41 apache-beam-2.8.0/apache_beam/metrics/execution.py
--rw-r-----  2.0 unx      956 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/metrics/execution.pxd
--rw-r-----  2.0 unx      924 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/metrics/__init__.py
--rw-r-----  2.0 unx    14452 b- defN 18-Oct-18 13:41 apache-beam-2.8.0/apache_beam/metrics/cells.py
--rw-r-----  2.0 unx     5004 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/metrics/cells_test.py
--rw-r-----  2.0 unx     8076 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/metrics/metric.py
--rw-r-----  2.0 unx     3900 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/metrics/execution_test.py
--rw-r-----  2.0 unx     3620 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/metrics/metricbase.py
--rw-r-----  2.0 unx     5639 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/metrics/metric_test.py
--rw-r-----  2.0 unx    19736 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/typehints/decorators.py
--rw-r-----  2.0 unx    39334 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/typehints/typehints_test.py
--rw-r-----  2.0 unx     5347 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/typehints/native_type_compatibility.py
--rw-r-----  2.0 unx     4087 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/typehints/native_type_compatibility_test.py
--rw-r-----  2.0 unx     1029 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/typehints/__init__.py
--rw-r-----  2.0 unx    38894 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/typehints/typehints.py
--rw-r-----  2.0 unx     7100 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/typehints/trivial_inference_test.py
--rw-r-----  2.0 unx    10910 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/typehints/typed_pipeline_test.py
--rw-r-----  2.0 unx     9490 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/typehints/opcodes.py
--rw-r-----  2.0 unx    13688 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/typehints/trivial_inference.py
--rw-r-----  2.0 unx     9808 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/typehints/typecheck.py
--rw-r-----  2.0 unx     2590 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/tools/map_fn_microbenchmark.py
--rw-r-----  2.0 unx     4346 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/tools/utils.py
--rw-r-----  2.0 unx      931 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/tools/__init__.py
--rw-r-----  2.0 unx     2468 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/tools/distribution_counter_microbenchmark.py
--rw-r-----  2.0 unx     5714 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/tools/coders_microbenchmark.py
--rw-r-----  2.0 unx     1298 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/tools/microbenchmarks_test.py
--rw-r-----  2.0 unx     2896 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/tools/sideinput_microbenchmark.py
--rw-r-----  2.0 unx     4291 b- defN 18-Oct-18 13:41 apache-beam-2.8.0/apache_beam/coders/coder_impl.pxd
--rw-r-----  2.0 unx     6099 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/coders/stream_test.py
--rw-r-----  2.0 unx      915 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/coders/__init__.py
--rw-r-----  2.0 unx     5171 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/coders/slow_stream.py
--rw-r-----  2.0 unx     2443 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/coders/stream.pxd
--rw-r-----  2.0 unx    16801 b- defN 18-Oct-18 13:41 apache-beam-2.8.0/apache_beam/coders/coders_test_common.py
--rw-r-----  2.0 unx    11595 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/coders/proto2_coder_test_messages_pb2.py
--rw-r-----  2.0 unx     1770 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/coders/observable_test.py
--rw-r-----  2.0 unx     6311 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/coders/standard_coders_test.py
--rw-r-----  2.0 unx     4003 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/coders/coders_test.py
--rw-r-----  2.0 unx     7600 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/coders/typecoders.py
--rw-r-----  2.0 unx     7852 b- defN 18-Oct-18 13:41 apache-beam-2.8.0/apache_beam/coders/stream.pyx
--rw-r-----  2.0 unx     4504 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/coders/typecoders_test.py
--rw-r-----  2.0 unx     1678 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/coders/slow_coders_test.py
--rw-r-----  2.0 unx    31592 b- defN 18-Oct-18 13:41 apache-beam-2.8.0/apache_beam/coders/coder_impl.py
--rw-r-----  2.0 unx     1317 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/coders/fast_coders_test.py
--rw-r-----  2.0 unx    30318 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/coders/coders.py
--rw-r-----  2.0 unx     1466 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/coders/observable.py
--rw-r-----  2.0 unx    12074 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/transforms/window_test.py
--rw-r-----  2.0 unx    10593 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/transforms/userstate.py
--rw-r-----  2.0 unx     3591 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/transforms/create_source.py
--rw-r-----  2.0 unx    20402 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/transforms/userstate_test.py
--rw-r-----  2.0 unx    18200 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/transforms/window.py
--rw-r-----  2.0 unx     4882 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/transforms/cy_dataflow_distribution_counter.pyx
--rw-r-----  2.0 unx     4552 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/transforms/timeutil.py
--rw-r-----  2.0 unx     9459 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/transforms/display_test.py
--rw-r-----  2.0 unx     1389 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/transforms/__init__.py
--rw-r-----  2.0 unx     1663 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/transforms/cy_dataflow_distribution_counter.pxd
--rw-r-----  2.0 unx    19571 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/transforms/combiners.py
--rw-r-----  2.0 unx     2917 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/transforms/sideinputs.py
--rw-r-----  2.0 unx    16909 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/transforms/util_test.py
--rw-r-----  2.0 unx     2929 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/transforms/dataflow_distribution_counter_test.py
--rw-r-----  2.0 unx    11516 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/transforms/display.py
--rw-r-----  2.0 unx     3073 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/transforms/cy_combiners.pxd
--rw-r-----  2.0 unx    13965 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/transforms/combiners_test.py
--rw-r-----  2.0 unx     5648 b- defN 18-Oct-18 13:41 apache-beam-2.8.0/apache_beam/transforms/create_test.py
--rw-r-----  2.0 unx     8232 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/transforms/cy_combiners.py
--rw-r-----  2.0 unx    30318 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/transforms/ptransform.py
--rw-r-----  2.0 unx    74776 b- defN 18-Oct-18 13:41 apache-beam-2.8.0/apache_beam/transforms/core.py
--rw-r-----  2.0 unx    21611 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/transforms/trigger_test.py
--rw-r-----  2.0 unx     4371 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/transforms/write_ptransform_test.py
--rw-r-----  2.0 unx    43254 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/transforms/trigger.py
--rw-r-----  2.0 unx    81197 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/transforms/ptransform_test.py
--rw-r-----  2.0 unx    23690 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/transforms/util.py
--rw-r-----  2.0 unx    11833 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/transforms/sideinputs_test.py
--rw-r-----  2.0 unx     4109 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/transforms/py_dataflow_distribution_counter.py
--rw-r-----  2.0 unx     2752 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/utils/windowed_value_test.py
--rw-r-----  2.0 unx     6779 b- defN 18-Oct-18 13:41 apache-beam-2.8.0/apache_beam/utils/annotations_test.py
--rw-r-----  2.0 unx     8489 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/utils/retry.py
--rw-r-----  2.0 unx     8061 b- defN 18-Oct-18 13:41 apache-beam-2.8.0/apache_beam/utils/counters.py
--rw-r-----  2.0 unx     3836 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/utils/counters_test.py
--rw-r-----  2.0 unx     1660 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/utils/plugin.py
--rw-r-----  2.0 unx     7723 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/utils/retry_test.py
--rw-r-----  2.0 unx     4476 b- defN 18-Oct-18 13:41 apache-beam-2.8.0/apache_beam/utils/annotations.py
--rw-r-----  2.0 unx      936 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/utils/__init__.py
--rw-r-----  2.0 unx     7476 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/utils/windowed_value.py
--rw-r-----  2.0 unx     1784 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/utils/processes.py
--rw-r-----  2.0 unx     4782 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/utils/profiler.py
--rw-r-----  2.0 unx     4073 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/utils/urns.py
--rw-r-----  2.0 unx     1252 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/utils/windowed_value.pxd
--rw-r-----  2.0 unx     2003 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/utils/proto_utils.py
--rw-r-----  2.0 unx     8551 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/utils/timestamp.py
--rw-r-----  2.0 unx     1130 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/utils/counters.pxd
--rw-r-----  2.0 unx     7930 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/utils/timestamp_test.py
--rw-r-----  2.0 unx     3466 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/utils/processes_test.py
--rw-r-----  2.0 unx     1529 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/portability/python_urns.py
--rw-r-----  2.0 unx     2538 b- defN 18-Oct-18 13:41 apache-beam-2.8.0/apache_beam/portability/common_urns.py
--rw-r-----  2.0 unx      892 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/portability/__init__.py
--rw-r-----  2.0 unx    12717 b- defN 18-Oct-18 13:43 apache-beam-2.8.0/apache_beam/portability/api/standard_window_fns_pb2.py
--rw-r-----  2.0 unx     6047 b- defN 18-Oct-18 13:43 apache-beam-2.8.0/apache_beam/portability/api/beam_fn_api_pb2_grpc.py
--rw-r-----  2.0 unx       83 b- defN 18-Oct-18 13:43 apache-beam-2.8.0/apache_beam/portability/api/endpoints_pb2_grpc.py
--rw-r-----  2.0 unx   209590 b- defN 18-Oct-18 13:43 apache-beam-2.8.0/apache_beam/portability/api/beam_runner_api_pb2.py
--rw-r-----  2.0 unx      922 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/portability/api/__init__.py
--rw-r-----  2.0 unx       83 b- defN 18-Oct-18 13:43 apache-beam-2.8.0/apache_beam/portability/api/standard_window_fns_pb2_grpc.py
--rw-r-----  2.0 unx   168555 b- defN 18-Oct-18 13:43 apache-beam-2.8.0/apache_beam/portability/api/beam_fn_api_pb2.py
--rw-r-----  2.0 unx     5779 b- defN 18-Oct-18 13:43 apache-beam-2.8.0/apache_beam/portability/api/beam_job_api_pb2_grpc.py
--rw-r-----  2.0 unx    14492 b- defN 18-Oct-18 13:43 apache-beam-2.8.0/apache_beam/portability/api/beam_provision_api_pb2.py
--rw-r-----  2.0 unx    25734 b- defN 18-Oct-18 13:43 apache-beam-2.8.0/apache_beam/portability/api/beam_artifact_api_pb2.py
--rw-r-----  2.0 unx     2070 b- defN 18-Oct-18 13:43 apache-beam-2.8.0/apache_beam/portability/api/beam_provision_api_pb2_grpc.py
--rw-r-----  2.0 unx     4989 b- defN 18-Oct-18 13:43 apache-beam-2.8.0/apache_beam/portability/api/endpoints_pb2.py
--rw-r-----  2.0 unx     5279 b- defN 18-Oct-18 13:43 apache-beam-2.8.0/apache_beam/portability/api/beam_artifact_api_pb2_grpc.py
--rw-r-----  2.0 unx    29004 b- defN 18-Oct-18 13:43 apache-beam-2.8.0/apache_beam/portability/api/beam_job_api_pb2.py
--rw-r-----  2.0 unx       83 b- defN 18-Oct-18 13:43 apache-beam-2.8.0/apache_beam/portability/api/beam_runner_api_pb2_grpc.py
--rw-r-----  2.0 unx     8280 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/options/pipeline_options_validator.py
--rw-r-----  2.0 unx      823 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/options/__init__.py
--rw-r-----  2.0 unx     4001 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/options/value_provider.py
--rw-r-----  2.0 unx    28733 b- defN 18-Oct-18 13:41 apache-beam-2.8.0/apache_beam/options/pipeline_options.py
--rw-r-----  2.0 unx    11811 b- defN 18-Oct-18 13:41 apache-beam-2.8.0/apache_beam/options/pipeline_options_test.py
--rw-r-----  2.0 unx    11364 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/options/pipeline_options_validator_test.py
--rw-r-----  2.0 unx     9145 b- defN 18-Oct-18 13:40 apache-beam-2.8.0/apache_beam/options/value_provider_test.py
--rw-r-----  2.0 unx      729 b- defN 18-Oct-18 13:43 apache-beam-2.8.0/apache_beam.egg-info/requires.txt
--rw-r-----  2.0 unx        1 b- defN 18-Oct-18 13:43 apache-beam-2.8.0/apache_beam.egg-info/dependency_links.txt
--rw-r-----  2.0 unx        1 b- defN 18-Oct-18 13:43 apache-beam-2.8.0/apache_beam.egg-info/not-zip-safe
--rw-r-----  2.0 unx    18434 b- defN 18-Oct-18 13:43 apache-beam-2.8.0/apache_beam.egg-info/SOURCES.txt
--rw-r-----  2.0 unx     1072 b- defN 18-Oct-18 13:43 apache-beam-2.8.0/apache_beam.egg-info/PKG-INFO
--rw-r-----  2.0 unx       67 b- defN 18-Oct-18 13:43 apache-beam-2.8.0/apache_beam.egg-info/entry_points.txt
--rw-r-----  2.0 unx       12 b- defN 18-Oct-18 13:43 apache-beam-2.8.0/apache_beam.egg-info/top_level.txt
-425 files, 4496872 bytes uncompressed, 1116320 bytes compressed:  75.2%
+Zip file size: 2496764 bytes, number of entries: 450
+-rw-r--r--  2.0 unx      856 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/MANIFEST.in
+-rw-r--r--  2.0 unx     6267 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/gen_protos.py
+-rw-r--r--  2.0 unx     7535 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/setup.py
+-rw-r--r--  2.0 unx     1010 b- defN 18-Dec-05 23:24 apache-beam-2.9.0/PKG-INFO
+-rw-r--r--  2.0 unx      758 b- defN 18-Dec-05 23:24 apache-beam-2.9.0/setup.cfg
+-rw-r--r--  2.0 unx     3529 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/__init__.py
+-rw-r--r--  2.0 unx    38341 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/pipeline.py
+-rw-r--r--  2.0 unx      866 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/version.py
+-rw-r--r--  2.0 unx    22449 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/pipeline_test.py
+-rw-r--r--  2.0 unx    18550 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/pvalue.py
+-rw-r--r--  2.0 unx     1569 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/error.py
+-rw-r--r--  2.0 unx     1880 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/pvalue_test.py
+-rw-r--r--  2.0 unx     5336 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/internal/util.py
+-rw-r--r--  2.0 unx      893 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/internal/__init__.py
+-rw-r--r--  2.0 unx     2317 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/internal/util_test.py
+-rw-r--r--  2.0 unx     1862 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/internal/module_test.py
+-rw-r--r--  2.0 unx     3143 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/internal/pickler_test.py
+-rw-r--r--  2.0 unx     9351 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/internal/pickler.py
+-rw-r--r--  2.0 unx      893 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/internal/gcp/__init__.py
+-rw-r--r--  2.0 unx     4755 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/internal/gcp/auth.py
+-rw-r--r--  2.0 unx     3775 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/internal/gcp/json_value_test.py
+-rw-r--r--  2.0 unx     6106 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/internal/gcp/json_value.py
+-rw-r--r--  2.0 unx      915 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/coders/__init__.py
+-rw-r--r--  2.0 unx     4619 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/coders/typecoders_test.py
+-rw-r--r--  2.0 unx     6099 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/coders/stream_test.py
+-rw-r--r--  2.0 unx     7600 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/coders/typecoders.py
+-rw-r--r--  2.0 unx    30433 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/coders/coders.py
+-rw-r--r--  2.0 unx  2652241 b- defN 18-Dec-05 23:24 apache-beam-2.9.0/apache_beam/coders/coder_impl.c
+-rw-r--r--  2.0 unx     5171 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/coders/slow_stream.py
+-rw-r--r--  2.0 unx     4118 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/coders/coders_test.py
+-rw-r--r--  2.0 unx     6311 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/coders/standard_coders_test.py
+-rw-r--r--  2.0 unx     1770 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/coders/observable_test.py
+-rw-r--r--  2.0 unx    32563 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/coders/coder_impl.py
+-rw-r--r--  2.0 unx     2443 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/coders/stream.pxd
+-rw-r--r--  2.0 unx     7861 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/coders/stream.pyx
+-rw-r--r--  2.0 unx     1317 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/coders/fast_coders_test.py
+-rw-r--r--  2.0 unx     1466 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/coders/observable.py
+-rw-r--r--  2.0 unx   637003 b- defN 18-Dec-05 23:24 apache-beam-2.9.0/apache_beam/coders/stream.c
+-rw-r--r--  2.0 unx    11595 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/coders/proto2_coder_test_messages_pb2.py
+-rw-r--r--  2.0 unx     1678 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/coders/slow_coders_test.py
+-rw-r--r--  2.0 unx    16932 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/coders/coders_test_common.py
+-rw-r--r--  2.0 unx     4358 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/coders/coder_impl.pxd
+-rw-r--r--  2.0 unx     1392 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/runners/__init__.py
+-rw-r--r--  2.0 unx    14033 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/runners/runner.py
+-rw-r--r--  2.0 unx    34731 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/runners/common.py
+-rw-r--r--  2.0 unx     5899 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/runners/sdf_common.py
+-rw-r--r--  2.0 unx     1856 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/runners/pipeline_context_test.py
+-rw-r--r--  2.0 unx     1784 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/runners/common_test.py
+-rw-r--r--  2.0 unx     3126 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/runners/runner_test.py
+-rw-r--r--  2.0 unx  1922797 b- defN 18-Dec-05 23:24 apache-beam-2.9.0/apache_beam/runners/common.c
+-rw-r--r--  2.0 unx     3773 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/runners/common.pxd
+-rw-r--r--  2.0 unx     5124 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/runners/pipeline_context.py
+-rw-r--r--  2.0 unx     1360 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/runners/test/__init__.py
+-rw-r--r--  2.0 unx      784 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/runners/internal/__init__.py
+-rw-r--r--  2.0 unx     1186 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/runners/internal/names.py
+-rw-r--r--  2.0 unx      892 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/runners/worker/__init__.py
+-rw-r--r--  2.0 unx    33047 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/runners/worker/operations.py
+-rw-r--r--  2.0 unx   316976 b- defN 18-Dec-05 23:24 apache-beam-2.9.0/apache_beam/runners/worker/logger.c
+-rw-r--r--  2.0 unx     3266 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/runners/worker/operations.pxd
+-rw-r--r--  2.0 unx    18101 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/runners/worker/sdk_worker.py
+-rw-r--r--  2.0 unx     6745 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/runners/worker/logger.py
+-rw-r--r--  2.0 unx     5860 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/runners/worker/sideinputs_test.py
+-rw-r--r--  2.0 unx     4459 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/runners/worker/statesampler_test.py
+-rw-r--r--  2.0 unx     2949 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/runners/worker/statesampler_slow.py
+-rw-r--r--  2.0 unx     7687 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/runners/worker/sdk_worker_main.py
+-rw-r--r--  2.0 unx   894235 b- defN 18-Dec-05 23:24 apache-beam-2.9.0/apache_beam/runners/worker/opcounters.c
+-rw-r--r--  2.0 unx    36356 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/runners/worker/bundle_processor.py
+-rw-r--r--  2.0 unx     2296 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/runners/worker/opcounters.pxd
+-rw-r--r--  2.0 unx     4902 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/runners/worker/statesampler.py
+-rw-r--r--  2.0 unx     9349 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/runners/worker/statesampler_fast.pyx
+-rw-r--r--  2.0 unx     7941 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/runners/worker/logger_test.py
+-rw-r--r--  2.0 unx     7410 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/runners/worker/sideinputs.py
+-rw-r--r--  2.0 unx     2175 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/runners/worker/statesampler_fast.pxd
+-rw-r--r--  2.0 unx  2161685 b- defN 18-Dec-05 23:24 apache-beam-2.9.0/apache_beam/runners/worker/operations.c
+-rw-r--r--  2.0 unx     2592 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/runners/worker/worker_id_interceptor_test.py
+-rw-r--r--  2.0 unx    12021 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/runners/worker/data_plane.py
+-rw-r--r--  2.0 unx     8160 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/runners/worker/opcounters_test.py
+-rw-r--r--  2.0 unx     4633 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/runners/worker/data_plane_test.py
+-rw-r--r--  2.0 unx    11419 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/runners/worker/opcounters.py
+-rw-r--r--  2.0 unx     5350 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/runners/worker/sdk_worker_main_test.py
+-rw-r--r--  2.0 unx     4358 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/runners/worker/sdk_worker_test.py
+-rw-r--r--  2.0 unx   495198 b- defN 18-Dec-05 23:24 apache-beam-2.9.0/apache_beam/runners/worker/statesampler_fast.c
+-rw-r--r--  2.0 unx     2162 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/runners/worker/worker_id_interceptor.py
+-rw-r--r--  2.0 unx    17246 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/runners/worker/operation_specs.py
+-rw-r--r--  2.0 unx     3856 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/runners/worker/log_handler_test.py
+-rw-r--r--  2.0 unx     4931 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/runners/worker/log_handler.py
+-rw-r--r--  2.0 unx    14767 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/runners/direct/sdf_direct_runner.py
+-rw-r--r--  2.0 unx     3121 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/runners/direct/util.py
+-rw-r--r--  2.0 unx     1088 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/runners/direct/__init__.py
+-rw-r--r--  2.0 unx     4895 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/runners/direct/direct_metrics.py
+-rw-r--r--  2.0 unx     3879 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/runners/direct/helper_transforms.py
+-rw-r--r--  2.0 unx    15203 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/runners/direct/evaluation_context.py
+-rw-r--r--  2.0 unx     8377 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/runners/direct/sdf_direct_runner_test.py
+-rw-r--r--  2.0 unx     1572 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/runners/direct/clock.py
+-rw-r--r--  2.0 unx     2388 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/runners/direct/consumer_tracking_pipeline_visitor.py
+-rw-r--r--  2.0 unx    11225 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/runners/direct/watermark_manager.py
+-rw-r--r--  2.0 unx     4539 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/runners/direct/direct_runner_test.py
+-rw-r--r--  2.0 unx     2197 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/runners/direct/test_direct_runner.py
+-rw-r--r--  2.0 unx     6862 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/runners/direct/bundle_factory.py
+-rw-r--r--  2.0 unx     4659 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/runners/direct/direct_userstate.py
+-rw-r--r--  2.0 unx     4457 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/runners/direct/consumer_tracking_pipeline_visitor_test.py
+-rw-r--r--  2.0 unx    18219 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/runners/direct/direct_runner.py
+-rw-r--r--  2.0 unx    25292 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/runners/direct/executor.py
+-rw-r--r--  2.0 unx     9689 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/runners/direct/direct_metrics_test.py
+-rw-r--r--  2.0 unx    38523 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/runners/direct/transform_evaluator.py
+-rw-r--r--  2.0 unx      823 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/runners/job/__init__.py
+-rw-r--r--  2.0 unx     1853 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/runners/job/manager.py
+-rw-r--r--  2.0 unx     1174 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/runners/job/utils.py
+-rw-r--r--  2.0 unx      899 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/runners/portability/__init__.py
+-rw-r--r--  2.0 unx     3863 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/runners/portability/job_server.py
+-rw-r--r--  2.0 unx    10661 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/runners/portability/local_job_service.py
+-rw-r--r--  2.0 unx     1646 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/runners/portability/local_job_service_main.py
+-rw-r--r--  2.0 unx    22428 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/runners/portability/stager_test.py
+-rw-r--r--  2.0 unx    25989 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/runners/portability/fn_api_runner_test.py
+-rw-r--r--  2.0 unx     6385 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/runners/portability/portable_stager_test.py
+-rw-r--r--  2.0 unx     8638 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/runners/portability/portable_runner_test.py
+-rw-r--r--  2.0 unx     4118 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/runners/portability/portable_stager.py
+-rw-r--r--  2.0 unx    70063 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/runners/portability/fn_api_runner.py
+-rw-r--r--  2.0 unx    25648 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/runners/portability/stager.py
+-rw-r--r--  2.0 unx    10198 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/runners/portability/portable_runner.py
+-rw-r--r--  2.0 unx     4705 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/runners/portability/flink_runner_test.py
+-rw-r--r--  2.0 unx     1175 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/runners/dataflow/__init__.py
+-rw-r--r--  2.0 unx    10106 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/runners/dataflow/dataflow_metrics_test.py
+-rw-r--r--  2.0 unx     3979 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/runners/dataflow/template_runner_test.py
+-rw-r--r--  2.0 unx    53855 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/runners/dataflow/dataflow_runner.py
+-rw-r--r--  2.0 unx    16329 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/runners/dataflow/dataflow_runner_test.py
+-rw-r--r--  2.0 unx     8693 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/runners/dataflow/dataflow_metrics.py
+-rw-r--r--  2.0 unx     3958 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/runners/dataflow/test_dataflow_runner.py
+-rw-r--r--  2.0 unx     2050 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/runners/dataflow/ptransform_overrides.py
+-rw-r--r--  2.0 unx      823 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/runners/dataflow/internal/__init__.py
+-rw-r--r--  2.0 unx    15365 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/runners/dataflow/internal/apiclient_test.py
+-rw-r--r--  2.0 unx     4445 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/runners/dataflow/internal/names.py
+-rw-r--r--  2.0 unx    41088 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/runners/dataflow/internal/apiclient.py
+-rw-r--r--  2.0 unx      823 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/runners/dataflow/internal/clients/__init__.py
+-rw-r--r--  2.0 unx     1429 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/runners/dataflow/internal/clients/dataflow/__init__.py
+-rw-r--r--  2.0 unx     2820 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/runners/dataflow/internal/clients/dataflow/message_matchers_test.py
+-rw-r--r--  2.0 unx    37884 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/runners/dataflow/internal/clients/dataflow/dataflow_v1b3_client.py
+-rw-r--r--  2.0 unx   209905 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/runners/dataflow/internal/clients/dataflow/dataflow_v1b3_messages.py
+-rw-r--r--  2.0 unx     4365 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/runners/dataflow/internal/clients/dataflow/message_matchers.py
+-rw-r--r--  2.0 unx      823 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/runners/dataflow/native_io/__init__.py
+-rw-r--r--  2.0 unx     6718 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/runners/dataflow/native_io/iobase_test.py
+-rw-r--r--  2.0 unx    10660 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/runners/dataflow/native_io/iobase.py
+-rw-r--r--  2.0 unx     2611 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/runners/dataflow/native_io/streaming_create.py
+-rw-r--r--  2.0 unx      784 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/runners/interactive/__init__.py
+-rw-r--r--  2.0 unx    12113 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/runners/interactive/pipeline_analyzer_test.py
+-rw-r--r--  2.0 unx     7128 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/runners/interactive/cache_manager.py
+-rw-r--r--  2.0 unx     8330 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/runners/interactive/interactive_runner.py
+-rw-r--r--  2.0 unx     3934 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/runners/interactive/interactive_runner_test.py
+-rw-r--r--  2.0 unx    17534 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/runners/interactive/pipeline_analyzer.py
+-rw-r--r--  2.0 unx     6674 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/runners/interactive/cache_manager_test.py
+-rw-r--r--  2.0 unx      784 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/runners/interactive/display/__init__.py
+-rw-r--r--  2.0 unx     8459 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/runners/interactive/display/pipeline_graph.py
+-rw-r--r--  2.0 unx     3477 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/runners/interactive/display/pipeline_graph_renderer.py
+-rw-r--r--  2.0 unx     5606 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/runners/interactive/display/interactive_pipeline_graph.py
+-rw-r--r--  2.0 unx     6764 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/runners/interactive/display/display_manager.py
+-rw-r--r--  2.0 unx     7100 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/typehints/trivial_inference_test.py
+-rw-r--r--  2.0 unx     1029 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/typehints/__init__.py
+-rw-r--r--  2.0 unx    10951 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/typehints/typed_pipeline_test.py
+-rw-r--r--  2.0 unx     4087 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/typehints/native_type_compatibility_test.py
+-rw-r--r--  2.0 unx    39009 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/typehints/typehints.py
+-rw-r--r--  2.0 unx     9490 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/typehints/opcodes.py
+-rw-r--r--  2.0 unx    13918 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/typehints/trivial_inference.py
+-rw-r--r--  2.0 unx     5347 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/typehints/native_type_compatibility.py
+-rw-r--r--  2.0 unx    20576 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/typehints/decorators.py
+-rw-r--r--  2.0 unx     9808 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/typehints/typecheck.py
+-rw-r--r--  2.0 unx    39390 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/typehints/typehints_test.py
+-rw-r--r--  2.0 unx     1630 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/io/__init__.py
+-rw-r--r--  2.0 unx     5935 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/io/restriction_trackers_test.py
+-rw-r--r--  2.0 unx     4944 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/io/restriction_trackers.py
+-rw-r--r--  2.0 unx    16947 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/io/avroio_test.py
+-rw-r--r--  2.0 unx    26295 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/io/source_test_utils.py
+-rw-r--r--  2.0 unx    27839 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/io/filebasedsource_test.py
+-rw-r--r--  2.0 unx     8510 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/io/filesystems_test.py
+-rw-r--r--  2.0 unx    10504 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/io/localfilesystem.py
+-rw-r--r--  2.0 unx    37807 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/io/textio_test.py
+-rw-r--r--  2.0 unx    24394 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/io/textio.py
+-rw-r--r--  2.0 unx    19669 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/io/vcfio.py
+-rw-r--r--  2.0 unx     6351 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/io/filesystemio_test.py
+-rw-r--r--  2.0 unx    27132 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/io/vcfio_test.py
+-rw-r--r--  2.0 unx     6430 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/io/source_test_utils_test.py
+-rw-r--r--  2.0 unx     4179 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/io/sources_test.py
+-rw-r--r--  2.0 unx    16092 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/io/localfilesystem_test.py
+-rw-r--r--  2.0 unx     2388 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/io/utils.py
+-rw-r--r--  2.0 unx    15384 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/io/filebasedsink.py
+-rw-r--r--  2.0 unx    19420 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/io/hadoopfilesystem_test.py
+-rw-r--r--  2.0 unx    12508 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/io/tfrecordio.py
+-rw-r--r--  2.0 unx    14059 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/io/range_trackers.py
+-rw-r--r--  2.0 unx    21582 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/io/avroio.py
+-rw-r--r--  2.0 unx    14473 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/io/filebasedsink_test.py
+-rw-r--r--  2.0 unx    16066 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/io/filesystem_test.py
+-rw-r--r--  2.0 unx     9913 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/io/concat_source_test.py
+-rw-r--r--  2.0 unx    12068 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/io/hadoopfilesystem.py
+-rw-r--r--  2.0 unx    48759 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/io/iobase.py
+-rw-r--r--  2.0 unx    17891 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/io/range_trackers_test.py
+-rw-r--r--  2.0 unx     7178 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/io/filesystemio.py
+-rw-r--r--  2.0 unx    28047 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/io/filesystem.py
+-rw-r--r--  2.0 unx    17352 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/io/filebasedsource.py
+-rw-r--r--  2.0 unx    18265 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/io/tfrecordio_test.py
+-rw-r--r--  2.0 unx    10482 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/io/concat_source.py
+-rw-r--r--  2.0 unx    10676 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/io/filesystems.py
+-rw-r--r--  2.0 unx      823 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/io/gcp/__init__.py
+-rw-r--r--  2.0 unx    22037 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/io/gcp/gcsio.py
+-rw-r--r--  2.0 unx     2656 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/io/gcp/bigquery_io_read_pipeline.py
+-rw-r--r--  2.0 unx     2712 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/io/gcp/big_query_query_to_table_pipeline.py
+-rw-r--r--  2.0 unx    24875 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/io/gcp/gcsio_test.py
+-rw-r--r--  2.0 unx    10907 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/io/gcp/gcsfilesystem.py
+-rw-r--r--  2.0 unx     7176 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/io/gcp/big_query_query_to_table_it_test.py
+-rw-r--r--  2.0 unx    65383 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/io/gcp/bigquery.py
+-rw-r--r--  2.0 unx    22738 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/io/gcp/pubsub_test.py
+-rw-r--r--  2.0 unx    46269 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/io/gcp/bigquery_test.py
+-rw-r--r--  2.0 unx     2405 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/io/gcp/datastore_write_it_test.py
+-rw-r--r--  2.0 unx     7531 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/io/gcp/pubsub_integration_test.py
+-rw-r--r--  2.0 unx     3588 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/io/gcp/pubsub_it_pipeline.py
+-rw-r--r--  2.0 unx    15291 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/io/gcp/pubsub.py
+-rw-r--r--  2.0 unx    12160 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/io/gcp/gcsfilesystem_test.py
+-rw-r--r--  2.0 unx     7522 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/io/gcp/datastore_write_it_pipeline.py
+-rw-r--r--  2.0 unx     2252 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/io/gcp/bigquery_io_read_it_test.py
+-rw-r--r--  2.0 unx      823 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/io/gcp/internal/__init__.py
+-rw-r--r--  2.0 unx      823 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/io/gcp/internal/clients/__init__.py
+-rw-r--r--  2.0 unx     1400 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/io/gcp/internal/clients/storage/__init__.py
+-rw-r--r--  2.0 unx    56041 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/io/gcp/internal/clients/storage/storage_v1_client.py
+-rw-r--r--  2.0 unx   115633 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/io/gcp/internal/clients/storage/storage_v1_messages.py
+-rw-r--r--  2.0 unx     1405 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/io/gcp/internal/clients/bigquery/__init__.py
+-rw-r--r--  2.0 unx    28371 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/io/gcp/internal/clients/bigquery/bigquery_v2_client.py
+-rw-r--r--  2.0 unx    81889 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/io/gcp/internal/clients/bigquery/bigquery_v2_messages.py
+-rw-r--r--  2.0 unx      824 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/io/gcp/tests/__init__.py
+-rw-r--r--  2.0 unx     2876 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/io/gcp/tests/utils_test.py
+-rw-r--r--  2.0 unx     3042 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/io/gcp/tests/utils.py
+-rw-r--r--  2.0 unx     5517 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/io/gcp/tests/pubsub_matcher.py
+-rw-r--r--  2.0 unx     6388 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/io/gcp/tests/pubsub_matcher_test.py
+-rw-r--r--  2.0 unx     2836 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/io/gcp/tests/bigquery_matcher_test.py
+-rw-r--r--  2.0 unx     3731 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/io/gcp/tests/bigquery_matcher.py
+-rw-r--r--  2.0 unx      823 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/io/gcp/datastore/__init__.py
+-rw-r--r--  2.0 unx     3789 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/io/gcp/datastore/v1/util.py
+-rw-r--r--  2.0 unx      823 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/io/gcp/datastore/v1/__init__.py
+-rw-r--r--  2.0 unx     2394 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/io/gcp/datastore/v1/util_test.py
+-rw-r--r--  2.0 unx     3916 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/io/gcp/datastore/v1/adaptive_throttler_test.py
+-rw-r--r--  2.0 unx     9650 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/io/gcp/datastore/v1/query_splitter.py
+-rw-r--r--  2.0 unx    10589 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/io/gcp/datastore/v1/helper.py
+-rw-r--r--  2.0 unx    12817 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/io/gcp/datastore/v1/datastoreio_test.py
+-rw-r--r--  2.0 unx     3789 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/io/gcp/datastore/v1/adaptive_throttler.py
+-rw-r--r--  2.0 unx     7915 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/io/gcp/datastore/v1/query_splitter_test.py
+-rw-r--r--  2.0 unx    10175 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/io/gcp/datastore/v1/helper_test.py
+-rw-r--r--  2.0 unx     3614 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/io/gcp/datastore/v1/fake_datastore.py
+-rw-r--r--  2.0 unx    20150 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/io/gcp/datastore/v1/datastoreio.py
+-rw-r--r--  2.0 unx      824 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/io/flink/__init__.py
+-rw-r--r--  2.0 unx     2708 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/io/flink/flink_streaming_impulse_source.py
+-rw-r--r--  2.0 unx      924 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/metrics/__init__.py
+-rw-r--r--  2.0 unx     3900 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/metrics/execution_test.py
+-rw-r--r--  2.0 unx    16147 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/metrics/cells.py
+-rw-r--r--  2.0 unx     9665 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/metrics/execution.py
+-rw-r--r--  2.0 unx     5004 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/metrics/cells_test.py
+-rw-r--r--  2.0 unx      956 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/metrics/execution.pxd
+-rw-r--r--  2.0 unx   607489 b- defN 18-Dec-05 23:24 apache-beam-2.9.0/apache_beam/metrics/execution.c
+-rw-r--r--  2.0 unx     3735 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/metrics/metricbase.py
+-rw-r--r--  2.0 unx     8076 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/metrics/metric.py
+-rw-r--r--  2.0 unx     8902 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/metrics/monitoring_infos.py
+-rw-r--r--  2.0 unx     5639 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/metrics/metric_test.py
+-rw-r--r--  2.0 unx     6652 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/examples/wordcount_debugging.py
+-rw-r--r--  2.0 unx      824 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/examples/__init__.py
+-rw-r--r--  2.0 unx     5019 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/examples/wordcount.py
+-rw-r--r--  2.0 unx     2172 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/examples/wordcount_test.py
+-rw-r--r--  2.0 unx     6266 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/examples/fastavro_it_test.py
+-rw-r--r--  2.0 unx     6628 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/examples/streaming_wordcount_debugging.py
+-rw-r--r--  2.0 unx     4954 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/examples/wordcount_minimal.py
+-rw-r--r--  2.0 unx     2146 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/examples/wordcount_minimal_test.py
+-rw-r--r--  2.0 unx     3012 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/examples/wordcount_it_test.py
+-rw-r--r--  2.0 unx     3354 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/examples/windowed_wordcount.py
+-rw-r--r--  2.0 unx     2101 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/examples/wordcount_debugging_test.py
+-rw-r--r--  2.0 unx     4462 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/examples/streaming_wordcount_it_test.py
+-rw-r--r--  2.0 unx     5910 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/examples/avro_bitcoin.py
+-rw-r--r--  2.0 unx     3845 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/examples/streaming_wordcount.py
+-rw-r--r--  2.0 unx      824 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/examples/cookbook/__init__.py
+-rw-r--r--  2.0 unx    11008 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/examples/cookbook/datastore_wordcount.py
+-rw-r--r--  2.0 unx     4743 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/examples/cookbook/group_with_coder.py
+-rw-r--r--  2.0 unx     3379 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/examples/cookbook/coders.py
+-rw-r--r--  2.0 unx     6140 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/examples/cookbook/mergecontacts.py
+-rw-r--r--  2.0 unx     6949 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/examples/cookbook/multiple_output_pardo.py
+-rw-r--r--  2.0 unx     1804 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/examples/cookbook/coders_test.py
+-rw-r--r--  2.0 unx     1797 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/examples/cookbook/bigquery_tornadoes_test.py
+-rw-r--r--  2.0 unx     2581 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/examples/cookbook/datastore_wordcount_it_test.py
+-rw-r--r--  2.0 unx     3583 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/examples/cookbook/bigquery_tornadoes.py
+-rw-r--r--  2.0 unx     1860 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/examples/cookbook/custom_ptransform_test.py
+-rw-r--r--  2.0 unx     3905 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/examples/cookbook/custom_ptransform.py
+-rw-r--r--  2.0 unx     3520 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/examples/cookbook/group_with_coder_test.py
+-rw-r--r--  2.0 unx     2307 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/examples/cookbook/bigquery_side_input_test.py
+-rw-r--r--  2.0 unx     5374 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/examples/cookbook/mergecontacts_test.py
+-rw-r--r--  2.0 unx     2936 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/examples/cookbook/bigquery_tornadoes_it_test.py
+-rw-r--r--  2.0 unx     2645 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/examples/cookbook/multiple_output_pardo_test.py
+-rw-r--r--  2.0 unx     4510 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/examples/cookbook/bigquery_schema.py
+-rw-r--r--  2.0 unx     4017 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/examples/cookbook/filters.py
+-rw-r--r--  2.0 unx     4484 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/examples/cookbook/bigquery_side_input.py
+-rw-r--r--  2.0 unx     2719 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/examples/cookbook/filters_test.py
+-rw-r--r--  2.0 unx     2680 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/examples/cookbook/combiners_test.py
+-rw-r--r--  2.0 unx      824 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/examples/complete/__init__.py
+-rw-r--r--  2.0 unx     2947 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/examples/complete/distribopt_test.py
+-rw-r--r--  2.0 unx     8683 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/examples/complete/tfidf.py
+-rw-r--r--  2.0 unx     2520 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/examples/complete/autocomplete_test.py
+-rw-r--r--  2.0 unx     1864 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/examples/complete/estimate_pi_test.py
+-rw-r--r--  2.0 unx     6055 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/examples/complete/top_wikipedia_sessions.py
+-rw-r--r--  2.0 unx     3166 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/examples/complete/autocomplete.py
+-rw-r--r--  2.0 unx     3460 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/examples/complete/tfidf_test.py
+-rw-r--r--  2.0 unx    13825 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/examples/complete/distribopt.py
+-rw-r--r--  2.0 unx     2437 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/examples/complete/top_wikipedia_sessions_test.py
+-rw-r--r--  2.0 unx     4502 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/examples/complete/estimate_pi.py
+-rw-r--r--  2.0 unx      824 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/examples/complete/juliaset/__init__.py
+-rw-r--r--  2.0 unx     4772 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/examples/complete/juliaset/setup.py
+-rw-r--r--  2.0 unx     2305 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/examples/complete/juliaset/juliaset_main.py
+-rw-r--r--  2.0 unx      824 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/examples/complete/juliaset/juliaset/__init__.py
+-rw-r--r--  2.0 unx     2977 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/examples/complete/juliaset/juliaset/juliaset_test.py
+-rw-r--r--  2.0 unx     4578 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/examples/complete/juliaset/juliaset/juliaset.py
+-rw-r--r--  2.0 unx      824 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/examples/complete/game/__init__.py
+-rw-r--r--  2.0 unx     5732 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/examples/complete/game/game_stats_it_test.py
+-rw-r--r--  2.0 unx    15062 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/examples/complete/game/game_stats.py
+-rw-r--r--  2.0 unx     3628 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/examples/complete/game/hourly_team_score_it_test.py
+-rw-r--r--  2.0 unx     5677 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/examples/complete/game/user_score.py
+-rw-r--r--  2.0 unx     2145 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/examples/complete/game/hourly_team_score_test.py
+-rw-r--r--  2.0 unx     6131 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/examples/complete/game/leader_board_it_test.py
+-rw-r--r--  2.0 unx     1994 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/examples/complete/game/user_score_test.py
+-rw-r--r--  2.0 unx    13517 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/examples/complete/game/leader_board.py
+-rw-r--r--  2.0 unx    11757 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/examples/complete/game/hourly_team_score.py
+-rw-r--r--  2.0 unx     3320 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/examples/complete/game/game_stats_test.py
+-rw-r--r--  2.0 unx     2569 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/examples/complete/game/leader_board_test.py
+-rw-r--r--  2.0 unx     3239 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/examples/complete/game/user_score_it_test.py
+-rw-r--r--  2.0 unx     3252 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/examples/flink/flink_streaming_impulse.py
+-rw-r--r--  2.0 unx      824 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/examples/flink/__init__.py
+-rw-r--r--  2.0 unx      824 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/examples/snippets/__init__.py
+-rw-r--r--  2.0 unx    47110 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/examples/snippets/snippets.py
+-rw-r--r--  2.0 unx    47921 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/examples/snippets/snippets_test.py
+-rw-r--r--  2.0 unx      936 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/utils/__init__.py
+-rw-r--r--  2.0 unx     7706 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/utils/windowed_value.py
+-rw-r--r--  2.0 unx     1252 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/utils/windowed_value.pxd
+-rw-r--r--  2.0 unx     5776 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/utils/annotations.py
+-rw-r--r--  2.0 unx    12285 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/utils/annotations_test.py
+-rw-r--r--  2.0 unx   497711 b- defN 18-Dec-05 23:24 apache-beam-2.9.0/apache_beam/utils/windowed_value.c
+-rw-r--r--  2.0 unx   504762 b- defN 18-Dec-05 23:24 apache-beam-2.9.0/apache_beam/utils/counters.c
+-rw-r--r--  2.0 unx     7930 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/utils/timestamp_test.py
+-rw-r--r--  2.0 unx     1660 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/utils/plugin.py
+-rw-r--r--  2.0 unx     7723 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/utils/retry_test.py
+-rw-r--r--  2.0 unx     2752 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/utils/windowed_value_test.py
+-rw-r--r--  2.0 unx     8406 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/utils/counters.py
+-rw-r--r--  2.0 unx     4073 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/utils/urns.py
+-rw-r--r--  2.0 unx     8489 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/utils/retry.py
+-rw-r--r--  2.0 unx     3466 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/utils/processes_test.py
+-rw-r--r--  2.0 unx     3836 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/utils/counters_test.py
+-rw-r--r--  2.0 unx     1130 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/utils/counters.pxd
+-rw-r--r--  2.0 unx     8921 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/utils/timestamp.py
+-rw-r--r--  2.0 unx     5564 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/utils/profiler.py
+-rw-r--r--  2.0 unx     2003 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/utils/proto_utils.py
+-rw-r--r--  2.0 unx     1835 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/utils/processes.py
+-rw-r--r--  2.0 unx      892 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/portability/__init__.py
+-rw-r--r--  2.0 unx     2986 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/portability/common_urns.py
+-rw-r--r--  2.0 unx     1529 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/portability/python_urns.py
+-rw-r--r--  2.0 unx      922 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/portability/api/__init__.py
+-rw-r--r--  2.0 unx    13593 b- defN 18-Dec-05 21:37 apache-beam-2.9.0/apache_beam/portability/api/beam_provision_api_pb2.py
+-rw-r--r--  2.0 unx       83 b- defN 18-Dec-05 21:37 apache-beam-2.9.0/apache_beam/portability/api/endpoints_pb2_grpc.py
+-rw-r--r--  2.0 unx   211319 b- defN 18-Dec-05 21:37 apache-beam-2.9.0/apache_beam/portability/api/beam_runner_api_pb2.py
+-rw-r--r--  2.0 unx    23813 b- defN 18-Dec-05 21:37 apache-beam-2.9.0/apache_beam/portability/api/beam_artifact_api_pb2.py
+-rw-r--r--  2.0 unx    26707 b- defN 18-Dec-05 21:37 apache-beam-2.9.0/apache_beam/portability/api/beam_job_api_pb2.py
+-rw-r--r--  2.0 unx     4938 b- defN 18-Dec-05 21:37 apache-beam-2.9.0/apache_beam/portability/api/endpoints_pb2.py
+-rw-r--r--  2.0 unx     5913 b- defN 18-Dec-05 21:37 apache-beam-2.9.0/apache_beam/portability/api/beam_fn_api_pb2_grpc.py
+-rw-r--r--  2.0 unx     2070 b- defN 18-Dec-05 21:37 apache-beam-2.9.0/apache_beam/portability/api/beam_provision_api_pb2_grpc.py
+-rw-r--r--  2.0 unx       83 b- defN 18-Dec-05 21:37 apache-beam-2.9.0/apache_beam/portability/api/standard_window_fns_pb2_grpc.py
+-rw-r--r--  2.0 unx     5779 b- defN 18-Dec-05 21:37 apache-beam-2.9.0/apache_beam/portability/api/beam_job_api_pb2_grpc.py
+-rw-r--r--  2.0 unx       83 b- defN 18-Dec-05 21:37 apache-beam-2.9.0/apache_beam/portability/api/beam_runner_api_pb2_grpc.py
+-rw-r--r--  2.0 unx     5279 b- defN 18-Dec-05 21:37 apache-beam-2.9.0/apache_beam/portability/api/beam_artifact_api_pb2_grpc.py
+-rw-r--r--  2.0 unx   184105 b- defN 18-Dec-05 21:37 apache-beam-2.9.0/apache_beam/portability/api/beam_fn_api_pb2.py
+-rw-r--r--  2.0 unx    12615 b- defN 18-Dec-05 21:37 apache-beam-2.9.0/apache_beam/portability/api/standard_window_fns_pb2.py
+-rw-r--r--  2.0 unx     7744 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/testing/util.py
+-rw-r--r--  2.0 unx     6125 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/testing/pipeline_verifiers_test.py
+-rw-r--r--  2.0 unx      823 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/testing/__init__.py
+-rw-r--r--  2.0 unx     5830 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/testing/test_utils.py
+-rw-r--r--  2.0 unx     5408 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/testing/synthetic_pipeline_test.py
+-rw-r--r--  2.0 unx     3949 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/testing/util_test.py
+-rw-r--r--  2.0 unx     5340 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/testing/pipeline_verifiers.py
+-rw-r--r--  2.0 unx    14604 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/testing/test_stream_test.py
+-rw-r--r--  2.0 unx     3285 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/testing/test_utils_test.py
+-rw-r--r--  2.0 unx     7250 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/testing/test_pipeline.py
+-rw-r--r--  2.0 unx     4610 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/testing/test_pipeline_test.py
+-rw-r--r--  2.0 unx    18558 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/testing/synthetic_pipeline.py
+-rw-r--r--  2.0 unx     6807 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/testing/test_stream.py
+-rw-r--r--  2.0 unx      823 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/testing/benchmarks/__init__.py
+-rw-r--r--  2.0 unx      823 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/testing/benchmarks/nexmark/__init__.py
+-rw-r--r--  2.0 unx     9445 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/testing/benchmarks/nexmark/nexmark_launcher.py
+-rw-r--r--  2.0 unx     3246 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/testing/benchmarks/nexmark/nexmark_util.py
+-rw-r--r--  2.0 unx      784 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/testing/benchmarks/nexmark/queries/__init__.py
+-rw-r--r--  2.0 unx     1421 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/testing/benchmarks/nexmark/queries/query0.py
+-rw-r--r--  2.0 unx     1923 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/testing/benchmarks/nexmark/queries/query1.py
+-rw-r--r--  2.0 unx     1759 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/testing/benchmarks/nexmark/queries/query2.py
+-rw-r--r--  2.0 unx      823 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/testing/benchmarks/nexmark/models/__init__.py
+-rw-r--r--  2.0 unx     2735 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/testing/benchmarks/nexmark/models/nexmark_model.py
+-rw-r--r--  2.0 unx      784 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/testing/load_tests/__init__.py
+-rw-r--r--  2.0 unx     4979 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/testing/load_tests/pardo_test.py
+-rw-r--r--  2.0 unx     4964 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/testing/load_tests/co_group_by_key_test.py
+-rw-r--r--  2.0 unx     3571 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/testing/load_tests/group_by_key_test.py
+-rw-r--r--  2.0 unx     1374 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/testing/load_tests/load_test_metrics_utils.py
+-rw-r--r--  2.0 unx     3674 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/testing/load_tests/combine_test.py
+-rw-r--r--  2.0 unx     6753 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/testing/data/trigger_transcripts.yaml
+-rw-r--r--  2.0 unx     6537 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/testing/data/standard_coders.yaml
+-rw-r--r--  2.0 unx     8347 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/transforms/cy_combiners.py
+-rw-r--r--  2.0 unx    23685 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/transforms/util.py
+-rw-r--r--  2.0 unx     1389 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/transforms/__init__.py
+-rw-r--r--  2.0 unx    43427 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/transforms/trigger.py
+-rw-r--r--  2.0 unx     4882 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/transforms/cy_dataflow_distribution_counter.pyx
+-rw-r--r--  2.0 unx    18027 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/transforms/window.py
+-rw-r--r--  2.0 unx    76746 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/transforms/core.py
+-rw-r--r--  2.0 unx     9459 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/transforms/display_test.py
+-rw-r--r--  2.0 unx     4371 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/transforms/write_ptransform_test.py
+-rw-r--r--  2.0 unx    16909 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/transforms/util_test.py
+-rw-r--r--  2.0 unx    11833 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/transforms/sideinputs_test.py
+-rw-r--r--  2.0 unx     4109 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/transforms/py_dataflow_distribution_counter.py
+-rw-r--r--  2.0 unx     1663 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/transforms/cy_dataflow_distribution_counter.pxd
+-rw-r--r--  2.0 unx    12074 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/transforms/window_test.py
+-rw-r--r--  2.0 unx    10739 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/transforms/userstate.py
+-rw-r--r--  2.0 unx    21611 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/transforms/trigger_test.py
+-rw-r--r--  2.0 unx     2917 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/transforms/sideinputs.py
+-rw-r--r--  2.0 unx  1168674 b- defN 18-Dec-05 23:24 apache-beam-2.9.0/apache_beam/transforms/cy_combiners.c
+-rw-r--r--  2.0 unx    30477 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/transforms/ptransform.py
+-rw-r--r--  2.0 unx   348250 b- defN 18-Dec-05 23:24 apache-beam-2.9.0/apache_beam/transforms/cy_dataflow_distribution_counter.c
+-rw-r--r--  2.0 unx     4552 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/transforms/timeutil.py
+-rw-r--r--  2.0 unx    23917 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/transforms/combiners.py
+-rw-r--r--  2.0 unx     3073 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/transforms/cy_combiners.pxd
+-rw-r--r--  2.0 unx     2929 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/transforms/dataflow_distribution_counter_test.py
+-rw-r--r--  2.0 unx     5800 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/transforms/create_test.py
+-rw-r--r--  2.0 unx    11574 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/transforms/display.py
+-rw-r--r--  2.0 unx    20402 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/transforms/userstate_test.py
+-rw-r--r--  2.0 unx     3591 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/transforms/create_source.py
+-rw-r--r--  2.0 unx    14524 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/transforms/combiners_test.py
+-rw-r--r--  2.0 unx    81197 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/transforms/ptransform_test.py
+-rw-r--r--  2.0 unx      931 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/tools/__init__.py
+-rw-r--r--  2.0 unx     2590 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/tools/map_fn_microbenchmark.py
+-rw-r--r--  2.0 unx     1298 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/tools/microbenchmarks_test.py
+-rw-r--r--  2.0 unx     2896 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/tools/sideinput_microbenchmark.py
+-rw-r--r--  2.0 unx     4346 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/tools/utils.py
+-rw-r--r--  2.0 unx     2468 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/tools/distribution_counter_microbenchmark.py
+-rw-r--r--  2.0 unx     5714 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/tools/coders_microbenchmark.py
+-rw-r--r--  2.0 unx      823 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/options/__init__.py
+-rw-r--r--  2.0 unx    12601 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/options/pipeline_options_test.py
+-rw-r--r--  2.0 unx     8449 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/options/pipeline_options_validator.py
+-rw-r--r--  2.0 unx    30333 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/options/pipeline_options.py
+-rw-r--r--  2.0 unx    11364 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/options/pipeline_options_validator_test.py
+-rw-r--r--  2.0 unx     4116 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/options/value_provider.py
+-rw-r--r--  2.0 unx     9145 b- defN 18-Dec-05 20:52 apache-beam-2.9.0/apache_beam/options/value_provider_test.py
+-rw-r--r--  2.0 unx      694 b- defN 18-Dec-05 23:24 apache-beam-2.9.0/apache_beam.egg-info/requires.txt
+-rw-r--r--  2.0 unx       12 b- defN 18-Dec-05 23:24 apache-beam-2.9.0/apache_beam.egg-info/top_level.txt
+-rw-r--r--  2.0 unx        1 b- defN 18-Dec-05 21:37 apache-beam-2.9.0/apache_beam.egg-info/not-zip-safe
+-rw-r--r--  2.0 unx        1 b- defN 18-Dec-05 23:24 apache-beam-2.9.0/apache_beam.egg-info/dependency_links.txt
+-rw-r--r--  2.0 unx    19478 b- defN 18-Dec-05 23:24 apache-beam-2.9.0/apache_beam.egg-info/SOURCES.txt
+-rw-r--r--  2.0 unx     1010 b- defN 18-Dec-05 23:24 apache-beam-2.9.0/apache_beam.egg-info/PKG-INFO
+-rw-r--r--  2.0 unx       67 b- defN 18-Dec-05 23:24 apache-beam-2.9.0/apache_beam.egg-info/entry_points.txt
+450 files, 16850741 bytes uncompressed, 2408266 bytes compressed:  85.7%
```

## zipnote {}

```diff
@@ -1,1276 +1,1351 @@
-Filename: apache-beam-2.8.0/MANIFEST.in
+Filename: apache-beam-2.9.0/MANIFEST.in
 Comment: 
 
-Filename: apache-beam-2.8.0/gen_protos.py
+Filename: apache-beam-2.9.0/gen_protos.py
 Comment: 
 
-Filename: apache-beam-2.8.0/setup.cfg
+Filename: apache-beam-2.9.0/setup.py
 Comment: 
 
-Filename: apache-beam-2.8.0/setup.py
+Filename: apache-beam-2.9.0/PKG-INFO
 Comment: 
 
-Filename: apache-beam-2.8.0/PKG-INFO
+Filename: apache-beam-2.9.0/setup.cfg
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/version.py
+Filename: apache-beam-2.9.0/apache_beam/__init__.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/pvalue_test.py
+Filename: apache-beam-2.9.0/apache_beam/pipeline.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/__init__.py
+Filename: apache-beam-2.9.0/apache_beam/version.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/pipeline.py
+Filename: apache-beam-2.9.0/apache_beam/pipeline_test.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/pvalue.py
+Filename: apache-beam-2.9.0/apache_beam/pvalue.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/pipeline_test.py
+Filename: apache-beam-2.9.0/apache_beam/error.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/error.py
+Filename: apache-beam-2.9.0/apache_beam/pvalue_test.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/runners/__init__.py
+Filename: apache-beam-2.9.0/apache_beam/internal/util.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/runners/pipeline_context_test.py
+Filename: apache-beam-2.9.0/apache_beam/internal/__init__.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/runners/pipeline_context.py
+Filename: apache-beam-2.9.0/apache_beam/internal/util_test.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/runners/sdf_common.py
+Filename: apache-beam-2.9.0/apache_beam/internal/module_test.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/runners/runner_test.py
+Filename: apache-beam-2.9.0/apache_beam/internal/pickler_test.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/runners/common.pxd
+Filename: apache-beam-2.9.0/apache_beam/internal/pickler.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/runners/common.py
+Filename: apache-beam-2.9.0/apache_beam/internal/gcp/__init__.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/runners/common_test.py
+Filename: apache-beam-2.9.0/apache_beam/internal/gcp/auth.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/runners/runner.py
+Filename: apache-beam-2.9.0/apache_beam/internal/gcp/json_value_test.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/runners/test/__init__.py
+Filename: apache-beam-2.9.0/apache_beam/internal/gcp/json_value.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/runners/direct/test_direct_runner.py
+Filename: apache-beam-2.9.0/apache_beam/coders/__init__.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/runners/direct/bundle_factory.py
+Filename: apache-beam-2.9.0/apache_beam/coders/typecoders_test.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/runners/direct/direct_runner_test.py
+Filename: apache-beam-2.9.0/apache_beam/coders/stream_test.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/runners/direct/__init__.py
+Filename: apache-beam-2.9.0/apache_beam/coders/typecoders.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/runners/direct/direct_metrics_test.py
+Filename: apache-beam-2.9.0/apache_beam/coders/coders.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/runners/direct/helper_transforms.py
+Filename: apache-beam-2.9.0/apache_beam/coders/coder_impl.c
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/runners/direct/sdf_direct_runner_test.py
+Filename: apache-beam-2.9.0/apache_beam/coders/slow_stream.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/runners/direct/direct_metrics.py
+Filename: apache-beam-2.9.0/apache_beam/coders/coders_test.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/runners/direct/consumer_tracking_pipeline_visitor.py
+Filename: apache-beam-2.9.0/apache_beam/coders/standard_coders_test.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/runners/direct/watermark_manager.py
+Filename: apache-beam-2.9.0/apache_beam/coders/observable_test.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/runners/direct/direct_runner.py
+Filename: apache-beam-2.9.0/apache_beam/coders/coder_impl.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/runners/direct/executor.py
+Filename: apache-beam-2.9.0/apache_beam/coders/stream.pxd
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/runners/direct/clock.py
+Filename: apache-beam-2.9.0/apache_beam/coders/stream.pyx
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/runners/direct/consumer_tracking_pipeline_visitor_test.py
+Filename: apache-beam-2.9.0/apache_beam/coders/fast_coders_test.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/runners/direct/transform_evaluator.py
+Filename: apache-beam-2.9.0/apache_beam/coders/observable.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/runners/direct/evaluation_context.py
+Filename: apache-beam-2.9.0/apache_beam/coders/stream.c
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/runners/direct/direct_userstate.py
+Filename: apache-beam-2.9.0/apache_beam/coders/proto2_coder_test_messages_pb2.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/runners/direct/util.py
+Filename: apache-beam-2.9.0/apache_beam/coders/slow_coders_test.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/runners/direct/sdf_direct_runner.py
+Filename: apache-beam-2.9.0/apache_beam/coders/coders_test_common.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/runners/interactive/pipeline_analyzer.py
+Filename: apache-beam-2.9.0/apache_beam/coders/coder_impl.pxd
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/runners/interactive/__init__.py
+Filename: apache-beam-2.9.0/apache_beam/runners/__init__.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/runners/interactive/pipeline_analyzer_test.py
+Filename: apache-beam-2.9.0/apache_beam/runners/runner.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/runners/interactive/interactive_runner_test.py
+Filename: apache-beam-2.9.0/apache_beam/runners/common.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/runners/interactive/interactive_runner.py
+Filename: apache-beam-2.9.0/apache_beam/runners/sdf_common.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/runners/interactive/cache_manager.py
+Filename: apache-beam-2.9.0/apache_beam/runners/pipeline_context_test.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/runners/interactive/cache_manager_test.py
+Filename: apache-beam-2.9.0/apache_beam/runners/common_test.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/runners/interactive/display/display_manager.py
+Filename: apache-beam-2.9.0/apache_beam/runners/runner_test.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/runners/interactive/display/interactive_pipeline_graph.py
+Filename: apache-beam-2.9.0/apache_beam/runners/common.c
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/runners/interactive/display/pipeline_graph_renderer.py
+Filename: apache-beam-2.9.0/apache_beam/runners/common.pxd
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/runners/interactive/display/__init__.py
+Filename: apache-beam-2.9.0/apache_beam/runners/pipeline_context.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/runners/interactive/display/pipeline_graph.py
+Filename: apache-beam-2.9.0/apache_beam/runners/test/__init__.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/runners/job/manager.py
+Filename: apache-beam-2.9.0/apache_beam/runners/internal/__init__.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/runners/job/utils.py
+Filename: apache-beam-2.9.0/apache_beam/runners/internal/names.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/runners/job/__init__.py
+Filename: apache-beam-2.9.0/apache_beam/runners/worker/__init__.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/runners/dataflow/dataflow_metrics_test.py
+Filename: apache-beam-2.9.0/apache_beam/runners/worker/operations.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/runners/dataflow/dataflow_runner.py
+Filename: apache-beam-2.9.0/apache_beam/runners/worker/logger.c
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/runners/dataflow/dataflow_metrics.py
+Filename: apache-beam-2.9.0/apache_beam/runners/worker/operations.pxd
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/runners/dataflow/__init__.py
+Filename: apache-beam-2.9.0/apache_beam/runners/worker/sdk_worker.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/runners/dataflow/test_dataflow_runner.py
+Filename: apache-beam-2.9.0/apache_beam/runners/worker/logger.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/runners/dataflow/template_runner_test.py
+Filename: apache-beam-2.9.0/apache_beam/runners/worker/sideinputs_test.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/runners/dataflow/dataflow_runner_test.py
+Filename: apache-beam-2.9.0/apache_beam/runners/worker/statesampler_test.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/runners/dataflow/ptransform_overrides.py
+Filename: apache-beam-2.9.0/apache_beam/runners/worker/statesampler_slow.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/runners/dataflow/native_io/__init__.py
+Filename: apache-beam-2.9.0/apache_beam/runners/worker/sdk_worker_main.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/runners/dataflow/native_io/iobase_test.py
+Filename: apache-beam-2.9.0/apache_beam/runners/worker/opcounters.c
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/runners/dataflow/native_io/streaming_create.py
+Filename: apache-beam-2.9.0/apache_beam/runners/worker/bundle_processor.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/runners/dataflow/native_io/iobase.py
+Filename: apache-beam-2.9.0/apache_beam/runners/worker/opcounters.pxd
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/runners/dataflow/internal/names.py
+Filename: apache-beam-2.9.0/apache_beam/runners/worker/statesampler.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/runners/dataflow/internal/apiclient_test.py
+Filename: apache-beam-2.9.0/apache_beam/runners/worker/statesampler_fast.pyx
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/runners/dataflow/internal/__init__.py
+Filename: apache-beam-2.9.0/apache_beam/runners/worker/logger_test.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/runners/dataflow/internal/apiclient.py
+Filename: apache-beam-2.9.0/apache_beam/runners/worker/sideinputs.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/runners/dataflow/internal/clients/__init__.py
+Filename: apache-beam-2.9.0/apache_beam/runners/worker/statesampler_fast.pxd
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/runners/dataflow/internal/clients/dataflow/dataflow_v1b3_client.py
+Filename: apache-beam-2.9.0/apache_beam/runners/worker/operations.c
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/runners/dataflow/internal/clients/dataflow/__init__.py
+Filename: apache-beam-2.9.0/apache_beam/runners/worker/worker_id_interceptor_test.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/runners/dataflow/internal/clients/dataflow/message_matchers.py
+Filename: apache-beam-2.9.0/apache_beam/runners/worker/data_plane.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/runners/dataflow/internal/clients/dataflow/dataflow_v1b3_messages.py
+Filename: apache-beam-2.9.0/apache_beam/runners/worker/opcounters_test.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/runners/dataflow/internal/clients/dataflow/message_matchers_test.py
+Filename: apache-beam-2.9.0/apache_beam/runners/worker/data_plane_test.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/runners/portability/local_job_service.py
+Filename: apache-beam-2.9.0/apache_beam/runners/worker/opcounters.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/runners/portability/stager.py
+Filename: apache-beam-2.9.0/apache_beam/runners/worker/sdk_worker_main_test.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/runners/portability/local_job_service_main.py
+Filename: apache-beam-2.9.0/apache_beam/runners/worker/sdk_worker_test.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/runners/portability/portable_runner_test.py
+Filename: apache-beam-2.9.0/apache_beam/runners/worker/statesampler_fast.c
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/runners/portability/__init__.py
+Filename: apache-beam-2.9.0/apache_beam/runners/worker/worker_id_interceptor.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/runners/portability/stager_test.py
+Filename: apache-beam-2.9.0/apache_beam/runners/worker/operation_specs.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/runners/portability/fn_api_runner.py
+Filename: apache-beam-2.9.0/apache_beam/runners/worker/log_handler_test.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/runners/portability/flink_runner_test.py
+Filename: apache-beam-2.9.0/apache_beam/runners/worker/log_handler.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/runners/portability/portable_stager.py
+Filename: apache-beam-2.9.0/apache_beam/runners/direct/sdf_direct_runner.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/runners/portability/portable_runner.py
+Filename: apache-beam-2.9.0/apache_beam/runners/direct/util.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/runners/portability/job_server.py
+Filename: apache-beam-2.9.0/apache_beam/runners/direct/__init__.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/runners/portability/portable_stager_test.py
+Filename: apache-beam-2.9.0/apache_beam/runners/direct/direct_metrics.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/runners/portability/fn_api_runner_test.py
+Filename: apache-beam-2.9.0/apache_beam/runners/direct/helper_transforms.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/runners/worker/statesampler_fast.pyx
+Filename: apache-beam-2.9.0/apache_beam/runners/direct/evaluation_context.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/runners/worker/sdk_worker_test.py
+Filename: apache-beam-2.9.0/apache_beam/runners/direct/sdf_direct_runner_test.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/runners/worker/sdk_worker_main.py
+Filename: apache-beam-2.9.0/apache_beam/runners/direct/clock.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/runners/worker/statesampler.py
+Filename: apache-beam-2.9.0/apache_beam/runners/direct/consumer_tracking_pipeline_visitor.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/runners/worker/statesampler_slow.py
+Filename: apache-beam-2.9.0/apache_beam/runners/direct/watermark_manager.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/runners/worker/sdk_worker_main_test.py
+Filename: apache-beam-2.9.0/apache_beam/runners/direct/direct_runner_test.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/runners/worker/log_handler.py
+Filename: apache-beam-2.9.0/apache_beam/runners/direct/test_direct_runner.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/runners/worker/__init__.py
+Filename: apache-beam-2.9.0/apache_beam/runners/direct/bundle_factory.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/runners/worker/statesampler_fast.pxd
+Filename: apache-beam-2.9.0/apache_beam/runners/direct/direct_userstate.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/runners/worker/operations.py
+Filename: apache-beam-2.9.0/apache_beam/runners/direct/consumer_tracking_pipeline_visitor_test.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/runners/worker/sdk_worker.py
+Filename: apache-beam-2.9.0/apache_beam/runners/direct/direct_runner.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/runners/worker/sideinputs.py
+Filename: apache-beam-2.9.0/apache_beam/runners/direct/executor.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/runners/worker/opcounters_test.py
+Filename: apache-beam-2.9.0/apache_beam/runners/direct/direct_metrics_test.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/runners/worker/operation_specs.py
+Filename: apache-beam-2.9.0/apache_beam/runners/direct/transform_evaluator.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/runners/worker/opcounters.py
+Filename: apache-beam-2.9.0/apache_beam/runners/job/__init__.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/runners/worker/worker_id_interceptor_test.py
+Filename: apache-beam-2.9.0/apache_beam/runners/job/manager.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/runners/worker/data_plane_test.py
+Filename: apache-beam-2.9.0/apache_beam/runners/job/utils.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/runners/worker/opcounters.pxd
+Filename: apache-beam-2.9.0/apache_beam/runners/portability/__init__.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/runners/worker/data_plane.py
+Filename: apache-beam-2.9.0/apache_beam/runners/portability/job_server.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/runners/worker/worker_id_interceptor.py
+Filename: apache-beam-2.9.0/apache_beam/runners/portability/local_job_service.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/runners/worker/logger_test.py
+Filename: apache-beam-2.9.0/apache_beam/runners/portability/local_job_service_main.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/runners/worker/log_handler_test.py
+Filename: apache-beam-2.9.0/apache_beam/runners/portability/stager_test.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/runners/worker/bundle_processor.py
+Filename: apache-beam-2.9.0/apache_beam/runners/portability/fn_api_runner_test.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/runners/worker/statesampler_test.py
+Filename: apache-beam-2.9.0/apache_beam/runners/portability/portable_stager_test.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/runners/worker/operations.pxd
+Filename: apache-beam-2.9.0/apache_beam/runners/portability/portable_runner_test.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/runners/worker/logger.py
+Filename: apache-beam-2.9.0/apache_beam/runners/portability/portable_stager.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/runners/worker/sideinputs_test.py
+Filename: apache-beam-2.9.0/apache_beam/runners/portability/fn_api_runner.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/examples/wordcount_debugging.py
+Filename: apache-beam-2.9.0/apache_beam/runners/portability/stager.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/examples/wordcount_minimal_test.py
+Filename: apache-beam-2.9.0/apache_beam/runners/portability/portable_runner.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/examples/streaming_wordcount_debugging.py
+Filename: apache-beam-2.9.0/apache_beam/runners/portability/flink_runner_test.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/examples/__init__.py
+Filename: apache-beam-2.9.0/apache_beam/runners/dataflow/__init__.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/examples/wordcount_test.py
+Filename: apache-beam-2.9.0/apache_beam/runners/dataflow/dataflow_metrics_test.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/examples/wordcount_it_test.py
+Filename: apache-beam-2.9.0/apache_beam/runners/dataflow/template_runner_test.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/examples/fastavro_it_test.py
+Filename: apache-beam-2.9.0/apache_beam/runners/dataflow/dataflow_runner.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/examples/windowed_wordcount.py
+Filename: apache-beam-2.9.0/apache_beam/runners/dataflow/dataflow_runner_test.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/examples/wordcount.py
+Filename: apache-beam-2.9.0/apache_beam/runners/dataflow/dataflow_metrics.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/examples/streaming_wordcount_it_test.py
+Filename: apache-beam-2.9.0/apache_beam/runners/dataflow/test_dataflow_runner.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/examples/wordcount_debugging_test.py
+Filename: apache-beam-2.9.0/apache_beam/runners/dataflow/ptransform_overrides.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/examples/wordcount_minimal.py
+Filename: apache-beam-2.9.0/apache_beam/runners/dataflow/internal/__init__.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/examples/avro_bitcoin.py
+Filename: apache-beam-2.9.0/apache_beam/runners/dataflow/internal/apiclient_test.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/examples/streaming_wordcount.py
+Filename: apache-beam-2.9.0/apache_beam/runners/dataflow/internal/names.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/examples/snippets/snippets_test.py
+Filename: apache-beam-2.9.0/apache_beam/runners/dataflow/internal/apiclient.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/examples/snippets/__init__.py
+Filename: apache-beam-2.9.0/apache_beam/runners/dataflow/internal/clients/__init__.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/examples/snippets/snippets.py
+Filename: apache-beam-2.9.0/apache_beam/runners/dataflow/internal/clients/dataflow/__init__.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/examples/complete/estimate_pi_test.py
+Filename: apache-beam-2.9.0/apache_beam/runners/dataflow/internal/clients/dataflow/message_matchers_test.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/examples/complete/autocomplete.py
+Filename: apache-beam-2.9.0/apache_beam/runners/dataflow/internal/clients/dataflow/dataflow_v1b3_client.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/examples/complete/distribopt.py
+Filename: apache-beam-2.9.0/apache_beam/runners/dataflow/internal/clients/dataflow/dataflow_v1b3_messages.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/examples/complete/tfidf_test.py
+Filename: apache-beam-2.9.0/apache_beam/runners/dataflow/internal/clients/dataflow/message_matchers.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/examples/complete/__init__.py
+Filename: apache-beam-2.9.0/apache_beam/runners/dataflow/native_io/__init__.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/examples/complete/distribopt_test.py
+Filename: apache-beam-2.9.0/apache_beam/runners/dataflow/native_io/iobase_test.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/examples/complete/estimate_pi.py
+Filename: apache-beam-2.9.0/apache_beam/runners/dataflow/native_io/iobase.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/examples/complete/top_wikipedia_sessions.py
+Filename: apache-beam-2.9.0/apache_beam/runners/dataflow/native_io/streaming_create.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/examples/complete/top_wikipedia_sessions_test.py
+Filename: apache-beam-2.9.0/apache_beam/runners/interactive/__init__.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/examples/complete/tfidf.py
+Filename: apache-beam-2.9.0/apache_beam/runners/interactive/pipeline_analyzer_test.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/examples/complete/autocomplete_test.py
+Filename: apache-beam-2.9.0/apache_beam/runners/interactive/cache_manager.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/examples/complete/juliaset/__init__.py
+Filename: apache-beam-2.9.0/apache_beam/runners/interactive/interactive_runner.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/examples/complete/juliaset/setup.py
+Filename: apache-beam-2.9.0/apache_beam/runners/interactive/interactive_runner_test.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/examples/complete/juliaset/juliaset_main.py
+Filename: apache-beam-2.9.0/apache_beam/runners/interactive/pipeline_analyzer.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/examples/complete/juliaset/juliaset/__init__.py
+Filename: apache-beam-2.9.0/apache_beam/runners/interactive/cache_manager_test.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/examples/complete/juliaset/juliaset/juliaset_test.py
+Filename: apache-beam-2.9.0/apache_beam/runners/interactive/display/__init__.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/examples/complete/juliaset/juliaset/juliaset.py
+Filename: apache-beam-2.9.0/apache_beam/runners/interactive/display/pipeline_graph.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/examples/complete/game/game_stats_it_test.py
+Filename: apache-beam-2.9.0/apache_beam/runners/interactive/display/pipeline_graph_renderer.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/examples/complete/game/game_stats_test.py
+Filename: apache-beam-2.9.0/apache_beam/runners/interactive/display/interactive_pipeline_graph.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/examples/complete/game/hourly_team_score_test.py
+Filename: apache-beam-2.9.0/apache_beam/runners/interactive/display/display_manager.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/examples/complete/game/user_score_it_test.py
+Filename: apache-beam-2.9.0/apache_beam/typehints/trivial_inference_test.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/examples/complete/game/leader_board_it_test.py
+Filename: apache-beam-2.9.0/apache_beam/typehints/__init__.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/examples/complete/game/game_stats.py
+Filename: apache-beam-2.9.0/apache_beam/typehints/typed_pipeline_test.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/examples/complete/game/__init__.py
+Filename: apache-beam-2.9.0/apache_beam/typehints/native_type_compatibility_test.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/examples/complete/game/hourly_team_score_it_test.py
+Filename: apache-beam-2.9.0/apache_beam/typehints/typehints.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/examples/complete/game/hourly_team_score.py
+Filename: apache-beam-2.9.0/apache_beam/typehints/opcodes.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/examples/complete/game/leader_board.py
+Filename: apache-beam-2.9.0/apache_beam/typehints/trivial_inference.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/examples/complete/game/user_score.py
+Filename: apache-beam-2.9.0/apache_beam/typehints/native_type_compatibility.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/examples/complete/game/leader_board_test.py
+Filename: apache-beam-2.9.0/apache_beam/typehints/decorators.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/examples/complete/game/user_score_test.py
+Filename: apache-beam-2.9.0/apache_beam/typehints/typecheck.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/examples/cookbook/group_with_coder_test.py
+Filename: apache-beam-2.9.0/apache_beam/typehints/typehints_test.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/examples/cookbook/filters.py
+Filename: apache-beam-2.9.0/apache_beam/io/__init__.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/examples/cookbook/datastore_wordcount_it_test.py
+Filename: apache-beam-2.9.0/apache_beam/io/restriction_trackers_test.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/examples/cookbook/bigquery_tornadoes_it_test.py
+Filename: apache-beam-2.9.0/apache_beam/io/restriction_trackers.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/examples/cookbook/__init__.py
+Filename: apache-beam-2.9.0/apache_beam/io/avroio_test.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/examples/cookbook/multiple_output_pardo_test.py
+Filename: apache-beam-2.9.0/apache_beam/io/source_test_utils.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/examples/cookbook/custom_ptransform.py
+Filename: apache-beam-2.9.0/apache_beam/io/filebasedsource_test.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/examples/cookbook/multiple_output_pardo.py
+Filename: apache-beam-2.9.0/apache_beam/io/filesystems_test.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/examples/cookbook/group_with_coder.py
+Filename: apache-beam-2.9.0/apache_beam/io/localfilesystem.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/examples/cookbook/bigquery_tornadoes_test.py
+Filename: apache-beam-2.9.0/apache_beam/io/textio_test.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/examples/cookbook/mergecontacts_test.py
+Filename: apache-beam-2.9.0/apache_beam/io/textio.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/examples/cookbook/custom_ptransform_test.py
+Filename: apache-beam-2.9.0/apache_beam/io/vcfio.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/examples/cookbook/combiners_test.py
+Filename: apache-beam-2.9.0/apache_beam/io/filesystemio_test.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/examples/cookbook/coders_test.py
+Filename: apache-beam-2.9.0/apache_beam/io/vcfio_test.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/examples/cookbook/bigquery_side_input.py
+Filename: apache-beam-2.9.0/apache_beam/io/source_test_utils_test.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/examples/cookbook/bigquery_tornadoes.py
+Filename: apache-beam-2.9.0/apache_beam/io/sources_test.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/examples/cookbook/mergecontacts.py
+Filename: apache-beam-2.9.0/apache_beam/io/localfilesystem_test.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/examples/cookbook/datastore_wordcount.py
+Filename: apache-beam-2.9.0/apache_beam/io/utils.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/examples/cookbook/coders.py
+Filename: apache-beam-2.9.0/apache_beam/io/filebasedsink.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/examples/cookbook/bigquery_side_input_test.py
+Filename: apache-beam-2.9.0/apache_beam/io/hadoopfilesystem_test.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/examples/cookbook/bigquery_schema.py
+Filename: apache-beam-2.9.0/apache_beam/io/tfrecordio.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/examples/cookbook/filters_test.py
+Filename: apache-beam-2.9.0/apache_beam/io/range_trackers.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/internal/__init__.py
+Filename: apache-beam-2.9.0/apache_beam/io/avroio.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/internal/util_test.py
+Filename: apache-beam-2.9.0/apache_beam/io/filebasedsink_test.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/internal/pickler.py
+Filename: apache-beam-2.9.0/apache_beam/io/filesystem_test.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/internal/module_test.py
+Filename: apache-beam-2.9.0/apache_beam/io/concat_source_test.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/internal/pickler_test.py
+Filename: apache-beam-2.9.0/apache_beam/io/hadoopfilesystem.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/internal/util.py
+Filename: apache-beam-2.9.0/apache_beam/io/iobase.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/internal/gcp/__init__.py
+Filename: apache-beam-2.9.0/apache_beam/io/range_trackers_test.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/internal/gcp/auth.py
+Filename: apache-beam-2.9.0/apache_beam/io/filesystemio.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/internal/gcp/json_value.py
+Filename: apache-beam-2.9.0/apache_beam/io/filesystem.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/internal/gcp/json_value_test.py
+Filename: apache-beam-2.9.0/apache_beam/io/filebasedsource.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/io/source_test_utils_test.py
+Filename: apache-beam-2.9.0/apache_beam/io/tfrecordio_test.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/io/filebasedsink.py
+Filename: apache-beam-2.9.0/apache_beam/io/concat_source.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/io/localfilesystem_test.py
+Filename: apache-beam-2.9.0/apache_beam/io/filesystems.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/io/filesystems_test.py
+Filename: apache-beam-2.9.0/apache_beam/io/gcp/__init__.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/io/filesystems.py
+Filename: apache-beam-2.9.0/apache_beam/io/gcp/gcsio.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/io/concat_source.py
+Filename: apache-beam-2.9.0/apache_beam/io/gcp/bigquery_io_read_pipeline.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/io/filebasedsource_test.py
+Filename: apache-beam-2.9.0/apache_beam/io/gcp/big_query_query_to_table_pipeline.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/io/restriction_trackers_test.py
+Filename: apache-beam-2.9.0/apache_beam/io/gcp/gcsio_test.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/io/filesystemio.py
+Filename: apache-beam-2.9.0/apache_beam/io/gcp/gcsfilesystem.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/io/range_trackers_test.py
+Filename: apache-beam-2.9.0/apache_beam/io/gcp/big_query_query_to_table_it_test.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/io/source_test_utils.py
+Filename: apache-beam-2.9.0/apache_beam/io/gcp/bigquery.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/io/tfrecordio_test.py
+Filename: apache-beam-2.9.0/apache_beam/io/gcp/pubsub_test.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/io/utils.py
+Filename: apache-beam-2.9.0/apache_beam/io/gcp/bigquery_test.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/io/filebasedsource.py
+Filename: apache-beam-2.9.0/apache_beam/io/gcp/datastore_write_it_test.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/io/filesystem_test.py
+Filename: apache-beam-2.9.0/apache_beam/io/gcp/pubsub_integration_test.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/io/textio.py
+Filename: apache-beam-2.9.0/apache_beam/io/gcp/pubsub_it_pipeline.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/io/__init__.py
+Filename: apache-beam-2.9.0/apache_beam/io/gcp/pubsub.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/io/filesystemio_test.py
+Filename: apache-beam-2.9.0/apache_beam/io/gcp/gcsfilesystem_test.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/io/restriction_trackers.py
+Filename: apache-beam-2.9.0/apache_beam/io/gcp/datastore_write_it_pipeline.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/io/localfilesystem.py
+Filename: apache-beam-2.9.0/apache_beam/io/gcp/bigquery_io_read_it_test.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/io/hadoopfilesystem_test.py
+Filename: apache-beam-2.9.0/apache_beam/io/gcp/internal/__init__.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/io/tfrecordio.py
+Filename: apache-beam-2.9.0/apache_beam/io/gcp/internal/clients/__init__.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/io/filesystem.py
+Filename: apache-beam-2.9.0/apache_beam/io/gcp/internal/clients/storage/__init__.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/io/iobase.py
+Filename: apache-beam-2.9.0/apache_beam/io/gcp/internal/clients/storage/storage_v1_client.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/io/vcfio_test.py
+Filename: apache-beam-2.9.0/apache_beam/io/gcp/internal/clients/storage/storage_v1_messages.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/io/textio_test.py
+Filename: apache-beam-2.9.0/apache_beam/io/gcp/internal/clients/bigquery/__init__.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/io/hadoopfilesystem.py
+Filename: apache-beam-2.9.0/apache_beam/io/gcp/internal/clients/bigquery/bigquery_v2_client.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/io/concat_source_test.py
+Filename: apache-beam-2.9.0/apache_beam/io/gcp/internal/clients/bigquery/bigquery_v2_messages.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/io/range_trackers.py
+Filename: apache-beam-2.9.0/apache_beam/io/gcp/tests/__init__.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/io/avroio.py
+Filename: apache-beam-2.9.0/apache_beam/io/gcp/tests/utils_test.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/io/filebasedsink_test.py
+Filename: apache-beam-2.9.0/apache_beam/io/gcp/tests/utils.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/io/vcfio.py
+Filename: apache-beam-2.9.0/apache_beam/io/gcp/tests/pubsub_matcher.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/io/sources_test.py
+Filename: apache-beam-2.9.0/apache_beam/io/gcp/tests/pubsub_matcher_test.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/io/avroio_test.py
+Filename: apache-beam-2.9.0/apache_beam/io/gcp/tests/bigquery_matcher_test.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/io/gcp/bigquery_io_read_it_test.py
+Filename: apache-beam-2.9.0/apache_beam/io/gcp/tests/bigquery_matcher.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/io/gcp/big_query_query_to_table_it_test.py
+Filename: apache-beam-2.9.0/apache_beam/io/gcp/datastore/__init__.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/io/gcp/bigquery_io_read_pipeline.py
+Filename: apache-beam-2.9.0/apache_beam/io/gcp/datastore/v1/util.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/io/gcp/datastore_write_it_test.py
+Filename: apache-beam-2.9.0/apache_beam/io/gcp/datastore/v1/__init__.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/io/gcp/big_query_query_to_table_pipeline.py
+Filename: apache-beam-2.9.0/apache_beam/io/gcp/datastore/v1/util_test.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/io/gcp/gcsfilesystem_test.py
+Filename: apache-beam-2.9.0/apache_beam/io/gcp/datastore/v1/adaptive_throttler_test.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/io/gcp/__init__.py
+Filename: apache-beam-2.9.0/apache_beam/io/gcp/datastore/v1/query_splitter.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/io/gcp/pubsub_test.py
+Filename: apache-beam-2.9.0/apache_beam/io/gcp/datastore/v1/helper.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/io/gcp/pubsub_integration_test.py
+Filename: apache-beam-2.9.0/apache_beam/io/gcp/datastore/v1/datastoreio_test.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/io/gcp/pubsub_it_pipeline.py
+Filename: apache-beam-2.9.0/apache_beam/io/gcp/datastore/v1/adaptive_throttler.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/io/gcp/bigquery.py
+Filename: apache-beam-2.9.0/apache_beam/io/gcp/datastore/v1/query_splitter_test.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/io/gcp/datastore_write_it_pipeline.py
+Filename: apache-beam-2.9.0/apache_beam/io/gcp/datastore/v1/helper_test.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/io/gcp/bigquery_test.py
+Filename: apache-beam-2.9.0/apache_beam/io/gcp/datastore/v1/fake_datastore.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/io/gcp/gcsio_test.py
+Filename: apache-beam-2.9.0/apache_beam/io/gcp/datastore/v1/datastoreio.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/io/gcp/gcsio.py
+Filename: apache-beam-2.9.0/apache_beam/io/flink/__init__.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/io/gcp/gcsfilesystem.py
+Filename: apache-beam-2.9.0/apache_beam/io/flink/flink_streaming_impulse_source.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/io/gcp/pubsub.py
+Filename: apache-beam-2.9.0/apache_beam/metrics/__init__.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/io/gcp/tests/utils.py
+Filename: apache-beam-2.9.0/apache_beam/metrics/execution_test.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/io/gcp/tests/__init__.py
+Filename: apache-beam-2.9.0/apache_beam/metrics/cells.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/io/gcp/tests/bigquery_matcher_test.py
+Filename: apache-beam-2.9.0/apache_beam/metrics/execution.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/io/gcp/tests/pubsub_matcher.py
+Filename: apache-beam-2.9.0/apache_beam/metrics/cells_test.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/io/gcp/tests/utils_test.py
+Filename: apache-beam-2.9.0/apache_beam/metrics/execution.pxd
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/io/gcp/tests/bigquery_matcher.py
+Filename: apache-beam-2.9.0/apache_beam/metrics/execution.c
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/io/gcp/tests/pubsub_matcher_test.py
+Filename: apache-beam-2.9.0/apache_beam/metrics/metricbase.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/io/gcp/datastore/__init__.py
+Filename: apache-beam-2.9.0/apache_beam/metrics/metric.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/io/gcp/datastore/v1/helper_test.py
+Filename: apache-beam-2.9.0/apache_beam/metrics/monitoring_infos.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/io/gcp/datastore/v1/fake_datastore.py
+Filename: apache-beam-2.9.0/apache_beam/metrics/metric_test.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/io/gcp/datastore/v1/adaptive_throttler.py
+Filename: apache-beam-2.9.0/apache_beam/examples/wordcount_debugging.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/io/gcp/datastore/v1/__init__.py
+Filename: apache-beam-2.9.0/apache_beam/examples/__init__.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/io/gcp/datastore/v1/util_test.py
+Filename: apache-beam-2.9.0/apache_beam/examples/wordcount.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/io/gcp/datastore/v1/adaptive_throttler_test.py
+Filename: apache-beam-2.9.0/apache_beam/examples/wordcount_test.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/io/gcp/datastore/v1/query_splitter_test.py
+Filename: apache-beam-2.9.0/apache_beam/examples/fastavro_it_test.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/io/gcp/datastore/v1/query_splitter.py
+Filename: apache-beam-2.9.0/apache_beam/examples/streaming_wordcount_debugging.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/io/gcp/datastore/v1/helper.py
+Filename: apache-beam-2.9.0/apache_beam/examples/wordcount_minimal.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/io/gcp/datastore/v1/datastoreio_test.py
+Filename: apache-beam-2.9.0/apache_beam/examples/wordcount_minimal_test.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/io/gcp/datastore/v1/datastoreio.py
+Filename: apache-beam-2.9.0/apache_beam/examples/wordcount_it_test.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/io/gcp/datastore/v1/util.py
+Filename: apache-beam-2.9.0/apache_beam/examples/windowed_wordcount.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/io/gcp/internal/__init__.py
+Filename: apache-beam-2.9.0/apache_beam/examples/wordcount_debugging_test.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/io/gcp/internal/clients/__init__.py
+Filename: apache-beam-2.9.0/apache_beam/examples/streaming_wordcount_it_test.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/io/gcp/internal/clients/bigquery/__init__.py
+Filename: apache-beam-2.9.0/apache_beam/examples/avro_bitcoin.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/io/gcp/internal/clients/bigquery/bigquery_v2_client.py
+Filename: apache-beam-2.9.0/apache_beam/examples/streaming_wordcount.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/io/gcp/internal/clients/bigquery/bigquery_v2_messages.py
+Filename: apache-beam-2.9.0/apache_beam/examples/cookbook/__init__.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/io/gcp/internal/clients/storage/storage_v1_client.py
+Filename: apache-beam-2.9.0/apache_beam/examples/cookbook/datastore_wordcount.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/io/gcp/internal/clients/storage/storage_v1_messages.py
+Filename: apache-beam-2.9.0/apache_beam/examples/cookbook/group_with_coder.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/io/gcp/internal/clients/storage/__init__.py
+Filename: apache-beam-2.9.0/apache_beam/examples/cookbook/coders.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/testing/pipeline_verifiers_test.py
+Filename: apache-beam-2.9.0/apache_beam/examples/cookbook/mergecontacts.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/testing/synthetic_pipeline.py
+Filename: apache-beam-2.9.0/apache_beam/examples/cookbook/multiple_output_pardo.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/testing/__init__.py
+Filename: apache-beam-2.9.0/apache_beam/examples/cookbook/coders_test.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/testing/util_test.py
+Filename: apache-beam-2.9.0/apache_beam/examples/cookbook/bigquery_tornadoes_test.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/testing/test_pipeline.py
+Filename: apache-beam-2.9.0/apache_beam/examples/cookbook/datastore_wordcount_it_test.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/testing/test_stream_test.py
+Filename: apache-beam-2.9.0/apache_beam/examples/cookbook/bigquery_tornadoes.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/testing/test_utils_test.py
+Filename: apache-beam-2.9.0/apache_beam/examples/cookbook/custom_ptransform_test.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/testing/test_stream.py
+Filename: apache-beam-2.9.0/apache_beam/examples/cookbook/custom_ptransform.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/testing/pipeline_verifiers.py
+Filename: apache-beam-2.9.0/apache_beam/examples/cookbook/group_with_coder_test.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/testing/test_pipeline_test.py
+Filename: apache-beam-2.9.0/apache_beam/examples/cookbook/bigquery_side_input_test.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/testing/test_utils.py
+Filename: apache-beam-2.9.0/apache_beam/examples/cookbook/mergecontacts_test.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/testing/synthetic_pipeline_test.py
+Filename: apache-beam-2.9.0/apache_beam/examples/cookbook/bigquery_tornadoes_it_test.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/testing/util.py
+Filename: apache-beam-2.9.0/apache_beam/examples/cookbook/multiple_output_pardo_test.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/testing/data/trigger_transcripts.yaml
+Filename: apache-beam-2.9.0/apache_beam/examples/cookbook/bigquery_schema.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/testing/data/standard_coders.yaml
+Filename: apache-beam-2.9.0/apache_beam/examples/cookbook/filters.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/testing/benchmarks/__init__.py
+Filename: apache-beam-2.9.0/apache_beam/examples/cookbook/bigquery_side_input.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/testing/benchmarks/nexmark/nexmark_launcher.py
+Filename: apache-beam-2.9.0/apache_beam/examples/cookbook/filters_test.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/testing/benchmarks/nexmark/__init__.py
+Filename: apache-beam-2.9.0/apache_beam/examples/cookbook/combiners_test.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/testing/benchmarks/nexmark/nexmark_util.py
+Filename: apache-beam-2.9.0/apache_beam/examples/complete/__init__.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/testing/benchmarks/nexmark/queries/query2.py
+Filename: apache-beam-2.9.0/apache_beam/examples/complete/distribopt_test.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/testing/benchmarks/nexmark/queries/query0.py
+Filename: apache-beam-2.9.0/apache_beam/examples/complete/tfidf.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/testing/benchmarks/nexmark/queries/query1.py
+Filename: apache-beam-2.9.0/apache_beam/examples/complete/autocomplete_test.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/testing/benchmarks/nexmark/queries/__init__.py
+Filename: apache-beam-2.9.0/apache_beam/examples/complete/estimate_pi_test.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/testing/benchmarks/nexmark/models/__init__.py
+Filename: apache-beam-2.9.0/apache_beam/examples/complete/top_wikipedia_sessions.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/testing/benchmarks/nexmark/models/nexmark_model.py
+Filename: apache-beam-2.9.0/apache_beam/examples/complete/autocomplete.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/metrics/execution.py
+Filename: apache-beam-2.9.0/apache_beam/examples/complete/tfidf_test.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/metrics/execution.pxd
+Filename: apache-beam-2.9.0/apache_beam/examples/complete/distribopt.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/metrics/__init__.py
+Filename: apache-beam-2.9.0/apache_beam/examples/complete/top_wikipedia_sessions_test.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/metrics/cells.py
+Filename: apache-beam-2.9.0/apache_beam/examples/complete/estimate_pi.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/metrics/cells_test.py
+Filename: apache-beam-2.9.0/apache_beam/examples/complete/juliaset/__init__.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/metrics/metric.py
+Filename: apache-beam-2.9.0/apache_beam/examples/complete/juliaset/setup.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/metrics/execution_test.py
+Filename: apache-beam-2.9.0/apache_beam/examples/complete/juliaset/juliaset_main.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/metrics/metricbase.py
+Filename: apache-beam-2.9.0/apache_beam/examples/complete/juliaset/juliaset/__init__.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/metrics/metric_test.py
+Filename: apache-beam-2.9.0/apache_beam/examples/complete/juliaset/juliaset/juliaset_test.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/typehints/decorators.py
+Filename: apache-beam-2.9.0/apache_beam/examples/complete/juliaset/juliaset/juliaset.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/typehints/typehints_test.py
+Filename: apache-beam-2.9.0/apache_beam/examples/complete/game/__init__.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/typehints/native_type_compatibility.py
+Filename: apache-beam-2.9.0/apache_beam/examples/complete/game/game_stats_it_test.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/typehints/native_type_compatibility_test.py
+Filename: apache-beam-2.9.0/apache_beam/examples/complete/game/game_stats.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/typehints/__init__.py
+Filename: apache-beam-2.9.0/apache_beam/examples/complete/game/hourly_team_score_it_test.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/typehints/typehints.py
+Filename: apache-beam-2.9.0/apache_beam/examples/complete/game/user_score.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/typehints/trivial_inference_test.py
+Filename: apache-beam-2.9.0/apache_beam/examples/complete/game/hourly_team_score_test.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/typehints/typed_pipeline_test.py
+Filename: apache-beam-2.9.0/apache_beam/examples/complete/game/leader_board_it_test.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/typehints/opcodes.py
+Filename: apache-beam-2.9.0/apache_beam/examples/complete/game/user_score_test.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/typehints/trivial_inference.py
+Filename: apache-beam-2.9.0/apache_beam/examples/complete/game/leader_board.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/typehints/typecheck.py
+Filename: apache-beam-2.9.0/apache_beam/examples/complete/game/hourly_team_score.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/tools/map_fn_microbenchmark.py
+Filename: apache-beam-2.9.0/apache_beam/examples/complete/game/game_stats_test.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/tools/utils.py
+Filename: apache-beam-2.9.0/apache_beam/examples/complete/game/leader_board_test.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/tools/__init__.py
+Filename: apache-beam-2.9.0/apache_beam/examples/complete/game/user_score_it_test.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/tools/distribution_counter_microbenchmark.py
+Filename: apache-beam-2.9.0/apache_beam/examples/flink/flink_streaming_impulse.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/tools/coders_microbenchmark.py
+Filename: apache-beam-2.9.0/apache_beam/examples/flink/__init__.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/tools/microbenchmarks_test.py
+Filename: apache-beam-2.9.0/apache_beam/examples/snippets/__init__.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/tools/sideinput_microbenchmark.py
+Filename: apache-beam-2.9.0/apache_beam/examples/snippets/snippets.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/coders/coder_impl.pxd
+Filename: apache-beam-2.9.0/apache_beam/examples/snippets/snippets_test.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/coders/stream_test.py
+Filename: apache-beam-2.9.0/apache_beam/utils/__init__.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/coders/__init__.py
+Filename: apache-beam-2.9.0/apache_beam/utils/windowed_value.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/coders/slow_stream.py
+Filename: apache-beam-2.9.0/apache_beam/utils/windowed_value.pxd
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/coders/stream.pxd
+Filename: apache-beam-2.9.0/apache_beam/utils/annotations.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/coders/coders_test_common.py
+Filename: apache-beam-2.9.0/apache_beam/utils/annotations_test.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/coders/proto2_coder_test_messages_pb2.py
+Filename: apache-beam-2.9.0/apache_beam/utils/windowed_value.c
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/coders/observable_test.py
+Filename: apache-beam-2.9.0/apache_beam/utils/counters.c
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/coders/standard_coders_test.py
+Filename: apache-beam-2.9.0/apache_beam/utils/timestamp_test.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/coders/coders_test.py
+Filename: apache-beam-2.9.0/apache_beam/utils/plugin.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/coders/typecoders.py
+Filename: apache-beam-2.9.0/apache_beam/utils/retry_test.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/coders/stream.pyx
+Filename: apache-beam-2.9.0/apache_beam/utils/windowed_value_test.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/coders/typecoders_test.py
+Filename: apache-beam-2.9.0/apache_beam/utils/counters.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/coders/slow_coders_test.py
+Filename: apache-beam-2.9.0/apache_beam/utils/urns.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/coders/coder_impl.py
+Filename: apache-beam-2.9.0/apache_beam/utils/retry.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/coders/fast_coders_test.py
+Filename: apache-beam-2.9.0/apache_beam/utils/processes_test.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/coders/coders.py
+Filename: apache-beam-2.9.0/apache_beam/utils/counters_test.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/coders/observable.py
+Filename: apache-beam-2.9.0/apache_beam/utils/counters.pxd
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/transforms/window_test.py
+Filename: apache-beam-2.9.0/apache_beam/utils/timestamp.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/transforms/userstate.py
+Filename: apache-beam-2.9.0/apache_beam/utils/profiler.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/transforms/create_source.py
+Filename: apache-beam-2.9.0/apache_beam/utils/proto_utils.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/transforms/userstate_test.py
+Filename: apache-beam-2.9.0/apache_beam/utils/processes.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/transforms/window.py
+Filename: apache-beam-2.9.0/apache_beam/portability/__init__.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/transforms/cy_dataflow_distribution_counter.pyx
+Filename: apache-beam-2.9.0/apache_beam/portability/common_urns.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/transforms/timeutil.py
+Filename: apache-beam-2.9.0/apache_beam/portability/python_urns.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/transforms/display_test.py
+Filename: apache-beam-2.9.0/apache_beam/portability/api/__init__.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/transforms/__init__.py
+Filename: apache-beam-2.9.0/apache_beam/portability/api/beam_provision_api_pb2.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/transforms/cy_dataflow_distribution_counter.pxd
+Filename: apache-beam-2.9.0/apache_beam/portability/api/endpoints_pb2_grpc.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/transforms/combiners.py
+Filename: apache-beam-2.9.0/apache_beam/portability/api/beam_runner_api_pb2.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/transforms/sideinputs.py
+Filename: apache-beam-2.9.0/apache_beam/portability/api/beam_artifact_api_pb2.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/transforms/util_test.py
+Filename: apache-beam-2.9.0/apache_beam/portability/api/beam_job_api_pb2.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/transforms/dataflow_distribution_counter_test.py
+Filename: apache-beam-2.9.0/apache_beam/portability/api/endpoints_pb2.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/transforms/display.py
+Filename: apache-beam-2.9.0/apache_beam/portability/api/beam_fn_api_pb2_grpc.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/transforms/cy_combiners.pxd
+Filename: apache-beam-2.9.0/apache_beam/portability/api/beam_provision_api_pb2_grpc.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/transforms/combiners_test.py
+Filename: apache-beam-2.9.0/apache_beam/portability/api/standard_window_fns_pb2_grpc.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/transforms/create_test.py
+Filename: apache-beam-2.9.0/apache_beam/portability/api/beam_job_api_pb2_grpc.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/transforms/cy_combiners.py
+Filename: apache-beam-2.9.0/apache_beam/portability/api/beam_runner_api_pb2_grpc.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/transforms/ptransform.py
+Filename: apache-beam-2.9.0/apache_beam/portability/api/beam_artifact_api_pb2_grpc.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/transforms/core.py
+Filename: apache-beam-2.9.0/apache_beam/portability/api/beam_fn_api_pb2.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/transforms/trigger_test.py
+Filename: apache-beam-2.9.0/apache_beam/portability/api/standard_window_fns_pb2.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/transforms/write_ptransform_test.py
+Filename: apache-beam-2.9.0/apache_beam/testing/util.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/transforms/trigger.py
+Filename: apache-beam-2.9.0/apache_beam/testing/pipeline_verifiers_test.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/transforms/ptransform_test.py
+Filename: apache-beam-2.9.0/apache_beam/testing/__init__.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/transforms/util.py
+Filename: apache-beam-2.9.0/apache_beam/testing/test_utils.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/transforms/sideinputs_test.py
+Filename: apache-beam-2.9.0/apache_beam/testing/synthetic_pipeline_test.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/transforms/py_dataflow_distribution_counter.py
+Filename: apache-beam-2.9.0/apache_beam/testing/util_test.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/utils/windowed_value_test.py
+Filename: apache-beam-2.9.0/apache_beam/testing/pipeline_verifiers.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/utils/annotations_test.py
+Filename: apache-beam-2.9.0/apache_beam/testing/test_stream_test.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/utils/retry.py
+Filename: apache-beam-2.9.0/apache_beam/testing/test_utils_test.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/utils/counters.py
+Filename: apache-beam-2.9.0/apache_beam/testing/test_pipeline.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/utils/counters_test.py
+Filename: apache-beam-2.9.0/apache_beam/testing/test_pipeline_test.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/utils/plugin.py
+Filename: apache-beam-2.9.0/apache_beam/testing/synthetic_pipeline.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/utils/retry_test.py
+Filename: apache-beam-2.9.0/apache_beam/testing/test_stream.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/utils/annotations.py
+Filename: apache-beam-2.9.0/apache_beam/testing/benchmarks/__init__.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/utils/__init__.py
+Filename: apache-beam-2.9.0/apache_beam/testing/benchmarks/nexmark/__init__.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/utils/windowed_value.py
+Filename: apache-beam-2.9.0/apache_beam/testing/benchmarks/nexmark/nexmark_launcher.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/utils/processes.py
+Filename: apache-beam-2.9.0/apache_beam/testing/benchmarks/nexmark/nexmark_util.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/utils/profiler.py
+Filename: apache-beam-2.9.0/apache_beam/testing/benchmarks/nexmark/queries/__init__.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/utils/urns.py
+Filename: apache-beam-2.9.0/apache_beam/testing/benchmarks/nexmark/queries/query0.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/utils/windowed_value.pxd
+Filename: apache-beam-2.9.0/apache_beam/testing/benchmarks/nexmark/queries/query1.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/utils/proto_utils.py
+Filename: apache-beam-2.9.0/apache_beam/testing/benchmarks/nexmark/queries/query2.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/utils/timestamp.py
+Filename: apache-beam-2.9.0/apache_beam/testing/benchmarks/nexmark/models/__init__.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/utils/counters.pxd
+Filename: apache-beam-2.9.0/apache_beam/testing/benchmarks/nexmark/models/nexmark_model.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/utils/timestamp_test.py
+Filename: apache-beam-2.9.0/apache_beam/testing/load_tests/__init__.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/utils/processes_test.py
+Filename: apache-beam-2.9.0/apache_beam/testing/load_tests/pardo_test.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/portability/python_urns.py
+Filename: apache-beam-2.9.0/apache_beam/testing/load_tests/co_group_by_key_test.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/portability/common_urns.py
+Filename: apache-beam-2.9.0/apache_beam/testing/load_tests/group_by_key_test.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/portability/__init__.py
+Filename: apache-beam-2.9.0/apache_beam/testing/load_tests/load_test_metrics_utils.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/portability/api/standard_window_fns_pb2.py
+Filename: apache-beam-2.9.0/apache_beam/testing/load_tests/combine_test.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/portability/api/beam_fn_api_pb2_grpc.py
+Filename: apache-beam-2.9.0/apache_beam/testing/data/trigger_transcripts.yaml
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/portability/api/endpoints_pb2_grpc.py
+Filename: apache-beam-2.9.0/apache_beam/testing/data/standard_coders.yaml
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/portability/api/beam_runner_api_pb2.py
+Filename: apache-beam-2.9.0/apache_beam/transforms/cy_combiners.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/portability/api/__init__.py
+Filename: apache-beam-2.9.0/apache_beam/transforms/util.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/portability/api/standard_window_fns_pb2_grpc.py
+Filename: apache-beam-2.9.0/apache_beam/transforms/__init__.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/portability/api/beam_fn_api_pb2.py
+Filename: apache-beam-2.9.0/apache_beam/transforms/trigger.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/portability/api/beam_job_api_pb2_grpc.py
+Filename: apache-beam-2.9.0/apache_beam/transforms/cy_dataflow_distribution_counter.pyx
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/portability/api/beam_provision_api_pb2.py
+Filename: apache-beam-2.9.0/apache_beam/transforms/window.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/portability/api/beam_artifact_api_pb2.py
+Filename: apache-beam-2.9.0/apache_beam/transforms/core.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/portability/api/beam_provision_api_pb2_grpc.py
+Filename: apache-beam-2.9.0/apache_beam/transforms/display_test.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/portability/api/endpoints_pb2.py
+Filename: apache-beam-2.9.0/apache_beam/transforms/write_ptransform_test.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/portability/api/beam_artifact_api_pb2_grpc.py
+Filename: apache-beam-2.9.0/apache_beam/transforms/util_test.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/portability/api/beam_job_api_pb2.py
+Filename: apache-beam-2.9.0/apache_beam/transforms/sideinputs_test.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/portability/api/beam_runner_api_pb2_grpc.py
+Filename: apache-beam-2.9.0/apache_beam/transforms/py_dataflow_distribution_counter.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/options/pipeline_options_validator.py
+Filename: apache-beam-2.9.0/apache_beam/transforms/cy_dataflow_distribution_counter.pxd
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/options/__init__.py
+Filename: apache-beam-2.9.0/apache_beam/transforms/window_test.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/options/value_provider.py
+Filename: apache-beam-2.9.0/apache_beam/transforms/userstate.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/options/pipeline_options.py
+Filename: apache-beam-2.9.0/apache_beam/transforms/trigger_test.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/options/pipeline_options_test.py
+Filename: apache-beam-2.9.0/apache_beam/transforms/sideinputs.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/options/pipeline_options_validator_test.py
+Filename: apache-beam-2.9.0/apache_beam/transforms/cy_combiners.c
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam/options/value_provider_test.py
+Filename: apache-beam-2.9.0/apache_beam/transforms/ptransform.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam.egg-info/requires.txt
+Filename: apache-beam-2.9.0/apache_beam/transforms/cy_dataflow_distribution_counter.c
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam.egg-info/dependency_links.txt
+Filename: apache-beam-2.9.0/apache_beam/transforms/timeutil.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam.egg-info/not-zip-safe
+Filename: apache-beam-2.9.0/apache_beam/transforms/combiners.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam.egg-info/SOURCES.txt
+Filename: apache-beam-2.9.0/apache_beam/transforms/cy_combiners.pxd
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam.egg-info/PKG-INFO
+Filename: apache-beam-2.9.0/apache_beam/transforms/dataflow_distribution_counter_test.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam.egg-info/entry_points.txt
+Filename: apache-beam-2.9.0/apache_beam/transforms/create_test.py
 Comment: 
 
-Filename: apache-beam-2.8.0/apache_beam.egg-info/top_level.txt
+Filename: apache-beam-2.9.0/apache_beam/transforms/display.py
+Comment: 
+
+Filename: apache-beam-2.9.0/apache_beam/transforms/userstate_test.py
+Comment: 
+
+Filename: apache-beam-2.9.0/apache_beam/transforms/create_source.py
+Comment: 
+
+Filename: apache-beam-2.9.0/apache_beam/transforms/combiners_test.py
+Comment: 
+
+Filename: apache-beam-2.9.0/apache_beam/transforms/ptransform_test.py
+Comment: 
+
+Filename: apache-beam-2.9.0/apache_beam/tools/__init__.py
+Comment: 
+
+Filename: apache-beam-2.9.0/apache_beam/tools/map_fn_microbenchmark.py
+Comment: 
+
+Filename: apache-beam-2.9.0/apache_beam/tools/microbenchmarks_test.py
+Comment: 
+
+Filename: apache-beam-2.9.0/apache_beam/tools/sideinput_microbenchmark.py
+Comment: 
+
+Filename: apache-beam-2.9.0/apache_beam/tools/utils.py
+Comment: 
+
+Filename: apache-beam-2.9.0/apache_beam/tools/distribution_counter_microbenchmark.py
+Comment: 
+
+Filename: apache-beam-2.9.0/apache_beam/tools/coders_microbenchmark.py
+Comment: 
+
+Filename: apache-beam-2.9.0/apache_beam/options/__init__.py
+Comment: 
+
+Filename: apache-beam-2.9.0/apache_beam/options/pipeline_options_test.py
+Comment: 
+
+Filename: apache-beam-2.9.0/apache_beam/options/pipeline_options_validator.py
+Comment: 
+
+Filename: apache-beam-2.9.0/apache_beam/options/pipeline_options.py
+Comment: 
+
+Filename: apache-beam-2.9.0/apache_beam/options/pipeline_options_validator_test.py
+Comment: 
+
+Filename: apache-beam-2.9.0/apache_beam/options/value_provider.py
+Comment: 
+
+Filename: apache-beam-2.9.0/apache_beam/options/value_provider_test.py
+Comment: 
+
+Filename: apache-beam-2.9.0/apache_beam.egg-info/requires.txt
+Comment: 
+
+Filename: apache-beam-2.9.0/apache_beam.egg-info/top_level.txt
+Comment: 
+
+Filename: apache-beam-2.9.0/apache_beam.egg-info/not-zip-safe
+Comment: 
+
+Filename: apache-beam-2.9.0/apache_beam.egg-info/dependency_links.txt
+Comment: 
+
+Filename: apache-beam-2.9.0/apache_beam.egg-info/SOURCES.txt
+Comment: 
+
+Filename: apache-beam-2.9.0/apache_beam.egg-info/PKG-INFO
+Comment: 
+
+Filename: apache-beam-2.9.0/apache_beam.egg-info/entry_points.txt
 Comment: 
 
 Zip file comment:
```

## Comparing `apache-beam-2.8.0/MANIFEST.in` & `apache-beam-2.9.0/MANIFEST.in`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/gen_protos.py` & `apache-beam-2.9.0/gen_protos.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/setup.cfg` & `apache-beam-2.9.0/setup.cfg`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/setup.py` & `apache-beam-2.9.0/setup.py`

 * *Files 2% similar despite different names*

```diff
@@ -113,15 +113,15 @@
     'crcmod>=1.7,<2.0',
     'dill>=0.2.6,<=0.2.8.2',
     'fastavro>=0.21.4,<0.22',
     'grpcio>=1.8,<2',
     'hdfs>=2.1.0,<3.0.0',
     'httplib2>=0.8,<=0.11.3',
     'mock>=1.0.1,<3.0.0',
-    'oauth2client>=2.0.1,<5',
+    'oauth2client>=2.0.1,<4',
     # grpcio 1.8.1 and above requires protobuf 3.5.0.post1.
     'protobuf>=3.5.0.post1,<4',
     'pydot>=1.2.0,<1.3',
     'pytz>=2018.3,<=2018.4',
     'pyyaml>=3.12,<4.0.0',
     'pyvcf>=0.6.8,<0.7.0',
     'typing>=3.6.0,<3.7.0; python_version < "3.5.0"',
@@ -133,22 +133,21 @@
     'nose>=1.3.7',
     'parameterized>=0.6.0,<0.7.0',
     'numpy>=1.14.3,<2',
     'pyhamcrest>=1.9,<2.0',
     ]
 
 GCP_REQUIREMENTS = [
-    # oauth2client >=4 only works with google-apitools>=0.5.18.
-    'google-apitools>=0.5.18,<=0.5.20',
+    # google-apitools 0.5.23 and above has important Python 3 supports.
+    'google-apitools>=0.5.23,<=0.5.24',
     'proto-google-cloud-datastore-v1>=0.90.0,<=0.90.4',
-    'googledatastore==7.0.1; python_version < "3.0"',
-    'google-cloud-pubsub==0.26.0',
-    'proto-google-cloud-pubsub-v1==0.15.4',
+    'googledatastore>=7.0.1,<7.1; python_version < "3.0"',
+    'google-cloud-pubsub==0.35.4',
     # GCP packages required by tests
-    'google-cloud-bigquery==0.25.0',
+    'google-cloud-bigquery>=1.6.0,<1.7.0',
 ]
 
 if sys.version_info[0] == 2:
   REQUIRED_PACKAGES = REQUIRED_PACKAGES + REQUIRED_PACKAGES_PY2_ONLY
 elif sys.version_info[0] >= 3:
   REQUIRED_PACKAGES = REQUIRED_PACKAGES + REQUIRED_PACKAGES_PY3_ONLY
```

## Comparing `apache-beam-2.8.0/PKG-INFO` & `apache-beam-2.9.0/PKG-INFO`

 * *Files 8% similar despite different names*

```diff
@@ -1,10 +1,10 @@
-Metadata-Version: 2.1
+Metadata-Version: 1.2
 Name: apache-beam
-Version: 2.8.0
+Version: 2.9.0
 Summary: Apache Beam SDK for Python
 Home-page: https://beam.apache.org
 Author: Apache Software Foundation
 Author-email: dev@beam.apache.org
 License: Apache License, Version 2.0
 Download-URL: https://pypi.python.org/pypi/apache-beam
 Description: 
@@ -18,10 +18,7 @@
 Classifier: Intended Audience :: End Users/Desktop
 Classifier: License :: OSI Approved :: Apache Software License
 Classifier: Operating System :: POSIX :: Linux
 Classifier: Programming Language :: Python :: 2.7
 Classifier: Topic :: Software Development :: Libraries
 Classifier: Topic :: Software Development :: Libraries :: Python Modules
 Requires-Python: >=2.7,<3.0
-Provides-Extra: test
-Provides-Extra: docs
-Provides-Extra: gcp
```

## Comparing `apache-beam-2.8.0/apache_beam/version.py` & `apache-beam-2.9.0/apache_beam/version.py`

 * *Files 2% similar despite different names*

```diff
@@ -14,8 +14,8 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 #
 
 """Apache Beam SDK version information and utilities."""
 
 
-__version__ = '2.8.0'
+__version__ = '2.9.0'
```

## Comparing `apache-beam-2.8.0/apache_beam/pvalue_test.py` & `apache-beam-2.9.0/apache_beam/pvalue_test.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/__init__.py` & `apache-beam-2.9.0/apache_beam/__init__.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/pipeline.py` & `apache-beam-2.9.0/apache_beam/pipeline.py`

 * *Files 2% similar despite different names*

```diff
@@ -396,15 +396,17 @@
 
   def run(self, test_runner_api=True):
     """Runs the pipeline. Returns whatever our runner returns after running."""
 
     # When possible, invoke a round trip through the runner API.
     if test_runner_api and self._verify_runner_api_compatible():
       return Pipeline.from_runner_api(
-          self.to_runner_api(), self.runner, self._options).run(False)
+          self.to_runner_api(use_fake_coders=True),
+          self.runner,
+          self._options).run(False)
 
     if self._options.view_as(TypeOptions).runtime_type_check:
       from apache_beam.typehints import typecheck
       self.visit(typecheck.TypeCheckVisitor())
 
     if self._options.view_as(SetupOptions).save_main_session:
       # If this option is chosen, verify we can pickle the main session early.
@@ -599,20 +601,47 @@
       def visit_value(self, value, _):
         if isinstance(value, pvalue.PDone):
           Visitor.ok = False
 
     self.visit(Visitor())
     return Visitor.ok
 
-  def to_runner_api(self, return_context=False, context=None):
+  def to_runner_api(
+      self, return_context=False, context=None, use_fake_coders=False):
     """For internal use only; no backwards-compatibility guarantees."""
     from apache_beam.runners import pipeline_context
     from apache_beam.portability.api import beam_runner_api_pb2
     if context is None:
-      context = pipeline_context.PipelineContext()
+      context = pipeline_context.PipelineContext(
+          use_fake_coders=use_fake_coders)
+
+    # The RunnerAPI spec requires certain transforms to have KV inputs
+    # (and corresponding outputs).
+    # Currently we only upgrade to KV pairs.  If there is a need for more
+    # general shapes, potential conflicts will have to be resolved.
+    # We also only handle single-input, and (for fixing the output) single
+    # output, which is sufficient.
+    class ForceKvInputTypes(PipelineVisitor):
+      def enter_composite_transform(self, transform_node):
+        self.visit_transform(transform_node)
+
+      def visit_transform(self, transform_node):
+        if (transform_node.transform
+            and transform_node.transform.runner_api_requires_keyed_input()):
+          pcoll = transform_node.inputs[0]
+          pcoll.element_type = typehints.coerce_to_kv_type(
+              pcoll.element_type, transform_node.full_label)
+          if len(transform_node.outputs) == 1:
+            # The runner often has expectations about the output types as well.
+            output, = transform_node.outputs.values()
+            output.element_type = transform_node.transform.infer_output_type(
+                pcoll.element_type)
+
+    self.visit(ForceKvInputTypes())
+
     # Mutates context; placing inline would force dependence on
     # argument evaluation order.
     root_transform_id = context.transforms.get_id(self._root_transform())
     proto = beam_runner_api_pb2.Pipeline(
         root_transform_ids=[root_transform_id],
         components=context.to_runner_api())
     proto.components.transforms[root_transform_id].unique_name = (
```

## Comparing `apache-beam-2.8.0/apache_beam/pvalue.py` & `apache-beam-2.9.0/apache_beam/pvalue.py`

 * *Files 2% similar despite different names*

```diff
@@ -24,14 +24,15 @@
 produced when the pipeline gets executed.
 """
 
 from __future__ import absolute_import
 
 import collections
 import itertools
+import typing
 from builtins import hex
 from builtins import object
 
 from past.builtins import unicode
 
 from apache_beam import coders
 from apache_beam import typehints
@@ -107,25 +108,29 @@
     arglist.insert(1, self)
     return self.pipeline.apply(*arglist, **kwargs)
 
   def __or__(self, ptransform):
     return self.pipeline.apply(ptransform, self)
 
 
-class PCollection(PValue):
+class PCollection(PValue, typing.Generic[typing.TypeVar('T')]):
   """A multiple values (potentially huge) container.
 
   Dataflow users should not construct PCollection objects directly in their
   pipelines.
   """
 
   def __eq__(self, other):
     if isinstance(other, PCollection):
       return self.tag == other.tag and self.producer == other.producer
 
+  def __ne__(self, other):
+    # TODO(BEAM-5949): Needed for Python 2 compatibility.
+    return not self == other
+
   def __hash__(self):
     return hash((self.tag, self.producer))
 
   @property
   def windowing(self):
     if not hasattr(self, '_windowing'):
       self._windowing = self.producer.transform.get_windowing(
@@ -138,15 +143,15 @@
     # of a closure).
     return _InvalidUnpickledPCollection, ()
 
   def to_runner_api(self, context):
     return beam_runner_api_pb2.PCollection(
         unique_name='%d%s.%s' % (
             len(self.producer.full_label), self.producer.full_label, self.tag),
-        coder_id=pickler.dumps(self.element_type),
+        coder_id=context.coder_id_from_element_type(self.element_type),
         is_bounded=beam_runner_api_pb2.IsBounded.BOUNDED,
         windowing_strategy_id=context.windowing_strategies.get_id(
             self.windowing))
 
   @staticmethod
   def from_runner_api(proto, context):
     # Producer and tag will be filled in later, the key point is that the
```

## Comparing `apache-beam-2.8.0/apache_beam/pipeline_test.py` & `apache-beam-2.9.0/apache_beam/pipeline_test.py`

 * *Files 0% similar despite different names*

```diff
@@ -545,15 +545,16 @@
 
       def expand(self, p):
         self.p = p
         return p | beam.Create([None])
 
     p = beam.Pipeline()
     p | MyPTransform()  # pylint: disable=expression-not-assigned
-    p = Pipeline.from_runner_api(Pipeline.to_runner_api(p), None, None)
+    p = Pipeline.from_runner_api(
+        Pipeline.to_runner_api(p, use_fake_coders=True), None, None)
     self.assertIsNotNone(p.transforms_stack[0].parts[0].parent)
     self.assertEquals(p.transforms_stack[0].parts[0].parent,
                       p.transforms_stack[0])
 
 
 class DirectRunnerRetryTests(unittest.TestCase):
```

## Comparing `apache-beam-2.8.0/apache_beam/error.py` & `apache-beam-2.9.0/apache_beam/error.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/runners/__init__.py` & `apache-beam-2.9.0/apache_beam/runners/__init__.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/runners/pipeline_context_test.py` & `apache-beam-2.9.0/apache_beam/runners/pipeline_context_test.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/runners/pipeline_context.py` & `apache-beam-2.9.0/apache_beam/runners/pipeline_context.py`

 * *Files 13% similar despite different names*

```diff
@@ -23,14 +23,15 @@
 from __future__ import absolute_import
 
 from builtins import object
 
 from apache_beam import coders
 from apache_beam import pipeline
 from apache_beam import pvalue
+from apache_beam.internal import pickler
 from apache_beam.portability.api import beam_fn_api_pb2
 from apache_beam.portability.api import beam_runner_api_pb2
 from apache_beam.transforms import core
 
 
 class Environment(object):
   """A wrapper around the environment proto.
@@ -105,29 +106,41 @@
       'transforms': pipeline.AppliedPTransform,
       'pcollections': pvalue.PCollection,
       'coders': coders.Coder,
       'windowing_strategies': core.Windowing,
       'environments': Environment,
   }
 
-  def __init__(self, proto=None, default_environment=None):
+  def __init__(
+      self, proto=None, default_environment=None, use_fake_coders=False):
     if isinstance(proto, beam_fn_api_pb2.ProcessBundleDescriptor):
       proto = beam_runner_api_pb2.Components(
           coders=dict(proto.coders.items()),
           windowing_strategies=dict(proto.windowing_strategies.items()),
           environments=dict(proto.environments.items()))
     for name, cls in self._COMPONENT_TYPES.items():
       setattr(
           self, name, _PipelineContextMap(
               self, cls, getattr(proto, name, None)))
     if default_environment:
       self._default_environment_id = self.environments.get_id(
           Environment(default_environment))
     else:
       self._default_environment_id = None
+    self.use_fake_coders = use_fake_coders
+
+  # If fake coders are requested, return a pickled version of the element type
+  # rather than an actual coder. The element type is required for some runners,
+  # as well as performing a round-trip through protos.
+  # TODO(BEAM-2717): Remove once this is no longer needed.
+  def coder_id_from_element_type(self, element_type):
+    if self.use_fake_coders:
+      return pickler.dumps(element_type)
+    else:
+      return self.coders.get_id(coders.registry.get_coder(element_type))
 
   @staticmethod
   def from_runner_api(proto):
     return PipelineContext(proto)
 
   def to_runner_api(self):
     context_proto = beam_runner_api_pb2.Components()
```

## Comparing `apache-beam-2.8.0/apache_beam/runners/sdf_common.py` & `apache-beam-2.9.0/apache_beam/runners/sdf_common.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/runners/common.pxd` & `apache-beam-2.9.0/apache_beam/runners/common.pxd`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/runners/common.py` & `apache-beam-2.9.0/apache_beam/runners/common.py`

 * *Files 2% similar despite different names*

```diff
@@ -58,14 +58,15 @@
     """
     self.step_name = step_name
 
   def __eq__(self, other):
     return self.step_name == other.step_name
 
   def __ne__(self, other):
+    # TODO(BEAM-5949): Needed for Python 2 compatibility.
     return not self == other
 
   def __repr__(self):
     return 'NameContext(%s)' % self.__dict__
 
   def __hash__(self):
     return hash(self.step_name)
@@ -100,14 +101,15 @@
 
   def __eq__(self, other):
     return (self.step_name == other.step_name and
             self.user_name == other.user_name and
             self.system_name == other.system_name)
 
   def __ne__(self, other):
+    # TODO(BEAM-5949): Needed for Python 2 compatibility.
     return not self == other
 
   def __hash__(self):
     return hash((self.step_name, self.user_name, self.system_name))
 
   def __repr__(self):
     return 'DataflowNameContext(%s)' % self.__dict__
@@ -146,16 +148,21 @@
     """
 
     if not isinstance(obj_to_invoke, (DoFn, RestrictionProvider)):
       raise ValueError('\'obj_to_invoke\' has to be either a \'DoFn\' or '
                        'a \'RestrictionProvider\'. Received %r instead.'
                        % obj_to_invoke)
 
-    args, _, _, defaults = core.get_function_arguments(
+    fullargspec = core.get_function_arguments(
         obj_to_invoke, method_name)
+
+    # TODO(BEAM-5878) support kwonlyargs on Python 3.
+    args = fullargspec[0]
+    defaults = fullargspec[3]
+
     defaults = defaults if defaults else []
     method_value = getattr(obj_to_invoke, method_name)
     self.method_value = method_value
     self.args = args
     self.defaults = defaults
 
     self.has_userstate_arguments = False
```

## Comparing `apache-beam-2.8.0/apache_beam/runners/common_test.py` & `apache-beam-2.9.0/apache_beam/runners/common_test.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/runners/runner.py` & `apache-beam-2.9.0/apache_beam/runners/runner.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/runners/test/__init__.py` & `apache-beam-2.9.0/apache_beam/runners/test/__init__.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/runners/direct/test_direct_runner.py` & `apache-beam-2.9.0/apache_beam/runners/direct/test_direct_runner.py`

 * *Files 6% similar despite different names*

```diff
@@ -48,9 +48,10 @@
 
       if on_success_matcher:
         from hamcrest import assert_that as hc_assert_that
         hc_assert_that(self.result, pickler.loads(on_success_matcher))
     finally:
       if not PipelineState.is_terminal(self.result.state):
         self.result.cancel()
+        self.result.wait_until_finish()
 
     return self.result
```

## Comparing `apache-beam-2.8.0/apache_beam/runners/direct/bundle_factory.py` & `apache-beam-2.9.0/apache_beam/runners/direct/bundle_factory.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/runners/direct/__init__.py` & `apache-beam-2.9.0/apache_beam/runners/direct/__init__.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/runners/direct/direct_metrics_test.py` & `apache-beam-2.9.0/apache_beam/runners/direct/direct_metrics_test.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/runners/direct/helper_transforms.py` & `apache-beam-2.9.0/apache_beam/runners/direct/helper_transforms.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/runners/direct/sdf_direct_runner_test.py` & `apache-beam-2.9.0/apache_beam/runners/direct/sdf_direct_runner_test.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/runners/direct/direct_metrics.py` & `apache-beam-2.9.0/apache_beam/runners/direct/direct_metrics.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/runners/direct/consumer_tracking_pipeline_visitor.py` & `apache-beam-2.9.0/apache_beam/runners/direct/consumer_tracking_pipeline_visitor.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/runners/direct/watermark_manager.py` & `apache-beam-2.9.0/apache_beam/runners/direct/watermark_manager.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/runners/direct/direct_runner.py` & `apache-beam-2.9.0/apache_beam/runners/direct/direct_runner.py`

 * *Files 2% similar despite different names*

```diff
@@ -21,14 +21,15 @@
 graph of transformations belonging to a pipeline on the local machine.
 """
 
 from __future__ import absolute_import
 
 import itertools
 import logging
+import time
 
 from google.protobuf import wrappers_pb2
 
 import apache_beam as beam
 from apache_beam import coders
 from apache_beam import typehints
 from apache_beam.internal.util import ArgumentPlaceholder
@@ -260,56 +261,60 @@
 
   def expand(self, pvalue):
     # This is handled as a native transform.
     return PCollection(self.pipeline)
 
 
 class _DirectWriteToPubSubFn(DoFn):
-  _topic = None
+  BUFFER_SIZE_ELEMENTS = 100
+  FLUSH_TIMEOUT_SECS = BUFFER_SIZE_ELEMENTS * 0.5
 
   def __init__(self, sink):
     self.project = sink.project
-    self.topic_name = sink.topic_name
+    self.short_topic_name = sink.topic_name
     self.id_label = sink.id_label
     self.timestamp_attribute = sink.timestamp_attribute
     self.with_attributes = sink.with_attributes
 
     # TODO(BEAM-4275): Add support for id_label and timestamp_attribute.
     if sink.id_label:
       raise NotImplementedError('DirectRunner: id_label is not supported for '
                                 'PubSub writes')
     if sink.timestamp_attribute:
       raise NotImplementedError('DirectRunner: timestamp_attribute is not '
                                 'supported for PubSub writes')
 
   def start_bundle(self):
-    from google.cloud import pubsub
-
-    if self._topic is None:
-      self._topic = pubsub.Client(project=self.project).topic(
-          self.topic_name)
     self._buffer = []
 
   def process(self, elem):
     self._buffer.append(elem)
-    if len(self._buffer) >= 100:
+    if len(self._buffer) >= self.BUFFER_SIZE_ELEMENTS:
       self._flush()
 
   def finish_bundle(self):
     self._flush()
 
   def _flush(self):
-    if self._buffer:
-      with self._topic.batch() as batch:
-        for elem in self._buffer:
-          if self.with_attributes:
-            batch.publish(elem.data, **elem.attributes)
-          else:
-            batch.publish(elem)
-      self._buffer = []
+    from google.cloud import pubsub
+    pub_client = pubsub.PublisherClient()
+    topic = pub_client.topic_path(self.project, self.short_topic_name)
+
+    if self.with_attributes:
+      futures = [pub_client.publish(topic, elem.data, **elem.attributes)
+                 for elem in self._buffer]
+    else:
+      futures = [pub_client.publish(topic, elem)
+                 for elem in self._buffer]
+
+    timer_start = time.time()
+    for future in futures:
+      remaining = self.FLUSH_TIMEOUT_SECS - (time.time() - timer_start)
+      future.result(remaining)
+    self._buffer = []
 
 
 def _get_pubsub_transform_overrides(pipeline_options):
   from apache_beam.io.gcp import pubsub as beam_pubsub
   from apache_beam.pipeline import PTransformOverride
 
   class ReadFromPubSubOverride(PTransformOverride):
```

## Comparing `apache-beam-2.8.0/apache_beam/runners/direct/executor.py` & `apache-beam-2.9.0/apache_beam/runners/direct/executor.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/runners/direct/clock.py` & `apache-beam-2.9.0/apache_beam/runners/direct/clock.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/runners/direct/consumer_tracking_pipeline_visitor_test.py` & `apache-beam-2.9.0/apache_beam/runners/direct/consumer_tracking_pipeline_visitor_test.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/runners/direct/transform_evaluator.py` & `apache-beam-2.9.0/apache_beam/runners/direct/transform_evaluator.py`

 * *Files 2% similar despite different names*

```diff
@@ -373,97 +373,117 @@
       hold = self.watermark
 
     return TransformResult(
         self, self.bundles, unprocessed_bundles, None, {None: hold})
 
 
 class _PubSubSubscriptionWrapper(object):
-  """Wrapper for garbage-collecting temporary PubSub subscriptions."""
+  """Wrapper for managing temporary PubSub subscriptions."""
 
-  def __init__(self, subscription, should_cleanup):
-    self.subscription = subscription
-    self.should_cleanup = should_cleanup
+  def __init__(self, project, short_topic_name, short_sub_name):
+    """Initialize subscription wrapper.
+
+    If sub_name is None, will create a temporary subscription to topic_name.
+
+    Args:
+      project: GCP project name for topic and subscription. May be None.
+        Required if sub_name is None.
+      short_topic_name: Valid topic name without
+        'projects/{project}/topics/' prefix. May be None.
+        Required if sub_name is None.
+      short_sub_name: Valid subscription name without
+        'projects/{project}/subscriptions/' prefix. May be None.
+    """
+    from google.cloud import pubsub
+    self.sub_client = pubsub.SubscriberClient()
+
+    if short_sub_name is None:
+      self.sub_name = self.sub_client.subscription_path(
+          project, 'beam_%d_%x' % (int(time.time()), random.randrange(1 << 32)))
+      topic_name = self.sub_client.topic_path(project, short_topic_name)
+      self.sub_client.create_subscription(self.sub_name, topic_name)
+      self._should_cleanup = True
+    else:
+      self.sub_name = self.sub_client.subscription_path(project, short_sub_name)
+      self._should_cleanup = False
 
   def __del__(self):
-    if self.should_cleanup:
-      self.subscription.delete()
+    if self._should_cleanup:
+      self.sub_client.delete_subscription(self.sub_name)
 
 
 class _PubSubReadEvaluator(_TransformEvaluator):
   """TransformEvaluator for PubSub read."""
 
+  # A mapping of transform to _PubSubSubscriptionWrapper.
   _subscription_cache = {}
 
   def __init__(self, evaluation_context, applied_ptransform,
                input_committed_bundle, side_inputs):
     assert not side_inputs
     super(_PubSubReadEvaluator, self).__init__(
         evaluation_context, applied_ptransform, input_committed_bundle,
         side_inputs)
 
     self.source = self._applied_ptransform.transform._source
     if self.source.id_label:
       raise NotImplementedError(
           'DirectRunner: id_label is not supported for PubSub reads')
-    self._subscription = _PubSubReadEvaluator.get_subscription(
+    self._sub_name = _PubSubReadEvaluator.get_subscription(
         self._applied_ptransform, self.source.project, self.source.topic_name,
         self.source.subscription_name)
 
   @classmethod
-  def get_subscription(cls, transform, project, topic, subscription_name):
+  def get_subscription(cls, transform, project, topic, short_sub_name):
     if transform not in cls._subscription_cache:
-      from google.cloud import pubsub
-      should_create = not subscription_name
-      if should_create:
-        subscription_name = 'beam_%d_%x' % (
-            int(time.time()), random.randrange(1 << 32))
-      wrapper = _PubSubSubscriptionWrapper(
-          pubsub.Client(project=project).topic(topic).subscription(
-              subscription_name),
-          should_create)
-      if should_create:
-        wrapper.subscription.create()
+      wrapper = _PubSubSubscriptionWrapper(project, topic, short_sub_name)
       cls._subscription_cache[transform] = wrapper
-    return cls._subscription_cache[transform].subscription
+    return cls._subscription_cache[transform].sub_name
 
   def start_bundle(self):
     pass
 
   def process_element(self, element):
     pass
 
   def _read_from_pubsub(self, timestamp_attribute):
     from apache_beam.io.gcp.pubsub import PubsubMessage
     from google.cloud import pubsub
     # Because of the AutoAck, we are not able to reread messages if this
     # evaluator fails with an exception before emitting a bundle. However,
     # the DirectRunner currently doesn't retry work items anyway, so the
     # pipeline would enter an inconsistent state on any error.
-    with pubsub.subscription.AutoAck(
-        self._subscription, return_immediately=True,
-        max_messages=10) as results:
-      def _get_element(message):
-        parsed_message = PubsubMessage._from_message(message)
-        if (timestamp_attribute and
-            timestamp_attribute in parsed_message.attributes):
-          rfc3339_or_milli = parsed_message.attributes[timestamp_attribute]
+    sub_client = pubsub.SubscriberClient()
+    response = sub_client.pull(self._sub_name, max_messages=10,
+                               return_immediately=True)
+
+    def _get_element(message):
+      parsed_message = PubsubMessage._from_message(message)
+      if (timestamp_attribute and
+          timestamp_attribute in parsed_message.attributes):
+        rfc3339_or_milli = parsed_message.attributes[timestamp_attribute]
+        try:
+          timestamp = Timestamp.from_rfc3339(rfc3339_or_milli)
+        except ValueError:
           try:
-            timestamp = Timestamp.from_rfc3339(rfc3339_or_milli)
-          except ValueError:
-            try:
-              timestamp = Timestamp(micros=int(rfc3339_or_milli) * 1000)
-            except ValueError as e:
-              raise ValueError('Bad timestamp value: %s' % e)
-        else:
-          timestamp = Timestamp.from_rfc3339(message.service_timestamp)
-
-        return timestamp, parsed_message
+            timestamp = Timestamp(micros=int(rfc3339_or_milli) * 1000)
+          except ValueError as e:
+            raise ValueError('Bad timestamp value: %s' % e)
+      else:
+        timestamp = Timestamp(message.publish_time.seconds,
+                              message.publish_time.nanos // 1000)
+
+      return timestamp, parsed_message
+
+    results = [_get_element(rm.message) for rm in response.received_messages]
+    ack_ids = [rm.ack_id for rm in response.received_messages]
+    if ack_ids:
+      sub_client.acknowledge(self._sub_name, ack_ids)
 
-      return [_get_element(message)
-              for unused_ack_id, message in iteritems(results)]
+    return results
 
   def finish_bundle(self):
     data = self._read_from_pubsub(self.source.timestamp_attribute)
     if data:
       output_pcollection = list(self._outputs)[0]
       bundle = self._evaluation_context.create_bundle(output_pcollection)
       # TODO(ccy): Respect the PubSub source's id_label field.
```

## Comparing `apache-beam-2.8.0/apache_beam/runners/direct/evaluation_context.py` & `apache-beam-2.9.0/apache_beam/runners/direct/evaluation_context.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/runners/direct/direct_userstate.py` & `apache-beam-2.9.0/apache_beam/runners/direct/direct_userstate.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/runners/direct/util.py` & `apache-beam-2.9.0/apache_beam/runners/direct/util.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/runners/direct/sdf_direct_runner.py` & `apache-beam-2.9.0/apache_beam/runners/direct/sdf_direct_runner.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/runners/interactive/pipeline_analyzer.py` & `apache-beam-2.9.0/apache_beam/runners/interactive/pipeline_analyzer.py`

 * *Files 1% similar despite different names*

```diff
@@ -50,15 +50,16 @@
 
     self._pipeline = beam.pipeline.Pipeline.from_runner_api(
         self._pipeline_proto,
         runner=underlying_runner,
         options=options)
     # context returned from to_runner_api is more informative than that returned
     # from from_runner_api.
-    _, self._context = self._pipeline.to_runner_api(return_context=True)
+    _, self._context = self._pipeline.to_runner_api(
+        return_context=True, use_fake_coders=True)
     self._pipeline_info = PipelineInfo(self._pipeline_proto.components)
 
     # Result of the analysis that can be queried by the user.
     self._pipeline_proto_to_execute = None
     self._top_level_referenced_pcoll_ids = None
     self._top_level_required_transforms = None
 
@@ -428,14 +429,18 @@
 
     def __eq__(self, other):
       if isinstance(other, self.Derivation):
         # pylint: disable=protected-access
         return (self._inputs == other._inputs and
                 self._transform_info == other._transform_info)
 
+    def __ne__(self, other):
+      # TODO(BEAM-5949): Needed for Python 2 compatibility.
+      return not self == other
+
     def __hash__(self):
       if self._hash is None:
         self._hash = (hash(tuple(sorted(self._transform_info.items())))
                       + sum(hash(tag) * hash(input)
                             for tag, input in self._inputs.items())
                       + hash(self._output_tag))
       return self._hash
```

## Comparing `apache-beam-2.8.0/apache_beam/runners/interactive/__init__.py` & `apache-beam-2.9.0/apache_beam/runners/internal/__init__.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/runners/interactive/pipeline_analyzer_test.py` & `apache-beam-2.9.0/apache_beam/runners/interactive/pipeline_analyzer_test.py`

 * *Files 1% similar despite different names*

```diff
@@ -34,17 +34,17 @@
 from apache_beam.runners.interactive import pipeline_analyzer
 
 
 def to_stable_runner_api(p):
   """The extra round trip ensures a stable pipeline proto.
   """
   return (beam.pipeline.Pipeline.from_runner_api(
-      p.to_runner_api(),
+      p.to_runner_api(use_fake_coders=True),
       p.runner,
-      p._options).to_runner_api())
+      p._options).to_runner_api(use_fake_coders=True))
 
 
 class PipelineAnalyzerTest(unittest.TestCase):
 
   def setUp(self):
     self.runner = direct_runner.DirectRunner()
     self.cache_manager = cache.FileBasedCacheManager()
```

## Comparing `apache-beam-2.8.0/apache_beam/runners/interactive/interactive_runner_test.py` & `apache-beam-2.9.0/apache_beam/runners/interactive/interactive_runner_test.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/runners/interactive/interactive_runner.py` & `apache-beam-2.9.0/apache_beam/runners/interactive/interactive_runner.py`

 * *Files 1% similar despite different names*

```diff
@@ -105,21 +105,21 @@
   def run_pipeline(self, pipeline):
     if not hasattr(self, '_desired_cache_labels'):
       self._desired_cache_labels = set()
 
     # Invoke a round trip through the runner API. This makes sure the Pipeline
     # proto is stable.
     pipeline = beam.pipeline.Pipeline.from_runner_api(
-        pipeline.to_runner_api(),
+        pipeline.to_runner_api(use_fake_coders=True),
         pipeline.runner,
         pipeline._options)
 
     # Snapshot the pipeline in a portable proto before mutating it.
     pipeline_proto, original_context = pipeline.to_runner_api(
-        return_context=True)
+        return_context=True, use_fake_coders=True)
     pcolls_to_pcoll_id = self._pcolls_to_pcoll_id(pipeline, original_context)
 
     analyzer = pipeline_analyzer.PipelineAnalyzer(self._cache_manager,
                                                   pipeline_proto,
                                                   self._underlying_runner,
                                                   pipeline._options,
                                                   self._desired_cache_labels)
```

## Comparing `apache-beam-2.8.0/apache_beam/runners/interactive/cache_manager.py` & `apache-beam-2.9.0/apache_beam/runners/interactive/cache_manager.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/runners/interactive/cache_manager_test.py` & `apache-beam-2.9.0/apache_beam/runners/interactive/cache_manager_test.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/runners/interactive/display/display_manager.py` & `apache-beam-2.9.0/apache_beam/runners/interactive/display/display_manager.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/runners/interactive/display/interactive_pipeline_graph.py` & `apache-beam-2.9.0/apache_beam/runners/interactive/display/interactive_pipeline_graph.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/runners/interactive/display/pipeline_graph_renderer.py` & `apache-beam-2.9.0/apache_beam/runners/interactive/display/pipeline_graph_renderer.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/runners/interactive/display/__init__.py` & `apache-beam-2.9.0/apache_beam/runners/interactive/__init__.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/runners/interactive/display/pipeline_graph.py` & `apache-beam-2.9.0/apache_beam/runners/interactive/display/pipeline_graph.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/runners/job/manager.py` & `apache-beam-2.9.0/apache_beam/runners/job/manager.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/runners/job/utils.py` & `apache-beam-2.9.0/apache_beam/runners/job/utils.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/runners/job/__init__.py` & `apache-beam-2.9.0/apache_beam/runners/job/__init__.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/runners/dataflow/dataflow_metrics_test.py` & `apache-beam-2.9.0/apache_beam/runners/dataflow/dataflow_metrics_test.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/runners/dataflow/dataflow_runner.py` & `apache-beam-2.9.0/apache_beam/runners/dataflow/dataflow_runner.py`

 * *Files 1% similar despite different names*

```diff
@@ -51,14 +51,15 @@
 from apache_beam.runners.dataflow.internal.clients import dataflow as dataflow_api
 from apache_beam.runners.dataflow.internal.names import PropertyNames
 from apache_beam.runners.dataflow.internal.names import TransformNames
 from apache_beam.runners.runner import PipelineResult
 from apache_beam.runners.runner import PipelineRunner
 from apache_beam.runners.runner import PipelineState
 from apache_beam.runners.runner import PValueCache
+from apache_beam.transforms import window
 from apache_beam.transforms.display import DisplayData
 from apache_beam.typehints import typehints
 from apache_beam.utils import proto_utils
 from apache_beam.utils.plugin import BeamPlugin
 
 __all__ = ['DataflowRunner']
 
@@ -170,15 +171,15 @@
       page_token = None
       while True:
         messages, page_token = runner.dataflow_client.list_messages(
             job_id, page_token=page_token, start_time=last_message_time)
         for m in messages:
           message = '%s: %s: %s' % (m.time, m.messageImportance, m.messageText)
 
-          if m.time > last_message_time:
+          if not last_message_time or m.time > last_message_time:
             last_message_time = m.time
             current_seen_messages = set()
 
           if message in current_seen_messages:
             # Skip the message if it has already been seen at the current
             # time. This could be the case since the list_messages API is
             # queried starting at last_message_time.
@@ -330,31 +331,23 @@
     # any added PTransforms.
     pipeline.replace_all(DataflowRunner._PTRANSFORM_OVERRIDES)
 
     # Snapshot the pipeline in a portable proto.
     self.proto_pipeline, self.proto_context = pipeline.to_runner_api(
         return_context=True)
 
-    # TODO(BEAM-2717): Remove once Coders are already in proto.
-    for pcoll in self.proto_pipeline.components.pcollections.values():
-      if pcoll.coder_id not in self.proto_context.coders:
-        coder = coders.registry.get_coder(pickler.loads(pcoll.coder_id))
-        pcoll.coder_id = self.proto_context.coders.get_id(coder)
-    self.proto_context.coders.populate_map(
-        self.proto_pipeline.components.coders)
-
     # Add setup_options for all the BeamPlugin imports
     setup_options = pipeline._options.view_as(SetupOptions)
     plugins = BeamPlugin.get_all_plugin_paths()
     if setup_options.beam_plugins is not None:
       plugins = list(set(plugins + setup_options.beam_plugins))
     setup_options.beam_plugins = plugins
 
     # Elevate "min_cpu_platform" to pipeline option, but using the existing
-    # experiment
+    # experiment.
     debug_options = pipeline._options.view_as(DebugOptions)
     worker_options = pipeline._options.view_as(WorkerOptions)
     if worker_options.min_cpu_platform:
       experiments = ["min_cpu_platform=%s" % worker_options.min_cpu_platform]
       if debug_options.experiments is not None:
         experiments = list(set(experiments + debug_options.experiments))
       debug_options.experiments = experiments
@@ -377,14 +370,26 @@
     if test_options.dry_run:
       return None
 
     # Get a Dataflow API client and set its options
     self.dataflow_client = apiclient.DataflowApplicationClient(
         pipeline._options)
 
+    dataflow_worker_jar = getattr(worker_options, 'dataflow_worker_jar', None)
+    if dataflow_worker_jar is not None:
+      if not apiclient._use_fnapi(pipeline._options):
+        logging.fatal(
+            'Typical end users should not use this worker jar feature. '
+            'It can only be used when fnapi is enabled.')
+
+      experiments = ["use_staged_dataflow_worker_jar"]
+      if debug_options.experiments is not None:
+        experiments = list(set(experiments + debug_options.experiments))
+      debug_options.experiments = experiments
+
     # Create the job description and send a request to the service. The result
     # can be None if there is no need to send a request to the service (e.g.
     # template creation). If a request was sent and failed then the call will
     # raise an exception.
     result = DataflowPipelineResult(
         self.dataflow_client.create_job(self.job), self)
 
@@ -503,30 +508,36 @@
         PropertyNames.WINDOWING_STRATEGY,
         self.serialize_windowing_strategy(windowing_strategy))
     return step
 
   def run_Impulse(self, transform_node):
     standard_options = (
         transform_node.outputs[None].pipeline._options.view_as(StandardOptions))
+    step = self._add_step(
+        TransformNames.READ, transform_node.full_label, transform_node)
     if standard_options.streaming:
-      step = self._add_step(
-          TransformNames.READ, transform_node.full_label, transform_node)
       step.add_property(PropertyNames.FORMAT, 'pubsub')
       step.add_property(PropertyNames.PUBSUB_SUBSCRIPTION, '_starting_signal/')
-
-      step.encoding = self._get_encoded_output_coder(transform_node)
-      step.add_property(
-          PropertyNames.OUTPUT_INFO,
-          [{PropertyNames.USER_NAME: (
-              '%s.%s' % (
-                  transform_node.full_label, PropertyNames.OUT)),
-            PropertyNames.ENCODING: step.encoding,
-            PropertyNames.OUTPUT_NAME: PropertyNames.OUT}])
     else:
-      ValueError('Impulse source for batch pipelines has not been defined.')
+      step.add_property(PropertyNames.FORMAT, 'impulse')
+      encoded_impulse_element = coders.WindowedValueCoder(
+          coders.BytesCoder(),
+          coders.coders.GlobalWindowCoder()).get_impl().encode_nested(
+              window.GlobalWindows.windowed_value(b''))
+      step.add_property(PropertyNames.IMPULSE_ELEMENT,
+                        self.byte_array_to_json_string(encoded_impulse_element))
+
+    step.encoding = self._get_encoded_output_coder(transform_node)
+    step.add_property(
+        PropertyNames.OUTPUT_INFO,
+        [{PropertyNames.USER_NAME: (
+            '%s.%s' % (
+                transform_node.full_label, PropertyNames.OUT)),
+          PropertyNames.ENCODING: step.encoding,
+          PropertyNames.OUTPUT_NAME: PropertyNames.OUT}])
 
   def run_Flatten(self, transform_node):
     step = self._add_step(TransformNames.FLATTEN,
                           transform_node.full_label, transform_node)
     inputs = []
     for one_input in transform_node.inputs:
       input_step = self._cache.get_pvalue(one_input)
@@ -777,17 +788,26 @@
     outputs.append(
         {PropertyNames.USER_NAME: (
             '%s.%s' % (transform_node.full_label, PropertyNames.OUT)),
          PropertyNames.ENCODING: step.encoding,
          PropertyNames.OUTPUT_NAME: PropertyNames.OUT})
     step.add_property(PropertyNames.OUTPUT_INFO, outputs)
 
-  def apply_Read(self, unused_transform, pbegin):
-    # Always consider Read to be a primitive for dataflow.
-    return beam.pvalue.PCollection(pbegin.pipeline)
+  def apply_Read(self, transform, pbegin):
+    if hasattr(transform.source, 'format'):
+      # Consider native Read to be a primitive for dataflow.
+      return beam.pvalue.PCollection(pbegin.pipeline)
+    else:
+      options = pbegin.pipeline.options.view_as(DebugOptions)
+      if options.experiments and 'beam_fn_api' in options.experiments:
+        # Expand according to FnAPI primitives.
+        return self.apply_PTransform(transform, pbegin)
+      else:
+        # Custom Read is also a primitive for non-FnAPI on dataflow.
+        return beam.pvalue.PCollection(pbegin.pipeline)
 
   def run_Read(self, transform_node):
     transform = transform_node.transform
     step = self._add_step(
         TransformNames.READ, transform_node.full_label, transform_node)
     # TODO(mairbek): refactor if-else tree to use registerable functions.
     # Initialize the source specific properties.
```

## Comparing `apache-beam-2.8.0/apache_beam/runners/dataflow/dataflow_metrics.py` & `apache-beam-2.9.0/apache_beam/runners/dataflow/dataflow_metrics.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/runners/dataflow/__init__.py` & `apache-beam-2.9.0/apache_beam/runners/dataflow/__init__.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/runners/dataflow/test_dataflow_runner.py` & `apache-beam-2.9.0/apache_beam/runners/dataflow/test_dataflow_runner.py`

 * *Files 10% similar despite different names*

```diff
@@ -28,15 +28,17 @@
 from apache_beam.options.pipeline_options import StandardOptions
 from apache_beam.options.pipeline_options import TestOptions
 from apache_beam.runners.dataflow.dataflow_runner import DataflowRunner
 from apache_beam.runners.runner import PipelineState
 
 __all__ = ['TestDataflowRunner']
 
-WAIT_TIMEOUT = 2 * 60
+# Dataflow take up to 10mins for the long tail of starting/stopping worker
+# pool.
+WAIT_IN_STATE_TIMEOUT = 10 * 60
 
 
 class TestDataflowRunner(DataflowRunner):
   def run_pipeline(self, pipeline):
     """Execute test pipeline and verify test matcher"""
     options = pipeline._options.view_as(TestOptions)
     on_success_matcher = options.on_success_matcher
@@ -47,51 +49,51 @@
     # send this option to remote executors.
     options.on_success_matcher = None
 
     self.result = super(TestDataflowRunner, self).run_pipeline(pipeline)
     if self.result.has_job:
       # TODO(markflyhigh)(BEAM-1890): Use print since Nose dosen't show logs
       # in some cases.
-      print('Found: %s.' % self.build_console_url(pipeline.options))
+      print('Found: %s.' % self.build_console_url(options))
 
     try:
       self.wait_until_in_state(PipelineState.RUNNING)
 
       if is_streaming and not wait_duration:
         logging.warning('Waiting indefinitely for streaming job.')
       self.result.wait_until_finish(duration=wait_duration)
 
       if on_success_matcher:
         from hamcrest import assert_that as hc_assert_that
         hc_assert_that(self.result, pickler.loads(on_success_matcher))
     finally:
       if not self.result.is_in_terminal_state():
         self.result.cancel()
-        self.wait_until_in_state(PipelineState.CANCELLED, timeout=300)
+        self.wait_until_in_state(PipelineState.CANCELLED)
 
     return self.result
 
   def build_console_url(self, options):
     """Build a console url of Dataflow job."""
     project = options.view_as(GoogleCloudOptions).project
     region_id = options.view_as(GoogleCloudOptions).region
     job_id = self.result.job_id()
     return (
         'https://console.cloud.google.com/dataflow/jobsDetail/locations'
         '/%s/jobs/%s?project=%s' % (region_id, job_id, project))
 
-  def wait_until_in_state(self, expected_state, timeout=WAIT_TIMEOUT):
-    """Wait until Dataflow pipeline terminate or enter RUNNING state."""
+  def wait_until_in_state(self, expected_state, timeout=WAIT_IN_STATE_TIMEOUT):
+    """Wait until Dataflow pipeline enters a certain state."""
     if not self.result.has_job:
       raise IOError('Failed to get the Dataflow job id.')
 
     start_time = time.time()
     while time.time() - start_time <= timeout:
       job_state = self.result.state
       if self.result.is_in_terminal_state() or job_state == expected_state:
         return job_state
       time.sleep(5)
 
     raise RuntimeError('Timeout after %d seconds while waiting for job %s '
                        'enters expected state %s. Current state is %s.' %
-                       (WAIT_TIMEOUT, self.result.job_id,
+                       (timeout, self.result.job_id(),
                         expected_state, self.result.state))
```

## Comparing `apache-beam-2.8.0/apache_beam/runners/dataflow/template_runner_test.py` & `apache-beam-2.9.0/apache_beam/runners/dataflow/template_runner_test.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/runners/dataflow/dataflow_runner_test.py` & `apache-beam-2.9.0/apache_beam/runners/dataflow/dataflow_runner_test.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/runners/dataflow/ptransform_overrides.py` & `apache-beam-2.9.0/apache_beam/runners/dataflow/ptransform_overrides.py`

 * *Files 1% similar despite different names*

```diff
@@ -43,8 +43,8 @@
 
   def get_replacement_transform(self, ptransform):
     # Imported here to avoid circular dependencies.
     # pylint: disable=wrong-import-order, wrong-import-position
     from apache_beam.runners.dataflow.native_io.streaming_create import \
       StreamingCreate
     coder = typecoders.registry.get_coder(ptransform.get_output_type())
-    return StreamingCreate(ptransform.value, coder)
+    return StreamingCreate(ptransform.values, coder)
```

## Comparing `apache-beam-2.8.0/apache_beam/runners/dataflow/native_io/__init__.py` & `apache-beam-2.9.0/apache_beam/runners/dataflow/internal/__init__.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/runners/dataflow/native_io/iobase_test.py` & `apache-beam-2.9.0/apache_beam/runners/dataflow/native_io/iobase_test.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/runners/dataflow/native_io/streaming_create.py` & `apache-beam-2.9.0/apache_beam/runners/dataflow/native_io/streaming_create.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/runners/dataflow/native_io/iobase.py` & `apache-beam-2.9.0/apache_beam/runners/dataflow/native_io/iobase.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/runners/dataflow/internal/names.py` & `apache-beam-2.9.0/apache_beam/runners/dataflow/internal/names.py`

 * *Files 11% similar despite different names*

```diff
@@ -18,48 +18,48 @@
 """Various names for properties, transforms, etc."""
 
 # All constants are for internal use only; no backwards-compatibility
 # guarantees.
 
 from __future__ import absolute_import
 
-# TODO (altay): Move shared names to a common location.
 # Standard file names used for staging files.
 from builtins import object
 
-PICKLED_MAIN_SESSION_FILE = 'pickled_main_session'
 DATAFLOW_SDK_TARBALL_FILE = 'dataflow_python_sdk.tar'
-STAGED_PIPELINE_FILENAME = "pipeline.pb"
-STAGED_PIPELINE_URL_METADATA_FIELD = "pipeline_url"
 
 # String constants related to sources framework
 SOURCE_FORMAT = 'custom_source'
 SOURCE_TYPE = 'CustomSourcesType'
 SERIALIZED_SOURCE_KEY = 'serialized_source'
 
 # In a released SDK, container tags are selected based on the SDK version.
 # Unreleased versions use container versions based on values of
 # BEAM_CONTAINER_VERSION and BEAM_FNAPI_CONTAINER_VERSION (see below).
 
 # Update this version to the next version whenever there is a change that will
 # require changes to legacy Dataflow worker execution environment.
-BEAM_CONTAINER_VERSION = 'beam-master-20180927'
+BEAM_CONTAINER_VERSION = 'beam-master-20181106'
 # Update this version to the next version whenever there is a change that
 # requires changes to SDK harness container or SDK harness launcher.
-BEAM_FNAPI_CONTAINER_VERSION = 'beam-master-20180920'
+BEAM_FNAPI_CONTAINER_VERSION = 'beam-master-20181106'
+
+# TODO(BEAM-5939): Remove these shared names once Dataflow worker is updated.
+PICKLED_MAIN_SESSION_FILE = 'pickled_main_session'
+STAGED_PIPELINE_FILENAME = "pipeline.pb"
+STAGED_PIPELINE_URL_METADATA_FIELD = "pipeline_url"
 
 # Package names for different distributions
-GOOGLE_PACKAGE_NAME = 'google-cloud-dataflow'
 BEAM_PACKAGE_NAME = 'apache-beam'
 
 # SDK identifiers for different distributions
-GOOGLE_SDK_NAME = 'Google Cloud Dataflow SDK for Python'
 BEAM_SDK_NAME = 'Apache Beam SDK for Python'
+# TODO(BEAM-5393): End duplicated constants (see above).
 
-DATAFLOW_CONTAINER_IMAGE_REPOSITORY = 'dataflow.gcr.io/v1beta3'
+DATAFLOW_CONTAINER_IMAGE_REPOSITORY = 'gcr.io/cloud-dataflow/v1beta3'
 
 
 class TransformNames(object):
   """For internal use only; no backwards-compatibility guarantees.
 
   Transform strings as they are expected in the CloudWorkflow protos."""
   COLLECTION_TO_SINGLETON = 'CollectionToSingleton'
@@ -91,14 +91,15 @@
   ELEMENTS = 'elements'
   ENCODING = 'encoding'
   FILE_PATTERN = 'filepattern'
   FILE_NAME_PREFIX = 'filename_prefix'
   FILE_NAME_SUFFIX = 'filename_suffix'
   FORMAT = 'format'
   INPUTS = 'inputs'
+  IMPULSE_ELEMENT = 'impulse_element'
   NON_PARALLEL_INPUTS = 'non_parallel_inputs'
   NUM_SHARDS = 'num_shards'
   OUT = 'out'
   OUTPUT = 'output'
   OUTPUT_INFO = 'output_info'
   OUTPUT_NAME = 'output_name'
   PARALLEL_INPUT = 'parallel_input'
```

## Comparing `apache-beam-2.8.0/apache_beam/runners/dataflow/internal/apiclient_test.py` & `apache-beam-2.9.0/apache_beam/runners/dataflow/internal/apiclient_test.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/runners/dataflow/internal/__init__.py` & `apache-beam-2.9.0/apache_beam/runners/dataflow/internal/clients/__init__.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/runners/dataflow/internal/apiclient.py` & `apache-beam-2.9.0/apache_beam/runners/dataflow/internal/apiclient.py`

 * *Files 2% similar despite different names*

```diff
@@ -20,26 +20,26 @@
 Dataflow client utility functions."""
 
 from __future__ import absolute_import
 
 from builtins import object
 import codecs
 import getpass
+import httplib2
 import json
 import logging
 import os
 import re
 import tempfile
 import time
 from datetime import datetime
 import io
 
 from past.builtins import unicode
 
-import pkg_resources
 from apitools.base.py import encoding
 from apitools.base.py import exceptions
 
 from apache_beam import version as beam_version
 from apache_beam.internal.gcp.auth import get_service_credentials
 from apache_beam.internal.gcp.json_value import to_json_value
 from apache_beam.io.filesystems import FileSystems
@@ -47,14 +47,15 @@
 from apache_beam.options.pipeline_options import DebugOptions
 from apache_beam.options.pipeline_options import GoogleCloudOptions
 from apache_beam.options.pipeline_options import StandardOptions
 from apache_beam.options.pipeline_options import WorkerOptions
 from apache_beam.runners.dataflow.internal import names
 from apache_beam.runners.dataflow.internal.clients import dataflow
 from apache_beam.runners.dataflow.internal.names import PropertyNames
+from apache_beam.runners.internal import names as shared_names
 from apache_beam.runners.portability.stager import Stager
 from apache_beam.transforms import cy_combiners
 from apache_beam.transforms import DataflowDistributionCounter
 from apache_beam.transforms.display import DisplayData
 from apache_beam.utils import retry
 
 # Environment version information. It is passed to the service during a
@@ -145,22 +146,20 @@
     self.proto.userAgent = dataflow.Environment.UserAgentValue()
     self.local = 'localhost' in self.google_cloud_options.dataflow_endpoint
 
     if self.google_cloud_options.service_account_email:
       self.proto.serviceAccountEmail = (
           self.google_cloud_options.service_account_email)
 
-    sdk_name, version_string = get_sdk_name_and_version()
-
     self.proto.userAgent.additionalProperties.extend([
         dataflow.Environment.UserAgentValue.AdditionalProperty(
             key='name',
-            value=to_json_value(sdk_name)),
+            value=to_json_value(shared_names.BEAM_SDK_NAME)),
         dataflow.Environment.UserAgentValue.AdditionalProperty(
-            key='version', value=to_json_value(version_string))])
+            key='version', value=to_json_value(beam_version.__version__))])
     # Version information.
     self.proto.version = dataflow.Environment.VersionValue()
     if self.standard_options.streaming:
       job_type = 'FNAPI_STREAMING'
     else:
       if _use_fnapi(options):
         job_type = 'FNAPI_BATCH'
@@ -379,28 +378,34 @@
           self.google_cloud_options.temp_location, path_suffix)
 
     self.proto = dataflow.Job(name=self.google_cloud_options.job_name)
     if self.options.view_as(StandardOptions).streaming:
       self.proto.type = dataflow.Job.TypeValueValuesEnum.JOB_TYPE_STREAMING
     else:
       self.proto.type = dataflow.Job.TypeValueValuesEnum.JOB_TYPE_BATCH
+    if self.google_cloud_options.update:
+      self.proto.replaceJobId = self.job_id_for_name(self.proto.name)
 
     # Labels.
     if self.google_cloud_options.labels:
       self.proto.labels = dataflow.Job.LabelsValue()
       for label in self.google_cloud_options.labels:
         parts = label.split('=', 1)
         key = parts[0]
         value = parts[1] if len(parts) > 1 else ''
         self.proto.labels.additionalProperties.append(
             dataflow.Job.LabelsValue.AdditionalProperty(key=key, value=value))
 
     self.base64_str_re = re.compile(r'^[A-Za-z0-9+/]*=*$')
     self.coder_str_re = re.compile(r'^([A-Za-z]+\$)([A-Za-z0-9+/]*=*)$')
 
+  def job_id_for_name(self, job_name):
+    return DataflowApplicationClient(
+        self.google_cloud_options).job_id_for_name(job_name)
+
   def json(self):
     return encoding.MessageToJson(self.proto)
 
   def __reduce__(self):
     """Reduce hook for pickling the Job class more easily."""
     return (Job, (self.options,))
 
@@ -418,22 +423,27 @@
     else:
       self.environment_version = _LEGACY_ENVIRONMENT_MAJOR_VERSION
 
     if self.google_cloud_options.no_auth:
       credentials = None
     else:
       credentials = get_service_credentials()
+
+    # Use 60 second socket timeout avoid hangs during network flakiness.
+    http_client = httplib2.Http(timeout=60)
     self._client = dataflow.DataflowV1b3(
         url=self.google_cloud_options.dataflow_endpoint,
         credentials=credentials,
-        get_credentials=(not self.google_cloud_options.no_auth))
+        get_credentials=(not self.google_cloud_options.no_auth),
+        http=http_client)
     self._storage_client = storage.StorageV1(
         url='https://www.googleapis.com/storage/v1',
         credentials=credentials,
-        get_credentials=(not self.google_cloud_options.no_auth))
+        get_credentials=(not self.google_cloud_options.no_auth),
+        http=http_client)
 
   # TODO(silviuc): Refactor so that retry logic can be applied.
   @retry.no_retries  # Using no_retries marks this as an integration point.
   def _gcs_file_copy(self, from_path, to_path):
     to_folder, to_name = os.path.split(to_path)
     with open(from_path, 'rb') as f:
       self.stage_file(to_folder, to_name, f)
@@ -508,23 +518,23 @@
     return None
 
   def create_job_description(self, job):
     """Creates a job described by the workflow proto."""
 
     # Stage the pipeline for the runner harness
     self.stage_file(job.google_cloud_options.staging_location,
-                    names.STAGED_PIPELINE_FILENAME,
+                    shared_names.STAGED_PIPELINE_FILENAME,
                     io.BytesIO(job.proto_pipeline.SerializeToString()))
 
     # Stage other resources for the SDK harness
     resources = self._stage_resources(job.options)
 
     job.proto.environment = Environment(
         pipeline_url=FileSystems.join(job.google_cloud_options.staging_location,
-                                      names.STAGED_PIPELINE_FILENAME),
+                                      shared_names.STAGED_PIPELINE_FILENAME),
         packages=resources, options=job.options,
         environment_version=self.environment_version).proto
     logging.debug('JOB: %s', job)
 
   @retry.with_exponential_backoff(num_retries=3, initial_delay_secs=3)
   def get_job_metrics(self, job_id):
     request = dataflow.DataflowProjectsLocationsJobsGetMetricsRequest()
@@ -710,14 +720,31 @@
       else:
         raise RuntimeError(
             'Unexpected value for minimum_importance argument: %r'
             % minimum_importance)
     response = self._client.projects_locations_jobs_messages.List(request)
     return response.jobMessages, response.nextPageToken
 
+  def job_id_for_name(self, job_name):
+    token = None
+    while True:
+      request = dataflow.DataflowProjectsLocationsJobsListRequest(
+          projectId=self.google_cloud_options.project,
+          location=self.google_cloud_options.region,
+          pageToken=token)
+      response = self._client.projects_locations_jobs.List(request)
+      for job in response.jobs:
+        if (job.name == job_name
+            and job.currentState
+            == dataflow.Job.CurrentStateValueValuesEnum.JOB_STATE_RUNNING):
+          return job.id
+      token = response.nextPageToken
+      if token is None:
+        raise ValueError("No running job found with name '%s'" % job_name)
+
 
 class MetricUpdateTranslators(object):
   """Translators between accumulators and dataflow metric updates."""
 
   @staticmethod
   def translate_boolean(accumulator, metric_update_proto):
     metric_update_proto.boolean = accumulator.value
@@ -764,19 +791,15 @@
 
   @staticmethod
   def get_sdk_package_name():
     """For internal use only; no backwards-compatibility guarantees.
 
           Returns the PyPI package name to be staged to Google Cloud Dataflow.
     """
-    sdk_name, _ = get_sdk_name_and_version()
-    if sdk_name == names.GOOGLE_SDK_NAME:
-      return names.GOOGLE_PACKAGE_NAME
-    else:
-      return names.BEAM_PACKAGE_NAME
+    return shared_names.BEAM_PACKAGE_NAME
 
 
 def to_split_int(n):
   res = dataflow.SplitInt64()
   res.lowBits = n & 0xffffffff
   res.highBits = n >> 32
   return res
@@ -820,25 +843,14 @@
   standard_options = pipeline_options.view_as(StandardOptions)
   debug_options = pipeline_options.view_as(DebugOptions)
 
   return standard_options.streaming or (
       debug_options.experiments and 'beam_fn_api' in debug_options.experiments)
 
 
-def get_sdk_name_and_version():
-  """For internal use only; no backwards-compatibility guarantees.
-
-    Returns name and version of SDK reported to Google Cloud Dataflow."""
-  try:
-    pkg_resources.get_distribution(names.GOOGLE_PACKAGE_NAME)
-    return (names.GOOGLE_SDK_NAME, beam_version.__version__)
-  except pkg_resources.DistributionNotFound:
-    return (names.BEAM_SDK_NAME, beam_version.__version__)
-
-
 def get_default_container_image_for_current_sdk(job_type):
   """For internal use only; no backwards-compatibility guarantees.
 
     Args:
       job_type (str): BEAM job type.
 
     Returns:
```

## Comparing `apache-beam-2.8.0/apache_beam/runners/dataflow/internal/clients/__init__.py` & `apache-beam-2.9.0/apache_beam/runners/dataflow/native_io/__init__.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/runners/dataflow/internal/clients/dataflow/dataflow_v1b3_client.py` & `apache-beam-2.9.0/apache_beam/runners/dataflow/internal/clients/dataflow/dataflow_v1b3_client.py`

 * *Files 1% similar despite different names*

```diff
@@ -41,24 +41,25 @@
   _URL_VERSION = u'v1b3'
   _API_KEY = None
 
   def __init__(self, url='', credentials=None,
                get_credentials=True, http=None, model=None,
                log_request=False, log_response=False,
                credentials_args=None, default_global_params=None,
-               additional_http_headers=None):
+               additional_http_headers=None, response_encoding=None):
     """Create a new dataflow handle."""
     url = url or self.BASE_URL
     super(DataflowV1b3, self).__init__(
         url, credentials=credentials,
         get_credentials=get_credentials, http=http, model=model,
         log_request=log_request, log_response=log_response,
         credentials_args=credentials_args,
         default_global_params=default_global_params,
-        additional_http_headers=additional_http_headers)
+        additional_http_headers=additional_http_headers,
+        response_encoding=response_encoding)
     self.projects_jobs_debug = self.ProjectsJobsDebugService(self)
     self.projects_jobs_messages = self.ProjectsJobsMessagesService(self)
     self.projects_jobs_workItems = self.ProjectsJobsWorkItemsService(self)
     self.projects_jobs = self.ProjectsJobsService(self)
     self.projects_locations_jobs_debug = self.ProjectsLocationsJobsDebugService(self)
     self.projects_locations_jobs_messages = self.ProjectsLocationsJobsMessagesService(self)
     self.projects_locations_jobs_workItems = self.ProjectsLocationsJobsWorkItemsService(self)
@@ -75,15 +76,15 @@
 
     def __init__(self, client):
       super(DataflowV1b3.ProjectsJobsDebugService, self).__init__(client)
       self._upload_configs = {
           }
 
     def GetConfig(self, request, global_params=None):
-      """Get encoded debug configuration for component. Not cacheable.
+      r"""Get encoded debug configuration for component. Not cacheable.
 
       Args:
         request: (DataflowProjectsJobsDebugGetConfigRequest) input message
         global_params: (StandardQueryParameters, default: None) global arguments
       Returns:
         (GetDebugConfigResponse) The response message.
       """
@@ -101,15 +102,15 @@
         request_field=u'getDebugConfigRequest',
         request_type_name=u'DataflowProjectsJobsDebugGetConfigRequest',
         response_type_name=u'GetDebugConfigResponse',
         supports_download=False,
     )
 
     def SendCapture(self, request, global_params=None):
-      """Send encoded debug capture data for component.
+      r"""Send encoded debug capture data for component.
 
       Args:
         request: (DataflowProjectsJobsDebugSendCaptureRequest) input message
         global_params: (StandardQueryParameters, default: None) global arguments
       Returns:
         (SendDebugCaptureResponse) The response message.
       """
@@ -137,15 +138,15 @@
 
     def __init__(self, client):
       super(DataflowV1b3.ProjectsJobsMessagesService, self).__init__(client)
       self._upload_configs = {
           }
 
     def List(self, request, global_params=None):
-      """Request the job status.
+      r"""Request the job status.
 
       Args:
         request: (DataflowProjectsJobsMessagesListRequest) input message
         global_params: (StandardQueryParameters, default: None) global arguments
       Returns:
         (ListJobMessagesResponse) The response message.
       """
@@ -173,15 +174,15 @@
 
     def __init__(self, client):
       super(DataflowV1b3.ProjectsJobsWorkItemsService, self).__init__(client)
       self._upload_configs = {
           }
 
     def Lease(self, request, global_params=None):
-      """Leases a dataflow WorkItem to run.
+      r"""Leases a dataflow WorkItem to run.
 
       Args:
         request: (DataflowProjectsJobsWorkItemsLeaseRequest) input message
         global_params: (StandardQueryParameters, default: None) global arguments
       Returns:
         (LeaseWorkItemResponse) The response message.
       """
@@ -199,15 +200,15 @@
         request_field=u'leaseWorkItemRequest',
         request_type_name=u'DataflowProjectsJobsWorkItemsLeaseRequest',
         response_type_name=u'LeaseWorkItemResponse',
         supports_download=False,
     )
 
     def ReportStatus(self, request, global_params=None):
-      """Reports the status of dataflow WorkItems leased by a worker.
+      r"""Reports the status of dataflow WorkItems leased by a worker.
 
       Args:
         request: (DataflowProjectsJobsWorkItemsReportStatusRequest) input message
         global_params: (StandardQueryParameters, default: None) global arguments
       Returns:
         (ReportWorkItemStatusResponse) The response message.
       """
@@ -235,15 +236,15 @@
 
     def __init__(self, client):
       super(DataflowV1b3.ProjectsJobsService, self).__init__(client)
       self._upload_configs = {
           }
 
     def Aggregated(self, request, global_params=None):
-      """List the jobs of a project across all regions.
+      r"""List the jobs of a project across all regions.
 
       Args:
         request: (DataflowProjectsJobsAggregatedRequest) input message
         global_params: (StandardQueryParameters, default: None) global arguments
       Returns:
         (ListJobsResponse) The response message.
       """
@@ -261,15 +262,15 @@
         request_field='',
         request_type_name=u'DataflowProjectsJobsAggregatedRequest',
         response_type_name=u'ListJobsResponse',
         supports_download=False,
     )
 
     def Create(self, request, global_params=None):
-      """Creates a Cloud Dataflow job.
+      r"""Creates a Cloud Dataflow job.
 
       Args:
         request: (DataflowProjectsJobsCreateRequest) input message
         global_params: (StandardQueryParameters, default: None) global arguments
       Returns:
         (Job) The response message.
       """
@@ -287,15 +288,15 @@
         request_field=u'job',
         request_type_name=u'DataflowProjectsJobsCreateRequest',
         response_type_name=u'Job',
         supports_download=False,
     )
 
     def Get(self, request, global_params=None):
-      """Gets the state of the specified Cloud Dataflow job.
+      r"""Gets the state of the specified Cloud Dataflow job.
 
       Args:
         request: (DataflowProjectsJobsGetRequest) input message
         global_params: (StandardQueryParameters, default: None) global arguments
       Returns:
         (Job) The response message.
       """
@@ -313,15 +314,15 @@
         request_field='',
         request_type_name=u'DataflowProjectsJobsGetRequest',
         response_type_name=u'Job',
         supports_download=False,
     )
 
     def GetMetrics(self, request, global_params=None):
-      """Request the job status.
+      r"""Request the job status.
 
       Args:
         request: (DataflowProjectsJobsGetMetricsRequest) input message
         global_params: (StandardQueryParameters, default: None) global arguments
       Returns:
         (JobMetrics) The response message.
       """
@@ -339,15 +340,15 @@
         request_field='',
         request_type_name=u'DataflowProjectsJobsGetMetricsRequest',
         response_type_name=u'JobMetrics',
         supports_download=False,
     )
 
     def List(self, request, global_params=None):
-      """List the jobs of a project in a given region.
+      r"""List the jobs of a project in a given region.
 
       Args:
         request: (DataflowProjectsJobsListRequest) input message
         global_params: (StandardQueryParameters, default: None) global arguments
       Returns:
         (ListJobsResponse) The response message.
       """
@@ -365,15 +366,15 @@
         request_field='',
         request_type_name=u'DataflowProjectsJobsListRequest',
         response_type_name=u'ListJobsResponse',
         supports_download=False,
     )
 
     def Update(self, request, global_params=None):
-      """Updates the state of an existing Cloud Dataflow job.
+      r"""Updates the state of an existing Cloud Dataflow job.
 
       Args:
         request: (DataflowProjectsJobsUpdateRequest) input message
         global_params: (StandardQueryParameters, default: None) global arguments
       Returns:
         (Job) The response message.
       """
@@ -401,15 +402,15 @@
 
     def __init__(self, client):
       super(DataflowV1b3.ProjectsLocationsJobsDebugService, self).__init__(client)
       self._upload_configs = {
           }
 
     def GetConfig(self, request, global_params=None):
-      """Get encoded debug configuration for component. Not cacheable.
+      r"""Get encoded debug configuration for component. Not cacheable.
 
       Args:
         request: (DataflowProjectsLocationsJobsDebugGetConfigRequest) input message
         global_params: (StandardQueryParameters, default: None) global arguments
       Returns:
         (GetDebugConfigResponse) The response message.
       """
@@ -427,15 +428,15 @@
         request_field=u'getDebugConfigRequest',
         request_type_name=u'DataflowProjectsLocationsJobsDebugGetConfigRequest',
         response_type_name=u'GetDebugConfigResponse',
         supports_download=False,
     )
 
     def SendCapture(self, request, global_params=None):
-      """Send encoded debug capture data for component.
+      r"""Send encoded debug capture data for component.
 
       Args:
         request: (DataflowProjectsLocationsJobsDebugSendCaptureRequest) input message
         global_params: (StandardQueryParameters, default: None) global arguments
       Returns:
         (SendDebugCaptureResponse) The response message.
       """
@@ -463,15 +464,15 @@
 
     def __init__(self, client):
       super(DataflowV1b3.ProjectsLocationsJobsMessagesService, self).__init__(client)
       self._upload_configs = {
           }
 
     def List(self, request, global_params=None):
-      """Request the job status.
+      r"""Request the job status.
 
       Args:
         request: (DataflowProjectsLocationsJobsMessagesListRequest) input message
         global_params: (StandardQueryParameters, default: None) global arguments
       Returns:
         (ListJobMessagesResponse) The response message.
       """
@@ -499,15 +500,15 @@
 
     def __init__(self, client):
       super(DataflowV1b3.ProjectsLocationsJobsWorkItemsService, self).__init__(client)
       self._upload_configs = {
           }
 
     def Lease(self, request, global_params=None):
-      """Leases a dataflow WorkItem to run.
+      r"""Leases a dataflow WorkItem to run.
 
       Args:
         request: (DataflowProjectsLocationsJobsWorkItemsLeaseRequest) input message
         global_params: (StandardQueryParameters, default: None) global arguments
       Returns:
         (LeaseWorkItemResponse) The response message.
       """
@@ -525,15 +526,15 @@
         request_field=u'leaseWorkItemRequest',
         request_type_name=u'DataflowProjectsLocationsJobsWorkItemsLeaseRequest',
         response_type_name=u'LeaseWorkItemResponse',
         supports_download=False,
     )
 
     def ReportStatus(self, request, global_params=None):
-      """Reports the status of dataflow WorkItems leased by a worker.
+      r"""Reports the status of dataflow WorkItems leased by a worker.
 
       Args:
         request: (DataflowProjectsLocationsJobsWorkItemsReportStatusRequest) input message
         global_params: (StandardQueryParameters, default: None) global arguments
       Returns:
         (ReportWorkItemStatusResponse) The response message.
       """
@@ -561,15 +562,15 @@
 
     def __init__(self, client):
       super(DataflowV1b3.ProjectsLocationsJobsService, self).__init__(client)
       self._upload_configs = {
           }
 
     def Create(self, request, global_params=None):
-      """Creates a Cloud Dataflow job.
+      r"""Creates a Cloud Dataflow job.
 
       Args:
         request: (DataflowProjectsLocationsJobsCreateRequest) input message
         global_params: (StandardQueryParameters, default: None) global arguments
       Returns:
         (Job) The response message.
       """
@@ -587,15 +588,15 @@
         request_field=u'job',
         request_type_name=u'DataflowProjectsLocationsJobsCreateRequest',
         response_type_name=u'Job',
         supports_download=False,
     )
 
     def Get(self, request, global_params=None):
-      """Gets the state of the specified Cloud Dataflow job.
+      r"""Gets the state of the specified Cloud Dataflow job.
 
       Args:
         request: (DataflowProjectsLocationsJobsGetRequest) input message
         global_params: (StandardQueryParameters, default: None) global arguments
       Returns:
         (Job) The response message.
       """
@@ -613,15 +614,15 @@
         request_field='',
         request_type_name=u'DataflowProjectsLocationsJobsGetRequest',
         response_type_name=u'Job',
         supports_download=False,
     )
 
     def GetMetrics(self, request, global_params=None):
-      """Request the job status.
+      r"""Request the job status.
 
       Args:
         request: (DataflowProjectsLocationsJobsGetMetricsRequest) input message
         global_params: (StandardQueryParameters, default: None) global arguments
       Returns:
         (JobMetrics) The response message.
       """
@@ -639,15 +640,15 @@
         request_field='',
         request_type_name=u'DataflowProjectsLocationsJobsGetMetricsRequest',
         response_type_name=u'JobMetrics',
         supports_download=False,
     )
 
     def List(self, request, global_params=None):
-      """List the jobs of a project in a given region.
+      r"""List the jobs of a project in a given region.
 
       Args:
         request: (DataflowProjectsLocationsJobsListRequest) input message
         global_params: (StandardQueryParameters, default: None) global arguments
       Returns:
         (ListJobsResponse) The response message.
       """
@@ -665,15 +666,15 @@
         request_field='',
         request_type_name=u'DataflowProjectsLocationsJobsListRequest',
         response_type_name=u'ListJobsResponse',
         supports_download=False,
     )
 
     def Update(self, request, global_params=None):
-      """Updates the state of an existing Cloud Dataflow job.
+      r"""Updates the state of an existing Cloud Dataflow job.
 
       Args:
         request: (DataflowProjectsLocationsJobsUpdateRequest) input message
         global_params: (StandardQueryParameters, default: None) global arguments
       Returns:
         (Job) The response message.
       """
@@ -701,15 +702,15 @@
 
     def __init__(self, client):
       super(DataflowV1b3.ProjectsLocationsTemplatesService, self).__init__(client)
       self._upload_configs = {
           }
 
     def Create(self, request, global_params=None):
-      """Creates a Cloud Dataflow job from a template.
+      r"""Creates a Cloud Dataflow job from a template.
 
       Args:
         request: (DataflowProjectsLocationsTemplatesCreateRequest) input message
         global_params: (StandardQueryParameters, default: None) global arguments
       Returns:
         (Job) The response message.
       """
@@ -727,15 +728,15 @@
         request_field=u'createJobFromTemplateRequest',
         request_type_name=u'DataflowProjectsLocationsTemplatesCreateRequest',
         response_type_name=u'Job',
         supports_download=False,
     )
 
     def Get(self, request, global_params=None):
-      """Get the template associated with a template.
+      r"""Get the template associated with a template.
 
       Args:
         request: (DataflowProjectsLocationsTemplatesGetRequest) input message
         global_params: (StandardQueryParameters, default: None) global arguments
       Returns:
         (GetTemplateResponse) The response message.
       """
@@ -753,15 +754,15 @@
         request_field='',
         request_type_name=u'DataflowProjectsLocationsTemplatesGetRequest',
         response_type_name=u'GetTemplateResponse',
         supports_download=False,
     )
 
     def Launch(self, request, global_params=None):
-      """Launch a template.
+      r"""Launch a template.
 
       Args:
         request: (DataflowProjectsLocationsTemplatesLaunchRequest) input message
         global_params: (StandardQueryParameters, default: None) global arguments
       Returns:
         (LaunchTemplateResponse) The response message.
       """
@@ -789,15 +790,15 @@
 
     def __init__(self, client):
       super(DataflowV1b3.ProjectsLocationsService, self).__init__(client)
       self._upload_configs = {
           }
 
     def WorkerMessages(self, request, global_params=None):
-      """Send a worker_message to the service.
+      r"""Send a worker_message to the service.
 
       Args:
         request: (DataflowProjectsLocationsWorkerMessagesRequest) input message
         global_params: (StandardQueryParameters, default: None) global arguments
       Returns:
         (SendWorkerMessagesResponse) The response message.
       """
@@ -825,15 +826,15 @@
 
     def __init__(self, client):
       super(DataflowV1b3.ProjectsTemplatesService, self).__init__(client)
       self._upload_configs = {
           }
 
     def Create(self, request, global_params=None):
-      """Creates a Cloud Dataflow job from a template.
+      r"""Creates a Cloud Dataflow job from a template.
 
       Args:
         request: (DataflowProjectsTemplatesCreateRequest) input message
         global_params: (StandardQueryParameters, default: None) global arguments
       Returns:
         (Job) The response message.
       """
@@ -851,15 +852,15 @@
         request_field=u'createJobFromTemplateRequest',
         request_type_name=u'DataflowProjectsTemplatesCreateRequest',
         response_type_name=u'Job',
         supports_download=False,
     )
 
     def Get(self, request, global_params=None):
-      """Get the template associated with a template.
+      r"""Get the template associated with a template.
 
       Args:
         request: (DataflowProjectsTemplatesGetRequest) input message
         global_params: (StandardQueryParameters, default: None) global arguments
       Returns:
         (GetTemplateResponse) The response message.
       """
@@ -877,15 +878,15 @@
         request_field='',
         request_type_name=u'DataflowProjectsTemplatesGetRequest',
         response_type_name=u'GetTemplateResponse',
         supports_download=False,
     )
 
     def Launch(self, request, global_params=None):
-      """Launch a template.
+      r"""Launch a template.
 
       Args:
         request: (DataflowProjectsTemplatesLaunchRequest) input message
         global_params: (StandardQueryParameters, default: None) global arguments
       Returns:
         (LaunchTemplateResponse) The response message.
       """
@@ -913,15 +914,15 @@
 
     def __init__(self, client):
       super(DataflowV1b3.ProjectsService, self).__init__(client)
       self._upload_configs = {
           }
 
     def WorkerMessages(self, request, global_params=None):
-      """Send a worker_message to the service.
+      r"""Send a worker_message to the service.
 
       Args:
         request: (DataflowProjectsWorkerMessagesRequest) input message
         global_params: (StandardQueryParameters, default: None) global arguments
       Returns:
         (SendWorkerMessagesResponse) The response message.
       """
```

## Comparing `apache-beam-2.8.0/apache_beam/runners/dataflow/internal/clients/dataflow/__init__.py` & `apache-beam-2.9.0/apache_beam/runners/dataflow/internal/clients/dataflow/__init__.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/runners/dataflow/internal/clients/dataflow/message_matchers.py` & `apache-beam-2.9.0/apache_beam/runners/dataflow/internal/clients/dataflow/message_matchers.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/runners/dataflow/internal/clients/dataflow/dataflow_v1b3_messages.py` & `apache-beam-2.9.0/apache_beam/runners/dataflow/internal/clients/dataflow/dataflow_v1b3_messages.py`

 * *Files 3% similar despite different names*

```diff
@@ -13,46 +13,45 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 #
 
 """Generated message classes for dataflow version v1b3.
 
-Develops and executes data processing patterns like ETL, batch computation,
-and continuous computation.
+Manages Google Cloud Dataflow projects on Google Cloud Platform.
 """
 # NOTE: This file is autogenerated and should not be edited by hand.
 
 from __future__ import absolute_import
 
 from apitools.base.protorpclite import messages as _messages
 from apitools.base.py import encoding
 from apitools.base.py import extra_types
 
 
 package = 'dataflow'
 
 
 class ApproximateProgress(_messages.Message):
-  """Obsolete in favor of ApproximateReportedProgress and
+  r"""Obsolete in favor of ApproximateReportedProgress and
   ApproximateSplitRequest.
 
   Fields:
     percentComplete: Obsolete.
     position: Obsolete.
     remainingTime: Obsolete.
   """
 
   percentComplete = _messages.FloatField(1, variant=_messages.Variant.FLOAT)
   position = _messages.MessageField('Position', 2)
   remainingTime = _messages.StringField(3)
 
 
 class ApproximateReportedProgress(_messages.Message):
-  """A progress measurement of a WorkItem by a worker.
+  r"""A progress measurement of a WorkItem by a worker.
 
   Fields:
     consumedParallelism: Total amount of parallelism in the portion of input
       of this task that has already been consumed and is no longer active. In
       the first two examples above (see remaining_parallelism), the value
       should be 29 or 2 respectively.  The sum of remaining_parallelism and
       consumed_parallelism should equal the total amount of parallelism in
@@ -86,29 +85,29 @@
   consumedParallelism = _messages.MessageField('ReportedParallelism', 1)
   fractionConsumed = _messages.FloatField(2)
   position = _messages.MessageField('Position', 3)
   remainingParallelism = _messages.MessageField('ReportedParallelism', 4)
 
 
 class ApproximateSplitRequest(_messages.Message):
-  """A suggestion by the service to the worker to dynamically split the
+  r"""A suggestion by the service to the worker to dynamically split the
   WorkItem.
 
   Fields:
     fractionConsumed: A fraction at which to split the work item, from 0.0
       (beginning of the input) to 1.0 (end of the input).
     position: A Position at which to split the work item.
   """
 
   fractionConsumed = _messages.FloatField(1)
   position = _messages.MessageField('Position', 2)
 
 
 class AutoscalingEvent(_messages.Message):
-  """A structured message reporting an autoscaling decision made by the
+  r"""A structured message reporting an autoscaling decision made by the
   Dataflow service.
 
   Enums:
     EventTypeValueValuesEnum: The type of autoscaling event to report.
 
   Fields:
     currentNumWorkers: The current number of workers the job has.
@@ -122,15 +121,15 @@
       num_workers value.
     workerPool: A short and friendly name for the worker pool this event
       refers to, populated from the value of
       PoolStageRelation::user_pool_name.
   """
 
   class EventTypeValueValuesEnum(_messages.Enum):
-    """The type of autoscaling event to report.
+    r"""The type of autoscaling event to report.
 
     Values:
       TYPE_UNKNOWN: Default type for the enum.  Value should never be
         returned.
       TARGET_NUM_WORKERS_CHANGED: The TARGET_NUM_WORKERS_CHANGED type should
         be used when the target worker pool size has changed at the start of
         an actuation. An event should always be specified as
@@ -158,26 +157,26 @@
   eventType = _messages.EnumField('EventTypeValueValuesEnum', 3)
   targetNumWorkers = _messages.IntegerField(4)
   time = _messages.StringField(5)
   workerPool = _messages.StringField(6)
 
 
 class AutoscalingSettings(_messages.Message):
-  """Settings for WorkerPool autoscaling.
+  r"""Settings for WorkerPool autoscaling.
 
   Enums:
     AlgorithmValueValuesEnum: The algorithm to use for autoscaling.
 
   Fields:
     algorithm: The algorithm to use for autoscaling.
     maxNumWorkers: The maximum number of workers to cap scaling at.
   """
 
   class AlgorithmValueValuesEnum(_messages.Enum):
-    """The algorithm to use for autoscaling.
+    r"""The algorithm to use for autoscaling.
 
     Values:
       AUTOSCALING_ALGORITHM_UNKNOWN: The algorithm is unknown, or unspecified.
       AUTOSCALING_ALGORITHM_NONE: Disable autoscaling.
       AUTOSCALING_ALGORITHM_BASIC: Increase worker count over time to reduce
         job execution time.
     """
@@ -186,15 +185,15 @@
     AUTOSCALING_ALGORITHM_BASIC = 2
 
   algorithm = _messages.EnumField('AlgorithmValueValuesEnum', 1)
   maxNumWorkers = _messages.IntegerField(2, variant=_messages.Variant.INT32)
 
 
 class BigQueryIODetails(_messages.Message):
-  """Metadata for a BigQuery connector used by the job.
+  r"""Metadata for a BigQuery connector used by the job.
 
   Fields:
     dataset: Dataset accessed in the connection.
     projectId: Project accessed in the connection.
     query: Query used to access data in the connection.
     table: Table accessed in the connection.
   """
@@ -202,29 +201,29 @@
   dataset = _messages.StringField(1)
   projectId = _messages.StringField(2)
   query = _messages.StringField(3)
   table = _messages.StringField(4)
 
 
 class BigTableIODetails(_messages.Message):
-  """Metadata for a BigTable connector used by the job.
+  r"""Metadata for a BigTable connector used by the job.
 
   Fields:
     instanceId: InstanceId accessed in the connection.
     projectId: ProjectId accessed in the connection.
     tableId: TableId accessed in the connection.
   """
 
   instanceId = _messages.StringField(1)
   projectId = _messages.StringField(2)
   tableId = _messages.StringField(3)
 
 
 class CPUTime(_messages.Message):
-  """Modeled after information exposed by /proc/stat.
+  r"""Modeled after information exposed by /proc/stat.
 
   Fields:
     rate: Average CPU utilization rate (% non-idle cpu / second) since
       previous sample.
     timestamp: Timestamp of the measurement.
     totalMs: Total active CPU time across all cores (ie., non-idle) in
       milliseconds since start-up.
@@ -232,15 +231,15 @@
 
   rate = _messages.FloatField(1)
   timestamp = _messages.StringField(2)
   totalMs = _messages.IntegerField(3, variant=_messages.Variant.UINT64)
 
 
 class ComponentSource(_messages.Message):
-  """Description of an interstitial value between transforms in an execution
+  r"""Description of an interstitial value between transforms in an execution
   stage.
 
   Fields:
     name: Dataflow service generated name for this source.
     originalTransformOrCollection: User name for the original user transform
       or collection with which this source is most closely associated.
     userName: Human-readable name for this transform; may be user or system
@@ -249,15 +248,15 @@
 
   name = _messages.StringField(1)
   originalTransformOrCollection = _messages.StringField(2)
   userName = _messages.StringField(3)
 
 
 class ComponentTransform(_messages.Message):
-  """Description of a transform executed as part of an execution stage.
+  r"""Description of a transform executed as part of an execution stage.
 
   Fields:
     name: Dataflow service generated name for this source.
     originalTransform: User name for the original user transform with which
       this transform is most closely associated.
     userName: Human-readable name for this transform; may be user or system
       generated.
@@ -265,15 +264,15 @@
 
   name = _messages.StringField(1)
   originalTransform = _messages.StringField(2)
   userName = _messages.StringField(3)
 
 
 class ComputationTopology(_messages.Message):
-  """All configuration data for a particular Computation.
+  r"""All configuration data for a particular Computation.
 
   Fields:
     computationId: The ID of the computation.
     inputs: The inputs to the computation.
     keyRanges: The key ranges processed by the computation.
     outputs: The outputs from the computation.
     stateFamilies: The state family values.
@@ -285,44 +284,44 @@
   keyRanges = _messages.MessageField('KeyRangeLocation', 3, repeated=True)
   outputs = _messages.MessageField('StreamLocation', 4, repeated=True)
   stateFamilies = _messages.MessageField('StateFamilyConfig', 5, repeated=True)
   systemStageName = _messages.StringField(6)
 
 
 class ConcatPosition(_messages.Message):
-  """A position that encapsulates an inner position and an index for the inner
-  position. A ConcatPosition can be used by a reader of a source that
+  r"""A position that encapsulates an inner position and an index for the
+  inner position. A ConcatPosition can be used by a reader of a source that
   encapsulates a set of other sources.
 
   Fields:
     index: Index of the inner source.
     position: Position within the inner source.
   """
 
   index = _messages.IntegerField(1, variant=_messages.Variant.INT32)
   position = _messages.MessageField('Position', 2)
 
 
 class CounterMetadata(_messages.Message):
-  """CounterMetadata includes all static non-name non-value counter
+  r"""CounterMetadata includes all static non-name non-value counter
   attributes.
 
   Enums:
     KindValueValuesEnum: Counter aggregation kind.
     StandardUnitsValueValuesEnum: System defined Units, see above enum.
 
   Fields:
     description: Human-readable description of the counter semantics.
     kind: Counter aggregation kind.
     otherUnits: A string referring to the unit type.
     standardUnits: System defined Units, see above enum.
   """
 
   class KindValueValuesEnum(_messages.Enum):
-    """Counter aggregation kind.
+    r"""Counter aggregation kind.
 
     Values:
       INVALID: Counter aggregation kind was not set.
       SUM: Aggregated value is the sum of all contributed values.
       MAX: Aggregated value is the max of all contributed values.
       MIN: Aggregated value is the min of all contributed values.
       MEAN: Aggregated value is the mean of all contributed values.
@@ -342,15 +341,15 @@
     OR = 5
     AND = 6
     SET = 7
     DISTRIBUTION = 8
     LATEST_VALUE = 9
 
   class StandardUnitsValueValuesEnum(_messages.Enum):
-    """System defined Units, see above enum.
+    r"""System defined Units, see above enum.
 
     Values:
       BYTES: Counter returns a value in bytes.
       BYTES_PER_SEC: Counter returns a value in bytes per second.
       MILLISECONDS: Counter returns a value in milliseconds.
       MICROSECONDS: Counter returns a value in microseconds.
       NANOSECONDS: Counter returns a value in nanoseconds.
@@ -370,15 +369,15 @@
   description = _messages.StringField(1)
   kind = _messages.EnumField('KindValueValuesEnum', 2)
   otherUnits = _messages.StringField(3)
   standardUnits = _messages.EnumField('StandardUnitsValueValuesEnum', 4)
 
 
 class CounterStructuredName(_messages.Message):
-  """Identifies a counter within a per-job namespace. Counters whose
+  r"""Identifies a counter within a per-job namespace. Counters whose
   structured names are the same get merged into a single value for the job.
 
   Enums:
     OriginValueValuesEnum: One of the standard Origins defined above.
     PortionValueValuesEnum: Portion of this counter, either key or value.
 
   Fields:
@@ -402,25 +401,25 @@
     originalStepName: System generated name of the original step in the user's
       graph, before optimization.
     portion: Portion of this counter, either key or value.
     workerId: ID of a particular worker.
   """
 
   class OriginValueValuesEnum(_messages.Enum):
-    """One of the standard Origins defined above.
+    r"""One of the standard Origins defined above.
 
     Values:
       SYSTEM: Counter was created by the Dataflow system.
       USER: Counter was created by the user.
     """
     SYSTEM = 0
     USER = 1
 
   class PortionValueValuesEnum(_messages.Enum):
-    """Portion of this counter, either key or value.
+    r"""Portion of this counter, either key or value.
 
     Values:
       ALL: Counter portion has not been set.
       KEY: Counter reports a key.
       VALUE: Counter reports a value.
     """
     ALL = 0
@@ -436,28 +435,28 @@
   originalRequestingStepName = _messages.StringField(7)
   originalStepName = _messages.StringField(8)
   portion = _messages.EnumField('PortionValueValuesEnum', 9)
   workerId = _messages.StringField(10)
 
 
 class CounterStructuredNameAndMetadata(_messages.Message):
-  """A single message which encapsulates structured name and metadata for a
+  r"""A single message which encapsulates structured name and metadata for a
   given counter.
 
   Fields:
     metadata: Metadata associated with a counter
     name: Structured name of the counter.
   """
 
   metadata = _messages.MessageField('CounterMetadata', 1)
   name = _messages.MessageField('CounterStructuredName', 2)
 
 
 class CounterUpdate(_messages.Message):
-  """An update to a Counter sent from a worker.
+  r"""An update to a Counter sent from a worker.
 
   Fields:
     boolean: Boolean value for And, Or.
     cumulative: True if this counter is reported as the total cumulative
       aggregate value accumulated since the worker started working on this
       WorkItem. By default this is false, indicating that this counter is
       reported as a delta.
@@ -493,15 +492,15 @@
   nameAndKind = _messages.MessageField('NameAndKind', 12)
   shortId = _messages.IntegerField(13)
   stringList = _messages.MessageField('StringList', 14)
   structuredNameAndMetadata = _messages.MessageField('CounterStructuredNameAndMetadata', 15)
 
 
 class CreateJobFromTemplateRequest(_messages.Message):
-  """A request to create a Cloud Dataflow job from a template.
+  r"""A request to create a Cloud Dataflow job from a template.
 
   Messages:
     ParametersValue: The runtime parameters to pass to the job.
 
   Fields:
     environment: The runtime environment for the job.
     gcsPath: Required. A Cloud Storage path to the template from which to
@@ -510,25 +509,25 @@
     jobName: Required. The job name to use for the created job.
     location: The location to which to direct the request.
     parameters: The runtime parameters to pass to the job.
   """
 
   @encoding.MapUnrecognizedFields('additionalProperties')
   class ParametersValue(_messages.Message):
-    """The runtime parameters to pass to the job.
+    r"""The runtime parameters to pass to the job.
 
     Messages:
       AdditionalProperty: An additional property for a ParametersValue object.
 
     Fields:
       additionalProperties: Additional properties of type ParametersValue
     """
 
     class AdditionalProperty(_messages.Message):
-      """An additional property for a ParametersValue object.
+      r"""An additional property for a ParametersValue object.
 
       Fields:
         key: Name of the additional property.
         value: A string attribute.
       """
 
       key = _messages.StringField(1)
@@ -540,25 +539,25 @@
   gcsPath = _messages.StringField(2)
   jobName = _messages.StringField(3)
   location = _messages.StringField(4)
   parameters = _messages.MessageField('ParametersValue', 5)
 
 
 class CustomSourceLocation(_messages.Message):
-  """Identifies the location of a custom souce.
+  r"""Identifies the location of a custom souce.
 
   Fields:
     stateful: Whether this source is stateful.
   """
 
   stateful = _messages.BooleanField(1)
 
 
 class DataDiskAssignment(_messages.Message):
-  """Data disk assignment for a given VM instance.
+  r"""Data disk assignment for a given VM instance.
 
   Fields:
     dataDisks: Mounted data disks. The order is important a data disk's
       0-based index in this list defines which persistent directory the disk
       is mounted to, for example the list of {
       "myproject-1014-104817-4c2-harness-0-disk-0" }, {
       "myproject-1014-104817-4c2-harness-0-disk-1" }.
@@ -567,15 +566,15 @@
   """
 
   dataDisks = _messages.StringField(1, repeated=True)
   vmInstance = _messages.StringField(2)
 
 
 class DataflowProjectsJobsAggregatedRequest(_messages.Message):
-  """A DataflowProjectsJobsAggregatedRequest object.
+  r"""A DataflowProjectsJobsAggregatedRequest object.
 
   Enums:
     FilterValueValuesEnum: The kind of filter to use.
     ViewValueValuesEnum: Level of information requested in response. Default
       is `JOB_VIEW_SUMMARY`.
 
   Fields:
@@ -588,29 +587,29 @@
       to request additional results in a long list.
     projectId: The project which owns the jobs.
     view: Level of information requested in response. Default is
       `JOB_VIEW_SUMMARY`.
   """
 
   class FilterValueValuesEnum(_messages.Enum):
-    """The kind of filter to use.
+    r"""The kind of filter to use.
 
     Values:
       UNKNOWN: <no description>
       ALL: <no description>
       TERMINATED: <no description>
       ACTIVE: <no description>
     """
     UNKNOWN = 0
     ALL = 1
     TERMINATED = 2
     ACTIVE = 3
 
   class ViewValueValuesEnum(_messages.Enum):
-    """Level of information requested in response. Default is
+    r"""Level of information requested in response. Default is
     `JOB_VIEW_SUMMARY`.
 
     Values:
       JOB_VIEW_UNKNOWN: <no description>
       JOB_VIEW_SUMMARY: <no description>
       JOB_VIEW_ALL: <no description>
       JOB_VIEW_DESCRIPTION: <no description>
@@ -625,29 +624,29 @@
   pageSize = _messages.IntegerField(3, variant=_messages.Variant.INT32)
   pageToken = _messages.StringField(4)
   projectId = _messages.StringField(5, required=True)
   view = _messages.EnumField('ViewValueValuesEnum', 6)
 
 
 class DataflowProjectsJobsCreateRequest(_messages.Message):
-  """A DataflowProjectsJobsCreateRequest object.
+  r"""A DataflowProjectsJobsCreateRequest object.
 
   Enums:
     ViewValueValuesEnum: The level of information requested in response.
 
   Fields:
     job: A Job resource to be passed as the request body.
     location: The location that contains this job.
     projectId: The ID of the Cloud Platform project that the job belongs to.
     replaceJobId: Deprecated. This field is now in the Job message.
     view: The level of information requested in response.
   """
 
   class ViewValueValuesEnum(_messages.Enum):
-    """The level of information requested in response.
+    r"""The level of information requested in response.
 
     Values:
       JOB_VIEW_UNKNOWN: <no description>
       JOB_VIEW_SUMMARY: <no description>
       JOB_VIEW_ALL: <no description>
       JOB_VIEW_DESCRIPTION: <no description>
     """
@@ -660,45 +659,45 @@
   location = _messages.StringField(2)
   projectId = _messages.StringField(3, required=True)
   replaceJobId = _messages.StringField(4)
   view = _messages.EnumField('ViewValueValuesEnum', 5)
 
 
 class DataflowProjectsJobsDebugGetConfigRequest(_messages.Message):
-  """A DataflowProjectsJobsDebugGetConfigRequest object.
+  r"""A DataflowProjectsJobsDebugGetConfigRequest object.
 
   Fields:
     getDebugConfigRequest: A GetDebugConfigRequest resource to be passed as
       the request body.
     jobId: The job id.
     projectId: The project id.
   """
 
   getDebugConfigRequest = _messages.MessageField('GetDebugConfigRequest', 1)
   jobId = _messages.StringField(2, required=True)
   projectId = _messages.StringField(3, required=True)
 
 
 class DataflowProjectsJobsDebugSendCaptureRequest(_messages.Message):
-  """A DataflowProjectsJobsDebugSendCaptureRequest object.
+  r"""A DataflowProjectsJobsDebugSendCaptureRequest object.
 
   Fields:
     jobId: The job id.
     projectId: The project id.
     sendDebugCaptureRequest: A SendDebugCaptureRequest resource to be passed
       as the request body.
   """
 
   jobId = _messages.StringField(1, required=True)
   projectId = _messages.StringField(2, required=True)
   sendDebugCaptureRequest = _messages.MessageField('SendDebugCaptureRequest', 3)
 
 
 class DataflowProjectsJobsGetMetricsRequest(_messages.Message):
-  """A DataflowProjectsJobsGetMetricsRequest object.
+  r"""A DataflowProjectsJobsGetMetricsRequest object.
 
   Fields:
     jobId: The job to get messages for.
     location: The location which contains the job specified by job_id.
     projectId: A project id.
     startTime: Return only metric data that has changed since this time.
       Default is to return all information about all metrics for the job.
@@ -707,28 +706,28 @@
   jobId = _messages.StringField(1, required=True)
   location = _messages.StringField(2)
   projectId = _messages.StringField(3, required=True)
   startTime = _messages.StringField(4)
 
 
 class DataflowProjectsJobsGetRequest(_messages.Message):
-  """A DataflowProjectsJobsGetRequest object.
+  r"""A DataflowProjectsJobsGetRequest object.
 
   Enums:
     ViewValueValuesEnum: The level of information requested in response.
 
   Fields:
     jobId: The job ID.
     location: The location that contains this job.
     projectId: The ID of the Cloud Platform project that the job belongs to.
     view: The level of information requested in response.
   """
 
   class ViewValueValuesEnum(_messages.Enum):
-    """The level of information requested in response.
+    r"""The level of information requested in response.
 
     Values:
       JOB_VIEW_UNKNOWN: <no description>
       JOB_VIEW_SUMMARY: <no description>
       JOB_VIEW_ALL: <no description>
       JOB_VIEW_DESCRIPTION: <no description>
     """
@@ -740,15 +739,15 @@
   jobId = _messages.StringField(1, required=True)
   location = _messages.StringField(2)
   projectId = _messages.StringField(3, required=True)
   view = _messages.EnumField('ViewValueValuesEnum', 4)
 
 
 class DataflowProjectsJobsListRequest(_messages.Message):
-  """A DataflowProjectsJobsListRequest object.
+  r"""A DataflowProjectsJobsListRequest object.
 
   Enums:
     FilterValueValuesEnum: The kind of filter to use.
     ViewValueValuesEnum: Level of information requested in response. Default
       is `JOB_VIEW_SUMMARY`.
 
   Fields:
@@ -761,29 +760,29 @@
       to request additional results in a long list.
     projectId: The project which owns the jobs.
     view: Level of information requested in response. Default is
       `JOB_VIEW_SUMMARY`.
   """
 
   class FilterValueValuesEnum(_messages.Enum):
-    """The kind of filter to use.
+    r"""The kind of filter to use.
 
     Values:
       UNKNOWN: <no description>
       ALL: <no description>
       TERMINATED: <no description>
       ACTIVE: <no description>
     """
     UNKNOWN = 0
     ALL = 1
     TERMINATED = 2
     ACTIVE = 3
 
   class ViewValueValuesEnum(_messages.Enum):
-    """Level of information requested in response. Default is
+    r"""Level of information requested in response. Default is
     `JOB_VIEW_SUMMARY`.
 
     Values:
       JOB_VIEW_UNKNOWN: <no description>
       JOB_VIEW_SUMMARY: <no description>
       JOB_VIEW_ALL: <no description>
       JOB_VIEW_DESCRIPTION: <no description>
@@ -798,15 +797,15 @@
   pageSize = _messages.IntegerField(3, variant=_messages.Variant.INT32)
   pageToken = _messages.StringField(4)
   projectId = _messages.StringField(5, required=True)
   view = _messages.EnumField('ViewValueValuesEnum', 6)
 
 
 class DataflowProjectsJobsMessagesListRequest(_messages.Message):
-  """A DataflowProjectsJobsMessagesListRequest object.
+  r"""A DataflowProjectsJobsMessagesListRequest object.
 
   Enums:
     MinimumImportanceValueValuesEnum: Filter to only get messages with
       importance >= level
 
   Fields:
     endTime: Return only messages with timestamps < end_time. The default is
@@ -823,15 +822,15 @@
     projectId: A project id.
     startTime: If specified, return only messages with timestamps >=
       start_time. The default is the job creation time (i.e. beginning of
       messages).
   """
 
   class MinimumImportanceValueValuesEnum(_messages.Enum):
-    """Filter to only get messages with importance >= level
+    r"""Filter to only get messages with importance >= level
 
     Values:
       JOB_MESSAGE_IMPORTANCE_UNKNOWN: <no description>
       JOB_MESSAGE_DEBUG: <no description>
       JOB_MESSAGE_DETAILED: <no description>
       JOB_MESSAGE_BASIC: <no description>
       JOB_MESSAGE_WARNING: <no description>
@@ -851,15 +850,15 @@
   pageSize = _messages.IntegerField(5, variant=_messages.Variant.INT32)
   pageToken = _messages.StringField(6)
   projectId = _messages.StringField(7, required=True)
   startTime = _messages.StringField(8)
 
 
 class DataflowProjectsJobsUpdateRequest(_messages.Message):
-  """A DataflowProjectsJobsUpdateRequest object.
+  r"""A DataflowProjectsJobsUpdateRequest object.
 
   Fields:
     job: A Job resource to be passed as the request body.
     jobId: The job ID.
     location: The location that contains this job.
     projectId: The ID of the Cloud Platform project that the job belongs to.
   """
@@ -867,59 +866,59 @@
   job = _messages.MessageField('Job', 1)
   jobId = _messages.StringField(2, required=True)
   location = _messages.StringField(3)
   projectId = _messages.StringField(4, required=True)
 
 
 class DataflowProjectsJobsWorkItemsLeaseRequest(_messages.Message):
-  """A DataflowProjectsJobsWorkItemsLeaseRequest object.
+  r"""A DataflowProjectsJobsWorkItemsLeaseRequest object.
 
   Fields:
     jobId: Identifies the workflow job this worker belongs to.
     leaseWorkItemRequest: A LeaseWorkItemRequest resource to be passed as the
       request body.
     projectId: Identifies the project this worker belongs to.
   """
 
   jobId = _messages.StringField(1, required=True)
   leaseWorkItemRequest = _messages.MessageField('LeaseWorkItemRequest', 2)
   projectId = _messages.StringField(3, required=True)
 
 
 class DataflowProjectsJobsWorkItemsReportStatusRequest(_messages.Message):
-  """A DataflowProjectsJobsWorkItemsReportStatusRequest object.
+  r"""A DataflowProjectsJobsWorkItemsReportStatusRequest object.
 
   Fields:
     jobId: The job which the WorkItem is part of.
     projectId: The project which owns the WorkItem's job.
     reportWorkItemStatusRequest: A ReportWorkItemStatusRequest resource to be
       passed as the request body.
   """
 
   jobId = _messages.StringField(1, required=True)
   projectId = _messages.StringField(2, required=True)
   reportWorkItemStatusRequest = _messages.MessageField('ReportWorkItemStatusRequest', 3)
 
 
 class DataflowProjectsLocationsJobsCreateRequest(_messages.Message):
-  """A DataflowProjectsLocationsJobsCreateRequest object.
+  r"""A DataflowProjectsLocationsJobsCreateRequest object.
 
   Enums:
     ViewValueValuesEnum: The level of information requested in response.
 
   Fields:
     job: A Job resource to be passed as the request body.
     location: The location that contains this job.
     projectId: The ID of the Cloud Platform project that the job belongs to.
     replaceJobId: Deprecated. This field is now in the Job message.
     view: The level of information requested in response.
   """
 
   class ViewValueValuesEnum(_messages.Enum):
-    """The level of information requested in response.
+    r"""The level of information requested in response.
 
     Values:
       JOB_VIEW_UNKNOWN: <no description>
       JOB_VIEW_SUMMARY: <no description>
       JOB_VIEW_ALL: <no description>
       JOB_VIEW_DESCRIPTION: <no description>
     """
@@ -932,15 +931,15 @@
   location = _messages.StringField(2, required=True)
   projectId = _messages.StringField(3, required=True)
   replaceJobId = _messages.StringField(4)
   view = _messages.EnumField('ViewValueValuesEnum', 5)
 
 
 class DataflowProjectsLocationsJobsDebugGetConfigRequest(_messages.Message):
-  """A DataflowProjectsLocationsJobsDebugGetConfigRequest object.
+  r"""A DataflowProjectsLocationsJobsDebugGetConfigRequest object.
 
   Fields:
     getDebugConfigRequest: A GetDebugConfigRequest resource to be passed as
       the request body.
     jobId: The job id.
     location: The location which contains the job specified by job_id.
     projectId: The project id.
@@ -949,15 +948,15 @@
   getDebugConfigRequest = _messages.MessageField('GetDebugConfigRequest', 1)
   jobId = _messages.StringField(2, required=True)
   location = _messages.StringField(3, required=True)
   projectId = _messages.StringField(4, required=True)
 
 
 class DataflowProjectsLocationsJobsDebugSendCaptureRequest(_messages.Message):
-  """A DataflowProjectsLocationsJobsDebugSendCaptureRequest object.
+  r"""A DataflowProjectsLocationsJobsDebugSendCaptureRequest object.
 
   Fields:
     jobId: The job id.
     location: The location which contains the job specified by job_id.
     projectId: The project id.
     sendDebugCaptureRequest: A SendDebugCaptureRequest resource to be passed
       as the request body.
@@ -966,15 +965,15 @@
   jobId = _messages.StringField(1, required=True)
   location = _messages.StringField(2, required=True)
   projectId = _messages.StringField(3, required=True)
   sendDebugCaptureRequest = _messages.MessageField('SendDebugCaptureRequest', 4)
 
 
 class DataflowProjectsLocationsJobsGetMetricsRequest(_messages.Message):
-  """A DataflowProjectsLocationsJobsGetMetricsRequest object.
+  r"""A DataflowProjectsLocationsJobsGetMetricsRequest object.
 
   Fields:
     jobId: The job to get messages for.
     location: The location which contains the job specified by job_id.
     projectId: A project id.
     startTime: Return only metric data that has changed since this time.
       Default is to return all information about all metrics for the job.
@@ -983,28 +982,28 @@
   jobId = _messages.StringField(1, required=True)
   location = _messages.StringField(2, required=True)
   projectId = _messages.StringField(3, required=True)
   startTime = _messages.StringField(4)
 
 
 class DataflowProjectsLocationsJobsGetRequest(_messages.Message):
-  """A DataflowProjectsLocationsJobsGetRequest object.
+  r"""A DataflowProjectsLocationsJobsGetRequest object.
 
   Enums:
     ViewValueValuesEnum: The level of information requested in response.
 
   Fields:
     jobId: The job ID.
     location: The location that contains this job.
     projectId: The ID of the Cloud Platform project that the job belongs to.
     view: The level of information requested in response.
   """
 
   class ViewValueValuesEnum(_messages.Enum):
-    """The level of information requested in response.
+    r"""The level of information requested in response.
 
     Values:
       JOB_VIEW_UNKNOWN: <no description>
       JOB_VIEW_SUMMARY: <no description>
       JOB_VIEW_ALL: <no description>
       JOB_VIEW_DESCRIPTION: <no description>
     """
@@ -1016,15 +1015,15 @@
   jobId = _messages.StringField(1, required=True)
   location = _messages.StringField(2, required=True)
   projectId = _messages.StringField(3, required=True)
   view = _messages.EnumField('ViewValueValuesEnum', 4)
 
 
 class DataflowProjectsLocationsJobsListRequest(_messages.Message):
-  """A DataflowProjectsLocationsJobsListRequest object.
+  r"""A DataflowProjectsLocationsJobsListRequest object.
 
   Enums:
     FilterValueValuesEnum: The kind of filter to use.
     ViewValueValuesEnum: Level of information requested in response. Default
       is `JOB_VIEW_SUMMARY`.
 
   Fields:
@@ -1037,29 +1036,29 @@
       to request additional results in a long list.
     projectId: The project which owns the jobs.
     view: Level of information requested in response. Default is
       `JOB_VIEW_SUMMARY`.
   """
 
   class FilterValueValuesEnum(_messages.Enum):
-    """The kind of filter to use.
+    r"""The kind of filter to use.
 
     Values:
       UNKNOWN: <no description>
       ALL: <no description>
       TERMINATED: <no description>
       ACTIVE: <no description>
     """
     UNKNOWN = 0
     ALL = 1
     TERMINATED = 2
     ACTIVE = 3
 
   class ViewValueValuesEnum(_messages.Enum):
-    """Level of information requested in response. Default is
+    r"""Level of information requested in response. Default is
     `JOB_VIEW_SUMMARY`.
 
     Values:
       JOB_VIEW_UNKNOWN: <no description>
       JOB_VIEW_SUMMARY: <no description>
       JOB_VIEW_ALL: <no description>
       JOB_VIEW_DESCRIPTION: <no description>
@@ -1074,15 +1073,15 @@
   pageSize = _messages.IntegerField(3, variant=_messages.Variant.INT32)
   pageToken = _messages.StringField(4)
   projectId = _messages.StringField(5, required=True)
   view = _messages.EnumField('ViewValueValuesEnum', 6)
 
 
 class DataflowProjectsLocationsJobsMessagesListRequest(_messages.Message):
-  """A DataflowProjectsLocationsJobsMessagesListRequest object.
+  r"""A DataflowProjectsLocationsJobsMessagesListRequest object.
 
   Enums:
     MinimumImportanceValueValuesEnum: Filter to only get messages with
       importance >= level
 
   Fields:
     endTime: Return only messages with timestamps < end_time. The default is
@@ -1099,15 +1098,15 @@
     projectId: A project id.
     startTime: If specified, return only messages with timestamps >=
       start_time. The default is the job creation time (i.e. beginning of
       messages).
   """
 
   class MinimumImportanceValueValuesEnum(_messages.Enum):
-    """Filter to only get messages with importance >= level
+    r"""Filter to only get messages with importance >= level
 
     Values:
       JOB_MESSAGE_IMPORTANCE_UNKNOWN: <no description>
       JOB_MESSAGE_DEBUG: <no description>
       JOB_MESSAGE_DETAILED: <no description>
       JOB_MESSAGE_BASIC: <no description>
       JOB_MESSAGE_WARNING: <no description>
@@ -1127,15 +1126,15 @@
   pageSize = _messages.IntegerField(5, variant=_messages.Variant.INT32)
   pageToken = _messages.StringField(6)
   projectId = _messages.StringField(7, required=True)
   startTime = _messages.StringField(8)
 
 
 class DataflowProjectsLocationsJobsUpdateRequest(_messages.Message):
-  """A DataflowProjectsLocationsJobsUpdateRequest object.
+  r"""A DataflowProjectsLocationsJobsUpdateRequest object.
 
   Fields:
     job: A Job resource to be passed as the request body.
     jobId: The job ID.
     location: The location that contains this job.
     projectId: The ID of the Cloud Platform project that the job belongs to.
   """
@@ -1143,15 +1142,15 @@
   job = _messages.MessageField('Job', 1)
   jobId = _messages.StringField(2, required=True)
   location = _messages.StringField(3, required=True)
   projectId = _messages.StringField(4, required=True)
 
 
 class DataflowProjectsLocationsJobsWorkItemsLeaseRequest(_messages.Message):
-  """A DataflowProjectsLocationsJobsWorkItemsLeaseRequest object.
+  r"""A DataflowProjectsLocationsJobsWorkItemsLeaseRequest object.
 
   Fields:
     jobId: Identifies the workflow job this worker belongs to.
     leaseWorkItemRequest: A LeaseWorkItemRequest resource to be passed as the
       request body.
     location: The location which contains the WorkItem's job.
     projectId: Identifies the project this worker belongs to.
@@ -1160,15 +1159,15 @@
   jobId = _messages.StringField(1, required=True)
   leaseWorkItemRequest = _messages.MessageField('LeaseWorkItemRequest', 2)
   location = _messages.StringField(3, required=True)
   projectId = _messages.StringField(4, required=True)
 
 
 class DataflowProjectsLocationsJobsWorkItemsReportStatusRequest(_messages.Message):
-  """A DataflowProjectsLocationsJobsWorkItemsReportStatusRequest object.
+  r"""A DataflowProjectsLocationsJobsWorkItemsReportStatusRequest object.
 
   Fields:
     jobId: The job which the WorkItem is part of.
     location: The location which contains the WorkItem's job.
     projectId: The project which owns the WorkItem's job.
     reportWorkItemStatusRequest: A ReportWorkItemStatusRequest resource to be
       passed as the request body.
@@ -1177,15 +1176,15 @@
   jobId = _messages.StringField(1, required=True)
   location = _messages.StringField(2, required=True)
   projectId = _messages.StringField(3, required=True)
   reportWorkItemStatusRequest = _messages.MessageField('ReportWorkItemStatusRequest', 4)
 
 
 class DataflowProjectsLocationsTemplatesCreateRequest(_messages.Message):
-  """A DataflowProjectsLocationsTemplatesCreateRequest object.
+  r"""A DataflowProjectsLocationsTemplatesCreateRequest object.
 
   Fields:
     createJobFromTemplateRequest: A CreateJobFromTemplateRequest resource to
       be passed as the request body.
     location: The location to which to direct the request.
     projectId: Required. The ID of the Cloud Platform project that the job
       belongs to.
@@ -1193,15 +1192,15 @@
 
   createJobFromTemplateRequest = _messages.MessageField('CreateJobFromTemplateRequest', 1)
   location = _messages.StringField(2, required=True)
   projectId = _messages.StringField(3, required=True)
 
 
 class DataflowProjectsLocationsTemplatesGetRequest(_messages.Message):
-  """A DataflowProjectsLocationsTemplatesGetRequest object.
+  r"""A DataflowProjectsLocationsTemplatesGetRequest object.
 
   Enums:
     ViewValueValuesEnum: The view to retrieve. Defaults to METADATA_ONLY.
 
   Fields:
     gcsPath: Required. A Cloud Storage path to the template from which to
       create the job. Must be a valid Cloud Storage URL, beginning with
@@ -1209,29 +1208,29 @@
     location: The location to which to direct the request.
     projectId: Required. The ID of the Cloud Platform project that the job
       belongs to.
     view: The view to retrieve. Defaults to METADATA_ONLY.
   """
 
   class ViewValueValuesEnum(_messages.Enum):
-    """The view to retrieve. Defaults to METADATA_ONLY.
+    r"""The view to retrieve. Defaults to METADATA_ONLY.
 
     Values:
       METADATA_ONLY: <no description>
     """
     METADATA_ONLY = 0
 
   gcsPath = _messages.StringField(1)
   location = _messages.StringField(2, required=True)
   projectId = _messages.StringField(3, required=True)
   view = _messages.EnumField('ViewValueValuesEnum', 4)
 
 
 class DataflowProjectsLocationsTemplatesLaunchRequest(_messages.Message):
-  """A DataflowProjectsLocationsTemplatesLaunchRequest object.
+  r"""A DataflowProjectsLocationsTemplatesLaunchRequest object.
 
   Fields:
     gcsPath: Required. A Cloud Storage path to the template from which to
       create the job. Must be valid Cloud Storage URL, beginning with 'gs://'.
     launchTemplateParameters: A LaunchTemplateParameters resource to be passed
       as the request body.
     location: The location to which to direct the request.
@@ -1245,44 +1244,44 @@
   launchTemplateParameters = _messages.MessageField('LaunchTemplateParameters', 2)
   location = _messages.StringField(3, required=True)
   projectId = _messages.StringField(4, required=True)
   validateOnly = _messages.BooleanField(5)
 
 
 class DataflowProjectsLocationsWorkerMessagesRequest(_messages.Message):
-  """A DataflowProjectsLocationsWorkerMessagesRequest object.
+  r"""A DataflowProjectsLocationsWorkerMessagesRequest object.
 
   Fields:
     location: The location which contains the job
     projectId: The project to send the WorkerMessages to.
     sendWorkerMessagesRequest: A SendWorkerMessagesRequest resource to be
       passed as the request body.
   """
 
   location = _messages.StringField(1, required=True)
   projectId = _messages.StringField(2, required=True)
   sendWorkerMessagesRequest = _messages.MessageField('SendWorkerMessagesRequest', 3)
 
 
 class DataflowProjectsTemplatesCreateRequest(_messages.Message):
-  """A DataflowProjectsTemplatesCreateRequest object.
+  r"""A DataflowProjectsTemplatesCreateRequest object.
 
   Fields:
     createJobFromTemplateRequest: A CreateJobFromTemplateRequest resource to
       be passed as the request body.
     projectId: Required. The ID of the Cloud Platform project that the job
       belongs to.
   """
 
   createJobFromTemplateRequest = _messages.MessageField('CreateJobFromTemplateRequest', 1)
   projectId = _messages.StringField(2, required=True)
 
 
 class DataflowProjectsTemplatesGetRequest(_messages.Message):
-  """A DataflowProjectsTemplatesGetRequest object.
+  r"""A DataflowProjectsTemplatesGetRequest object.
 
   Enums:
     ViewValueValuesEnum: The view to retrieve. Defaults to METADATA_ONLY.
 
   Fields:
     gcsPath: Required. A Cloud Storage path to the template from which to
       create the job. Must be a valid Cloud Storage URL, beginning with
@@ -1290,29 +1289,29 @@
     location: The location to which to direct the request.
     projectId: Required. The ID of the Cloud Platform project that the job
       belongs to.
     view: The view to retrieve. Defaults to METADATA_ONLY.
   """
 
   class ViewValueValuesEnum(_messages.Enum):
-    """The view to retrieve. Defaults to METADATA_ONLY.
+    r"""The view to retrieve. Defaults to METADATA_ONLY.
 
     Values:
       METADATA_ONLY: <no description>
     """
     METADATA_ONLY = 0
 
   gcsPath = _messages.StringField(1)
   location = _messages.StringField(2)
   projectId = _messages.StringField(3, required=True)
   view = _messages.EnumField('ViewValueValuesEnum', 4)
 
 
 class DataflowProjectsTemplatesLaunchRequest(_messages.Message):
-  """A DataflowProjectsTemplatesLaunchRequest object.
+  r"""A DataflowProjectsTemplatesLaunchRequest object.
 
   Fields:
     gcsPath: Required. A Cloud Storage path to the template from which to
       create the job. Must be valid Cloud Storage URL, beginning with 'gs://'.
     launchTemplateParameters: A LaunchTemplateParameters resource to be passed
       as the request body.
     location: The location to which to direct the request.
@@ -1326,55 +1325,55 @@
   launchTemplateParameters = _messages.MessageField('LaunchTemplateParameters', 2)
   location = _messages.StringField(3)
   projectId = _messages.StringField(4, required=True)
   validateOnly = _messages.BooleanField(5)
 
 
 class DataflowProjectsWorkerMessagesRequest(_messages.Message):
-  """A DataflowProjectsWorkerMessagesRequest object.
+  r"""A DataflowProjectsWorkerMessagesRequest object.
 
   Fields:
     projectId: The project to send the WorkerMessages to.
     sendWorkerMessagesRequest: A SendWorkerMessagesRequest resource to be
       passed as the request body.
   """
 
   projectId = _messages.StringField(1, required=True)
   sendWorkerMessagesRequest = _messages.MessageField('SendWorkerMessagesRequest', 2)
 
 
 class DatastoreIODetails(_messages.Message):
-  """Metadata for a Datastore connector used by the job.
+  r"""Metadata for a Datastore connector used by the job.
 
   Fields:
     namespace: Namespace used in the connection.
     projectId: ProjectId accessed in the connection.
   """
 
   namespace = _messages.StringField(1)
   projectId = _messages.StringField(2)
 
 
 class DerivedSource(_messages.Message):
-  """Specification of one of the bundles produced as a result of splitting a
+  r"""Specification of one of the bundles produced as a result of splitting a
   Source (e.g. when executing a SourceSplitRequest, or when splitting an
   active task using WorkItemStatus.dynamic_source_split), relative to the
   source being split.
 
   Enums:
     DerivationModeValueValuesEnum: What source to base the produced source on
       (if any).
 
   Fields:
     derivationMode: What source to base the produced source on (if any).
     source: Specification of the source.
   """
 
   class DerivationModeValueValuesEnum(_messages.Enum):
-    """What source to base the produced source on (if any).
+    r"""What source to base the produced source on (if any).
 
     Values:
       SOURCE_DERIVATION_MODE_UNKNOWN: The source derivation is unknown, or
         unspecified.
       SOURCE_DERIVATION_MODE_INDEPENDENT: Produce a completely independent
         Source with no base.
       SOURCE_DERIVATION_MODE_CHILD_OF_CURRENT: Produce a Source based on the
@@ -1388,15 +1387,15 @@
     SOURCE_DERIVATION_MODE_SIBLING_OF_CURRENT = 3
 
   derivationMode = _messages.EnumField('DerivationModeValueValuesEnum', 1)
   source = _messages.MessageField('Source', 2)
 
 
 class Disk(_messages.Message):
-  """Describes the data disk used by a workflow job.
+  r"""Describes the data disk used by a workflow job.
 
   Fields:
     diskType: Disk storage type, as defined by Google Compute Engine.  This
       must be a disk type appropriate to the project and zone in which the
       workers will run.  If unknown or unspecified, the service will attempt
       to choose a reasonable default.  For example, the standard persistent
       disk type is a resource name typically ending in "pd-standard".  If SSD
@@ -1416,15 +1415,15 @@
 
   diskType = _messages.StringField(1)
   mountPoint = _messages.StringField(2)
   sizeGb = _messages.IntegerField(3, variant=_messages.Variant.INT32)
 
 
 class DisplayData(_messages.Message):
-  """Data provided with a pipeline or transform to provide descriptive info.
+  r"""Data provided with a pipeline or transform to provide descriptive info.
 
   Fields:
     boolValue: Contains value if the data is of a boolean type.
     durationValue: Contains value if the data is of duration type.
     floatValue: Contains value if the data is of float type.
     int64Value: Contains value if the data is of int64 type.
     javaClassValue: Contains value if the data is of java class type.
@@ -1456,15 +1455,15 @@
   shortStrValue = _messages.StringField(9)
   strValue = _messages.StringField(10)
   timestampValue = _messages.StringField(11)
   url = _messages.StringField(12)
 
 
 class DistributionUpdate(_messages.Message):
-  """A metric value representing a distribution.
+  r"""A metric value representing a distribution.
 
   Fields:
     count: The count of the number of elements present in the distribution.
     histogram: (Optional) Histogram of value counts for the distribution.
     max: The maximum value present in the distribution.
     min: The minimum value present in the distribution.
     sum: Use an int64 since we'd prefer the added precision. If overflow is a
@@ -1478,15 +1477,15 @@
   max = _messages.MessageField('SplitInt64', 3)
   min = _messages.MessageField('SplitInt64', 4)
   sum = _messages.MessageField('SplitInt64', 5)
   sumOfSquares = _messages.FloatField(6)
 
 
 class DynamicSourceSplit(_messages.Message):
-  """When a task splits using WorkItemStatus.dynamic_source_split, this
+  r"""When a task splits using WorkItemStatus.dynamic_source_split, this
   message describes the two parts of the split relative to the description of
   the current task's input.
 
   Fields:
     primary: Primary part (continued to be processed by worker). Specified
       relative to the previously-current source. Becomes current.
     residual: Residual part (returned to the pool of work). Specified relative
@@ -1494,15 +1493,15 @@
   """
 
   primary = _messages.MessageField('DerivedSource', 1)
   residual = _messages.MessageField('DerivedSource', 2)
 
 
 class Environment(_messages.Message):
-  """Describes the environment in which a Dataflow Job runs.
+  r"""Describes the environment in which a Dataflow Job runs.
 
   Messages:
     InternalExperimentsValue: Experimental settings.
     SdkPipelineOptionsValue: The Cloud Dataflow SDK pipeline options specified
       by the user. These options are passed through the service and are used
       to recreate the SDK pipeline options on the worker in a language
       agnostic and platform independent way.
@@ -1540,104 +1539,104 @@
       service are required in order to run the job.
     workerPools: The worker pools. At least one "harness" worker pool must be
       specified in order for the job to have workers.
   """
 
   @encoding.MapUnrecognizedFields('additionalProperties')
   class InternalExperimentsValue(_messages.Message):
-    """Experimental settings.
+    r"""Experimental settings.
 
     Messages:
       AdditionalProperty: An additional property for a
         InternalExperimentsValue object.
 
     Fields:
       additionalProperties: Properties of the object. Contains field @type
         with type URL.
     """
 
     class AdditionalProperty(_messages.Message):
-      """An additional property for a InternalExperimentsValue object.
+      r"""An additional property for a InternalExperimentsValue object.
 
       Fields:
         key: Name of the additional property.
         value: A extra_types.JsonValue attribute.
       """
 
       key = _messages.StringField(1)
       value = _messages.MessageField('extra_types.JsonValue', 2)
 
     additionalProperties = _messages.MessageField('AdditionalProperty', 1, repeated=True)
 
   @encoding.MapUnrecognizedFields('additionalProperties')
   class SdkPipelineOptionsValue(_messages.Message):
-    """The Cloud Dataflow SDK pipeline options specified by the user. These
+    r"""The Cloud Dataflow SDK pipeline options specified by the user. These
     options are passed through the service and are used to recreate the SDK
     pipeline options on the worker in a language agnostic and platform
     independent way.
 
     Messages:
       AdditionalProperty: An additional property for a SdkPipelineOptionsValue
         object.
 
     Fields:
       additionalProperties: Properties of the object.
     """
 
     class AdditionalProperty(_messages.Message):
-      """An additional property for a SdkPipelineOptionsValue object.
+      r"""An additional property for a SdkPipelineOptionsValue object.
 
       Fields:
         key: Name of the additional property.
         value: A extra_types.JsonValue attribute.
       """
 
       key = _messages.StringField(1)
       value = _messages.MessageField('extra_types.JsonValue', 2)
 
     additionalProperties = _messages.MessageField('AdditionalProperty', 1, repeated=True)
 
   @encoding.MapUnrecognizedFields('additionalProperties')
   class UserAgentValue(_messages.Message):
-    """A description of the process that generated the request.
+    r"""A description of the process that generated the request.
 
     Messages:
       AdditionalProperty: An additional property for a UserAgentValue object.
 
     Fields:
       additionalProperties: Properties of the object.
     """
 
     class AdditionalProperty(_messages.Message):
-      """An additional property for a UserAgentValue object.
+      r"""An additional property for a UserAgentValue object.
 
       Fields:
         key: Name of the additional property.
         value: A extra_types.JsonValue attribute.
       """
 
       key = _messages.StringField(1)
       value = _messages.MessageField('extra_types.JsonValue', 2)
 
     additionalProperties = _messages.MessageField('AdditionalProperty', 1, repeated=True)
 
   @encoding.MapUnrecognizedFields('additionalProperties')
   class VersionValue(_messages.Message):
-    """A structure describing which components and their versions of the
+    r"""A structure describing which components and their versions of the
     service are required in order to run the job.
 
     Messages:
       AdditionalProperty: An additional property for a VersionValue object.
 
     Fields:
       additionalProperties: Properties of the object.
     """
 
     class AdditionalProperty(_messages.Message):
-      """An additional property for a VersionValue object.
+      r"""An additional property for a VersionValue object.
 
       Fields:
         key: Name of the additional property.
         value: A extra_types.JsonValue attribute.
       """
 
       key = _messages.StringField(1)
@@ -1654,29 +1653,29 @@
   tempStoragePrefix = _messages.StringField(7)
   userAgent = _messages.MessageField('UserAgentValue', 8)
   version = _messages.MessageField('VersionValue', 9)
   workerPools = _messages.MessageField('WorkerPool', 10, repeated=True)
 
 
 class ExecutionStageState(_messages.Message):
-  """A message describing the state of a particular execution stage.
+  r"""A message describing the state of a particular execution stage.
 
   Enums:
     ExecutionStageStateValueValuesEnum: Executions stage states allow the same
       set of values as JobState.
 
   Fields:
     currentStateTime: The time at which the stage transitioned to this state.
     executionStageName: The name of the execution stage.
     executionStageState: Executions stage states allow the same set of values
       as JobState.
   """
 
   class ExecutionStageStateValueValuesEnum(_messages.Enum):
-    """Executions stage states allow the same set of values as JobState.
+    r"""Executions stage states allow the same set of values as JobState.
 
     Values:
       JOB_STATE_UNKNOWN: The job's run state isn't specified.
       JOB_STATE_STOPPED: `JOB_STATE_STOPPED` indicates that the job has not
         yet started to run.
       JOB_STATE_RUNNING: `JOB_STATE_RUNNING` indicates that the job is
         currently running.
@@ -1736,15 +1735,15 @@
 
   currentStateTime = _messages.StringField(1)
   executionStageName = _messages.StringField(2)
   executionStageState = _messages.EnumField('ExecutionStageStateValueValuesEnum', 3)
 
 
 class ExecutionStageSummary(_messages.Message):
-  """Description of the composing transforms, names/ids, and input/outputs of
+  r"""Description of the composing transforms, names/ids, and input/outputs of
   a stage of execution.  Some composing transforms and sources may have been
   generated by the Dataflow service during execution planning.
 
   Enums:
     KindValueValuesEnum: Type of tranform this stage is executing.
 
   Fields:
@@ -1755,15 +1754,15 @@
     inputSource: Input sources for this stage.
     kind: Type of tranform this stage is executing.
     name: Dataflow service generated name for this stage.
     outputSource: Output sources for this stage.
   """
 
   class KindValueValuesEnum(_messages.Enum):
-    """Type of tranform this stage is executing.
+    r"""Type of tranform this stage is executing.
 
     Values:
       UNKNOWN_KIND: Unrecognized transform type.
       PAR_DO_KIND: ParDo transform.
       GROUP_BY_KEY_KIND: Group By Key transform.
       FLATTEN_KIND: Flatten transform.
       READ_KIND: Read transform.
@@ -1789,107 +1788,107 @@
   inputSource = _messages.MessageField('StageSource', 4, repeated=True)
   kind = _messages.EnumField('KindValueValuesEnum', 5)
   name = _messages.StringField(6)
   outputSource = _messages.MessageField('StageSource', 7, repeated=True)
 
 
 class FailedLocation(_messages.Message):
-  """Indicates which location failed to respond to a request for data.
+  r"""Indicates which location failed to respond to a request for data.
 
   Fields:
     name: The name of the failed location.
   """
 
   name = _messages.StringField(1)
 
 
 class FileIODetails(_messages.Message):
-  """Metadata for a File connector used by the job.
+  r"""Metadata for a File connector used by the job.
 
   Fields:
     filePattern: File Pattern used to access files by the connector.
   """
 
   filePattern = _messages.StringField(1)
 
 
 class FlattenInstruction(_messages.Message):
-  """An instruction that copies its inputs (zero or more) to its (single)
+  r"""An instruction that copies its inputs (zero or more) to its (single)
   output.
 
   Fields:
     inputs: Describes the inputs to the flatten instruction.
   """
 
   inputs = _messages.MessageField('InstructionInput', 1, repeated=True)
 
 
 class FloatingPointList(_messages.Message):
-  """A metric value representing a list of floating point numbers.
+  r"""A metric value representing a list of floating point numbers.
 
   Fields:
     elements: Elements of the list.
   """
 
   elements = _messages.FloatField(1, repeated=True)
 
 
 class FloatingPointMean(_messages.Message):
-  """A representation of a floating point mean metric contribution.
+  r"""A representation of a floating point mean metric contribution.
 
   Fields:
     count: The number of values being aggregated.
     sum: The sum of all values being aggregated.
   """
 
   count = _messages.MessageField('SplitInt64', 1)
   sum = _messages.FloatField(2)
 
 
 class GetDebugConfigRequest(_messages.Message):
-  """Request to get updated debug configuration for component.
+  r"""Request to get updated debug configuration for component.
 
   Fields:
     componentId: The internal component id for which debug configuration is
       requested.
     location: The location which contains the job specified by job_id.
     workerId: The worker id, i.e., VM hostname.
   """
 
   componentId = _messages.StringField(1)
   location = _messages.StringField(2)
   workerId = _messages.StringField(3)
 
 
 class GetDebugConfigResponse(_messages.Message):
-  """Response to a get debug configuration request.
+  r"""Response to a get debug configuration request.
 
   Fields:
     config: The encoded debug configuration for the requested component.
   """
 
   config = _messages.StringField(1)
 
 
 class GetTemplateResponse(_messages.Message):
-  """The response to a GetTemplate request.
+  r"""The response to a GetTemplate request.
 
   Fields:
     metadata: The template metadata describing the template name, available
       parameters, etc.
     status: The status of the get template request. Any problems with the
       request will be indicated in the error_details.
   """
 
   metadata = _messages.MessageField('TemplateMetadata', 1)
   status = _messages.MessageField('Status', 2)
 
 
 class Histogram(_messages.Message):
-  """Histogram of value counts for a distribution.  Buckets have an inclusive
+  r"""Histogram of value counts for a distribution.  Buckets have an inclusive
   lower bound and exclusive upper bound and use "1,2,5 bucketing": The first
   bucket range is from [0,1) and all subsequent bucket boundaries are powers
   of ten multiplied by 1, 2, or 5. Thus, bucket boundaries are 0, 1, 2, 5, 10,
   20, 50, 100, 200, 500, 1000, ... Negative values are not supported.
 
   Fields:
     bucketCounts: Counts of values in each bucket. For efficiency, prefix and
@@ -1902,15 +1901,15 @@
   """
 
   bucketCounts = _messages.IntegerField(1, repeated=True)
   firstBucketOffset = _messages.IntegerField(2, variant=_messages.Variant.INT32)
 
 
 class InstructionInput(_messages.Message):
-  """An input of an instruction, as a reference to an output of a producer
+  r"""An input of an instruction, as a reference to an output of a producer
   instruction.
 
   Fields:
     outputNum: The output index (origin zero) within the producer.
     producerInstructionIndex: The index (origin zero) of the parallel
       instruction that produces the output to be consumed by this input.  This
       index is relative to the list of instructions in this input's
@@ -1918,15 +1917,15 @@
   """
 
   outputNum = _messages.IntegerField(1, variant=_messages.Variant.INT32)
   producerInstructionIndex = _messages.IntegerField(2, variant=_messages.Variant.INT32)
 
 
 class InstructionOutput(_messages.Message):
-  """An output of an instruction.
+  r"""An output of an instruction.
 
   Messages:
     CodecValue: The codec to use to encode data being written via this output.
 
   Fields:
     codec: The codec to use to encode data being written via this output.
     name: The user-provided name of this output.
@@ -1939,25 +1938,25 @@
       set this.
     systemName: System-defined name of this output. Unique across the
       workflow.
   """
 
   @encoding.MapUnrecognizedFields('additionalProperties')
   class CodecValue(_messages.Message):
-    """The codec to use to encode data being written via this output.
+    r"""The codec to use to encode data being written via this output.
 
     Messages:
       AdditionalProperty: An additional property for a CodecValue object.
 
     Fields:
       additionalProperties: Properties of the object.
     """
 
     class AdditionalProperty(_messages.Message):
-      """An additional property for a CodecValue object.
+      r"""An additional property for a CodecValue object.
 
       Fields:
         key: Name of the additional property.
         value: A extra_types.JsonValue attribute.
       """
 
       key = _messages.StringField(1)
@@ -1970,50 +1969,50 @@
   onlyCountKeyBytes = _messages.BooleanField(3)
   onlyCountValueBytes = _messages.BooleanField(4)
   originalName = _messages.StringField(5)
   systemName = _messages.StringField(6)
 
 
 class IntegerGauge(_messages.Message):
-  """A metric value representing temporal values of a variable.
+  r"""A metric value representing temporal values of a variable.
 
   Fields:
     timestamp: The time at which this value was measured. Measured as msecs
       from epoch.
     value: The value of the variable represented by this gauge.
   """
 
   timestamp = _messages.StringField(1)
   value = _messages.MessageField('SplitInt64', 2)
 
 
 class IntegerList(_messages.Message):
-  """A metric value representing a list of integers.
+  r"""A metric value representing a list of integers.
 
   Fields:
     elements: Elements of the list.
   """
 
   elements = _messages.MessageField('SplitInt64', 1, repeated=True)
 
 
 class IntegerMean(_messages.Message):
-  """A representation of an integer mean metric contribution.
+  r"""A representation of an integer mean metric contribution.
 
   Fields:
     count: The number of values being aggregated.
     sum: The sum of all values being aggregated.
   """
 
   count = _messages.MessageField('SplitInt64', 1)
   sum = _messages.MessageField('SplitInt64', 2)
 
 
 class Job(_messages.Message):
-  """Defines a job to be run by the Cloud Dataflow service.
+  r"""Defines a job to be run by the Cloud Dataflow service.
 
   Enums:
     CurrentStateValueValuesEnum: The current state of the job.  Jobs are
       created in the `JOB_STATE_STOPPED` state unless otherwise specified.  A
       job in the `JOB_STATE_RUNNING` state may asynchronously enter a terminal
       state. After a job has reached a terminal state, no further state
       updates may be made.  This field may be mutated by the Cloud Dataflow
@@ -2088,28 +2087,34 @@
       switch between the `JOB_STATE_STOPPED` and `JOB_STATE_RUNNING` states,
       by setting requested_state.  `UpdateJob` may also be used to directly
       set a job's requested state to `JOB_STATE_CANCELLED` or
       `JOB_STATE_DONE`, irrevocably terminating the job if it has not already
       reached a terminal state.
     stageStates: This field may be mutated by the Cloud Dataflow service;
       callers cannot mutate it.
+    startTime: The timestamp when the job was started (transitioned to
+      JOB_STATE_PENDING). Flexible resource scheduling jobs are started with
+      some delay after job creation, so start_time is unset before start and
+      is updated when the job is started by the Cloud Dataflow service. For
+      other jobs, start_time always equals to create_time and is immutable and
+      set by the Cloud Dataflow service.
     steps: The top-level steps that constitute the entire job.
     tempFiles: A set of files the system should be aware of that are used for
       temporary storage. These temporary files will be removed on job
       completion. No duplicates are allowed. No file patterns are supported.
       The supported files are:  Google Cloud Storage:
       storage.googleapis.com/{bucket}/{object}
       bucket.storage.googleapis.com/{object}
     transformNameMapping: The map of transform name prefixes of the job to be
       replaced to the corresponding name prefixes of the new job.
     type: The type of Cloud Dataflow job.
   """
 
   class CurrentStateValueValuesEnum(_messages.Enum):
-    """The current state of the job.  Jobs are created in the
+    r"""The current state of the job.  Jobs are created in the
     `JOB_STATE_STOPPED` state unless otherwise specified.  A job in the
     `JOB_STATE_RUNNING` state may asynchronously enter a terminal state. After
     a job has reached a terminal state, no further state updates may be made.
     This field may be mutated by the Cloud Dataflow service; callers cannot
     mutate it.
 
     Values:
@@ -2169,15 +2174,15 @@
     JOB_STATE_DRAINING = 7
     JOB_STATE_DRAINED = 8
     JOB_STATE_PENDING = 9
     JOB_STATE_CANCELLING = 10
     JOB_STATE_QUEUED = 11
 
   class RequestedStateValueValuesEnum(_messages.Enum):
-    """The job's requested state.  `UpdateJob` may be used to switch between
+    r"""The job's requested state.  `UpdateJob` may be used to switch between
     the `JOB_STATE_STOPPED` and `JOB_STATE_RUNNING` states, by setting
     requested_state.  `UpdateJob` may also be used to directly set a job's
     requested state to `JOB_STATE_CANCELLED` or `JOB_STATE_DONE`, irrevocably
     terminating the job if it has not already reached a terminal state.
 
     Values:
       JOB_STATE_UNKNOWN: The job's run state isn't specified.
@@ -2236,72 +2241,72 @@
     JOB_STATE_DRAINING = 7
     JOB_STATE_DRAINED = 8
     JOB_STATE_PENDING = 9
     JOB_STATE_CANCELLING = 10
     JOB_STATE_QUEUED = 11
 
   class TypeValueValuesEnum(_messages.Enum):
-    """The type of Cloud Dataflow job.
+    r"""The type of Cloud Dataflow job.
 
     Values:
       JOB_TYPE_UNKNOWN: The type of the job is unspecified, or unknown.
       JOB_TYPE_BATCH: A batch job with a well-defined end point: data is read,
         data is processed, data is written, and the job is done.
       JOB_TYPE_STREAMING: A continuously streaming job with no end: data is
         read, processed, and written continuously.
     """
     JOB_TYPE_UNKNOWN = 0
     JOB_TYPE_BATCH = 1
     JOB_TYPE_STREAMING = 2
 
   @encoding.MapUnrecognizedFields('additionalProperties')
   class LabelsValue(_messages.Message):
-    """User-defined labels for this job.  The labels map can contain no more
+    r"""User-defined labels for this job.  The labels map can contain no more
     than 64 entries.  Entries of the labels map are UTF8 strings that comply
     with the following restrictions:  * Keys must conform to regexp:
     \p{Ll}\p{Lo}{0,62} * Values must conform to regexp:
     [\p{Ll}\p{Lo}\p{N}_-]{0,63} * Both keys and values are additionally
     constrained to be <= 128 bytes in size.
 
     Messages:
       AdditionalProperty: An additional property for a LabelsValue object.
 
     Fields:
       additionalProperties: Additional properties of type LabelsValue
     """
 
     class AdditionalProperty(_messages.Message):
-      """An additional property for a LabelsValue object.
+      r"""An additional property for a LabelsValue object.
 
       Fields:
         key: Name of the additional property.
         value: A string attribute.
       """
 
       key = _messages.StringField(1)
       value = _messages.StringField(2)
 
     additionalProperties = _messages.MessageField('AdditionalProperty', 1, repeated=True)
 
   @encoding.MapUnrecognizedFields('additionalProperties')
   class TransformNameMappingValue(_messages.Message):
-    """The map of transform name prefixes of the job to be replaced to the
+    r"""The map of transform name prefixes of the job to be replaced to the
     corresponding name prefixes of the new job.
 
     Messages:
       AdditionalProperty: An additional property for a
         TransformNameMappingValue object.
 
     Fields:
       additionalProperties: Additional properties of type
         TransformNameMappingValue
     """
 
     class AdditionalProperty(_messages.Message):
-      """An additional property for a TransformNameMappingValue object.
+      r"""An additional property for a TransformNameMappingValue object.
 
       Fields:
         key: Name of the additional property.
         value: A string attribute.
       """
 
       key = _messages.StringField(1)
@@ -2322,45 +2327,46 @@
   name = _messages.StringField(11)
   pipelineDescription = _messages.MessageField('PipelineDescription', 12)
   projectId = _messages.StringField(13)
   replaceJobId = _messages.StringField(14)
   replacedByJobId = _messages.StringField(15)
   requestedState = _messages.EnumField('RequestedStateValueValuesEnum', 16)
   stageStates = _messages.MessageField('ExecutionStageState', 17, repeated=True)
-  steps = _messages.MessageField('Step', 18, repeated=True)
-  tempFiles = _messages.StringField(19, repeated=True)
-  transformNameMapping = _messages.MessageField('TransformNameMappingValue', 20)
-  type = _messages.EnumField('TypeValueValuesEnum', 21)
+  startTime = _messages.StringField(18)
+  steps = _messages.MessageField('Step', 19, repeated=True)
+  tempFiles = _messages.StringField(20, repeated=True)
+  transformNameMapping = _messages.MessageField('TransformNameMappingValue', 21)
+  type = _messages.EnumField('TypeValueValuesEnum', 22)
 
 
 class JobExecutionInfo(_messages.Message):
-  """Additional information about how a Cloud Dataflow job will be executed
+  r"""Additional information about how a Cloud Dataflow job will be executed
   that isn't contained in the submitted job.
 
   Messages:
     StagesValue: A mapping from each stage to the information about that
       stage.
 
   Fields:
     stages: A mapping from each stage to the information about that stage.
   """
 
   @encoding.MapUnrecognizedFields('additionalProperties')
   class StagesValue(_messages.Message):
-    """A mapping from each stage to the information about that stage.
+    r"""A mapping from each stage to the information about that stage.
 
     Messages:
       AdditionalProperty: An additional property for a StagesValue object.
 
     Fields:
       additionalProperties: Additional properties of type StagesValue
     """
 
     class AdditionalProperty(_messages.Message):
-      """An additional property for a StagesValue object.
+      r"""An additional property for a StagesValue object.
 
       Fields:
         key: Name of the additional property.
         value: A JobExecutionStageInfo attribute.
       """
 
       key = _messages.StringField(1)
@@ -2368,41 +2374,41 @@
 
     additionalProperties = _messages.MessageField('AdditionalProperty', 1, repeated=True)
 
   stages = _messages.MessageField('StagesValue', 1)
 
 
 class JobExecutionStageInfo(_messages.Message):
-  """Contains information about how a particular google.dataflow.v1beta3.Step
+  r"""Contains information about how a particular google.dataflow.v1beta3.Step
   will be executed.
 
   Fields:
     stepName: The steps associated with the execution stage. Note that stages
       may have several steps, and that a given step might be run by more than
       one stage.
   """
 
   stepName = _messages.StringField(1, repeated=True)
 
 
 class JobMessage(_messages.Message):
-  """A particular message pertaining to a Dataflow job.
+  r"""A particular message pertaining to a Dataflow job.
 
   Enums:
     MessageImportanceValueValuesEnum: Importance level of the message.
 
   Fields:
     id: Deprecated.
     messageImportance: Importance level of the message.
     messageText: The text of the message.
     time: The timestamp of the message.
   """
 
   class MessageImportanceValueValuesEnum(_messages.Enum):
-    """Importance level of the message.
+    r"""Importance level of the message.
 
     Values:
       JOB_MESSAGE_IMPORTANCE_UNKNOWN: The message importance isn't specified,
         or is unknown.
       JOB_MESSAGE_DEBUG: The message is at the 'debug' level: typically only
         useful for software engineers working on the code the job is running.
         Typically, Dataflow pipeline runners do not display log messages at
@@ -2436,15 +2442,15 @@
   id = _messages.StringField(1)
   messageImportance = _messages.EnumField('MessageImportanceValueValuesEnum', 2)
   messageText = _messages.StringField(3)
   time = _messages.StringField(4)
 
 
 class JobMetadata(_messages.Message):
-  """Metadata available primarily for filtering jobs. Will be included in the
+  r"""Metadata available primarily for filtering jobs. Will be included in the
   ListJob response and Job SUMMARY view+.
 
   Fields:
     bigTableDetails: Identification of a BigTable source used in the Dataflow
       job.
     bigqueryDetails: Identification of a BigQuery source used in the Dataflow
       job.
@@ -2463,15 +2469,15 @@
   fileDetails = _messages.MessageField('FileIODetails', 4, repeated=True)
   pubsubDetails = _messages.MessageField('PubSubIODetails', 5, repeated=True)
   sdkVersion = _messages.MessageField('SdkVersion', 6)
   spannerDetails = _messages.MessageField('SpannerIODetails', 7, repeated=True)
 
 
 class JobMetrics(_messages.Message):
-  """JobMetrics contains a collection of metrics descibing the detailed
+  r"""JobMetrics contains a collection of metrics descibing the detailed
   progress of a Dataflow job. Metrics correspond to user-defined and system-
   defined metrics in the job.  This resource captures only the most recent
   values of each metric; time-series data can be queried for them (under the
   same metric names) from Cloud Monitoring.
 
   Fields:
     metricTime: Timestamp as of which metric values are current.
@@ -2479,15 +2485,15 @@
   """
 
   metricTime = _messages.StringField(1)
   metrics = _messages.MessageField('MetricUpdate', 2, repeated=True)
 
 
 class KeyRangeDataDiskAssignment(_messages.Message):
-  """Data disk assignment information for a specific key-range of a sharded
+  r"""Data disk assignment information for a specific key-range of a sharded
   computation. Currently we only support UTF-8 character splits to simplify
   encoding into JSON.
 
   Fields:
     dataDisk: The name of the data disk where data for this range is stored.
       This name is local to the Google Cloud Platform project and uniquely
       identifies the disk within that project, for example
@@ -2498,15 +2504,15 @@
 
   dataDisk = _messages.StringField(1)
   end = _messages.StringField(2)
   start = _messages.StringField(3)
 
 
 class KeyRangeLocation(_messages.Message):
-  """Location information for a specific key-range of a sharded computation.
+  r"""Location information for a specific key-range of a sharded computation.
   Currently we only support UTF-8 character splits to simplify encoding into
   JSON.
 
   Fields:
     dataDisk: The name of the data disk where data for this range is stored.
       This name is local to the Google Cloud Platform project and uniquely
       identifies the disk within that project, for example
@@ -2524,38 +2530,38 @@
   deliveryEndpoint = _messages.StringField(2)
   deprecatedPersistentDirectory = _messages.StringField(3)
   end = _messages.StringField(4)
   start = _messages.StringField(5)
 
 
 class LaunchTemplateParameters(_messages.Message):
-  """Parameters to provide to the template being launched.
+  r"""Parameters to provide to the template being launched.
 
   Messages:
     ParametersValue: The runtime parameters to pass to the job.
 
   Fields:
     environment: The runtime environment for the job.
     jobName: Required. The job name to use for the created job.
     parameters: The runtime parameters to pass to the job.
   """
 
   @encoding.MapUnrecognizedFields('additionalProperties')
   class ParametersValue(_messages.Message):
-    """The runtime parameters to pass to the job.
+    r"""The runtime parameters to pass to the job.
 
     Messages:
       AdditionalProperty: An additional property for a ParametersValue object.
 
     Fields:
       additionalProperties: Additional properties of type ParametersValue
     """
 
     class AdditionalProperty(_messages.Message):
-      """An additional property for a ParametersValue object.
+      r"""An additional property for a ParametersValue object.
 
       Fields:
         key: Name of the additional property.
         value: A string attribute.
       """
 
       key = _messages.StringField(1)
@@ -2565,26 +2571,26 @@
 
   environment = _messages.MessageField('RuntimeEnvironment', 1)
   jobName = _messages.StringField(2)
   parameters = _messages.MessageField('ParametersValue', 3)
 
 
 class LaunchTemplateResponse(_messages.Message):
-  """Response to the request to launch a template.
+  r"""Response to the request to launch a template.
 
   Fields:
     job: The job that was launched, if the request was not a dry run and the
       job was successfully launched.
   """
 
   job = _messages.MessageField('Job', 1)
 
 
 class LeaseWorkItemRequest(_messages.Message):
-  """Request to lease WorkItems.
+  r"""Request to lease WorkItems.
 
   Fields:
     currentWorkerTime: The current timestamp at the worker.
     location: The location which contains the WorkItem's job.
     requestedLeaseDuration: The initial lease period.
     workItemTypes: Filter for WorkItem type.
     workerCapabilities: Worker capabilities. WorkItems might be limited to
@@ -2598,56 +2604,56 @@
   requestedLeaseDuration = _messages.StringField(3)
   workItemTypes = _messages.StringField(4, repeated=True)
   workerCapabilities = _messages.StringField(5, repeated=True)
   workerId = _messages.StringField(6)
 
 
 class LeaseWorkItemResponse(_messages.Message):
-  """Response to a request to lease WorkItems.
+  r"""Response to a request to lease WorkItems.
 
   Fields:
     workItems: A list of the leased WorkItems.
   """
 
   workItems = _messages.MessageField('WorkItem', 1, repeated=True)
 
 
 class ListJobMessagesResponse(_messages.Message):
-  """Response to a request to list job messages.
+  r"""Response to a request to list job messages.
 
   Fields:
     autoscalingEvents: Autoscaling events in ascending timestamp order.
     jobMessages: Messages in ascending timestamp order.
     nextPageToken: The token to obtain the next page of results if there are
       more.
   """
 
   autoscalingEvents = _messages.MessageField('AutoscalingEvent', 1, repeated=True)
   jobMessages = _messages.MessageField('JobMessage', 2, repeated=True)
   nextPageToken = _messages.StringField(3)
 
 
 class ListJobsResponse(_messages.Message):
-  """Response to a request to list Cloud Dataflow jobs.  This may be a partial
-  response, depending on the page size in the ListJobsRequest.
+  r"""Response to a request to list Cloud Dataflow jobs.  This may be a
+  partial response, depending on the page size in the ListJobsRequest.
 
   Fields:
     failedLocation: Zero or more messages describing locations that failed to
       respond.
     jobs: A subset of the requested job information.
     nextPageToken: Set if there may be more results than fit in this response.
   """
 
   failedLocation = _messages.MessageField('FailedLocation', 1, repeated=True)
   jobs = _messages.MessageField('Job', 2, repeated=True)
   nextPageToken = _messages.StringField(3)
 
 
 class MapTask(_messages.Message):
-  """MapTask consists of an ordered set of instructions, each of which
+  r"""MapTask consists of an ordered set of instructions, each of which
   describes one particular low-level operation for the worker to perform in
   order to accomplish the MapTask's WorkItem.  Each instruction must appear in
   the list before any instructions which depends on its output.
 
   Fields:
     counterPrefix: Counter prefix that can be used to prefix counters. Not
       currently used in Dataflow.
@@ -2661,29 +2667,30 @@
   counterPrefix = _messages.StringField(1)
   instructions = _messages.MessageField('ParallelInstruction', 2, repeated=True)
   stageName = _messages.StringField(3)
   systemName = _messages.StringField(4)
 
 
 class MetricShortId(_messages.Message):
-  """The metric short id is returned to the user alongside an offset into
+  r"""The metric short id is returned to the user alongside an offset into
   ReportWorkItemStatusRequest
 
   Fields:
     metricIndex: The index of the corresponding metric in the
       ReportWorkItemStatusRequest. Required.
     shortId: The service-generated short identifier for the metric.
   """
 
   metricIndex = _messages.IntegerField(1, variant=_messages.Variant.INT32)
   shortId = _messages.IntegerField(2)
 
 
 class MetricStructuredName(_messages.Message):
-  """Identifies a metric, by describing the source which generated the metric.
+  r"""Identifies a metric, by describing the source which generated the
+  metric.
 
   Messages:
     ContextValue: Zero or more labeled fields which identify the part of the
       job this metric is associated with, such as the name of a step or
       collection.  For example, built-in counters associated with steps will
       have context['step'] = <step-name>. Counters associated with
       PCollections in the SDK will have context['pcollection'] = <pcollection-
@@ -2700,29 +2707,29 @@
     origin: Origin (namespace) of metric name. May be blank for user-define
       metrics; will be "dataflow" for metrics defined by the Dataflow service
       or SDK.
   """
 
   @encoding.MapUnrecognizedFields('additionalProperties')
   class ContextValue(_messages.Message):
-    """Zero or more labeled fields which identify the part of the job this
+    r"""Zero or more labeled fields which identify the part of the job this
     metric is associated with, such as the name of a step or collection.  For
     example, built-in counters associated with steps will have context['step']
     = <step-name>. Counters associated with PCollections in the SDK will have
     context['pcollection'] = <pcollection-name>.
 
     Messages:
       AdditionalProperty: An additional property for a ContextValue object.
 
     Fields:
       additionalProperties: Additional properties of type ContextValue
     """
 
     class AdditionalProperty(_messages.Message):
-      """An additional property for a ContextValue object.
+      r"""An additional property for a ContextValue object.
 
       Fields:
         key: Name of the additional property.
         value: A string attribute.
       """
 
       key = _messages.StringField(1)
@@ -2732,15 +2739,15 @@
 
   context = _messages.MessageField('ContextValue', 1)
   name = _messages.StringField(2)
   origin = _messages.StringField(3)
 
 
 class MetricUpdate(_messages.Message):
-  """Describes the state of a metric.
+  r"""Describes the state of a metric.
 
   Fields:
     cumulative: True if this metric is reported as the total cumulative
       aggregate value accumulated since the worker started working on this
       WorkItem. By default this is false, indicating that this metric is
       reported as a delta that is not associated with any WorkItem.
     distribution: A struct value describing properties of a distribution of
@@ -2785,49 +2792,49 @@
   name = _messages.MessageField('MetricStructuredName', 8)
   scalar = _messages.MessageField('extra_types.JsonValue', 9)
   set = _messages.MessageField('extra_types.JsonValue', 10)
   updateTime = _messages.StringField(11)
 
 
 class MountedDataDisk(_messages.Message):
-  """Describes mounted data disk.
+  r"""Describes mounted data disk.
 
   Fields:
     dataDisk: The name of the data disk. This name is local to the Google
       Cloud Platform project and uniquely identifies the disk within that
       project, for example "myproject-1014-104817-4c2-harness-0-disk-1".
   """
 
   dataDisk = _messages.StringField(1)
 
 
 class MultiOutputInfo(_messages.Message):
-  """Information about an output of a multi-output DoFn.
+  r"""Information about an output of a multi-output DoFn.
 
   Fields:
     tag: The id of the tag the user code will emit to this output by; this
       should correspond to the tag of some SideInputInfo.
   """
 
   tag = _messages.StringField(1)
 
 
 class NameAndKind(_messages.Message):
-  """Basic metadata about a counter.
+  r"""Basic metadata about a counter.
 
   Enums:
     KindValueValuesEnum: Counter aggregation kind.
 
   Fields:
     kind: Counter aggregation kind.
     name: Name of the counter.
   """
 
   class KindValueValuesEnum(_messages.Enum):
-    """Counter aggregation kind.
+    r"""Counter aggregation kind.
 
     Values:
       INVALID: Counter aggregation kind was not set.
       SUM: Aggregated value is the sum of all contributed values.
       MAX: Aggregated value is the max of all contributed values.
       MIN: Aggregated value is the min of all contributed values.
       MEAN: Aggregated value is the mean of all contributed values.
@@ -2851,15 +2858,15 @@
     LATEST_VALUE = 9
 
   kind = _messages.EnumField('KindValueValuesEnum', 1)
   name = _messages.StringField(2)
 
 
 class Package(_messages.Message):
-  """The packages that must be installed in order for a worker to run the
+  r"""The packages that must be installed in order for a worker to run the
   steps of the Cloud Dataflow job that will be assigned to its worker pool.
   This is the mechanism by which the Cloud Dataflow SDK causes code to be
   loaded onto the workers. For example, the Cloud Dataflow Java SDK might use
   this to install jars containing the user's code and all of the various
   dependencies (libraries, data files, etc.) required in order for that code
   to run.
 
@@ -2871,16 +2878,16 @@
   """
 
   location = _messages.StringField(1)
   name = _messages.StringField(2)
 
 
 class ParDoInstruction(_messages.Message):
-  """An instruction that does a ParDo operation. Takes one main input and zero
-  or more side inputs, and produces zero or more outputs. Runs user code.
+  r"""An instruction that does a ParDo operation. Takes one main input and
+  zero or more side inputs, and produces zero or more outputs. Runs user code.
 
   Messages:
     UserFnValue: The user function to invoke.
 
   Fields:
     input: The input.
     multiOutputInfos: Information about each of the outputs, if user_fn is a
@@ -2888,25 +2895,25 @@
     numOutputs: The number of outputs.
     sideInputs: Zero or more side inputs.
     userFn: The user function to invoke.
   """
 
   @encoding.MapUnrecognizedFields('additionalProperties')
   class UserFnValue(_messages.Message):
-    """The user function to invoke.
+    r"""The user function to invoke.
 
     Messages:
       AdditionalProperty: An additional property for a UserFnValue object.
 
     Fields:
       additionalProperties: Properties of the object.
     """
 
     class AdditionalProperty(_messages.Message):
-      """An additional property for a UserFnValue object.
+      r"""An additional property for a UserFnValue object.
 
       Fields:
         key: Name of the additional property.
         value: A extra_types.JsonValue attribute.
       """
 
       key = _messages.StringField(1)
@@ -2918,15 +2925,15 @@
   multiOutputInfos = _messages.MessageField('MultiOutputInfo', 2, repeated=True)
   numOutputs = _messages.IntegerField(3, variant=_messages.Variant.INT32)
   sideInputs = _messages.MessageField('SideInputInfo', 4, repeated=True)
   userFn = _messages.MessageField('UserFnValue', 5)
 
 
 class ParallelInstruction(_messages.Message):
-  """Describes a particular operation comprising a MapTask.
+  r"""Describes a particular operation comprising a MapTask.
 
   Fields:
     flatten: Additional information for Flatten instructions.
     name: User-provided name of this operation.
     originalName: System-defined name for the operation in the original
       workflow graph.
     outputs: Describes the outputs of the instruction.
@@ -2947,27 +2954,27 @@
   partialGroupByKey = _messages.MessageField('PartialGroupByKeyInstruction', 6)
   read = _messages.MessageField('ReadInstruction', 7)
   systemName = _messages.StringField(8)
   write = _messages.MessageField('WriteInstruction', 9)
 
 
 class Parameter(_messages.Message):
-  """Structured data associated with this message.
+  r"""Structured data associated with this message.
 
   Fields:
     key: Key or name for this parameter.
     value: Value for this parameter.
   """
 
   key = _messages.StringField(1)
   value = _messages.MessageField('extra_types.JsonValue', 2)
 
 
 class ParameterMetadata(_messages.Message):
-  """Metadata for a specific parameter.
+  r"""Metadata for a specific parameter.
 
   Fields:
     helpText: Required. The help text to display for the parameter.
     isOptional: Optional. Whether the parameter is optional. Defaults to
       false.
     label: Required. The label to display for the parameter.
     name: Required. The name of the parameter.
@@ -2978,15 +2985,15 @@
   isOptional = _messages.BooleanField(2)
   label = _messages.StringField(3)
   name = _messages.StringField(4)
   regexes = _messages.StringField(5, repeated=True)
 
 
 class PartialGroupByKeyInstruction(_messages.Message):
-  """An instruction that does a partial group-by-key. One input and one
+  r"""An instruction that does a partial group-by-key. One input and one
   output.
 
   Messages:
     InputElementCodecValue: The codec to use for interpreting an element in
       the input PTable.
     ValueCombiningFnValue: The value combining function to invoke.
 
@@ -3002,51 +3009,51 @@
       this instruction.
     sideInputs: Zero or more side inputs.
     valueCombiningFn: The value combining function to invoke.
   """
 
   @encoding.MapUnrecognizedFields('additionalProperties')
   class InputElementCodecValue(_messages.Message):
-    """The codec to use for interpreting an element in the input PTable.
+    r"""The codec to use for interpreting an element in the input PTable.
 
     Messages:
       AdditionalProperty: An additional property for a InputElementCodecValue
         object.
 
     Fields:
       additionalProperties: Properties of the object.
     """
 
     class AdditionalProperty(_messages.Message):
-      """An additional property for a InputElementCodecValue object.
+      r"""An additional property for a InputElementCodecValue object.
 
       Fields:
         key: Name of the additional property.
         value: A extra_types.JsonValue attribute.
       """
 
       key = _messages.StringField(1)
       value = _messages.MessageField('extra_types.JsonValue', 2)
 
     additionalProperties = _messages.MessageField('AdditionalProperty', 1, repeated=True)
 
   @encoding.MapUnrecognizedFields('additionalProperties')
   class ValueCombiningFnValue(_messages.Message):
-    """The value combining function to invoke.
+    r"""The value combining function to invoke.
 
     Messages:
       AdditionalProperty: An additional property for a ValueCombiningFnValue
         object.
 
     Fields:
       additionalProperties: Properties of the object.
     """
 
     class AdditionalProperty(_messages.Message):
-      """An additional property for a ValueCombiningFnValue object.
+      r"""An additional property for a ValueCombiningFnValue object.
 
       Fields:
         key: Name of the additional property.
         value: A extra_types.JsonValue attribute.
       """
 
       key = _messages.StringField(1)
@@ -3059,15 +3066,15 @@
   originalCombineValuesInputStoreName = _messages.StringField(3)
   originalCombineValuesStepName = _messages.StringField(4)
   sideInputs = _messages.MessageField('SideInputInfo', 5, repeated=True)
   valueCombiningFn = _messages.MessageField('ValueCombiningFnValue', 6)
 
 
 class PipelineDescription(_messages.Message):
-  """A descriptive representation of submitted pipeline as well as the
+  r"""A descriptive representation of submitted pipeline as well as the
   executed form.  This data is provided by the Dataflow service for ease of
   visualizing the pipeline and interpretting Dataflow provided metrics.
 
   Fields:
     displayData: Pipeline level display data.
     executionPipelineStage: Description of each stage of execution of the
       pipeline.
@@ -3077,15 +3084,15 @@
 
   displayData = _messages.MessageField('DisplayData', 1, repeated=True)
   executionPipelineStage = _messages.MessageField('ExecutionStageSummary', 2, repeated=True)
   originalPipelineTransform = _messages.MessageField('TransformSummary', 3, repeated=True)
 
 
 class Position(_messages.Message):
-  """Position defines a position within a collection of data.  The value can
+  r"""Position defines a position within a collection of data.  The value can
   be either the end position, a key (used with ordered collections), a byte
   offset, or a record index.
 
   Fields:
     byteOffset: Position is a byte offset.
     concatPosition: CloudPosition is a concat position.
     end: Position is past all other positions. Also useful for the end
@@ -3101,27 +3108,27 @@
   end = _messages.BooleanField(3)
   key = _messages.StringField(4)
   recordIndex = _messages.IntegerField(5)
   shufflePosition = _messages.StringField(6)
 
 
 class PubSubIODetails(_messages.Message):
-  """Metadata for a PubSub connector used by the job.
+  r"""Metadata for a PubSub connector used by the job.
 
   Fields:
     subscription: Subscription used in the connection.
     topic: Topic accessed in the connection.
   """
 
   subscription = _messages.StringField(1)
   topic = _messages.StringField(2)
 
 
 class PubsubLocation(_messages.Message):
-  """Identifies a pubsub location to use for transferring data into or out of
+  r"""Identifies a pubsub location to use for transferring data into or out of
   a streaming Dataflow job.
 
   Fields:
     dropLateData: Indicates whether the pipeline allows late-arriving data.
     idLabel: If set, contains a pubsub label from which to extract record ids.
       If left empty, record deduplication will be strictly best effort.
     subscription: A pubsub subscription, in the form of
@@ -3143,25 +3150,25 @@
   timestampLabel = _messages.StringField(4)
   topic = _messages.StringField(5)
   trackingSubscription = _messages.StringField(6)
   withAttributes = _messages.BooleanField(7)
 
 
 class ReadInstruction(_messages.Message):
-  """An instruction that reads records. Takes no inputs, produces one output.
+  r"""An instruction that reads records. Takes no inputs, produces one output.
 
   Fields:
     source: The source to read from.
   """
 
   source = _messages.MessageField('Source', 1)
 
 
 class ReportWorkItemStatusRequest(_messages.Message):
-  """Request to report the status of WorkItems.
+  r"""Request to report the status of WorkItems.
 
   Fields:
     currentWorkerTime: The current timestamp at the worker.
     location: The location which contains the WorkItem's job.
     workItemStatuses: The order is unimportant, except that the order of the
       WorkItemServiceState messages in the ReportWorkItemStatusResponse
       corresponds to the order of WorkItemStatus messages here.
@@ -3174,28 +3181,28 @@
   currentWorkerTime = _messages.StringField(1)
   location = _messages.StringField(2)
   workItemStatuses = _messages.MessageField('WorkItemStatus', 3, repeated=True)
   workerId = _messages.StringField(4)
 
 
 class ReportWorkItemStatusResponse(_messages.Message):
-  """Response from a request to report the status of WorkItems.
+  r"""Response from a request to report the status of WorkItems.
 
   Fields:
     workItemServiceStates: A set of messages indicating the service-side state
       for each WorkItem whose status was reported, in the same order as the
       WorkItemStatus messages in the ReportWorkItemStatusRequest which
       resulting in this response.
   """
 
   workItemServiceStates = _messages.MessageField('WorkItemServiceState', 1, repeated=True)
 
 
 class ReportedParallelism(_messages.Message):
-  """Represents the level of parallelism in a WorkItem's input, reported by
+  r"""Represents the level of parallelism in a WorkItem's input, reported by
   the worker.
 
   Fields:
     isInfinite: Specifies whether the parallelism is infinite. If true,
       "value" is ignored. Infinite parallelism means the service will assume
       that the work item can always be split into more non-empty work items by
       dynamic splitting. This is a work-around for lack of support for
@@ -3204,33 +3211,33 @@
   """
 
   isInfinite = _messages.BooleanField(1)
   value = _messages.FloatField(2)
 
 
 class ResourceUtilizationReport(_messages.Message):
-  """Worker metrics exported from workers. This contains resource utilization
+  r"""Worker metrics exported from workers. This contains resource utilization
   metrics accumulated from a variety of sources. For more information, see go
   /df-resource-signals.
 
   Fields:
     cpuTime: CPU utilization samples.
   """
 
   cpuTime = _messages.MessageField('CPUTime', 1, repeated=True)
 
 
 class ResourceUtilizationReportResponse(_messages.Message):
-  """Service-side response to WorkerMessage reporting resource utilization.
+  r"""Service-side response to WorkerMessage reporting resource utilization.
   """
 
 
 
 class RuntimeEnvironment(_messages.Message):
-  """The environment values to set at runtime.
+  r"""The environment values to set at runtime.
 
   Fields:
     additionalExperiments: Additional experiment flags for the job.
     bypassTempDirValidation: Whether to bypass the safety checks for the job's
       temporary directory. Use with caution.
     machineType: The machine type to use for the job. Defaults to the value
       from the template if not specified.
@@ -3257,27 +3264,27 @@
   serviceAccountEmail = _messages.StringField(6)
   subnetwork = _messages.StringField(7)
   tempLocation = _messages.StringField(8)
   zone = _messages.StringField(9)
 
 
 class SdkVersion(_messages.Message):
-  """The version of the SDK used to run the jobl
+  r"""The version of the SDK used to run the jobl
 
   Enums:
     SdkSupportStatusValueValuesEnum: The support status for this SDK version.
 
   Fields:
     sdkSupportStatus: The support status for this SDK version.
     version: The version of the SDK used to run the job.
     versionDisplayName: A readable string describing the version of the sdk.
   """
 
   class SdkSupportStatusValueValuesEnum(_messages.Enum):
-    """The support status for this SDK version.
+    r"""The support status for this SDK version.
 
     Values:
       UNKNOWN: Cloud Dataflow is unaware of this version.
       SUPPORTED: This is a known version of an SDK, and is supported.
       STALE: A newer version of the SDK family exists, and an update is
         recommended.
       DEPRECATED: This version of the SDK is deprecated and will eventually be
@@ -3293,15 +3300,15 @@
 
   sdkSupportStatus = _messages.EnumField('SdkSupportStatusValueValuesEnum', 1)
   version = _messages.StringField(2)
   versionDisplayName = _messages.StringField(3)
 
 
 class SendDebugCaptureRequest(_messages.Message):
-  """Request to send encoded debug information.
+  r"""Request to send encoded debug information.
 
   Fields:
     componentId: The internal component id for which debug information is
       sent.
     data: The encoded debug information.
     location: The location which contains the job specified by job_id.
     workerId: The worker id, i.e., VM hostname.
@@ -3310,42 +3317,42 @@
   componentId = _messages.StringField(1)
   data = _messages.StringField(2)
   location = _messages.StringField(3)
   workerId = _messages.StringField(4)
 
 
 class SendDebugCaptureResponse(_messages.Message):
-  """Response to a send capture request.
+  r"""Response to a send capture request.
 nothing"""
 
 
 class SendWorkerMessagesRequest(_messages.Message):
-  """A request for sending worker messages to the service.
+  r"""A request for sending worker messages to the service.
 
   Fields:
     location: The location which contains the job
     workerMessages: The WorkerMessages to send.
   """
 
   location = _messages.StringField(1)
   workerMessages = _messages.MessageField('WorkerMessage', 2, repeated=True)
 
 
 class SendWorkerMessagesResponse(_messages.Message):
-  """The response to the worker messages.
+  r"""The response to the worker messages.
 
   Fields:
     workerMessageResponses: The servers response to the worker messages.
   """
 
   workerMessageResponses = _messages.MessageField('WorkerMessageResponse', 1, repeated=True)
 
 
 class SeqMapTask(_messages.Message):
-  """Describes a particular function to invoke.
+  r"""Describes a particular function to invoke.
 
   Messages:
     UserFnValue: The user function to invoke.
 
   Fields:
     inputs: Information about each of the inputs.
     name: The user-provided name of the SeqDo operation.
@@ -3355,25 +3362,25 @@
     systemName: System-defined name of the SeqDo operation. Unique across the
       workflow.
     userFn: The user function to invoke.
   """
 
   @encoding.MapUnrecognizedFields('additionalProperties')
   class UserFnValue(_messages.Message):
-    """The user function to invoke.
+    r"""The user function to invoke.
 
     Messages:
       AdditionalProperty: An additional property for a UserFnValue object.
 
     Fields:
       additionalProperties: Properties of the object.
     """
 
     class AdditionalProperty(_messages.Message):
-      """An additional property for a UserFnValue object.
+      r"""An additional property for a UserFnValue object.
 
       Fields:
         key: Name of the additional property.
         value: A extra_types.JsonValue attribute.
       """
 
       key = _messages.StringField(1)
@@ -3386,39 +3393,39 @@
   outputInfos = _messages.MessageField('SeqMapTaskOutputInfo', 3, repeated=True)
   stageName = _messages.StringField(4)
   systemName = _messages.StringField(5)
   userFn = _messages.MessageField('UserFnValue', 6)
 
 
 class SeqMapTaskOutputInfo(_messages.Message):
-  """Information about an output of a SeqMapTask.
+  r"""Information about an output of a SeqMapTask.
 
   Fields:
     sink: The sink to write the output value to.
     tag: The id of the TupleTag the user code will tag the output value by.
   """
 
   sink = _messages.MessageField('Sink', 1)
   tag = _messages.StringField(2)
 
 
 class ShellTask(_messages.Message):
-  """A task which consists of a shell command for the worker to execute.
+  r"""A task which consists of a shell command for the worker to execute.
 
   Fields:
     command: The shell command to run.
     exitCode: Exit code for the task.
   """
 
   command = _messages.StringField(1)
   exitCode = _messages.IntegerField(2, variant=_messages.Variant.INT32)
 
 
 class SideInputInfo(_messages.Message):
-  """Information about a side input of a DoFn or an input of a SeqDoFn.
+  r"""Information about a side input of a DoFn or an input of a SeqDoFn.
 
   Messages:
     KindValue: How to interpret the source element(s) as a side input value.
 
   Fields:
     kind: How to interpret the source element(s) as a side input value.
     sources: The source(s) to read element(s) from to get the value of this
@@ -3427,25 +3434,25 @@
       source is required.
     tag: The id of the tag the user code will access this side input by; this
       should correspond to the tag of some MultiOutputInfo.
   """
 
   @encoding.MapUnrecognizedFields('additionalProperties')
   class KindValue(_messages.Message):
-    """How to interpret the source element(s) as a side input value.
+    r"""How to interpret the source element(s) as a side input value.
 
     Messages:
       AdditionalProperty: An additional property for a KindValue object.
 
     Fields:
       additionalProperties: Properties of the object.
     """
 
     class AdditionalProperty(_messages.Message):
-      """An additional property for a KindValue object.
+      r"""An additional property for a KindValue object.
 
       Fields:
         key: Name of the additional property.
         value: A extra_types.JsonValue attribute.
       """
 
       key = _messages.StringField(1)
@@ -3455,62 +3462,62 @@
 
   kind = _messages.MessageField('KindValue', 1)
   sources = _messages.MessageField('Source', 2, repeated=True)
   tag = _messages.StringField(3)
 
 
 class Sink(_messages.Message):
-  """A sink that records can be encoded and written to.
+  r"""A sink that records can be encoded and written to.
 
   Messages:
     CodecValue: The codec to use to encode data written to the sink.
     SpecValue: The sink to write to, plus its parameters.
 
   Fields:
     codec: The codec to use to encode data written to the sink.
     spec: The sink to write to, plus its parameters.
   """
 
   @encoding.MapUnrecognizedFields('additionalProperties')
   class CodecValue(_messages.Message):
-    """The codec to use to encode data written to the sink.
+    r"""The codec to use to encode data written to the sink.
 
     Messages:
       AdditionalProperty: An additional property for a CodecValue object.
 
     Fields:
       additionalProperties: Properties of the object.
     """
 
     class AdditionalProperty(_messages.Message):
-      """An additional property for a CodecValue object.
+      r"""An additional property for a CodecValue object.
 
       Fields:
         key: Name of the additional property.
         value: A extra_types.JsonValue attribute.
       """
 
       key = _messages.StringField(1)
       value = _messages.MessageField('extra_types.JsonValue', 2)
 
     additionalProperties = _messages.MessageField('AdditionalProperty', 1, repeated=True)
 
   @encoding.MapUnrecognizedFields('additionalProperties')
   class SpecValue(_messages.Message):
-    """The sink to write to, plus its parameters.
+    r"""The sink to write to, plus its parameters.
 
     Messages:
       AdditionalProperty: An additional property for a SpecValue object.
 
     Fields:
       additionalProperties: Properties of the object.
     """
 
     class AdditionalProperty(_messages.Message):
-      """An additional property for a SpecValue object.
+      r"""An additional property for a SpecValue object.
 
       Fields:
         key: Name of the additional property.
         value: A extra_types.JsonValue attribute.
       """
 
       key = _messages.StringField(1)
@@ -3519,15 +3526,15 @@
     additionalProperties = _messages.MessageField('AdditionalProperty', 1, repeated=True)
 
   codec = _messages.MessageField('CodecValue', 1)
   spec = _messages.MessageField('SpecValue', 2)
 
 
 class Source(_messages.Message):
-  """A source that records can be read and decoded from.
+  r"""A source that records can be read and decoded from.
 
   Messages:
     BaseSpecsValueListEntry: A BaseSpecsValueListEntry object.
     CodecValue: The codec to use to decode data read from the source.
     SpecValue: The source to read from, plus its parameters.
 
   Fields:
@@ -3558,74 +3565,74 @@
       Source objects supplied by the framework to the user don't have this
       field populated.
     spec: The source to read from, plus its parameters.
   """
 
   @encoding.MapUnrecognizedFields('additionalProperties')
   class BaseSpecsValueListEntry(_messages.Message):
-    """A BaseSpecsValueListEntry object.
+    r"""A BaseSpecsValueListEntry object.
 
     Messages:
       AdditionalProperty: An additional property for a BaseSpecsValueListEntry
         object.
 
     Fields:
       additionalProperties: Properties of the object.
     """
 
     class AdditionalProperty(_messages.Message):
-      """An additional property for a BaseSpecsValueListEntry object.
+      r"""An additional property for a BaseSpecsValueListEntry object.
 
       Fields:
         key: Name of the additional property.
         value: A extra_types.JsonValue attribute.
       """
 
       key = _messages.StringField(1)
       value = _messages.MessageField('extra_types.JsonValue', 2)
 
     additionalProperties = _messages.MessageField('AdditionalProperty', 1, repeated=True)
 
   @encoding.MapUnrecognizedFields('additionalProperties')
   class CodecValue(_messages.Message):
-    """The codec to use to decode data read from the source.
+    r"""The codec to use to decode data read from the source.
 
     Messages:
       AdditionalProperty: An additional property for a CodecValue object.
 
     Fields:
       additionalProperties: Properties of the object.
     """
 
     class AdditionalProperty(_messages.Message):
-      """An additional property for a CodecValue object.
+      r"""An additional property for a CodecValue object.
 
       Fields:
         key: Name of the additional property.
         value: A extra_types.JsonValue attribute.
       """
 
       key = _messages.StringField(1)
       value = _messages.MessageField('extra_types.JsonValue', 2)
 
     additionalProperties = _messages.MessageField('AdditionalProperty', 1, repeated=True)
 
   @encoding.MapUnrecognizedFields('additionalProperties')
   class SpecValue(_messages.Message):
-    """The source to read from, plus its parameters.
+    r"""The source to read from, plus its parameters.
 
     Messages:
       AdditionalProperty: An additional property for a SpecValue object.
 
     Fields:
       additionalProperties: Properties of the object.
     """
 
     class AdditionalProperty(_messages.Message):
-      """An additional property for a SpecValue object.
+      r"""An additional property for a SpecValue object.
 
       Fields:
         key: Name of the additional property.
         value: A extra_types.JsonValue attribute.
       """
 
       key = _messages.StringField(1)
@@ -3637,15 +3644,15 @@
   codec = _messages.MessageField('CodecValue', 2)
   doesNotNeedSplitting = _messages.BooleanField(3)
   metadata = _messages.MessageField('SourceMetadata', 4)
   spec = _messages.MessageField('SpecValue', 5)
 
 
 class SourceFork(_messages.Message):
-  """DEPRECATED in favor of DynamicSourceSplit.
+  r"""DEPRECATED in favor of DynamicSourceSplit.
 
   Fields:
     primary: DEPRECATED
     primarySource: DEPRECATED
     residual: DEPRECATED
     residualSource: DEPRECATED
   """
@@ -3653,35 +3660,35 @@
   primary = _messages.MessageField('SourceSplitShard', 1)
   primarySource = _messages.MessageField('DerivedSource', 2)
   residual = _messages.MessageField('SourceSplitShard', 3)
   residualSource = _messages.MessageField('DerivedSource', 4)
 
 
 class SourceGetMetadataRequest(_messages.Message):
-  """A request to compute the SourceMetadata of a Source.
+  r"""A request to compute the SourceMetadata of a Source.
 
   Fields:
     source: Specification of the source whose metadata should be computed.
   """
 
   source = _messages.MessageField('Source', 1)
 
 
 class SourceGetMetadataResponse(_messages.Message):
-  """The result of a SourceGetMetadataOperation.
+  r"""The result of a SourceGetMetadataOperation.
 
   Fields:
     metadata: The computed metadata.
   """
 
   metadata = _messages.MessageField('SourceMetadata', 1)
 
 
 class SourceMetadata(_messages.Message):
-  """Metadata about a Source useful for automatically optimizing and tuning
+  r"""Metadata about a Source useful for automatically optimizing and tuning
   the pipeline, etc.
 
   Fields:
     estimatedSizeBytes: An estimate of the total size (in bytes) of the data
       that would be read from this source.  This estimate is in terms of
       external storage size, before any decompression or other processing done
       by the reader.
@@ -3693,15 +3700,15 @@
 
   estimatedSizeBytes = _messages.IntegerField(1)
   infinite = _messages.BooleanField(2)
   producesSortedKeys = _messages.BooleanField(3)
 
 
 class SourceOperationRequest(_messages.Message):
-  """A work item that represents the different operations that can be
+  r"""A work item that represents the different operations that can be
   performed on a user-defined Source specification.
 
   Fields:
     getMetadata: Information about a request to get metadata about a source.
     name: User-provided name of the Read instruction for this source.
     originalName: System-defined name for the Read instruction for this source
       in the original workflow graph.
@@ -3717,43 +3724,43 @@
   originalName = _messages.StringField(3)
   split = _messages.MessageField('SourceSplitRequest', 4)
   stageName = _messages.StringField(5)
   systemName = _messages.StringField(6)
 
 
 class SourceOperationResponse(_messages.Message):
-  """The result of a SourceOperationRequest, specified in
+  r"""The result of a SourceOperationRequest, specified in
   ReportWorkItemStatusRequest.source_operation when the work item is
   completed.
 
   Fields:
     getMetadata: A response to a request to get metadata about a source.
     split: A response to a request to split a source.
   """
 
   getMetadata = _messages.MessageField('SourceGetMetadataResponse', 1)
   split = _messages.MessageField('SourceSplitResponse', 2)
 
 
 class SourceSplitOptions(_messages.Message):
-  """Hints for splitting a Source into bundles (parts for parallel processing)
-  using SourceSplitRequest.
+  r"""Hints for splitting a Source into bundles (parts for parallel
+  processing) using SourceSplitRequest.
 
   Fields:
     desiredBundleSizeBytes: The source should be split into a set of bundles
       where the estimated size of each is approximately this many bytes.
     desiredShardSizeBytes: DEPRECATED in favor of desired_bundle_size_bytes.
   """
 
   desiredBundleSizeBytes = _messages.IntegerField(1)
   desiredShardSizeBytes = _messages.IntegerField(2)
 
 
 class SourceSplitRequest(_messages.Message):
-  """Represents the operation to split a high-level Source specification into
+  r"""Represents the operation to split a high-level Source specification into
   bundles (parts for parallel processing).  At a high level, splitting of a
   source into bundles happens as follows: SourceSplitRequest is applied to the
   source. If it returns SOURCE_SPLIT_OUTCOME_USE_CURRENT, no further splitting
   happens and the source is used "as is". Otherwise, splitting is applied
   recursively to each produced DerivedSource.  As an optimization, for any
   Source, if its does_not_need_splitting is true, the framework assumes that
   splitting this source would return SOURCE_SPLIT_OUTCOME_USE_CURRENT, and
@@ -3766,15 +3773,15 @@
   """
 
   options = _messages.MessageField('SourceSplitOptions', 1)
   source = _messages.MessageField('Source', 2)
 
 
 class SourceSplitResponse(_messages.Message):
-  """The response to a SourceSplitRequest.
+  r"""The response to a SourceSplitRequest.
 
   Enums:
     OutcomeValueValuesEnum: Indicates whether splitting happened and produced
       a list of bundles. If this is USE_CURRENT_SOURCE_AS_IS, the current
       source should be processed "as is" without splitting. "bundles" is
       ignored in this case. If this is SPLITTING_HAPPENED, then "bundles"
       contains a list of bundles into which the source was split.
@@ -3788,19 +3795,19 @@
       be processed "as is" without splitting. "bundles" is ignored in this
       case. If this is SPLITTING_HAPPENED, then "bundles" contains a list of
       bundles into which the source was split.
     shards: DEPRECATED in favor of bundles.
   """
 
   class OutcomeValueValuesEnum(_messages.Enum):
-    """Indicates whether splitting happened and produced a list of bundles. If
-    this is USE_CURRENT_SOURCE_AS_IS, the current source should be processed
-    "as is" without splitting. "bundles" is ignored in this case. If this is
-    SPLITTING_HAPPENED, then "bundles" contains a list of bundles into which
-    the source was split.
+    r"""Indicates whether splitting happened and produced a list of bundles.
+    If this is USE_CURRENT_SOURCE_AS_IS, the current source should be
+    processed "as is" without splitting. "bundles" is ignored in this case. If
+    this is SPLITTING_HAPPENED, then "bundles" contains a list of bundles into
+    which the source was split.
 
     Values:
       SOURCE_SPLIT_OUTCOME_UNKNOWN: The source split outcome is unknown, or
         unspecified.
       SOURCE_SPLIT_OUTCOME_USE_CURRENT: The current source should be processed
         "as is" without splitting.
       SOURCE_SPLIT_OUTCOME_SPLITTING_HAPPENED: Splitting produced a list of
@@ -3812,26 +3819,26 @@
 
   bundles = _messages.MessageField('DerivedSource', 1, repeated=True)
   outcome = _messages.EnumField('OutcomeValueValuesEnum', 2)
   shards = _messages.MessageField('SourceSplitShard', 3, repeated=True)
 
 
 class SourceSplitShard(_messages.Message):
-  """DEPRECATED in favor of DerivedSource.
+  r"""DEPRECATED in favor of DerivedSource.
 
   Enums:
     DerivationModeValueValuesEnum: DEPRECATED
 
   Fields:
     derivationMode: DEPRECATED
     source: DEPRECATED
   """
 
   class DerivationModeValueValuesEnum(_messages.Enum):
-    """DEPRECATED
+    r"""DEPRECATED
 
     Values:
       SOURCE_DERIVATION_MODE_UNKNOWN: The source derivation is unknown, or
         unspecified.
       SOURCE_DERIVATION_MODE_INDEPENDENT: Produce a completely independent
         Source with no base.
       SOURCE_DERIVATION_MODE_CHILD_OF_CURRENT: Produce a Source based on the
@@ -3845,42 +3852,42 @@
     SOURCE_DERIVATION_MODE_SIBLING_OF_CURRENT = 3
 
   derivationMode = _messages.EnumField('DerivationModeValueValuesEnum', 1)
   source = _messages.MessageField('Source', 2)
 
 
 class SpannerIODetails(_messages.Message):
-  """Metadata for a Spanner connector used by the job.
+  r"""Metadata for a Spanner connector used by the job.
 
   Fields:
     databaseId: DatabaseId accessed in the connection.
     instanceId: InstanceId accessed in the connection.
     projectId: ProjectId accessed in the connection.
   """
 
   databaseId = _messages.StringField(1)
   instanceId = _messages.StringField(2)
   projectId = _messages.StringField(3)
 
 
 class SplitInt64(_messages.Message):
-  """A representation of an int64, n, that is immune to precision loss when
+  r"""A representation of an int64, n, that is immune to precision loss when
   encoded in JSON.
 
   Fields:
     highBits: The high order bits, including the sign: n >> 32.
     lowBits: The low order bits: n & 0xffffffff.
   """
 
   highBits = _messages.IntegerField(1, variant=_messages.Variant.INT32)
   lowBits = _messages.IntegerField(2, variant=_messages.Variant.UINT32)
 
 
 class StageSource(_messages.Message):
-  """Description of an input or output of an execution stage.
+  r"""Description of an input or output of an execution stage.
 
   Fields:
     name: Dataflow service generated name for this source.
     originalTransformOrCollection: User name for the original user transform
       or collection with which this source is most closely associated.
     sizeBytes: Size of the source, if measurable.
     userName: Human-readable name for this source; may be user or system
@@ -3890,15 +3897,15 @@
   name = _messages.StringField(1)
   originalTransformOrCollection = _messages.StringField(2)
   sizeBytes = _messages.IntegerField(3)
   userName = _messages.StringField(4)
 
 
 class StandardQueryParameters(_messages.Message):
-  """Query parameters accepted by all methods.
+  r"""Query parameters accepted by all methods.
 
   Enums:
     FXgafvValueValuesEnum: V1 error format.
     AltValueValuesEnum: Data format for response.
 
   Fields:
     f__xgafv: V1 error format.
@@ -3917,27 +3924,27 @@
     trace: A tracing token of the form "token:<tokenid>" to include in api
       requests.
     uploadType: Legacy upload protocol for media (e.g. "media", "multipart").
     upload_protocol: Upload protocol for media (e.g. "raw", "multipart").
   """
 
   class AltValueValuesEnum(_messages.Enum):
-    """Data format for response.
+    r"""Data format for response.
 
     Values:
       json: Responses with Content-Type of application/json
       media: Media download with context-dependent Content-Type
       proto: Responses with Content-Type of application/x-protobuf
     """
     json = 0
     media = 1
     proto = 2
 
   class FXgafvValueValuesEnum(_messages.Enum):
-    """V1 error format.
+    r"""V1 error format.
 
     Values:
       _1: v1 error format
       _2: v2 error format
     """
     _1 = 0
     _2 = 1
@@ -3953,27 +3960,27 @@
   quotaUser = _messages.StringField(9)
   trace = _messages.StringField(10)
   uploadType = _messages.StringField(11)
   upload_protocol = _messages.StringField(12)
 
 
 class StateFamilyConfig(_messages.Message):
-  """State family configuration.
+  r"""State family configuration.
 
   Fields:
     isRead: If true, this family corresponds to a read operation.
     stateFamily: The state family value.
   """
 
   isRead = _messages.BooleanField(1)
   stateFamily = _messages.StringField(2)
 
 
 class Status(_messages.Message):
-  """The `Status` type defines a logical error model that is suitable for
+  r"""The `Status` type defines a logical error model that is suitable for
   different programming environments, including REST APIs and RPC APIs. It is
   used by [gRPC](https://github.com/grpc). The error model is designed to be:
   - Simple to use and understand for most users - Flexible enough to meet
   unexpected needs  # Overview  The `Status` message contains three pieces of
   data: error code, error message, and error details. The error code should be
   an enum value of google.rpc.Code, but it may accept additional error codes
   if needed.  The error message should be a developer-facing English message
@@ -4013,27 +4020,27 @@
     message: A developer-facing error message, which should be in English. Any
       user-facing error message should be localized and sent in the
       google.rpc.Status.details field, or localized by the client.
   """
 
   @encoding.MapUnrecognizedFields('additionalProperties')
   class DetailsValueListEntry(_messages.Message):
-    """A DetailsValueListEntry object.
+    r"""A DetailsValueListEntry object.
 
     Messages:
       AdditionalProperty: An additional property for a DetailsValueListEntry
         object.
 
     Fields:
       additionalProperties: Properties of the object. Contains field @type
         with type URL.
     """
 
     class AdditionalProperty(_messages.Message):
-      """An additional property for a DetailsValueListEntry object.
+      r"""An additional property for a DetailsValueListEntry object.
 
       Fields:
         key: Name of the additional property.
         value: A extra_types.JsonValue attribute.
       """
 
       key = _messages.StringField(1)
@@ -4043,16 +4050,16 @@
 
   code = _messages.IntegerField(1, variant=_messages.Variant.INT32)
   details = _messages.MessageField('DetailsValueListEntry', 2, repeated=True)
   message = _messages.StringField(3)
 
 
 class Step(_messages.Message):
-  """Defines a particular step within a Cloud Dataflow job.  A job consists of
-  multiple steps, each of which performs some specific operation as part of
+  r"""Defines a particular step within a Cloud Dataflow job.  A job consists
+  of multiple steps, each of which performs some specific operation as part of
   the overall job.  Data is typically passed from one step to another as part
   of the job.  Here's an example of a sequence of steps which together
   implement a Map-Reduce job:    * Read a collection of data from some source,
   parsing the     collection's elements.    * Validate the elements.    *
   Apply a user-defined function to map each element to some value     and
   extract an element-specific key value.    * Group elements with the same key
   into a single element with     that key, transforming a multiply-keyed
@@ -4072,27 +4079,27 @@
     properties: Named properties associated with the step. Each kind of
       predefined step has its own required set of properties. Must be provided
       on Create.  Only retrieved with JOB_VIEW_ALL.
   """
 
   @encoding.MapUnrecognizedFields('additionalProperties')
   class PropertiesValue(_messages.Message):
-    """Named properties associated with the step. Each kind of predefined step
-    has its own required set of properties. Must be provided on Create.  Only
-    retrieved with JOB_VIEW_ALL.
+    r"""Named properties associated with the step. Each kind of predefined
+    step has its own required set of properties. Must be provided on Create.
+    Only retrieved with JOB_VIEW_ALL.
 
     Messages:
       AdditionalProperty: An additional property for a PropertiesValue object.
 
     Fields:
       additionalProperties: Properties of the object.
     """
 
     class AdditionalProperty(_messages.Message):
-      """An additional property for a PropertiesValue object.
+      r"""An additional property for a PropertiesValue object.
 
       Fields:
         key: Name of the additional property.
         value: A extra_types.JsonValue attribute.
       """
 
       key = _messages.StringField(1)
@@ -4102,15 +4109,15 @@
 
   kind = _messages.StringField(1)
   name = _messages.StringField(2)
   properties = _messages.MessageField('PropertiesValue', 3)
 
 
 class StreamLocation(_messages.Message):
-  """Describes a stream of data, either as input to be processed or as output
+  r"""Describes a stream of data, either as input to be processed or as output
   of a streaming Dataflow job.
 
   Fields:
     customSourceLocation: The stream is a custom source.
     pubsubLocation: The stream is a pubsub stream.
     sideInputLocation: The stream is a streaming side input.
     streamingStageLocation: The stream is part of another computation within
@@ -4119,16 +4126,30 @@
 
   customSourceLocation = _messages.MessageField('CustomSourceLocation', 1)
   pubsubLocation = _messages.MessageField('PubsubLocation', 2)
   sideInputLocation = _messages.MessageField('StreamingSideInputLocation', 3)
   streamingStageLocation = _messages.MessageField('StreamingStageLocation', 4)
 
 
+class StreamingApplianceSnapshotConfig(_messages.Message):
+  r"""Streaming appliance snapshot configuration.
+
+  Fields:
+    importStateEndpoint: Indicates which endpoint is used to import appliance
+      state.
+    snapshotId: If set, indicates the snapshot id for the snapshot being
+      performed.
+  """
+
+  importStateEndpoint = _messages.StringField(1)
+  snapshotId = _messages.StringField(2)
+
+
 class StreamingComputationConfig(_messages.Message):
-  """Configuration information for a single streaming computation.
+  r"""Configuration information for a single streaming computation.
 
   Fields:
     computationId: Unique identifier for this computation.
     instructions: Instructions that comprise the computation.
     stageName: Stage name of this computation.
     systemName: System defined name for this computation.
   """
@@ -4136,42 +4157,42 @@
   computationId = _messages.StringField(1)
   instructions = _messages.MessageField('ParallelInstruction', 2, repeated=True)
   stageName = _messages.StringField(3)
   systemName = _messages.StringField(4)
 
 
 class StreamingComputationRanges(_messages.Message):
-  """Describes full or partial data disk assignment information of the
+  r"""Describes full or partial data disk assignment information of the
   computation ranges.
 
   Fields:
     computationId: The ID of the computation.
     rangeAssignments: Data disk assignments for ranges from this computation.
   """
 
   computationId = _messages.StringField(1)
   rangeAssignments = _messages.MessageField('KeyRangeDataDiskAssignment', 2, repeated=True)
 
 
 class StreamingComputationTask(_messages.Message):
-  """A task which describes what action should be performed for the specified
+  r"""A task which describes what action should be performed for the specified
   streaming computation ranges.
 
   Enums:
     TaskTypeValueValuesEnum: A type of streaming computation task.
 
   Fields:
     computationRanges: Contains ranges of a streaming computation this task
       should apply to.
     dataDisks: Describes the set of data disks this task should apply to.
     taskType: A type of streaming computation task.
   """
 
   class TaskTypeValueValuesEnum(_messages.Enum):
-    """A type of streaming computation task.
+    r"""A type of streaming computation task.
 
     Values:
       STREAMING_COMPUTATION_TASK_UNKNOWN: The streaming computation task is
         unknown, or unspecified.
       STREAMING_COMPUTATION_TASK_STOP: Stop processing specified streaming
         computation range(s).
       STREAMING_COMPUTATION_TASK_START: Start processing specified streaming
@@ -4183,15 +4204,16 @@
 
   computationRanges = _messages.MessageField('StreamingComputationRanges', 1, repeated=True)
   dataDisks = _messages.MessageField('MountedDataDisk', 2, repeated=True)
   taskType = _messages.EnumField('TaskTypeValueValuesEnum', 3)
 
 
 class StreamingConfigTask(_messages.Message):
-  """A task that carries configuration information for streaming computations.
+  r"""A task that carries configuration information for streaming
+  computations.
 
   Messages:
     UserStepToStateFamilyNameMapValue: Map from user step names to state
       families.
 
   Fields:
     streamingComputationConfigs: Set of computation configuration information.
@@ -4202,27 +4224,27 @@
     windmillServicePort: If present, the worker must use this port to
       communicate with Windmill Service dispatchers. Only applicable when
       windmill_service_endpoint is specified.
   """
 
   @encoding.MapUnrecognizedFields('additionalProperties')
   class UserStepToStateFamilyNameMapValue(_messages.Message):
-    """Map from user step names to state families.
+    r"""Map from user step names to state families.
 
     Messages:
       AdditionalProperty: An additional property for a
         UserStepToStateFamilyNameMapValue object.
 
     Fields:
       additionalProperties: Additional properties of type
         UserStepToStateFamilyNameMapValue
     """
 
     class AdditionalProperty(_messages.Message):
-      """An additional property for a UserStepToStateFamilyNameMapValue
+      r"""An additional property for a UserStepToStateFamilyNameMapValue
       object.
 
       Fields:
         key: Name of the additional property.
         value: A string attribute.
       """
 
@@ -4234,69 +4256,71 @@
   streamingComputationConfigs = _messages.MessageField('StreamingComputationConfig', 1, repeated=True)
   userStepToStateFamilyNameMap = _messages.MessageField('UserStepToStateFamilyNameMapValue', 2)
   windmillServiceEndpoint = _messages.StringField(3)
   windmillServicePort = _messages.IntegerField(4)
 
 
 class StreamingSetupTask(_messages.Message):
-  """A task which initializes part of a streaming Dataflow job.
+  r"""A task which initializes part of a streaming Dataflow job.
 
   Fields:
     drain: The user has requested drain.
     receiveWorkPort: The TCP port on which the worker should listen for
       messages from other streaming computation workers.
+    snapshotConfig: Configures streaming appliance snapshot.
     streamingComputationTopology: The global topology of the streaming
       Dataflow job.
     workerHarnessPort: The TCP port used by the worker to communicate with the
       Dataflow worker harness.
   """
 
   drain = _messages.BooleanField(1)
   receiveWorkPort = _messages.IntegerField(2, variant=_messages.Variant.INT32)
-  streamingComputationTopology = _messages.MessageField('TopologyConfig', 3)
-  workerHarnessPort = _messages.IntegerField(4, variant=_messages.Variant.INT32)
+  snapshotConfig = _messages.MessageField('StreamingApplianceSnapshotConfig', 3)
+  streamingComputationTopology = _messages.MessageField('TopologyConfig', 4)
+  workerHarnessPort = _messages.IntegerField(5, variant=_messages.Variant.INT32)
 
 
 class StreamingSideInputLocation(_messages.Message):
-  """Identifies the location of a streaming side input.
+  r"""Identifies the location of a streaming side input.
 
   Fields:
     stateFamily: Identifies the state family where this side input is stored.
     tag: Identifies the particular side input within the streaming Dataflow
       job.
   """
 
   stateFamily = _messages.StringField(1)
   tag = _messages.StringField(2)
 
 
 class StreamingStageLocation(_messages.Message):
-  """Identifies the location of a streaming computation stage, for stage-to-
+  r"""Identifies the location of a streaming computation stage, for stage-to-
   stage communication.
 
   Fields:
     streamId: Identifies the particular stream within the streaming Dataflow
       job.
   """
 
   streamId = _messages.StringField(1)
 
 
 class StringList(_messages.Message):
-  """A metric value representing a list of strings.
+  r"""A metric value representing a list of strings.
 
   Fields:
     elements: Elements of the list.
   """
 
   elements = _messages.StringField(1, repeated=True)
 
 
 class StructuredMessage(_messages.Message):
-  """A rich message format, including a human readable string, a key for
+  r"""A rich message format, including a human readable string, a key for
   identifying the message, and structured data associated with the message for
   programmatic consumption.
 
   Fields:
     messageKey: Idenfier for this message type.  Used by external systems to
       internationalize or personalize message.
     messageText: Human-readable version of message.
@@ -4305,15 +4329,15 @@
 
   messageKey = _messages.StringField(1)
   messageText = _messages.StringField(2)
   parameters = _messages.MessageField('Parameter', 3, repeated=True)
 
 
 class TaskRunnerSettings(_messages.Message):
-  """Taskrunner configuration settings.
+  r"""Taskrunner configuration settings.
 
   Fields:
     alsologtostderr: Whether to also send taskrunner log info to stderr.
     baseTaskDir: The location on the worker for task-specific subdirectories.
     baseUrl: The base URL for the taskrunner to use when accessing Google
       Cloud APIs.  When workers access Google Cloud APIs, they logically do so
       via relative URLs.  If this field is specified, it supplies the base URL
@@ -4368,30 +4392,30 @@
   taskUser = _messages.StringField(16)
   tempStoragePrefix = _messages.StringField(17)
   vmId = _messages.StringField(18)
   workflowFileName = _messages.StringField(19)
 
 
 class TemplateMetadata(_messages.Message):
-  """Metadata describing a template.
+  r"""Metadata describing a template.
 
   Fields:
     description: Optional. A description of the template.
     name: Required. The name of the template.
     parameters: The parameters for the template.
   """
 
   description = _messages.StringField(1)
   name = _messages.StringField(2)
   parameters = _messages.MessageField('ParameterMetadata', 3, repeated=True)
 
 
 class TopologyConfig(_messages.Message):
-  """Global topology of the streaming Dataflow job, including all computations
-  and their sharded locations.
+  r"""Global topology of the streaming Dataflow job, including all
+  computations and their sharded locations.
 
   Messages:
     UserStageToComputationNameMapValue: Maps user stage names to stable
       computation names.
 
   Fields:
     computations: The computations associated with a streaming Dataflow job.
@@ -4401,27 +4425,27 @@
     persistentStateVersion: Version number for persistent state.
     userStageToComputationNameMap: Maps user stage names to stable computation
       names.
   """
 
   @encoding.MapUnrecognizedFields('additionalProperties')
   class UserStageToComputationNameMapValue(_messages.Message):
-    """Maps user stage names to stable computation names.
+    r"""Maps user stage names to stable computation names.
 
     Messages:
       AdditionalProperty: An additional property for a
         UserStageToComputationNameMapValue object.
 
     Fields:
       additionalProperties: Additional properties of type
         UserStageToComputationNameMapValue
     """
 
     class AdditionalProperty(_messages.Message):
-      """An additional property for a UserStageToComputationNameMapValue
+      r"""An additional property for a UserStageToComputationNameMapValue
       object.
 
       Fields:
         key: Name of the additional property.
         value: A string attribute.
       """
 
@@ -4434,15 +4458,15 @@
   dataDiskAssignments = _messages.MessageField('DataDiskAssignment', 2, repeated=True)
   forwardingKeyBits = _messages.IntegerField(3, variant=_messages.Variant.INT32)
   persistentStateVersion = _messages.IntegerField(4, variant=_messages.Variant.INT32)
   userStageToComputationNameMap = _messages.MessageField('UserStageToComputationNameMapValue', 5)
 
 
 class TransformSummary(_messages.Message):
-  """Description of the type, names/ids, and input/outputs for a transform.
+  r"""Description of the type, names/ids, and input/outputs for a transform.
 
   Enums:
     KindValueValuesEnum: Type of transform.
 
   Fields:
     displayData: Transform-specific display data.
     id: SDK generated id of this transform instance.
@@ -4451,15 +4475,15 @@
     kind: Type of transform.
     name: User provided name for this transform instance.
     outputCollectionName: User  names for all collection outputs to this
       transform.
   """
 
   class KindValueValuesEnum(_messages.Enum):
-    """Type of transform.
+    r"""Type of transform.
 
     Values:
       UNKNOWN_KIND: Unrecognized transform type.
       PAR_DO_KIND: ParDo transform.
       GROUP_BY_KEY_KIND: Group By Key transform.
       FLATTEN_KIND: Flatten transform.
       READ_KIND: Read transform.
@@ -4484,15 +4508,15 @@
   inputCollectionName = _messages.StringField(3, repeated=True)
   kind = _messages.EnumField('KindValueValuesEnum', 4)
   name = _messages.StringField(5)
   outputCollectionName = _messages.StringField(6, repeated=True)
 
 
 class WorkItem(_messages.Message):
-  """WorkItem represents basic information about a WorkItem to be executed in
+  r"""WorkItem represents basic information about a WorkItem to be executed in
   the cloud.
 
   Fields:
     configuration: Work item-specific configuration as an opaque blob.
     id: Identifies this WorkItem.
     initialReportIndex: The initial index to use when reporting the status of
       the WorkItem.
@@ -4529,15 +4553,15 @@
   sourceOperationTask = _messages.MessageField('SourceOperationRequest', 12)
   streamingComputationTask = _messages.MessageField('StreamingComputationTask', 13)
   streamingConfigTask = _messages.MessageField('StreamingConfigTask', 14)
   streamingSetupTask = _messages.MessageField('StreamingSetupTask', 15)
 
 
 class WorkItemServiceState(_messages.Message):
-  """The Dataflow service's idea of the current state of a WorkItem being
+  r"""The Dataflow service's idea of the current state of a WorkItem being
   processed by a worker.
 
   Messages:
     HarnessDataValue: Other data returned by the service, specific to the
       particular worker harness.
 
   Fields:
@@ -4557,27 +4581,27 @@
       service suggests that the worker truncate the task.
     suggestedStopPoint: DEPRECATED in favor of split_request.
     suggestedStopPosition: Obsolete, always empty.
   """
 
   @encoding.MapUnrecognizedFields('additionalProperties')
   class HarnessDataValue(_messages.Message):
-    """Other data returned by the service, specific to the particular worker
+    r"""Other data returned by the service, specific to the particular worker
     harness.
 
     Messages:
       AdditionalProperty: An additional property for a HarnessDataValue
         object.
 
     Fields:
       additionalProperties: Properties of the object.
     """
 
     class AdditionalProperty(_messages.Message):
-      """An additional property for a HarnessDataValue object.
+      r"""An additional property for a HarnessDataValue object.
 
       Fields:
         key: Name of the additional property.
         value: A extra_types.JsonValue attribute.
       """
 
       key = _messages.StringField(1)
@@ -4592,15 +4616,15 @@
   reportStatusInterval = _messages.StringField(5)
   splitRequest = _messages.MessageField('ApproximateSplitRequest', 6)
   suggestedStopPoint = _messages.MessageField('ApproximateProgress', 7)
   suggestedStopPosition = _messages.MessageField('Position', 8)
 
 
 class WorkItemStatus(_messages.Message):
-  """Conveys a worker's progress through the work described by a WorkItem.
+  r"""Conveys a worker's progress through the work described by a WorkItem.
 
   Fields:
     completed: True if the WorkItem was completed (successfully or
       unsuccessfully).
     counterUpdates: Worker output counters for this WorkItem.
     dynamicSourceSplit: See documentation of stop_position.
     errors: Specifies errors which occurred during processing.  If errors are
@@ -4666,15 +4690,15 @@
   sourceOperationResponse = _messages.MessageField('SourceOperationResponse', 11)
   stopPosition = _messages.MessageField('Position', 12)
   totalThrottlerWaitTimeSeconds = _messages.FloatField(13)
   workItemId = _messages.StringField(14)
 
 
 class WorkerHealthReport(_messages.Message):
-  """WorkerHealthReport contains information about the health of a worker.
+  r"""WorkerHealthReport contains information about the health of a worker.
   The VM should be identified by the labels attached to the WorkerMessage that
   this health ping belongs to.
 
   Messages:
     PodsValueListEntry: A PodsValueListEntry object.
 
   Fields:
@@ -4687,26 +4711,26 @@
       not being explicitly set by the worker.
     vmIsHealthy: Whether the VM is healthy.
     vmStartupTime: The time the VM was booted.
   """
 
   @encoding.MapUnrecognizedFields('additionalProperties')
   class PodsValueListEntry(_messages.Message):
-    """A PodsValueListEntry object.
+    r"""A PodsValueListEntry object.
 
     Messages:
       AdditionalProperty: An additional property for a PodsValueListEntry
         object.
 
     Fields:
       additionalProperties: Properties of the object.
     """
 
     class AdditionalProperty(_messages.Message):
-      """An additional property for a PodsValueListEntry object.
+      r"""An additional property for a PodsValueListEntry object.
 
       Fields:
         key: Name of the additional property.
         value: A extra_types.JsonValue attribute.
       """
 
       key = _messages.StringField(1)
@@ -4717,28 +4741,28 @@
   pods = _messages.MessageField('PodsValueListEntry', 1, repeated=True)
   reportInterval = _messages.StringField(2)
   vmIsHealthy = _messages.BooleanField(3)
   vmStartupTime = _messages.StringField(4)
 
 
 class WorkerHealthReportResponse(_messages.Message):
-  """WorkerHealthReportResponse contains information returned to the worker in
-  response to a health ping.
+  r"""WorkerHealthReportResponse contains information returned to the worker
+  in response to a health ping.
 
   Fields:
     reportInterval: A positive value indicates the worker should change its
       reporting interval to the specified value.  The default value of zero
       means no change in report rate is requested by the server.
   """
 
   reportInterval = _messages.StringField(1)
 
 
 class WorkerLifecycleEvent(_messages.Message):
-  """A report of an event in a worker's lifecycle. The proto contains one
+  r"""A report of an event in a worker's lifecycle. The proto contains one
   event, because the worker is expected to asynchronously send each message
   immediately after the event. Due to this asynchrony, messages may arrive out
   of order (or missing), and it is up to the consumer to interpret. The
   timestamp of the event is in the enclosing WorkerMessage proto.
 
   Enums:
     EventValueValuesEnum: The event being reported.
@@ -4753,15 +4777,15 @@
       restarts.
     event: The event being reported.
     metadata: Other stats that can accompany an event. E.g. {
       "downloaded_bytes" : "123456" }
   """
 
   class EventValueValuesEnum(_messages.Enum):
-    """The event being reported.
+    r"""The event being reported.
 
     Values:
       UNKNOWN_EVENT: Invalid event.
       OS_START: The time the VM started.
       CONTAINER_START: Our container code starts running. Multiple containers
         could be distinguished with WorkerMessage.labels if desired.
       NETWORK_UP: The worker has a functional external network connection.
@@ -4778,26 +4802,26 @@
     STAGING_FILES_DOWNLOAD_START = 4
     STAGING_FILES_DOWNLOAD_FINISH = 5
     SDK_INSTALL_START = 6
     SDK_INSTALL_FINISH = 7
 
   @encoding.MapUnrecognizedFields('additionalProperties')
   class MetadataValue(_messages.Message):
-    """Other stats that can accompany an event. E.g. { "downloaded_bytes" :
+    r"""Other stats that can accompany an event. E.g. { "downloaded_bytes" :
     "123456" }
 
     Messages:
       AdditionalProperty: An additional property for a MetadataValue object.
 
     Fields:
       additionalProperties: Additional properties of type MetadataValue
     """
 
     class AdditionalProperty(_messages.Message):
-      """An additional property for a MetadataValue object.
+      r"""An additional property for a MetadataValue object.
 
       Fields:
         key: Name of the additional property.
         value: A string attribute.
       """
 
       key = _messages.StringField(1)
@@ -4807,59 +4831,59 @@
 
   containerStartTime = _messages.StringField(1)
   event = _messages.EnumField('EventValueValuesEnum', 2)
   metadata = _messages.MessageField('MetadataValue', 3)
 
 
 class WorkerMessage(_messages.Message):
-  """WorkerMessage provides information to the backend about a worker.
+  r"""WorkerMessage provides information to the backend about a worker.
 
   Messages:
     LabelsValue: Labels are used to group WorkerMessages. For example, a
       worker_message about a particular container might have the labels: {
-      "JOB_ID": "2015-04-22",   "WORKER_ID": "wordcount-vm-2015\u2026"
+      "JOB_ID": "2015-04-22",   "WORKER_ID": "wordcount-vm-2015..."
       "CONTAINER_TYPE": "worker",   "CONTAINER_ID": "ac1234def"} Label tags
       typically correspond to Label enum values. However, for ease of
       development other strings can be used as tags. LABEL_UNSPECIFIED should
       not be used here.
 
   Fields:
     labels: Labels are used to group WorkerMessages. For example, a
       worker_message about a particular container might have the labels: {
-      "JOB_ID": "2015-04-22",   "WORKER_ID": "wordcount-vm-2015\u2026"
+      "JOB_ID": "2015-04-22",   "WORKER_ID": "wordcount-vm-2015..."
       "CONTAINER_TYPE": "worker",   "CONTAINER_ID": "ac1234def"} Label tags
       typically correspond to Label enum values. However, for ease of
       development other strings can be used as tags. LABEL_UNSPECIFIED should
       not be used here.
     time: The timestamp of the worker_message.
     workerHealthReport: The health of a worker.
     workerLifecycleEvent: Record of worker lifecycle events.
     workerMessageCode: A worker message code.
     workerMetrics: Resource metrics reported by workers.
     workerShutdownNotice: Shutdown notice by workers.
   """
 
   @encoding.MapUnrecognizedFields('additionalProperties')
   class LabelsValue(_messages.Message):
-    """Labels are used to group WorkerMessages. For example, a worker_message
+    r"""Labels are used to group WorkerMessages. For example, a worker_message
     about a particular container might have the labels: { "JOB_ID":
-    "2015-04-22",   "WORKER_ID": "wordcount-vm-2015\u2026"   "CONTAINER_TYPE":
+    "2015-04-22",   "WORKER_ID": "wordcount-vm-2015..."   "CONTAINER_TYPE":
     "worker",   "CONTAINER_ID": "ac1234def"} Label tags typically correspond
     to Label enum values. However, for ease of development other strings can
     be used as tags. LABEL_UNSPECIFIED should not be used here.
 
     Messages:
       AdditionalProperty: An additional property for a LabelsValue object.
 
     Fields:
       additionalProperties: Additional properties of type LabelsValue
     """
 
     class AdditionalProperty(_messages.Message):
-      """An additional property for a LabelsValue object.
+      r"""An additional property for a LabelsValue object.
 
       Fields:
         key: Name of the additional property.
         value: A string attribute.
       """
 
       key = _messages.StringField(1)
@@ -4873,15 +4897,15 @@
   workerLifecycleEvent = _messages.MessageField('WorkerLifecycleEvent', 4)
   workerMessageCode = _messages.MessageField('WorkerMessageCode', 5)
   workerMetrics = _messages.MessageField('ResourceUtilizationReport', 6)
   workerShutdownNotice = _messages.MessageField('WorkerShutdownNotice', 7)
 
 
 class WorkerMessageCode(_messages.Message):
-  """A message code is used to report status and error messages to the
+  r"""A message code is used to report status and error messages to the
   service. The message codes are intended to be machine readable. The service
   will take care of translating these into user understandable messages if
   necessary.  Example use cases:   1. Worker processes reporting successful
   startup.   2. Worker processes reporting specific errors (e.g. package
   staging      failure).
 
   Messages:
@@ -4920,15 +4944,15 @@
       typically passed  as a label.  hostname and other worker identifiers
       should almost always be passed  as labels since they will be included on
       most messages.
   """
 
   @encoding.MapUnrecognizedFields('additionalProperties')
   class ParametersValue(_messages.Message):
-    """Parameters contains specific information about the code.  This is a
+    r"""Parameters contains specific information about the code.  This is a
     struct to allow parameters of different types.  Examples:  1. For a
     "HARNESS_STARTED" message parameters might provide the name     of the
     worker and additional data like timing information.  2. For a
     "GCS_DOWNLOAD_ERROR" parameters might contain fields listing     the GCS
     objects being downloaded and fields containing errors.  In general complex
     data structures should be avoided. If a worker needs to send a specific
     and complicated data structure then please consider defining a new proto
@@ -4941,15 +4965,15 @@
       AdditionalProperty: An additional property for a ParametersValue object.
 
     Fields:
       additionalProperties: Properties of the object.
     """
 
     class AdditionalProperty(_messages.Message):
-      """An additional property for a ParametersValue object.
+      r"""An additional property for a ParametersValue object.
 
       Fields:
         key: Name of the additional property.
         value: A extra_types.JsonValue attribute.
       """
 
       key = _messages.StringField(1)
@@ -4958,15 +4982,15 @@
     additionalProperties = _messages.MessageField('AdditionalProperty', 1, repeated=True)
 
   code = _messages.StringField(1)
   parameters = _messages.MessageField('ParametersValue', 2)
 
 
 class WorkerMessageResponse(_messages.Message):
-  """A worker_message response allows the server to pass information to the
+  r"""A worker_message response allows the server to pass information to the
   sender.
 
   Fields:
     workerHealthReportResponse: The service's response to a worker's health
       report.
     workerMetricsResponse: Service's response to reporting worker metrics
       (currently empty).
@@ -4976,15 +5000,15 @@
 
   workerHealthReportResponse = _messages.MessageField('WorkerHealthReportResponse', 1)
   workerMetricsResponse = _messages.MessageField('ResourceUtilizationReportResponse', 2)
   workerShutdownNoticeResponse = _messages.MessageField('WorkerShutdownNoticeResponse', 3)
 
 
 class WorkerPool(_messages.Message):
-  """Describes one particular pool of Cloud Dataflow workers to be
+  r"""Describes one particular pool of Cloud Dataflow workers to be
   instantiated by the Cloud Dataflow service in order to perform the
   computations required by a job.  Note that a workflow job may use multiple
   pools, in order to match the various computational requirements of the
   various stages of the job.
 
   Enums:
     DefaultPackageSetValueValuesEnum: The default package set to install.
@@ -5058,15 +5082,15 @@
       executes the Cloud Dataflow worker harness, residing in Google Container
       Registry.
     zone: Zone to run the worker pools in.  If empty or unspecified, the
       service will attempt to choose a reasonable default.
   """
 
   class DefaultPackageSetValueValuesEnum(_messages.Enum):
-    """The default package set to install.  This allows the service to select
+    r"""The default package set to install.  This allows the service to select
     a default set of packages which are useful to worker harnesses written in
     a particular language.
 
     Values:
       DEFAULT_PACKAGE_SET_UNKNOWN: The default set of packages to stage is
         unknown, or unspecified.
       DEFAULT_PACKAGE_SET_NONE: Indicates that no packages should be staged at
@@ -5078,27 +5102,27 @@
     """
     DEFAULT_PACKAGE_SET_UNKNOWN = 0
     DEFAULT_PACKAGE_SET_NONE = 1
     DEFAULT_PACKAGE_SET_JAVA = 2
     DEFAULT_PACKAGE_SET_PYTHON = 3
 
   class IpConfigurationValueValuesEnum(_messages.Enum):
-    """Configuration for VM IPs.
+    r"""Configuration for VM IPs.
 
     Values:
       WORKER_IP_UNSPECIFIED: The configuration is unknown, or unspecified.
       WORKER_IP_PUBLIC: Workers should have public IP addresses.
       WORKER_IP_PRIVATE: Workers should have private IP addresses.
     """
     WORKER_IP_UNSPECIFIED = 0
     WORKER_IP_PUBLIC = 1
     WORKER_IP_PRIVATE = 2
 
   class TeardownPolicyValueValuesEnum(_messages.Enum):
-    """Sets the policy for determining when to turndown worker pool. Allowed
+    r"""Sets the policy for determining when to turndown worker pool. Allowed
     values are: `TEARDOWN_ALWAYS`, `TEARDOWN_ON_SUCCESS`, and
     `TEARDOWN_NEVER`. `TEARDOWN_ALWAYS` means workers are always torn down
     regardless of whether the job succeeds. `TEARDOWN_ON_SUCCESS` means
     workers are torn down if the job succeeds. `TEARDOWN_NEVER` means the
     workers are never torn down.  If the workers are not torn down by the
     service, they will continue to run and use Google Compute Engine VM
     resources in the user's project until they are explicitly terminated by
@@ -5118,50 +5142,50 @@
     TEARDOWN_POLICY_UNKNOWN = 0
     TEARDOWN_ALWAYS = 1
     TEARDOWN_ON_SUCCESS = 2
     TEARDOWN_NEVER = 3
 
   @encoding.MapUnrecognizedFields('additionalProperties')
   class MetadataValue(_messages.Message):
-    """Metadata to set on the Google Compute Engine VMs.
+    r"""Metadata to set on the Google Compute Engine VMs.
 
     Messages:
       AdditionalProperty: An additional property for a MetadataValue object.
 
     Fields:
       additionalProperties: Additional properties of type MetadataValue
     """
 
     class AdditionalProperty(_messages.Message):
-      """An additional property for a MetadataValue object.
+      r"""An additional property for a MetadataValue object.
 
       Fields:
         key: Name of the additional property.
         value: A string attribute.
       """
 
       key = _messages.StringField(1)
       value = _messages.StringField(2)
 
     additionalProperties = _messages.MessageField('AdditionalProperty', 1, repeated=True)
 
   @encoding.MapUnrecognizedFields('additionalProperties')
   class PoolArgsValue(_messages.Message):
-    """Extra arguments for this worker pool.
+    r"""Extra arguments for this worker pool.
 
     Messages:
       AdditionalProperty: An additional property for a PoolArgsValue object.
 
     Fields:
       additionalProperties: Properties of the object. Contains field @type
         with type URL.
     """
 
     class AdditionalProperty(_messages.Message):
-      """An additional property for a PoolArgsValue object.
+      r"""An additional property for a PoolArgsValue object.
 
       Fields:
         key: Name of the additional property.
         value: A extra_types.JsonValue attribute.
       """
 
       key = _messages.StringField(1)
@@ -5189,15 +5213,15 @@
   taskrunnerSettings = _messages.MessageField('TaskRunnerSettings', 18)
   teardownPolicy = _messages.EnumField('TeardownPolicyValueValuesEnum', 19)
   workerHarnessContainerImage = _messages.StringField(20)
   zone = _messages.StringField(21)
 
 
 class WorkerSettings(_messages.Message):
-  """Provides data to pass through to the worker harness.
+  r"""Provides data to pass through to the worker harness.
 
   Fields:
     baseUrl: The base URL for accessing Google Cloud APIs.  When workers
       access Google Cloud APIs, they logically do so via relative URLs.  If
       this field is specified, it supplies the base URL to use for resolving
       these relative URLs.  The normative algorithm used is defined by RFC
       1808, "Relative Uniform Resource Locators".  If not specified, the
@@ -5219,33 +5243,34 @@
   servicePath = _messages.StringField(3)
   shuffleServicePath = _messages.StringField(4)
   tempStoragePrefix = _messages.StringField(5)
   workerId = _messages.StringField(6)
 
 
 class WorkerShutdownNotice(_messages.Message):
-  """Shutdown notification from workers. This is to be sent by the shutdown
+  r"""Shutdown notification from workers. This is to be sent by the shutdown
   script of the worker VM so that the backend knows that the VM is being shut
   down.
 
   Fields:
     reason: The reason for the worker shutdown. Current possible values are:
       "UNKNOWN": shutdown reason is unknown.   "PREEMPTION": shutdown reason
       is preemption. Other possible reasons may be added in the future.
   """
 
   reason = _messages.StringField(1)
 
 
 class WorkerShutdownNoticeResponse(_messages.Message):
-  """Service-side response to WorkerMessage issuing shutdown notice."""
+  r"""Service-side response to WorkerMessage issuing shutdown notice."""
 
 
 class WriteInstruction(_messages.Message):
-  """An instruction that writes records. Takes one input, produces no outputs.
+  r"""An instruction that writes records. Takes one input, produces no
+  outputs.
 
   Fields:
     input: The input.
     sink: The sink to write to.
   """
 
   input = _messages.MessageField('InstructionInput', 1)
```

## Comparing `apache-beam-2.8.0/apache_beam/runners/dataflow/internal/clients/dataflow/message_matchers_test.py` & `apache-beam-2.9.0/apache_beam/runners/dataflow/internal/clients/dataflow/message_matchers_test.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/runners/portability/local_job_service.py` & `apache-beam-2.9.0/apache_beam/runners/portability/local_job_service.py`

 * *Files 0% similar despite different names*

```diff
@@ -204,14 +204,15 @@
     ]
     self._state = None
     self.state = beam_job_api_pb2.JobState.STARTING
     self.daemon = True
 
   def add_state_change_callback(self, f):
     self._state_change_callbacks.append(f)
+    f(self.state)
 
   @property
   def log_queue(self):
     return self._log_queue
 
   @property
   def state(self):
```

## Comparing `apache-beam-2.8.0/apache_beam/runners/portability/stager.py` & `apache-beam-2.9.0/apache_beam/runners/portability/stager.py`

 * *Files 8% similar despite different names*

```diff
@@ -46,37 +46,43 @@
 """
 from __future__ import absolute_import
 
 import glob
 import logging
 import os
 import shutil
-import subprocess
 import sys
 import tempfile
 
 import pkg_resources
 
 from apache_beam.internal import pickler
 from apache_beam.io.filesystems import FileSystems
 from apache_beam.options.pipeline_options import SetupOptions
+from apache_beam.options.pipeline_options import WorkerOptions
 # TODO(angoenka): Remove reference to dataflow internal names
-from apache_beam.runners.dataflow.internal import names
+from apache_beam.runners.dataflow.internal.names import DATAFLOW_SDK_TARBALL_FILE
+from apache_beam.runners.internal import names
 from apache_beam.utils import processes
+from apache_beam.utils import retry
 
 # All constants are for internal use only; no backwards-compatibility
 # guarantees.
 
 # Standard file names used for staging files.
 WORKFLOW_TARBALL_FILE = 'workflow.tar.gz'
 REQUIREMENTS_FILE = 'requirements.txt'
 EXTRA_PACKAGES_FILE = 'extra_packages.txt'
 
-# Package names for distributions
-BEAM_PACKAGE_NAME = 'apache-beam'
+
+def retry_on_non_zero_exit(exception):
+  if (isinstance(exception, processes.CalledProcessError) and
+      exception.returncode != 0):
+    return True
+  return False
 
 
 class Stager(object):
   """Abstract Stager identifies and copies the appropriate artifacts to the
   staging location.
   Implementation of this stager has to implement :func:`stage_artifact` and
   :func:`commit_manifest`.
@@ -91,15 +97,15 @@
     """Commits manifest."""
     raise NotImplementedError
 
   @staticmethod
   def get_sdk_package_name():
     """For internal use only; no backwards-compatibility guarantees.
         Returns the PyPI package name to be staged."""
-    return BEAM_PACKAGE_NAME
+    return names.BEAM_PACKAGE_NAME
 
   def stage_job_resources(self,
                           options,
                           build_setup_args=None,
                           temp_dir=None,
                           populate_requirements_cache=None,
                           staging_location=None):
@@ -119,16 +125,15 @@
             testing.
           populate_requirements_cache: Callable for populating the requirements
             cache. Used only for testing.
           staging_location: Location to stage the file.
 
         Returns:
           A list of file names (no paths) for the resources staged. All the
-          files
-          are assumed to be staged at staging_location.
+          files are assumed to be staged at staging_location.
 
         Raises:
           RuntimeError: If files specified are not found or error encountered
           while trying to create the resources (e.g., build a setup package).
         """
     temp_dir = temp_dir or tempfile.mkdtemp()
     resources = []
@@ -227,15 +232,15 @@
         pass
       else:
         # This branch is also used by internal tests running with the SDK built
         # at head.
         if os.path.isdir(setup_options.sdk_location):
           # TODO(angoenka): remove reference to Dataflow
           sdk_path = os.path.join(setup_options.sdk_location,
-                                  names.DATAFLOW_SDK_TARBALL_FILE)
+                                  DATAFLOW_SDK_TARBALL_FILE)
         else:
           sdk_path = setup_options.sdk_location
 
         if os.path.isfile(sdk_path):
           logging.info('Copying Beam SDK "%s" to staging location.', sdk_path)
           staged_path = FileSystems.join(
               staging_location,
@@ -252,14 +257,22 @@
             logging.info('Beam SDK will not be staged since --sdk_location '
                          'is empty.')
           else:
             raise RuntimeError(
                 'The file "%s" cannot be found. Its location was specified by '
                 'the --sdk_location command-line option.' % sdk_path)
 
+    worker_options = options.view_as(WorkerOptions)
+    dataflow_worker_jar = getattr(worker_options, 'dataflow_worker_jar', None)
+    if dataflow_worker_jar is not None:
+      jar_staged_filename = 'dataflow-worker.jar'
+      staged_path = FileSystems.join(staging_location, jar_staged_filename)
+      self.stage_artifact(dataflow_worker_jar, staged_path)
+      resources.append(jar_staged_filename)
+
     # Delete all temp files created while staging job resources.
     shutil.rmtree(temp_dir)
     retrieval_token = self.commit_manifest()
     return retrieval_token, resources
 
   @staticmethod
   def _download_file(from_url, to_path):
@@ -383,14 +396,16 @@
     # the current process.
     python_bin = os.environ.get('BEAM_PYTHON') or sys.executable
     if not python_bin:
       raise ValueError('Could not find Python executable.')
     return python_bin
 
   @staticmethod
+  @retry.with_exponential_backoff(num_retries=4,
+                                  retry_filter=retry_on_non_zero_exit)
   def _populate_requirements_cache(requirements_file, cache_dir):
     # The 'pip download' command will not download again if it finds the
     # tarball with the proper version already present.
     # It will get the packages downloaded in the order they are presented in
     # the requirements file and will not download package dependencies.
     cmd_args = [
         Stager._get_python_executable(),
@@ -404,28 +419,28 @@
         '--exists-action',
         'i',
         # Download from PyPI source distributions.
         '--no-binary',
         ':all:'
     ]
     logging.info('Executing command: %s', cmd_args)
-    processes.check_call(cmd_args)
+    processes.check_output(cmd_args, stderr=processes.STDOUT)
 
   @staticmethod
   def _build_setup_package(setup_file, temp_dir, build_setup_args=None):
     saved_current_directory = os.getcwd()
     try:
       os.chdir(os.path.dirname(setup_file))
       if build_setup_args is None:
         build_setup_args = [
             Stager._get_python_executable(),
             os.path.basename(setup_file), 'sdist', '--dist-dir', temp_dir
         ]
       logging.info('Executing command: %s', build_setup_args)
-      processes.check_call(build_setup_args)
+      processes.check_output(build_setup_args)
       output_files = glob.glob(os.path.join(temp_dir, '*.tar.gz'))
       if not output_files:
         raise RuntimeError(
             'File %s not found.' % os.path.join(temp_dir, '*.tar.gz'))
       return output_files[0]
     finally:
       os.chdir(saved_current_directory)
@@ -439,15 +454,15 @@
     if sdk_location.endswith('.whl'):
       _, wheel_filename = FileSystems.split(sdk_location)
       if wheel_filename.startswith('apache_beam'):
         return wheel_filename
       else:
         raise RuntimeError('Unrecognized SDK wheel file: %s' % sdk_location)
     else:
-      return names.DATAFLOW_SDK_TARBALL_FILE
+      return DATAFLOW_SDK_TARBALL_FILE
 
   def _stage_beam_sdk(self, sdk_remote_location, staging_location, temp_dir):
     """Stages a Beam SDK file with the appropriate version.
 
       Args:
         sdk_remote_location: A URL from which thefile can be downloaded or a
           remote file location. The SDK file can be a tarball or a wheel. Set
@@ -545,16 +560,16 @@
       expected_files = [
           os.path.join(temp_dir, '%s-%s.zip' % (package_name, version)),
           os.path.join(temp_dir, '%s-%s.tar.gz' % (package_name, version))
       ]
 
     logging.info('Executing command: %s', cmd_args)
     try:
-      processes.check_call(cmd_args)
-    except subprocess.CalledProcessError as e:
+      processes.check_output(cmd_args)
+    except processes.CalledProcessError as e:
       raise RuntimeError(repr(e))
 
     for sdk_file in expected_files:
       if os.path.exists(sdk_file):
         return sdk_file
 
     raise RuntimeError(
```

## Comparing `apache-beam-2.8.0/apache_beam/runners/portability/local_job_service_main.py` & `apache-beam-2.9.0/apache_beam/runners/portability/local_job_service_main.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/runners/portability/portable_runner_test.py` & `apache-beam-2.9.0/apache_beam/runners/portability/portable_runner_test.py`

 * *Files 12% similar despite different names*

```diff
@@ -15,15 +15,14 @@
 # limitations under the License.
 #
 from __future__ import absolute_import
 from __future__ import print_function
 
 import inspect
 import logging
-import os
 import platform
 import signal
 import socket
 import subprocess
 import sys
 import threading
 import time
@@ -39,16 +38,14 @@
 from apache_beam.portability.api import beam_job_api_pb2
 from apache_beam.portability.api import beam_job_api_pb2_grpc
 from apache_beam.portability.api import beam_runner_api_pb2
 from apache_beam.runners.portability import fn_api_runner_test
 from apache_beam.runners.portability import portable_runner
 from apache_beam.runners.portability.local_job_service import LocalJobServicer
 from apache_beam.runners.portability.portable_runner import PortableRunner
-from apache_beam.testing.util import assert_that
-from apache_beam.testing.util import equal_to
 
 
 class PortableRunnerTest(fn_api_runner_test.FnApiRunnerTest):
 
   TIMEOUT_SECS = 30
 
   _use_grpc = False
@@ -137,15 +134,15 @@
     else:
       # Do not use GRPC for worker.
       cls._servicer = LocalJobServicer(use_grpc=False)
       return 'localhost:%d' % cls._servicer.start_grpc_server()
 
   @classmethod
   def get_runner(cls):
-    return portable_runner.PortableRunner(is_embedded_fnapi_runner=True)
+    return portable_runner.PortableRunner()
 
   @classmethod
   def tearDownClass(cls):
     cls._maybe_kill_subprocess()
 
   @classmethod
   def _maybe_kill_subprocess(cls):
@@ -166,45 +163,14 @@
     })
     options.view_as(PortableOptions).job_endpoint = self._get_job_endpoint()
     return options
 
   def create_pipeline(self):
     return beam.Pipeline(self.get_runner(), self.create_options())
 
-  def test_assert_that(self):
-    # TODO: figure out a way for runner to parse and raise the
-    # underlying exception.
-    with self.assertRaises(Exception):
-      with self.create_pipeline() as p:
-        assert_that(p | beam.Create(['a', 'b']), equal_to(['a']))
-
-  @unittest.skipIf(sys.version_info[0] == 3 and
-                   os.environ.get('RUN_SKIPPED_PY3_TESTS') != '1',
-                   'This test is flaky on on Python 3. '
-                   'TODO: BEAM-5692')
-  def test_error_message_includes_stage(self):
-    # TODO: figure out a way for runner to parse and raise the
-    # underlying exception.
-    with self.assertRaises(Exception):
-      with self.create_pipeline() as p:
-        def raise_error(x):
-          raise RuntimeError('x')
-        # pylint: disable=expression-not-assigned
-        (p
-         | beam.Create(['a', 'b'])
-         | 'StageA' >> beam.Map(lambda x: x)
-         | 'StageB' >> beam.Map(lambda x: x)
-         | 'StageC' >> beam.Map(raise_error)
-         | 'StageD' >> beam.Map(lambda x: x))
-
-  def test_error_traceback_includes_user_code(self):
-    # TODO: figure out a way for runner to parse and raise the
-    # underlying exception.
-    raise unittest.SkipTest('TODO')
-
   # Inherits all tests from fn_api_runner_test.FnApiRunnerTest
 
 
 class PortableRunnerTestWithGrpc(PortableRunnerTest):
   _use_grpc = True
```

## Comparing `apache-beam-2.8.0/apache_beam/runners/portability/__init__.py` & `apache-beam-2.9.0/apache_beam/runners/portability/__init__.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/runners/portability/stager_test.py` & `apache-beam-2.9.0/apache_beam/runners/portability/stager_test.py`

 * *Files 1% similar despite different names*

```diff
@@ -27,14 +27,15 @@
 
 import mock
 
 from apache_beam.io.filesystems import FileSystems
 from apache_beam.options.pipeline_options import PipelineOptions
 from apache_beam.options.pipeline_options import SetupOptions
 from apache_beam.runners.dataflow.internal import names
+from apache_beam.runners.internal import names as shared_names
 from apache_beam.runners.portability import stager
 
 
 class StagerTest(unittest.TestCase):
 
   def setUp(self):
     self._temp_dir = None
@@ -61,15 +62,15 @@
 
   def populate_requirements_cache(self, requirements_file, cache_dir):
     _ = requirements_file
     self.create_temp_file(os.path.join(cache_dir, 'abc.txt'), 'nothing')
     self.create_temp_file(os.path.join(cache_dir, 'def.txt'), 'nothing')
 
   def build_fake_pip_download_command_handler(self, has_wheels):
-    """A stub for apache_beam.utils.processes.check_call that imitates pip.
+    """A stub for apache_beam.utils.processes.check_output that imitates pip.
 
       Args:
         has_wheels: Whether pip fake should have a whl distribution of packages.
       """
 
     def pip_fake(args):
       """Fakes fetching a package from pip by creating a temporary file.
@@ -141,20 +142,20 @@
   def test_with_main_session(self):
     staging_dir = self.make_temp_dir()
     options = PipelineOptions()
 
     options.view_as(SetupOptions).save_main_session = True
     self.update_options(options)
 
-    self.assertEqual([names.PICKLED_MAIN_SESSION_FILE],
+    self.assertEqual([shared_names.PICKLED_MAIN_SESSION_FILE],
                      self.stager.stage_job_resources(
                          options, staging_location=staging_dir)[1])
     self.assertTrue(
         os.path.isfile(
-            os.path.join(staging_dir, names.PICKLED_MAIN_SESSION_FILE)))
+            os.path.join(staging_dir, shared_names.PICKLED_MAIN_SESSION_FILE)))
 
   def test_default_resources(self):
     staging_dir = self.make_temp_dir()
     options = PipelineOptions()
     self.update_options(options)
 
     self.assertEqual([],
@@ -287,15 +288,15 @@
   def test_sdk_location_default(self):
     staging_dir = self.make_temp_dir()
     options = PipelineOptions()
     self.update_options(options)
     options.view_as(SetupOptions).sdk_location = 'default'
 
     with mock.patch(
-        'apache_beam.utils.processes.check_call',
+        'apache_beam.utils.processes.check_output',
         self.build_fake_pip_download_command_handler(has_wheels=False)):
       _, staged_resources = self.stager.stage_job_resources(
           options, temp_dir=self.make_temp_dir(), staging_location=staging_dir)
 
     self.assertEqual([names.DATAFLOW_SDK_TARBALL_FILE], staged_resources)
 
     with open(os.path.join(staging_dir, names.DATAFLOW_SDK_TARBALL_FILE)) as f:
@@ -305,15 +306,15 @@
     staging_dir = self.make_temp_dir()
 
     options = PipelineOptions()
     self.update_options(options)
     options.view_as(SetupOptions).sdk_location = 'default'
 
     with mock.patch(
-        'apache_beam.utils.processes.check_call',
+        'apache_beam.utils.processes.check_output',
         self.build_fake_pip_download_command_handler(has_wheels=True)):
       _, staged_resources = self.stager.stage_job_resources(
           options, temp_dir=self.make_temp_dir(), staging_location=staging_dir)
 
       self.assertEqual(len(staged_resources), 2)
       self.assertEqual(staged_resources[0], names.DATAFLOW_SDK_TARBALL_FILE)
       # Exact name depends on the version of the SDK.
```

## Comparing `apache-beam-2.8.0/apache_beam/runners/portability/fn_api_runner.py` & `apache-beam-2.9.0/apache_beam/runners/portability/fn_api_runner.py`

 * *Files 5% similar despite different names*

```diff
@@ -14,60 +14,65 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 #
 
 """A PipelineRunner using the SDK harness.
 """
 from __future__ import absolute_import
+from __future__ import print_function
 
 import collections
 import contextlib
 import copy
 import logging
+import os
 import queue
+import subprocess
+import sys
 import threading
 import time
 from builtins import object
 from concurrent import futures
 
 import grpc
 
 import apache_beam as beam  # pylint: disable=ungrouped-imports
 from apache_beam import coders
 from apache_beam import metrics
-from apache_beam.coders import WindowedValueCoder
-from apache_beam.coders import registry
 from apache_beam.coders.coder_impl import create_InputStream
 from apache_beam.coders.coder_impl import create_OutputStream
-from apache_beam.internal import pickler
+from apache_beam.metrics import monitoring_infos
+from apache_beam.metrics.execution import MetricKey
 from apache_beam.metrics.execution import MetricsEnvironment
+from apache_beam.options import pipeline_options
 from apache_beam.options.value_provider import RuntimeValueProvider
 from apache_beam.portability import common_urns
 from apache_beam.portability import python_urns
 from apache_beam.portability.api import beam_fn_api_pb2
 from apache_beam.portability.api import beam_fn_api_pb2_grpc
 from apache_beam.portability.api import beam_runner_api_pb2
 from apache_beam.portability.api import endpoints_pb2
 from apache_beam.runners import pipeline_context
 from apache_beam.runners import runner
 from apache_beam.runners.worker import bundle_processor
 from apache_beam.runners.worker import data_plane
 from apache_beam.runners.worker import sdk_worker
 from apache_beam.transforms import trigger
 from apache_beam.transforms.window import GlobalWindows
+from apache_beam.utils import profiler
 from apache_beam.utils import proto_utils
 
 # This module is experimental. No backwards-compatibility guarantees.
 
 ENCODED_IMPULSE_VALUE = beam.coders.WindowedValueCoder(
     beam.coders.BytesCoder(),
     beam.coders.coders.GlobalWindowCoder()).get_impl().encode_nested(
         beam.transforms.window.GlobalWindows.windowed_value(b''))
 
-IMPULSE_BUFFER_PREFIX = b'impulse:'
+IMPULSE_BUFFER = b'impulse'
 
 
 class BeamFnControlServicer(beam_fn_api_pb2_grpc.BeamFnControlServicer):
 
   _DONE = object()
 
   def __init__(self):
@@ -117,44 +122,51 @@
   """Used to accumulate groupded (shuffled) results."""
   def __init__(self, pre_grouped_coder, post_grouped_coder, windowing):
     self._key_coder = pre_grouped_coder.key_coder()
     self._pre_grouped_coder = pre_grouped_coder
     self._post_grouped_coder = post_grouped_coder
     self._table = collections.defaultdict(list)
     self._windowing = windowing
+    self._grouped_output = None
 
   def append(self, elements_data):
+    if self._grouped_output:
+      raise RuntimeError('Grouping table append after read.')
     input_stream = create_InputStream(elements_data)
     coder_impl = self._pre_grouped_coder.get_impl()
     key_coder_impl = self._key_coder.get_impl()
     # TODO(robertwb): We could optimize this even more by using a
     # window-dropping coder for the data plane.
     is_trivial_windowing = self._windowing.is_default()
     while input_stream.size() > 0:
       windowed_key_value = coder_impl.decode_from_stream(input_stream, True)
       key, value = windowed_key_value.value
       self._table[key_coder_impl.encode(key)].append(
           value if is_trivial_windowing
           else windowed_key_value.with_value(value))
 
   def __iter__(self):
-    output_stream = create_OutputStream()
-    if self._windowing.is_default():
-      globally_window = GlobalWindows.windowed_value(None).with_value
-      windowed_key_values = lambda key, values: [globally_window((key, values))]
-    else:
-      trigger_driver = trigger.create_trigger_driver(self._windowing, True)
-      windowed_key_values = trigger_driver.process_entire_key
-    coder_impl = self._post_grouped_coder.get_impl()
-    key_coder_impl = self._key_coder.get_impl()
-    for encoded_key, windowed_values in self._table.items():
-      key = key_coder_impl.decode(encoded_key)
-      for wkvs in windowed_key_values(key, windowed_values):
-        coder_impl.encode_to_stream(wkvs, output_stream, True)
-    return iter([output_stream.get()])
+    if not self._grouped_output:
+      output_stream = create_OutputStream()
+      if self._windowing.is_default():
+        globally_window = GlobalWindows.windowed_value(None).with_value
+        windowed_key_values = lambda key, values: [
+            globally_window((key, values))]
+      else:
+        trigger_driver = trigger.create_trigger_driver(self._windowing, True)
+        windowed_key_values = trigger_driver.process_entire_key
+      coder_impl = self._post_grouped_coder.get_impl()
+      key_coder_impl = self._key_coder.get_impl()
+      for encoded_key, windowed_values in self._table.items():
+        key = key_coder_impl.decode(encoded_key)
+        for wkvs in windowed_key_values(key, windowed_values):
+          coder_impl.encode_to_stream(wkvs, output_stream, True)
+      self._grouped_output = [output_stream.get()]
+      self._table = None
+    return iter(self._grouped_output)
 
 
 class _WindowGroupingBuffer(object):
   """Used to partition windowed side inputs."""
   def __init__(self, side_input_data, coder):
     # Here's where we would use a different type of partitioning
     # (e.g. also by key) for a different access pattern.
@@ -193,48 +205,95 @@
       for value in values:
         value_coder_impl.encode_to_stream(value, output_stream, True)
       yield encoded_key, encoded_window, output_stream.get()
 
 
 class FnApiRunner(runner.PipelineRunner):
 
-  def __init__(self, use_grpc=False, sdk_harness_factory=None):
+  def __init__(self, use_grpc=False, sdk_harness_factory=None, bundle_repeat=0):
     """Creates a new Fn API Runner.
 
     Args:
       use_grpc: whether to use grpc or simply make in-process calls
           defaults to False
       sdk_harness_factory: callable used to instantiate customized sdk harnesses
           typcially not set by users
+      bundle_repeat: replay every bundle this many extra times, for profiling
+          and debugging
     """
     super(FnApiRunner, self).__init__()
     self._last_uid = -1
     self._use_grpc = use_grpc
     if sdk_harness_factory and not use_grpc:
       raise ValueError('GRPC must be used if a harness factory is provided.')
     self._sdk_harness_factory = sdk_harness_factory
+    self._bundle_repeat = bundle_repeat
     self._progress_frequency = None
+    self._profiler_factory = None
 
   def _next_uid(self):
     self._last_uid += 1
     return str(self._last_uid)
 
   def run_pipeline(self, pipeline):
     MetricsEnvironment.set_metrics_supported(False)
     RuntimeValueProvider.set_runtime_options({})
     # This is sometimes needed if type checking is disabled
     # to enforce that the inputs (and outputs) of GroupByKey operations
     # are known to be KVs.
     from apache_beam.runners.dataflow.dataflow_runner import DataflowRunner
     pipeline.visit(DataflowRunner.group_by_key_input_visitor())
+    self._bundle_repeat = self._bundle_repeat or pipeline._options.view_as(
+        pipeline_options.DirectOptions).direct_runner_bundle_repeat
+    self._profiler_factory = profiler.Profile.factory_from_options(
+        pipeline._options.view_as(pipeline_options.ProfilingOptions))
     return self.run_via_runner_api(pipeline.to_runner_api())
 
   def run_via_runner_api(self, pipeline_proto):
     return self.run_stages(*self.create_stages(pipeline_proto))
 
+  @contextlib.contextmanager
+  def maybe_profile(self):
+    if self._profiler_factory:
+      try:
+        profile_id = 'direct-' + subprocess.check_output(
+            ['git', 'rev-parse', '--abbrev-ref', 'HEAD']
+        ).decode(errors='ignore').strip()
+      except subprocess.CalledProcessError:
+        profile_id = 'direct-unknown'
+      profiler = self._profiler_factory(profile_id, time_prefix='')
+    else:
+      profiler = None
+
+    if profiler:
+      with profiler:
+        yield
+      if not self._bundle_repeat:
+        logging.warning(
+            'The --direct_runner_bundle_repeat option is not set; '
+            'a significant portion of the profile may be one-time overhead.')
+      path = profiler.profile_output
+      print('CPU Profile written to %s' % path)
+      try:
+        import gprof2dot  # pylint: disable=unused-variable
+        if not subprocess.call([
+            sys.executable, '-m', 'gprof2dot',
+            '-f', 'pstats', path, '-o', path + '.dot']):
+          if not subprocess.call(
+              ['dot', '-Tsvg', '-o', path + '.svg', path + '.dot']):
+            print('CPU Profile rendering at file://%s.svg'
+                  % os.path.abspath(path))
+      except ImportError:
+        # pylint: disable=superfluous-parens
+        print('Please install gprof2dot and dot for profile renderings.')
+
+    else:
+      # Empty context.
+      yield
+
   def create_stages(self, pipeline_proto):
 
     # First define a couple of helpers.
 
     def union(a, b):
       # Minimize the number of distinct sets.
       if not a or a == b:
@@ -445,22 +504,20 @@
                     inputs={'in': impulse_pc},
                     outputs={'out': read_pc}))
 
         # Now map impulses to inputs.
         for transform in list(stage.transforms):
           if transform.spec.urn == common_urns.primitives.IMPULSE.urn:
             stage.transforms.remove(transform)
-            impulse_pc = only_element(transform.outputs.values())
             stage.transforms.append(
                 beam_runner_api_pb2.PTransform(
                     unique_name=transform.unique_name,
                     spec=beam_runner_api_pb2.FunctionSpec(
                         urn=bundle_processor.DATA_INPUT_URN,
-                        payload=IMPULSE_BUFFER_PREFIX +
-                        impulse_pc.encode('utf-8')),
+                        payload=IMPULSE_BUFFER),
                     outputs=transform.outputs))
 
         yield stage
 
     def lift_combiners(stages):
       """Expands CombinePerKey into pre- and post-grouping stages.
 
@@ -605,37 +662,37 @@
             length_prefix_unknown_coders(
                 pipeline_components.pcollections[pcoll_id], pipeline_components)
           for pcoll_id in transform.outputs.values():
             length_prefix_unknown_coders(
                 pipeline_components.pcollections[pcoll_id], pipeline_components)
 
           # This is used later to correlate the read and write.
-          param = str("group:%s" % stage.name).encode('utf-8')
+          grouping_buffer = create_buffer_id(stage.name, kind='group')
           if stage.name not in pipeline_components.transforms:
             pipeline_components.transforms[stage.name].CopyFrom(transform)
           gbk_write = Stage(
               transform.unique_name + '/Write',
               [beam_runner_api_pb2.PTransform(
                   unique_name=transform.unique_name + '/Write',
                   inputs=transform.inputs,
                   spec=beam_runner_api_pb2.FunctionSpec(
                       urn=bundle_processor.DATA_OUTPUT_URN,
-                      payload=param))],
+                      payload=grouping_buffer))],
               downstream_side_inputs=frozenset(),
               must_follow=stage.must_follow)
           yield gbk_write
 
           yield Stage(
               transform.unique_name + '/Read',
               [beam_runner_api_pb2.PTransform(
                   unique_name=transform.unique_name + '/Read',
                   outputs=transform.outputs,
                   spec=beam_runner_api_pb2.FunctionSpec(
                       urn=bundle_processor.DATA_INPUT_URN,
-                      payload=param))],
+                      payload=grouping_buffer))],
               downstream_side_inputs=stage.downstream_side_inputs,
               must_follow=union(frozenset([gbk_write]), stage.must_follow))
         else:
           yield stage
 
     def sink_flattens(stages):
       """Sink flattens and remove them from the graph.
@@ -647,15 +704,15 @@
       # TODO(robertwb): Possibly fuse this into one of the stages.
       pcollections = pipeline_components.pcollections
       for stage in stages:
         assert len(stage.transforms) == 1
         transform = stage.transforms[0]
         if transform.spec.urn == common_urns.primitives.FLATTEN.urn:
           # This is used later to correlate the read and writes.
-          param = str("materialize:%s" % transform.unique_name).encode('utf-8')
+          buffer_id = create_buffer_id(transform.unique_name)
           output_pcoll_id, = list(transform.outputs.values())
           output_coder_id = pcollections[output_pcoll_id].coder_id
           flatten_writes = []
           for local_in, pcoll_in in transform.inputs.items():
 
             if pcollections[pcoll_in].coder_id != output_coder_id:
               # Flatten inputs must all be written with the same coder as is
@@ -683,28 +740,28 @@
             flatten_write = Stage(
                 transform.unique_name + '/Write/' + local_in,
                 [beam_runner_api_pb2.PTransform(
                     unique_name=transform.unique_name + '/Write/' + local_in,
                     inputs={local_in: transcoded_pcollection},
                     spec=beam_runner_api_pb2.FunctionSpec(
                         urn=bundle_processor.DATA_OUTPUT_URN,
-                        payload=param))],
+                        payload=buffer_id))],
                 downstream_side_inputs=frozenset(),
                 must_follow=stage.must_follow)
             flatten_writes.append(flatten_write)
             yield flatten_write
 
           yield Stage(
               transform.unique_name + '/Read',
               [beam_runner_api_pb2.PTransform(
                   unique_name=transform.unique_name + '/Read',
                   outputs=transform.outputs,
                   spec=beam_runner_api_pb2.FunctionSpec(
                       urn=bundle_processor.DATA_INPUT_URN,
-                      payload=param))],
+                      payload=buffer_id))],
               downstream_side_inputs=stage.downstream_side_inputs,
               must_follow=union(frozenset(flatten_writes), stage.must_follow))
 
         else:
           yield stage
 
     def annotate_downstream_side_inputs(stages):
@@ -795,45 +852,45 @@
             producers_by_pcoll[output] = stage
 
       logging.debug('consumers\n%s', consumers_by_pcoll)
       logging.debug('producers\n%s', producers_by_pcoll)
 
       # Now try to fuse away all pcollections.
       for pcoll, producer in producers_by_pcoll.items():
-        pcoll_as_param = str("materialize:%s" % pcoll).encode('utf-8')
         write_pcoll = None
         for consumer in consumers_by_pcoll[pcoll]:
           producer = replacement(producer)
           consumer = replacement(consumer)
           # Update consumer.must_follow set, as it's used in can_fuse.
           consumer.must_follow = frozenset(
               replacement(s) for s in consumer.must_follow)
           if producer.can_fuse(consumer):
             fuse(producer, consumer)
           else:
             # If we can't fuse, do a read + write.
+            buffer_id = create_buffer_id(pcoll)
             if write_pcoll is None:
               write_pcoll = Stage(
                   pcoll + '/Write',
                   [beam_runner_api_pb2.PTransform(
                       unique_name=pcoll + '/Write',
                       inputs={'in': pcoll},
                       spec=beam_runner_api_pb2.FunctionSpec(
                           urn=bundle_processor.DATA_OUTPUT_URN,
-                          payload=pcoll_as_param))])
+                          payload=buffer_id))])
               fuse(producer, write_pcoll)
             if consumer.has_as_main_input(pcoll):
               read_pcoll = Stage(
                   pcoll + '/Read',
                   [beam_runner_api_pb2.PTransform(
                       unique_name=pcoll + '/Read',
                       outputs={'out': pcoll},
                       spec=beam_runner_api_pb2.FunctionSpec(
                           urn=bundle_processor.DATA_INPUT_URN,
-                          payload=pcoll_as_param))],
+                          payload=buffer_id))],
                   must_follow=frozenset([write_pcoll]))
               fuse(read_pcoll, consumer)
             else:
               consumer.must_follow = union(
                   consumer.must_follow, frozenset([write_pcoll]))
 
       # Everything that was originally a stage or a replacement, but wasn't
@@ -851,43 +908,35 @@
 
     def inject_timer_pcollections(stages):
       for stage in stages:
         for transform in list(stage.transforms):
           if transform.spec.urn == common_urns.primitives.PAR_DO.urn:
             payload = proto_utils.parse_Bytes(
                 transform.spec.payload, beam_runner_api_pb2.ParDoPayload)
-            for tag in payload.timer_specs.keys():
+            for tag, spec in payload.timer_specs.items():
               if len(transform.inputs) > 1:
                 raise NotImplementedError('Timers and side inputs.')
               input_pcoll = pipeline_components.pcollections[
                   next(iter(transform.inputs.values()))]
               # Create the appropriate coder for the timer PCollection.
-              void_coder_id = add_or_get_coder_id(
-                  beam.coders.SingletonCoder(None).to_runner_api(None))
-              timer_coder_id = add_or_get_coder_id(
-                  beam_runner_api_pb2.Coder(
-                      spec=beam_runner_api_pb2.SdkFunctionSpec(
-                          spec=beam_runner_api_pb2.FunctionSpec(
-                              urn=common_urns.coders.TIMER.urn)),
-                      component_coder_ids=[void_coder_id]))
               key_coder_id = input_pcoll.coder_id
               if (pipeline_components.coders[key_coder_id].spec.spec.urn
                   == common_urns.coders.WINDOWED_VALUE.urn):
                 key_coder_id = pipeline_components.coders[
                     key_coder_id].component_coder_ids[0]
               if (pipeline_components.coders[key_coder_id].spec.spec.urn
                   == common_urns.coders.KV.urn):
                 key_coder_id = pipeline_components.coders[
                     key_coder_id].component_coder_ids[0]
               key_timer_coder_id = add_or_get_coder_id(
                   beam_runner_api_pb2.Coder(
                       spec=beam_runner_api_pb2.SdkFunctionSpec(
                           spec=beam_runner_api_pb2.FunctionSpec(
                               urn=common_urns.coders.KV.urn)),
-                      component_coder_ids=[key_coder_id, timer_coder_id]))
+                      component_coder_ids=[key_coder_id, spec.timer_coder_id]))
               timer_pcoll_coder_id = windowed_coder_id(
                   key_timer_coder_id,
                   pipeline_components.windowing_strategies[
                       input_pcoll.windowing_strategy_id].window_coder_id)
               # Inject the read and write pcollections.
               timer_read_pcoll = unique_name(
                   pipeline_components.pcollections,
@@ -909,24 +958,24 @@
                       is_bounded=input_pcoll.is_bounded))
               stage.transforms.append(
                   beam_runner_api_pb2.PTransform(
                       unique_name=timer_read_pcoll + '/Read',
                       outputs={'out': timer_read_pcoll},
                       spec=beam_runner_api_pb2.FunctionSpec(
                           urn=bundle_processor.DATA_INPUT_URN,
-                          payload=('timers:%s' % timer_read_pcoll).encode(
-                              'utf-8'))))
+                          payload=create_buffer_id(
+                              timer_read_pcoll, kind='timers'))))
               stage.transforms.append(
                   beam_runner_api_pb2.PTransform(
                       unique_name=timer_write_pcoll + '/Write',
                       inputs={'in': timer_write_pcoll},
                       spec=beam_runner_api_pb2.FunctionSpec(
                           urn=bundle_processor.DATA_OUTPUT_URN,
-                          payload=('timers:%s' % timer_write_pcoll).encode(
-                              'utf-8'))))
+                          payload=create_buffer_id(
+                              timer_write_pcoll, kind='timers'))))
               assert tag not in transform.inputs
               transform.inputs[tag] = timer_read_pcoll
               assert tag not in transform.outputs
               transform.outputs[tag] = timer_write_pcoll
               stage.timer_pcollections.append(
                   (timer_read_pcoll + '/Read', timer_write_pcoll))
         yield stage
@@ -947,27 +996,23 @@
         process(stage)
       return ordered
 
     # Now actually apply the operations.
 
     pipeline_components = copy.deepcopy(pipeline_proto.components)
 
-    # Reify coders.
-    # TODO(BEAM-2717): Remove once Coders are already in proto.
-    coders = pipeline_context.PipelineContext(pipeline_components).coders
+    # Some SDK workers require windowed coders for their PCollections.
+    # TODO(BEAM-4150): Consistently use unwindowed coders everywhere.
     for pcoll in pipeline_components.pcollections.values():
-      if pcoll.coder_id not in coders:
-        window_coder = coders[
+      if (pipeline_components.coders[pcoll.coder_id].spec.spec.urn
+          != common_urns.coders.WINDOWED_VALUE.urn):
+        pcoll.coder_id = windowed_coder_id(
+            pcoll.coder_id,
             pipeline_components.windowing_strategies[
-                pcoll.windowing_strategy_id].window_coder_id]
-        coder = WindowedValueCoder(
-            registry.get_coder(pickler.loads(pcoll.coder_id)),
-            window_coder=window_coder)
-        pcoll.coder_id = coders.get_id(coder)
-    coders.populate_map(pipeline_components.coders)
+                pcoll.windowing_strategy_id].window_coder_id)
 
     known_composites = set(
         [common_urns.primitives.GROUP_BY_KEY.urn,
          common_urns.composites.COMBINE_PER_KEY.urn])
 
     def leaf_transforms(root_ids):
       for root_id in root_ids:
@@ -996,31 +1041,35 @@
       stages = list(phase(stages))
       logging.debug('Stages: %s', [str(s) for s in stages])
 
     # Return the (possibly mutated) context and ordered set of stages.
     return pipeline_components, stages, safe_coders
 
   def run_stages(self, pipeline_components, stages, safe_coders):
-
     if self._use_grpc:
       controller = FnApiRunner.GrpcController(self._sdk_harness_factory)
     else:
       controller = FnApiRunner.DirectController()
     metrics_by_stage = {}
+    monitoring_infos_by_stage = {}
 
     try:
-      pcoll_buffers = collections.defaultdict(list)
-      for stage in stages:
-        metrics_by_stage[stage.name] = self.run_stage(
-            controller, pipeline_components, stage,
-            pcoll_buffers, safe_coders).process_bundle.metrics
+      with self.maybe_profile():
+        pcoll_buffers = collections.defaultdict(list)
+        for stage in stages:
+          stage_results = self.run_stage(
+              controller, pipeline_components, stage,
+              pcoll_buffers, safe_coders)
+          metrics_by_stage[stage.name] = stage_results.process_bundle.metrics
+          monitoring_infos_by_stage[stage.name] = (
+              stage_results.process_bundle.monitoring_infos)
     finally:
       controller.close()
-
-    return RunnerResult(runner.PipelineState.DONE, metrics_by_stage)
+    return RunnerResult(
+        runner.PipelineState.DONE, monitoring_infos_by_stage, metrics_by_stage)
 
   def run_stage(
       self, controller, pipeline_components, stage, pcoll_buffers, safe_coders):
 
     context = pipeline_context.PipelineContext(pipeline_components)
     data_api_service_descriptor = controller.data_api_service_descriptor()
 
@@ -1032,15 +1081,15 @@
       data_output = {}
       for transform in stage.transforms:
         if transform.spec.urn in (bundle_processor.DATA_INPUT_URN,
                                   bundle_processor.DATA_OUTPUT_URN):
           pcoll_id = transform.spec.payload
           if transform.spec.urn == bundle_processor.DATA_INPUT_URN:
             target = transform.unique_name, only_element(transform.outputs)
-            if pcoll_id.startswith(IMPULSE_BUFFER_PREFIX):
+            if pcoll_id == IMPULSE_BUFFER:
               data_input[target] = [ENCODED_IMPULSE_VALUE]
             else:
               data_input[target] = pcoll_buffers[pcoll_id]
             coder_id = pipeline_components.pcollections[
                 only_element(transform.outputs.values())].coder_id
           elif transform.spec.urn == bundle_processor.DATA_OUTPUT_URN:
             target = transform.unique_name, only_element(transform.inputs)
@@ -1055,15 +1104,15 @@
                 data_api_service_descriptor.url)
           transform.spec.payload = data_spec.SerializeToString()
         elif transform.spec.urn == common_urns.primitives.PAR_DO.urn:
           payload = proto_utils.parse_Bytes(
               transform.spec.payload, beam_runner_api_pb2.ParDoPayload)
           for tag, si in payload.side_inputs.items():
             data_side_input[transform.unique_name, tag] = (
-                'materialize:' + transform.inputs[tag],
+                create_buffer_id(transform.inputs[tag]),
                 beam.pvalue.SideInputData.from_runner_api(si, context))
       return data_input, data_side_input, data_output
 
     logging.info('Running %s', stage.name)
     logging.debug('       %s', stage)
     data_input, data_side_input, data_output = extract_endpoints(stage)
 
@@ -1078,69 +1127,79 @@
         environments=dict(pipeline_components.environments.items()))
 
     if controller.state_api_service_descriptor():
       process_bundle_descriptor.state_api_service_descriptor.url = (
           controller.state_api_service_descriptor().url)
 
     # Store the required side inputs into state.
-    for (transform_id, tag), (pcoll_id, si) in data_side_input.items():
-      actual_pcoll_id = pcoll_id[len(b"materialize:"):]
+    for (transform_id, tag), (buffer_id, si) in data_side_input.items():
+      _, pcoll_id = split_buffer_id(buffer_id)
       value_coder = context.coders[safe_coders[
-          pipeline_components.pcollections[actual_pcoll_id].coder_id]]
+          pipeline_components.pcollections[pcoll_id].coder_id]]
       elements_by_window = _WindowGroupingBuffer(si, value_coder)
-      for element_data in pcoll_buffers[pcoll_id]:
+      for element_data in pcoll_buffers[buffer_id]:
         elements_by_window.append(element_data)
       for key, window, elements_data in elements_by_window.encoded_items():
         state_key = beam_fn_api_pb2.StateKey(
             multimap_side_input=beam_fn_api_pb2.StateKey.MultimapSideInput(
                 ptransform_id=transform_id,
                 side_input_id=tag,
                 window=window,
                 key=key))
         controller.state_handler.blocking_append(state_key, elements_data)
 
-    def get_buffer(pcoll_id):
-      if (pcoll_id.startswith(b'materialize:')
-          or pcoll_id.startswith(b'timers:')):
-        if pcoll_id not in pcoll_buffers:
+    def get_buffer(buffer_id):
+      kind, name = split_buffer_id(buffer_id)
+      if kind in ('materialize', 'timers'):
+        if buffer_id not in pcoll_buffers:
           # Just store the data chunks for replay.
-          pcoll_buffers[pcoll_id] = list()
-      elif pcoll_id.startswith(b'group:'):
+          pcoll_buffers[buffer_id] = list()
+      elif kind == 'group':
         # This is a grouping write, create a grouping buffer if needed.
-        if pcoll_id not in pcoll_buffers:
-          original_gbk_transform = pcoll_id.split(b':', 1)[1]
+        if buffer_id not in pcoll_buffers:
+          original_gbk_transform = name
           transform_proto = pipeline_components.transforms[
               original_gbk_transform]
           input_pcoll = only_element(list(transform_proto.inputs.values()))
           output_pcoll = only_element(list(transform_proto.outputs.values()))
           pre_gbk_coder = context.coders[safe_coders[
               pipeline_components.pcollections[input_pcoll].coder_id]]
           post_gbk_coder = context.coders[safe_coders[
               pipeline_components.pcollections[output_pcoll].coder_id]]
           windowing_strategy = context.windowing_strategies[
               pipeline_components
               .pcollections[output_pcoll].windowing_strategy_id]
-          pcoll_buffers[pcoll_id] = _GroupingBuffer(
+          pcoll_buffers[buffer_id] = _GroupingBuffer(
               pre_gbk_coder, post_gbk_coder, windowing_strategy)
       else:
         # These should be the only two identifiers we produce for now,
         # but special side input writes may go here.
-        raise NotImplementedError(pcoll_id)
-      return pcoll_buffers[pcoll_id]
+        raise NotImplementedError(buffer_id)
+      return pcoll_buffers[buffer_id]
+
+    for k in range(self._bundle_repeat):
+      try:
+        controller.state_handler.checkpoint()
+        BundleManager(
+            controller, lambda pcoll_id: [], process_bundle_descriptor,
+            self._progress_frequency, k).process_bundle(data_input, data_output)
+      finally:
+        controller.state_handler.restore()
 
     result = BundleManager(
         controller, get_buffer, process_bundle_descriptor,
         self._progress_frequency).process_bundle(data_input, data_output)
 
     while True:
       timer_inputs = {}
       for transform_id, timer_writes in stage.timer_pcollections:
         windowed_timer_coder_impl = context.coders[
             pipeline_components.pcollections[timer_writes].coder_id].get_impl()
-        written_timers = get_buffer(b'timers:' + timer_writes.encode('utf-8'))
+        written_timers = get_buffer(
+            create_buffer_id(timer_writes, kind='timers'))
         if written_timers:
           # Keep only the "last" timer set per key and window.
           timers_by_key_and_window = {}
           for elements_data in written_timers:
             input_stream = create_InputStream(elements_data)
             while input_stream.size() > 0:
               windowed_key_timer = windowed_timer_coder_impl.decode_from_stream(
@@ -1162,27 +1221,79 @@
           if other_input not in timer_inputs:
             timer_inputs[other_input] = []
         # TODO(robertwb): merge results
         BundleManager(
             controller,
             get_buffer,
             process_bundle_descriptor,
-            self._progress_frequency).process_bundle(timer_inputs, data_output)
+            self._progress_frequency,
+            True).process_bundle(timer_inputs, data_output)
       else:
         break
 
     return result
 
   # These classes are used to interact with the worker.
 
   class StateServicer(beam_fn_api_pb2_grpc.BeamFnStateServicer):
 
+    class CopyOnWriteState(object):
+      def __init__(self, underlying):
+        self._underlying = underlying
+        self._overlay = {}
+
+      def __getitem__(self, key):
+        if key in self._overlay:
+          return self._overlay[key]
+        else:
+          return FnApiRunner.StateServicer.CopyOnWriteList(
+              self._underlying, self._overlay, key)
+
+      def __delitem__(self, key):
+        self._overlay[key] = []
+
+      def commit(self):
+        self._underlying.update(self._overlay)
+        return self._underlying
+
+    class CopyOnWriteList(object):
+      def __init__(self, underlying, overlay, key):
+        self._underlying = underlying
+        self._overlay = overlay
+        self._key = key
+
+      def __iter__(self):
+        if self._key in self._overlay:
+          return iter(self._overlay[self._key])
+        else:
+          return iter(self._underlying[self._key])
+
+      def append(self, item):
+        if self._key not in self._overlay:
+          self._overlay[self._key] = list(self._underlying[self._key])
+        self._overlay[self._key].append(item)
+
     def __init__(self):
       self._lock = threading.Lock()
       self._state = collections.defaultdict(list)
+      self._checkpoint = None
+
+    def checkpoint(self):
+      assert self._checkpoint is None
+      self._checkpoint = self._state
+      self._state = FnApiRunner.StateServicer.CopyOnWriteState(self._state)
+
+    def commit(self):
+      self._state.commit()
+      self._state = self._checkpoint.commit()
+      self._checkpoint = None
+
+    def restore(self):
+      self._state = self._checkpoint
+      self._checkpoint = None
 
     @contextlib.contextmanager
     def process_instruction_id(self, unused_instruction_id):
       yield
 
     def blocking_get(self, state_key):
       with self._lock:
@@ -1347,19 +1458,20 @@
 
 
 class BundleManager(object):
 
   _uid_counter = 0
 
   def __init__(
-      self, controller, get_buffer, bundle_descriptor, progress_frequency=None):
+      self, controller, get_buffer, bundle_descriptor, progress_frequency=None,
+      skip_registration=False):
     self._controller = controller
     self._get_buffer = get_buffer
     self._bundle_descriptor = bundle_descriptor
-    self._registered = False
+    self._registered = skip_registration
     self._progress_frequency = progress_frequency
 
   def process_bundle(self, inputs, expected_outputs):
     # Unique id for the instruction processing this bundle.
     BundleManager._uid_counter += 1
     process_bundle_id = 'bundle_%s' % BundleManager._uid_counter
 
@@ -1467,66 +1579,92 @@
       with self._condition:
         if not self._response:
           self._condition.wait(timeout)
     return self._response
 
 
 class FnApiMetrics(metrics.metric.MetricResults):
-  def __init__(self, step_metrics):
+  def __init__(self, step_monitoring_infos, user_metrics_only=True):
+    """Used for querying metrics from the PipelineResult object.
+
+      step_monitoring_infos: Per step metrics specified as MonitoringInfos.
+      use_monitoring_infos: If true, return the metrics based on the
+          step_monitoring_infos.
+    """
     self._counters = {}
     self._distributions = {}
     self._gauges = {}
-    for step_metric in step_metrics.values():
-      for ptransform_id, ptransform in step_metric.ptransforms.items():
-        for proto in ptransform.user:
-          key = metrics.execution.MetricKey(
-              ptransform_id,
-              metrics.metricbase.MetricName.from_runner_api(proto.metric_name))
-          if proto.HasField('counter_data'):
-            self._counters[key] = proto.counter_data.value
-          elif proto.HasField('distribution_data'):
-            self._distributions[
-                key] = metrics.cells.DistributionResult(
-                    metrics.cells.DistributionData.from_runner_api(
-                        proto.distribution_data))
-          elif proto.HasField('gauge_data'):
-            self._gauges[
-                key] = metrics.cells.GaugeResult(
-                    metrics.cells.GaugeData.from_runner_api(
-                        proto.gauge_data))
+    self._user_metrics_only = user_metrics_only
+    self._init_metrics_from_monitoring_infos(step_monitoring_infos)
+
+  def _init_metrics_from_monitoring_infos(self, step_monitoring_infos):
+    for smi in step_monitoring_infos.values():
+      # Only include user metrics.
+      for mi in smi:
+        if (self._user_metrics_only and
+            not monitoring_infos.is_user_monitoring_info(mi)):
+          continue
+        key = self._to_metric_key(mi)
+        if monitoring_infos.is_counter(mi):
+          self._counters[key] = (
+              monitoring_infos.extract_metric_result_map_value(mi))
+        elif monitoring_infos.is_distribution(mi):
+          self._distributions[key] = (
+              monitoring_infos.extract_metric_result_map_value(mi))
+        elif monitoring_infos.is_gauge(mi):
+          self._gauges[key] = (
+              monitoring_infos.extract_metric_result_map_value(mi))
+
+  def _to_metric_key(self, monitoring_info):
+    # Right now this assumes that all metrics have a PTRANSFORM
+    ptransform_id = monitoring_info.labels['PTRANSFORM']
+    namespace, name = monitoring_infos.parse_namespace_and_name(monitoring_info)
+    return MetricKey(
+        ptransform_id, metrics.metricbase.MetricName(namespace, name))
 
   def query(self, filter=None):
     counters = [metrics.execution.MetricResult(k, v, v)
                 for k, v in self._counters.items()
                 if self.matches(filter, k)]
     distributions = [metrics.execution.MetricResult(k, v, v)
                      for k, v in self._distributions.items()
                      if self.matches(filter, k)]
     gauges = [metrics.execution.MetricResult(k, v, v)
               for k, v in self._gauges.items()
               if self.matches(filter, k)]
 
-    return {'counters': counters,
-            'distributions': distributions,
-            'gauges': gauges}
+    return {self.COUNTERS: counters,
+            self.DISTRIBUTIONS: distributions,
+            self.GAUGES: gauges}
 
 
 class RunnerResult(runner.PipelineResult):
-  def __init__(self, state, metrics_by_stage):
+  def __init__(self, state, monitoring_infos_by_stage, metrics_by_stage):
     super(RunnerResult, self).__init__(state)
+    self._monitoring_infos_by_stage = monitoring_infos_by_stage
     self._metrics_by_stage = metrics_by_stage
-    self._user_metrics = None
+    self._metrics = None
+    self._monitoring_metrics = None
 
   def wait_until_finish(self, duration=None):
     return self._state
 
   def metrics(self):
-    if self._user_metrics is None:
-      self._user_metrics = FnApiMetrics(self._metrics_by_stage)
-    return self._user_metrics
+    """Returns a queryable oject including user metrics only."""
+    if self._metrics is None:
+      self._metrics = FnApiMetrics(
+          self._monitoring_infos_by_stage, user_metrics_only=True)
+    return self._metrics
+
+  def monitoring_metrics(self):
+    """Returns a queryable object including all metrics."""
+    if self._monitoring_metrics is None:
+      self._monitoring_metrics = FnApiMetrics(
+          self._monitoring_infos_by_stage, user_metrics_only=False)
+    return self._monitoring_metrics
 
 
 def only_element(iterable):
   element, = iterable
   return element
 
 
@@ -1536,7 +1674,15 @@
     while True:
       counter += 1
       prefix_counter = prefix + "_%s" % counter
       if prefix_counter not in existing:
         return prefix_counter
   else:
     return prefix
+
+
+def create_buffer_id(name, kind='materialize'):
+  return ('%s:%s' % (kind, name)).encode('utf-8')
+
+
+def split_buffer_id(buffer_id):
+  return buffer_id.decode('utf-8').split(':', 1)
```

## Comparing `apache-beam-2.8.0/apache_beam/runners/portability/flink_runner_test.py` & `apache-beam-2.9.0/apache_beam/examples/flink/flink_streaming_impulse.py`

 * *Files 24% similar despite different names*

```diff
@@ -10,82 +10,89 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 #
+
+"""A streaming workflow that uses a synthetic streaming source.
+
+This can only be used with the Flink portable runner.
+"""
+
 from __future__ import absolute_import
-from __future__ import print_function
 
+import argparse
 import logging
-import shutil
 import sys
-import tempfile
-import unittest
 
 import apache_beam as beam
-from apache_beam.options.pipeline_options import DebugOptions
-from apache_beam.options.pipeline_options import StandardOptions
-from apache_beam.runners.portability import portable_runner
-from apache_beam.runners.portability import portable_runner_test
-from apache_beam.testing.util import assert_that
+import apache_beam.transforms.window as window
+from apache_beam.io.flink.flink_streaming_impulse_source import FlinkStreamingImpulseSource
+from apache_beam.options.pipeline_options import PipelineOptions
+from apache_beam.transforms.trigger import AccumulationMode
+from apache_beam.transforms.trigger import AfterProcessingTime
+from apache_beam.transforms.trigger import Repeatedly
+
+
+def split(s):
+  a = s.split("-")
+  return a[0], int(a[1])
+
+
+def count(x):
+  return x[0], sum(x[1])
+
+
+def apply_timestamp(element):
+  import time
+  yield window.TimestampedValue(element, time.time())
+
+
+def run(argv=None):
+  """Build and run the pipeline."""
+  args = ["--runner=PortableRunner",
+          "--job_endpoint=localhost:8099",
+          "--streaming"]
+  if argv:
+    args.extend(argv)
+
+  parser = argparse.ArgumentParser()
+  parser.add_argument('--count',
+                      dest='count',
+                      default=0,
+                      help='Number of triggers to generate '
+                           '(0 means emit forever).')
+  parser.add_argument('--interval_ms',
+                      dest='interval_ms',
+                      default=500,
+                      help='Interval between records per parallel '
+                           'Flink subtask.')
+
+  known_args, pipeline_args = parser.parse_known_args(args)
+
+  pipeline_options = PipelineOptions(pipeline_args)
+
+  p = beam.Pipeline(options=pipeline_options)
+
+  messages = (p | FlinkStreamingImpulseSource()
+              .set_message_count(known_args.count)
+              .set_interval_ms(known_args.interval_ms))
+
+  _ = (messages | 'decode' >> beam.Map(lambda x: ('', 1))
+       | 'window' >> beam.WindowInto(window.GlobalWindows(),
+                                     trigger=Repeatedly(
+                                         AfterProcessingTime(5 * 1000)),
+                                     accumulation_mode=
+                                     AccumulationMode.DISCARDING)
+       | 'group' >> beam.GroupByKey()
+       | 'count' >> beam.Map(count)
+       | 'log' >> beam.Map(lambda x: logging.info("%d" % x[1])))
 
-if __name__ == '__main__':
-  # Run as
-  #
-  # python -m apache_beam.runners.portability.flink_runner_test \
-  #     /path/to/job_server.jar \
-  #     [FlinkRunnerTest.test_method, ...]
-  flinkJobServerJar = sys.argv.pop(1)
-  streaming = sys.argv.pop(1).lower() == 'streaming'
-
-  # This is defined here to only be run when we invoke this file explicitly.
-  class FlinkRunnerTest(portable_runner_test.PortableRunnerTest):
-    _use_grpc = True
-    _use_subprocesses = True
-
-    @classmethod
-    def _subprocess_command(cls, port):
-      tmp_dir = tempfile.mkdtemp(prefix='flinktest')
-      try:
-        return [
-            'java',
-            '-jar', flinkJobServerJar,
-            '--artifacts-dir', tmp_dir,
-            '--job-host', 'localhost',
-            '--job-port', str(port),
-        ]
-      finally:
-        shutil.rmtree(tmp_dir)
-
-    @classmethod
-    def get_runner(cls):
-      return portable_runner.PortableRunner()
-
-    def create_options(self):
-      options = super(FlinkRunnerTest, self).create_options()
-      options.view_as(DebugOptions).experiments = ['beam_fn_api']
-      if streaming:
-        options.view_as(StandardOptions).streaming = True
-      return options
-
-    # Can't read host files from within docker, read a "local" file there.
-    def test_read(self):
-      with self.create_pipeline() as p:
-        lines = p | beam.io.ReadFromText('/etc/profile')
-        assert_that(lines, lambda lines: len(lines) > 0)
-
-    def test_no_subtransform_composite(self):
-      raise unittest.SkipTest("BEAM-4781")
-
-    def test_pardo_state_only(self):
-      raise unittest.SkipTest("BEAM-2918 - User state not yet supported.")
-
-    def test_pardo_timers(self):
-      raise unittest.SkipTest("BEAM-4681 - User timers not yet supported.")
+  result = p.run()
+  result.wait_until_finish()
 
-    # Inherits all other tests.
 
-  # Run the tests.
+if __name__ == '__main__':
   logging.getLogger().setLevel(logging.INFO)
-  unittest.main()
+  run(sys.argv[1:])
```

## Comparing `apache-beam-2.8.0/apache_beam/runners/portability/portable_stager.py` & `apache-beam-2.9.0/apache_beam/runners/portability/portable_stager.py`

 * *Files 2% similar despite different names*

```diff
@@ -16,15 +16,14 @@
 """A :class:`FileHandler` to work with :class:`ArtifactStagingServiceStub`.
 """
 
 from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
-import base64
 import hashlib
 import os
 
 from apache_beam.portability.api import beam_artifact_api_pb2
 from apache_beam.portability.api import beam_artifact_api_pb2_grpc
 from apache_beam.runners.portability.stager import Stager
 
@@ -68,15 +67,15 @@
       raise ValueError(
           'Cannot stage {0} to artifact server. Only local files can be staged.'
           .format(local_path_to_artifact))
 
     def artifact_request_generator():
       artifact_metadata = beam_artifact_api_pb2.ArtifactMetadata(
           name=artifact_name,
-          md5=_get_file_hash(local_path_to_artifact))
+          sha256=_get_file_hash(local_path_to_artifact))
       metadata = beam_artifact_api_pb2.PutArtifactMetadata(
           staging_session_token=self._staging_session_token,
           metadata=artifact_metadata)
       request = beam_artifact_api_pb2.PutArtifactRequest(metadata=metadata)
       yield request
       with open(local_path_to_artifact, 'rb') as f:
         while True:
@@ -96,15 +95,15 @@
     return self._artifact_staging_stub.CommitManifest(
         beam_artifact_api_pb2.CommitManifestRequest(
             manifest=manifest,
             staging_session_token=self._staging_session_token)).retrieval_token
 
 
 def _get_file_hash(path):
-  hasher = hashlib.md5()
+  hasher = hashlib.sha256()
   with open(path, 'rb') as f:
     while True:
       chunk = f.read(1 << 21)
       if chunk:
         hasher.update(chunk)
       else:
-        return base64.b64encode(hasher.digest())
+        return hasher.hexdigest()
```

## Comparing `apache-beam-2.8.0/apache_beam/runners/portability/portable_runner.py` & `apache-beam-2.9.0/apache_beam/runners/portability/portable_runner.py`

 * *Files 8% similar despite different names*

```diff
@@ -20,31 +20,38 @@
 import json
 import logging
 import os
 import threading
 
 import grpc
 
-from apache_beam import coders
 from apache_beam import metrics
-from apache_beam.internal import pickler
 from apache_beam.options.pipeline_options import PortableOptions
 from apache_beam.options.pipeline_options import SetupOptions
 from apache_beam.portability import common_urns
 from apache_beam.portability.api import beam_job_api_pb2
 from apache_beam.portability.api import beam_job_api_pb2_grpc
 from apache_beam.portability.api import beam_runner_api_pb2
 from apache_beam.runners import pipeline_context
 from apache_beam.runners import runner
 from apache_beam.runners.job import utils as job_utils
 from apache_beam.runners.portability import portable_stager
 from apache_beam.runners.portability.job_server import DockerizedJobServer
 
 __all__ = ['PortableRunner']
 
+MESSAGE_LOG_LEVELS = {
+    beam_job_api_pb2.JobMessage.MESSAGE_IMPORTANCE_UNSPECIFIED: logging.INFO,
+    beam_job_api_pb2.JobMessage.JOB_MESSAGE_DEBUG: logging.DEBUG,
+    beam_job_api_pb2.JobMessage.JOB_MESSAGE_DETAILED: logging.DEBUG,
+    beam_job_api_pb2.JobMessage.JOB_MESSAGE_BASIC: logging.INFO,
+    beam_job_api_pb2.JobMessage.JOB_MESSAGE_WARNING: logging.WARNING,
+    beam_job_api_pb2.JobMessage.JOB_MESSAGE_ERROR: logging.ERROR,
+}
+
 TERMINAL_STATES = [
     beam_job_api_pb2.JobState.DONE,
     beam_job_api_pb2.JobState.STOPPED,
     beam_job_api_pb2.JobState.FAILED,
     beam_job_api_pb2.JobState.CANCELLED,
 ]
 
@@ -54,18 +61,14 @@
     Experimental: No backward compatibility guaranteed.
     A BeamRunner that executes Python pipelines via the Beam Job API.
 
     This runner is a stub and does not run the actual job.
     This runner schedules the job on a job service. The responsibility of
     running and managing the job lies with the job service used.
   """
-
-  def __init__(self, is_embedded_fnapi_runner=False):
-    self.is_embedded_fnapi_runner = is_embedded_fnapi_runner
-
   @staticmethod
   def default_docker_image():
     if 'USER' in os.environ:
       # Perhaps also test if this was built?
       logging.info('Using latest locally built Python SDK docker image.')
       return os.environ['USER'] + '-docker-apache.bintray.io/beam/python:latest'
     else:
@@ -116,27 +119,14 @@
       job_endpoint = docker.start()
 
     proto_context = pipeline_context.PipelineContext(
         default_environment=PortableRunner._create_environment(
             portable_options))
     proto_pipeline = pipeline.to_runner_api(context=proto_context)
 
-    if not self.is_embedded_fnapi_runner:
-      # Java has different expectations about coders
-      # (windowed in Fn API, but *un*windowed in runner API), whereas the
-      # embedded FnApiRunner treats them consistently, so we must guard this
-      # for now, until FnApiRunner is fixed.
-      # See also BEAM-2717.
-      for pcoll in proto_pipeline.components.pcollections.values():
-        if pcoll.coder_id not in proto_context.coders:
-          # This is not really a coder id, but a pickled coder.
-          coder = coders.registry.get_coder(pickler.loads(pcoll.coder_id))
-          pcoll.coder_id = proto_context.coders.get_id(coder)
-      proto_context.coders.populate_map(proto_pipeline.components.coders)
-
     # Some runners won't detect the GroupByKey transform unless it has no
     # subtransforms.  Remove all sub-transforms until BEAM-4605 is resolved.
     for _, transform_proto in list(
         proto_pipeline.components.transforms.items()):
       if transform_proto.spec.urn == common_urns.primitives.GROUP_BY_KEY.urn:
         for sub_transform in transform_proto.subtransforms:
           del proto_pipeline.components.transforms[sub_transform]
@@ -223,28 +213,52 @@
   @staticmethod
   def _pipeline_state_to_runner_api_state(pipeline_state):
     return beam_job_api_pb2.JobState.Enum.Value(pipeline_state)
 
   def metrics(self):
     return PortableMetrics()
 
+  def _last_error_message(self):
+    # Python sort is stable.
+    ordered_messages = sorted(
+        [m.message_response for m in self._messages
+         if m.HasField('message_response')],
+        key=lambda m: m.importance)
+    if ordered_messages:
+      return ordered_messages[-1].message_text
+    else:
+      return 'unknown error'
+
   def wait_until_finish(self):
 
     def read_messages():
       for message in self._job_service.GetMessageStream(
           beam_job_api_pb2.JobMessagesRequest(job_id=self._job_id)):
+        if message.HasField('message_response'):
+          logging.log(
+              MESSAGE_LOG_LEVELS[message.message_response.importance],
+              "%s",
+              message.message_response.message_text)
+        else:
+          logging.info(
+              "Job state changed to %s",
+              self._runner_api_state_to_pipeline_state(
+                  message.state_response.state))
         self._messages.append(message)
 
     t = threading.Thread(target=read_messages, name='wait_until_finish_read')
     t.daemon = True
     t.start()
 
     for state_response in self._job_service.GetStateStream(
         beam_job_api_pb2.GetJobStateRequest(job_id=self._job_id)):
       self._state = self._runner_api_state_to_pipeline_state(
           state_response.state)
       if state_response.state in TERMINAL_STATES:
+        # Wait for any last messages.
+        t.join(10)
         break
     if self._state != runner.PipelineState.DONE:
       raise RuntimeError(
-          'Pipeline %s failed in state %s.' % (self._job_id, self._state))
+          'Pipeline %s failed in state %s: %s' % (
+              self._job_id, self._state, self._last_error_message()))
     return self._state
```

## Comparing `apache-beam-2.8.0/apache_beam/runners/portability/job_server.py` & `apache-beam-2.9.0/apache_beam/runners/portability/job_server.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/runners/portability/portable_stager_test.py` & `apache-beam-2.9.0/apache_beam/runners/portability/portable_stager_test.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/runners/portability/fn_api_runner_test.py` & `apache-beam-2.9.0/apache_beam/runners/portability/fn_api_runner_test.py`

 * *Files 14% similar despite different names*

```diff
@@ -24,14 +24,15 @@
 import tempfile
 import time
 import traceback
 import unittest
 from builtins import range
 
 import apache_beam as beam
+from apache_beam.metrics import monitoring_infos
 from apache_beam.metrics.execution import MetricKey
 from apache_beam.metrics.execution import MetricsEnvironment
 from apache_beam.metrics.metricbase import MetricName
 from apache_beam.runners.portability import fn_api_runner
 from apache_beam.runners.worker import data_plane
 from apache_beam.runners.worker import sdk_worker
 from apache_beam.runners.worker import statesampler
@@ -431,15 +432,14 @@
 
     with self.create_pipeline() as p:
       pcoll_a = p | 'a' >> beam.Create(['a'])
       pcoll_b = p | 'b' >> beam.Create(['b'])
       assert_that((pcoll_a, pcoll_b) | First(), equal_to(['a']))
 
   def test_metrics(self):
-
     p = self.create_pipeline()
     if not isinstance(p.runner, fn_api_runner.FnApiRunner):
       # This test is inherited by others that may not support the same
       # internal way of accessing progress metrics.
       self.skipTest('Metrics not supported.')
 
     counter = beam.metrics.Metrics.counter('ns', 'counter')
@@ -466,14 +466,54 @@
     gaug, = res.metrics().query(
         beam.metrics.MetricsFilter().with_step('gauge'))['gauges']
     self.assertEqual(
         dist.committed.data, beam.metrics.cells.DistributionData(4, 2, 1, 3))
     self.assertEqual(dist.committed.mean, 2.0)
     self.assertEqual(gaug.committed.value, 3)
 
+  def test_non_user_metrics(self):
+    p = self.create_pipeline()
+    if not isinstance(p.runner, fn_api_runner.FnApiRunner):
+      # This test is inherited by others that may not support the same
+      # internal way of accessing progress metrics.
+      self.skipTest('Metrics not supported.')
+
+    pcoll = p | beam.Create(['a', 'zzz'])
+    # pylint: disable=expression-not-assigned
+    pcoll | 'MyStep' >> beam.FlatMap(lambda x: None)
+    res = p.run()
+    res.wait_until_finish()
+
+    result_metrics = res.monitoring_metrics()
+    all_metrics_via_montoring_infos = result_metrics.query()
+
+    def assert_counter_exists(metrics, namespace, name, step):
+      found = 0
+      metric_key = MetricKey(step, MetricName(namespace, name))
+      for m in metrics['counters']:
+        if m.key == metric_key:
+          found = found + 1
+      self.assertEqual(
+          1, found, "Did not find exactly 1 metric for %s." % metric_key)
+    urns = [
+        monitoring_infos.ELEMENT_COUNT_URN,
+        monitoring_infos.START_BUNDLE_MSECS_URN,
+        monitoring_infos.PROCESS_BUNDLE_MSECS_URN,
+        monitoring_infos.FINISH_BUNDLE_MSECS_URN,
+        monitoring_infos.TOTAL_MSECS_URN,
+    ]
+    for urn in urns:
+      split = urn.split(':')
+      namespace = split[0]
+      name = ':'.join(split[1:])
+      assert_counter_exists(
+          all_metrics_via_montoring_infos, namespace, name, step='Create/Read')
+      assert_counter_exists(
+          all_metrics_via_montoring_infos, namespace, name, step='MyStep')
+
   def test_progress_metrics(self):
     p = self.create_pipeline()
     if not isinstance(p.runner, fn_api_runner.FnApiRunner):
       # This test is inherited by others that may not support the same
       # internal way of accessing progress metrics.
       self.skipTest('Progress metrics not supported.')
 
@@ -485,21 +525,29 @@
          | 'm_out' >> beam.FlatMap(lambda x: [
              1, 2, 3, 4, 5,
              beam.pvalue.TaggedOutput('once', x),
              beam.pvalue.TaggedOutput('twice', x),
              beam.pvalue.TaggedOutput('twice', x)]))
     res = p.run()
     res.wait_until_finish()
+
+    def has_mi_for_ptransform(monitoring_infos, ptransform):
+      for mi in monitoring_infos:
+        if ptransform in mi.labels['PTRANSFORM']:
+          return True
+      return False
+
     try:
-      self.assertEqual(2, len(res._metrics_by_stage))
-      pregbk_metrics, postgbk_metrics = list(res._metrics_by_stage.values())
+      # TODO(ajamato): Delete this block after deleting the legacy metrics code.
+      # Test the DEPRECATED legacy metrics
+      pregbk_metrics, postgbk_metrics = list(
+          res._metrics_by_stage.values())
       if 'Create/Read' not in pregbk_metrics.ptransforms:
         # The metrics above are actually unordered. Swap.
         pregbk_metrics, postgbk_metrics = postgbk_metrics, pregbk_metrics
-
       self.assertEqual(
           4,
           pregbk_metrics.ptransforms['Create/Read']
           .processed_elements.measured.output_element_counts['out'])
       self.assertEqual(
           4,
           pregbk_metrics.ptransforms['Map(sleep)']
@@ -523,16 +571,66 @@
       self.assertEqual(
           1,
           m_out.processed_elements.measured.output_element_counts['once'])
       self.assertEqual(
           2,
           m_out.processed_elements.measured.output_element_counts['twice'])
 
+      # Test the new MonitoringInfo monitoring format.
+      self.assertEqual(2, len(res._monitoring_infos_by_stage))
+      pregbk_mis, postgbk_mis = list(res._monitoring_infos_by_stage.values())
+      if not has_mi_for_ptransform(pregbk_mis, 'Create/Read'):
+        # The monitoring infos above are actually unordered. Swap.
+        pregbk_mis, postgbk_mis = postgbk_mis, pregbk_mis
+
+      def assert_has_monitoring_info(
+          monitoring_infos, urn, labels, value=None, ge_value=None):
+        # TODO(ajamato): Consider adding a matcher framework
+        found = 0
+        for m in monitoring_infos:
+          if m.labels == labels and m.urn == urn:
+            if (ge_value is not None and
+                m.metric.counter_data.int64_value >= ge_value):
+              found = found + 1
+            elif (value is not None and
+                  m.metric.counter_data.int64_value == value):
+              found = found + 1
+        ge_value_str = {'ge_value' : ge_value} if ge_value else ''
+        value_str = {'value' : value} if value else ''
+        self.assertEqual(
+            1, found, "Found (%s) Expected only 1 monitoring_info for %s." %
+            (found, (urn, labels, value_str, ge_value_str),))
+
+      # pregbk monitoring infos
+      labels = {'PTRANSFORM' : 'Create/Read', 'TAG' : 'out'}
+      assert_has_monitoring_info(
+          pregbk_mis, monitoring_infos.ELEMENT_COUNT_URN, labels, value=4)
+      labels = {'PTRANSFORM' : 'Map(sleep)', 'TAG' : 'None'}
+      assert_has_monitoring_info(
+          pregbk_mis, monitoring_infos.ELEMENT_COUNT_URN, labels, value=4)
+      labels = {'PTRANSFORM' : 'Map(sleep)'}
+      assert_has_monitoring_info(
+          pregbk_mis, monitoring_infos.TOTAL_MSECS_URN,
+          labels, ge_value=4 * DEFAULT_SAMPLING_PERIOD_MS)
+
+      # postgbk monitoring infos
+      labels = {'PTRANSFORM' : 'GroupByKey/Read', 'TAG' : 'None'}
+      assert_has_monitoring_info(
+          postgbk_mis, monitoring_infos.ELEMENT_COUNT_URN, labels, value=1)
+      labels = {'PTRANSFORM' : 'm_out', 'TAG' : 'None'}
+      assert_has_monitoring_info(
+          postgbk_mis, monitoring_infos.ELEMENT_COUNT_URN, labels, value=5)
+      labels = {'PTRANSFORM' : 'm_out', 'TAG' : 'once'}
+      assert_has_monitoring_info(
+          postgbk_mis, monitoring_infos.ELEMENT_COUNT_URN, labels, value=1)
+      labels = {'PTRANSFORM' : 'm_out', 'TAG' : 'twice'}
+      assert_has_monitoring_info(
+          postgbk_mis, monitoring_infos.ELEMENT_COUNT_URN, labels, value=2)
     except:
-      print(res._metrics_by_stage)
+      print(res._monitoring_infos_by_stage)
       raise
 
 
 class FnApiRunnerTestWithGrpc(FnApiRunnerTest):
 
   def create_pipeline(self):
     return beam.Pipeline(
@@ -545,10 +643,17 @@
     return beam.Pipeline(
         runner=fn_api_runner.FnApiRunner(
             use_grpc=True,
             sdk_harness_factory=functools.partial(
                 sdk_worker.SdkHarness, worker_count=2)))
 
 
+class FnApiRunnerTestWithBundleRepeat(FnApiRunnerTest):
+
+  def create_pipeline(self):
+    return beam.Pipeline(
+        runner=fn_api_runner.FnApiRunner(use_grpc=False, bundle_repeat=3))
+
+
 if __name__ == '__main__':
   logging.getLogger().setLevel(logging.INFO)
   unittest.main()
```

## Comparing `apache-beam-2.8.0/apache_beam/runners/worker/statesampler_fast.pyx` & `apache-beam-2.9.0/apache_beam/runners/worker/statesampler_fast.pyx`

 * *Files 2% similar despite different names*

```diff
@@ -47,15 +47,15 @@
 
 cdef extern from "Python.h":
   # This typically requires the GIL, but we synchronize the list modifications
   # we use this on via our own lock.
   cdef void* PyList_GET_ITEM(list, Py_ssize_t index) nogil
 
 cdef extern from "unistd.h" nogil:
-  void usleep(long)
+  void usleep(int)
 
 cdef extern from "<time.h>" nogil:
   struct timespec:
     long tv_sec  # seconds
     long tv_nsec  # nanoseconds
   int clock_gettime(int clock_id, timespec *result)
 
@@ -114,15 +114,15 @@
   def run(self):
     cdef int64_t last_nsecs = get_nsec_time()
     cdef int64_t elapsed_nsecs
     cdef int64_t latest_transition_count = self.state_transition_count
     cdef int64_t sampling_period_us = self._sampling_period_ms_start * 1000
     with nogil:
       while True:
-        usleep(sampling_period_us)
+        usleep(<int>sampling_period_us)
         sampling_period_us = <int64_t>math.fmin(
             sampling_period_us * self._sampling_period_ratio,
             self._sampling_period_ms * 1000)
         pythread.PyThread_acquire_lock(self.lock, pythread.WAIT_LOCK)
         try:
           if self.finished:
             break
@@ -150,14 +150,19 @@
     pythread.PyThread_acquire_lock(self.lock, pythread.WAIT_LOCK)
     self.finished = True
     pythread.PyThread_release_lock(self.lock)
     # May have to wait up to sampling_period_ms, but the platform-independent
     # pythread doesn't support conditions.
     self.sampling_thread.join()
 
+  def reset(self):
+    for state in self.scoped_states_by_index:
+      (<ScopedState>state)._nsecs = 0
+    self.started = self.finished = False
+
   def current_state(self):
     return self.scoped_states_by_index[self.current_state_index]
 
   cpdef _scoped_state(self, counter_name, name_context, output_counter,
                       metrics_container):
     """Returns a context manager managing transitions for a given state.
     Args:
@@ -205,14 +210,17 @@
   @property
   def nsecs(self):
     return self._nsecs
 
   def sampled_seconds(self):
     return 1e-9 * self.nsecs
 
+  def sampled_msecs_int(self):
+    return int(1e-6 * self.nsecs)
+
   def __repr__(self):
     return "ScopedState[%s, %s]" % (self.name, self.nsecs)
 
   cpdef __enter__(self):
     self.old_state_index = self.sampler.current_state_index
     pythread.PyThread_acquire_lock(self.sampler.lock, pythread.WAIT_LOCK)
     self.sampler.current_state_index = self.state_index
```

## Comparing `apache-beam-2.8.0/apache_beam/runners/worker/sdk_worker_test.py` & `apache-beam-2.9.0/apache_beam/runners/worker/sdk_worker_test.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/runners/worker/sdk_worker_main.py` & `apache-beam-2.9.0/apache_beam/runners/worker/sdk_worker_main.py`

 * *Files 4% similar despite different names*

```diff
@@ -27,20 +27,22 @@
 import threading
 import traceback
 from builtins import object
 
 from google.protobuf import text_format
 
 from apache_beam.internal import pickler
+from apache_beam.options import pipeline_options
 from apache_beam.options.pipeline_options import DebugOptions
 from apache_beam.options.pipeline_options import PipelineOptions
 from apache_beam.portability.api import endpoints_pb2
-from apache_beam.runners.dataflow.internal import names
+from apache_beam.runners.internal import names
 from apache_beam.runners.worker.log_handler import FnApiLogRecordHandler
 from apache_beam.runners.worker.sdk_worker import SdkHarness
+from apache_beam.utils import profiler
 
 # This module is experimental. No backwards-compatibility guarantees.
 
 
 class StatusServer(object):
 
   @classmethod
@@ -134,15 +136,18 @@
     service_descriptor = endpoints_pb2.ApiServiceDescriptor()
     text_format.Merge(os.environ['CONTROL_API_SERVICE_DESCRIPTOR'],
                       service_descriptor)
     # TODO(robertwb): Support credentials.
     assert not service_descriptor.oauth2_client_credentials_grant.url
     SdkHarness(
         control_address=service_descriptor.url,
-        worker_count=_get_worker_count(sdk_pipeline_options)).run()
+        worker_count=_get_worker_count(sdk_pipeline_options),
+        profiler_factory=profiler.Profile.factory_from_options(
+            sdk_pipeline_options.view_as(pipeline_options.ProfilingOptions))
+    ).run()
     logging.info('Python sdk harness exiting.')
   except:  # pylint: disable=broad-except
     logging.exception('Python sdk harness failed: ')
     raise
   finally:
     if fn_log_handler:
       fn_log_handler.close()
```

## Comparing `apache-beam-2.8.0/apache_beam/runners/worker/statesampler.py` & `apache-beam-2.9.0/apache_beam/runners/worker/statesampler.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/runners/worker/statesampler_slow.py` & `apache-beam-2.9.0/apache_beam/runners/worker/statesampler_slow.py`

 * *Files 6% similar despite different names*

```diff
@@ -61,14 +61,18 @@
   def start(self):
     # Sampling not yet supported. Only state tracking at the moment.
     pass
 
   def stop(self):
     pass
 
+  def reset(self):
+    for state in self._states_by_name.values():
+      state.nsecs = 0
+
 
 class ScopedState(object):
 
   def __init__(self, sampler, name, step_name_context,
                counter=None, metrics_container=None):
     self.state_sampler = sampler
     self.name = name
@@ -76,14 +80,17 @@
     self.counter = counter
     self.nsecs = 0
     self.metrics_container = metrics_container
 
   def sampled_seconds(self):
     return 1e-9 * self.nsecs
 
+  def sampled_msecs_int(self):
+    return int(1e-6 * self.nsecs)
+
   def __repr__(self):
     return "ScopedState[%s, %s]" % (self.name, self.nsecs)
 
   def __enter__(self):
     self.state_sampler._enter_state(self)
 
   def __exit__(self, exc_type, exc_value, traceback):
```

## Comparing `apache-beam-2.8.0/apache_beam/runners/worker/sdk_worker_main_test.py` & `apache-beam-2.9.0/apache_beam/runners/worker/sdk_worker_main_test.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/runners/worker/log_handler.py` & `apache-beam-2.9.0/apache_beam/runners/worker/log_handler.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/runners/worker/__init__.py` & `apache-beam-2.9.0/apache_beam/runners/worker/__init__.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/runners/worker/statesampler_fast.pxd` & `apache-beam-2.9.0/apache_beam/runners/worker/statesampler_fast.pxd`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/runners/worker/operations.py` & `apache-beam-2.9.0/apache_beam/runners/worker/operations.py`

 * *Files 10% similar despite different names*

```diff
@@ -27,14 +27,15 @@
 from builtins import filter
 from builtins import object
 from builtins import zip
 
 from apache_beam import pvalue
 from apache_beam.internal import pickler
 from apache_beam.io import iobase
+from apache_beam.metrics import monitoring_infos
 from apache_beam.metrics.execution import MetricsContainer
 from apache_beam.portability.api import beam_fn_api_pb2
 from apache_beam.runners import common
 from apache_beam.runners.common import Receiver
 from apache_beam.runners.dataflow.internal.names import PropertyNames
 from apache_beam.runners.worker import opcounters
 from apache_beam.runners.worker import operation_specs
@@ -138,36 +139,50 @@
     self.scoped_process_state = self.state_sampler.scoped_state(
         self.name_context, 'process', metrics_container=self.metrics_container)
     self.scoped_finish_state = self.state_sampler.scoped_state(
         self.name_context, 'finish', metrics_container=self.metrics_container)
     # TODO(ccy): the '-abort' state can be added when the abort is supported in
     # Operations.
     self.receivers = []
+    # Legacy workers cannot call setup() until after setting additional state
+    # on the operation.
+    self.setup_done = False
+
+  def setup(self):
+    with self.scoped_start_state:
+      self.debug_logging_enabled = logging.getLogger().isEnabledFor(
+          logging.DEBUG)
+      # Everything except WorkerSideInputSource, which is not a
+      # top-level operation, should have output_coders
+      #TODO(pabloem): Define better what step name is used here.
+      if getattr(self.spec, 'output_coders', None):
+        self.receivers = [ConsumerSet(self.counter_factory,
+                                      self.name_context.logging_name(),
+                                      i,
+                                      self.consumers[i], coder)
+                          for i, coder in enumerate(self.spec.output_coders)]
+    self.setup_done = True
 
   def start(self):
     """Start operation."""
-    self.debug_logging_enabled = logging.getLogger().isEnabledFor(logging.DEBUG)
-    # Everything except WorkerSideInputSource, which is not a
-    # top-level operation, should have output_coders
-    #TODO(pabloem): Define better what step name is used here.
-    if getattr(self.spec, 'output_coders', None):
-      self.receivers = [ConsumerSet(self.counter_factory,
-                                    self.name_context.logging_name(),
-                                    i,
-                                    self.consumers[i], coder)
-                        for i, coder in enumerate(self.spec.output_coders)]
+    if not self.setup_done:
+      # For legacy workers.
+      self.setup()
 
   def process(self, o):
     """Process element in operation."""
     pass
 
   def finish(self):
     """Finish operation."""
     pass
 
+  def reset(self):
+    self.metrics_container.reset()
+
   def output(self, windowed_value, output_index=0):
     cython.cast(Receiver, self.receivers[output_index]).receive(windowed_value)
 
   def add_receiver(self, operation, output_index=0):
     """Adds a receiver operation for the specified output."""
     self.consumers[output_index].append(operation)
 
@@ -186,14 +201,70 @@
                     # TODO(robertwb): Plumb the actual name here.
                     {'ONLY_OUTPUT': self.receivers[0].opcounter
                                     .element_counter.value()}
                     if len(self.receivers) == 1
                     else None))),
         user=self.metrics_container.to_runner_api())
 
+  def monitoring_infos(self, transform_id):
+    """Returns the list of MonitoringInfos collected by this operation."""
+    all_monitoring_infos = self.execution_time_monitoring_infos(transform_id)
+    all_monitoring_infos.update(
+        self.element_count_monitoring_infos(transform_id))
+    all_monitoring_infos.update(self.user_monitoring_infos(transform_id))
+    return all_monitoring_infos
+
+  def element_count_monitoring_infos(self, transform_id):
+    """Returns the element count MonitoringInfo collected by this operation."""
+    if len(self.receivers) == 1:
+      # If there is exactly one output, we can unambiguously
+      # fix its name later, which we do.
+      # TODO(robertwb): Plumb the actual name here.
+      mi = monitoring_infos.int64_counter(
+          monitoring_infos.ELEMENT_COUNT_URN,
+          self.receivers[0].opcounter.element_counter.value(),
+          ptransform=transform_id,
+          tag='ONLY_OUTPUT' if len(self.receivers) == 1 else str(None),
+      )
+      return {monitoring_infos.to_key(mi) : mi}
+    return {}
+
+  def user_monitoring_infos(self, transform_id):
+    """Returns the user MonitoringInfos collected by this operation."""
+    return self.metrics_container.to_runner_api_monitoring_infos(transform_id)
+
+  def execution_time_monitoring_infos(self, transform_id):
+    total_time_spent_msecs = (
+        self.scoped_start_state.sampled_msecs_int()
+        + self.scoped_process_state.sampled_msecs_int()
+        + self.scoped_finish_state.sampled_msecs_int())
+    mis = [
+        monitoring_infos.int64_counter(
+            monitoring_infos.START_BUNDLE_MSECS_URN,
+            self.scoped_start_state.sampled_msecs_int(),
+            ptransform=transform_id
+        ),
+        monitoring_infos.int64_counter(
+            monitoring_infos.PROCESS_BUNDLE_MSECS_URN,
+            self.scoped_process_state.sampled_msecs_int(),
+            ptransform=transform_id
+        ),
+        monitoring_infos.int64_counter(
+            monitoring_infos.FINISH_BUNDLE_MSECS_URN,
+            self.scoped_finish_state.sampled_msecs_int(),
+            ptransform=transform_id
+        ),
+        monitoring_infos.int64_counter(
+            monitoring_infos.TOTAL_MSECS_URN,
+            total_time_spent_msecs,
+            ptransform=transform_id
+        ),
+    ]
+    return {monitoring_infos.to_key(mi) : mi for mi in mis}
+
   def __str__(self):
     """Generates a useful string for this object.
 
     Compactly displays interesting fields.  In particular, pickled
     fields are not displayed.  Note that we collapse the fields of the
     contained Worker* object into this object, since there is a 1-1
     mapping between Operation and operation_specs.Worker*.
@@ -361,17 +432,17 @@
           view_options = {'default': default} if has_default else {}
         else:
           view_options = {}
 
       yield apache_sideinputs.SideInputMap(
           view_class, view_options, sideinputs.EmulatedIterable(iterator_fn))
 
-  def start(self):
+  def setup(self):
     with self.scoped_start_state:
-      super(DoOperation, self).start()
+      super(DoOperation, self).setup()
 
       # See fn_data in dataflow_runner.py
       fn, args, kwargs, tags_and_types, window_fn = (
           pickler.loads(self.spec.serialized_fn))
 
       state = common.DoFnState(self.counter_factory)
       state.step_name = self.name_context.logging_name()
@@ -413,14 +484,17 @@
           user_state_context=self.user_state_context,
           operation_name=self.name_context.metrics_name())
 
       self.dofn_receiver = (self.dofn_runner
                             if isinstance(self.dofn_runner, Receiver)
                             else DoFnRunnerReceiver(self.dofn_runner))
 
+  def start(self):
+    with self.scoped_start_state:
+      super(DoOperation, self).start()
       self.dofn_runner.start()
 
   def process(self, o):
     with self.scoped_process_state:
       self.dofn_receiver.receive(o)
 
   def process_timer(self, tag, windowed_timer):
@@ -429,23 +503,43 @@
     self.dofn_receiver.process_user_timer(
         timer_spec, key, windowed_timer.windows[0], timer_data['timestamp'])
 
   def finish(self):
     with self.scoped_finish_state:
       self.dofn_runner.finish()
 
+  def reset(self):
+    super(DoOperation, self).reset()
+    for side_input_map in self.side_input_maps:
+      side_input_map.reset()
+    if self.user_state_context:
+      self.user_state_context.reset()
+
   def progress_metrics(self):
     metrics = super(DoOperation, self).progress_metrics()
     if self.tagged_receivers:
       metrics.processed_elements.measured.output_element_counts.clear()
       for tag, receiver in self.tagged_receivers.items():
         metrics.processed_elements.measured.output_element_counts[
             str(tag)] = receiver.opcounter.element_counter.value()
     return metrics
 
+  def monitoring_infos(self, transform_id):
+    infos = super(DoOperation, self).monitoring_infos(transform_id)
+    if self.tagged_receivers:
+      for tag, receiver in self.tagged_receivers.items():
+        mi = monitoring_infos.int64_counter(
+            monitoring_infos.ELEMENT_COUNT_URN,
+            receiver.opcounter.element_counter.value(),
+            ptransform=transform_id,
+            tag=str(tag)
+        )
+        infos[monitoring_infos.to_key(mi)] = mi
+    return infos
+
 
 class DoFnRunnerReceiver(Receiver):
 
   def __init__(self, dofn_runner):
     self.dofn_runner = dofn_runner
 
   def receive(self, windowed_value):
```

## Comparing `apache-beam-2.8.0/apache_beam/runners/worker/sdk_worker.py` & `apache-beam-2.9.0/apache_beam/runners/worker/sdk_worker.py`

 * *Files 5% similar despite different names*

```diff
@@ -17,14 +17,15 @@
 """SDK harness for executing Python Fns via the Fn API."""
 
 from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
 import abc
+import collections
 import contextlib
 import logging
 import queue
 import sys
 import threading
 import traceback
 from builtins import object
@@ -41,15 +42,16 @@
 from apache_beam.runners.worker import data_plane
 from apache_beam.runners.worker.worker_id_interceptor import WorkerIdInterceptor
 
 
 class SdkHarness(object):
   REQUEST_METHOD_PREFIX = '_request_'
 
-  def __init__(self, control_address, worker_count, credentials=None):
+  def __init__(self, control_address, worker_count, credentials=None,
+               profiler_factory=None):
     self._worker_count = worker_count
     self._worker_index = 0
     if credentials is None:
       logging.info('Creating insecure control channel.')
       self._control_channel = grpc.insecure_channel(control_address)
     else:
       logging.info('Creating secure control channel.')
@@ -58,14 +60,15 @@
     logging.info('Control channel established.')
 
     self._control_channel = grpc.intercept_channel(
         self._control_channel, WorkerIdInterceptor())
     self._data_channel_factory = data_plane.GrpcClientDataChannelFactory(
         credentials)
     self._state_handler_factory = GrpcStateHandlerFactory()
+    self._profiler_factory = profiler_factory
     self.workers = queue.Queue()
     # one thread is enough for getting the progress report.
     # Assumption:
     # Progress report generation should not do IO or wait on other resources.
     #  Without wait, having multiple threads will not improve performance and
     #  will only add complexity.
     self._progress_thread_pool = futures.ThreadPoolExecutor(max_workers=1)
@@ -92,15 +95,16 @@
       # same function is reused by different process bundle calls and
       # potentially get executed by different worker. Hence we need a
       # centralized function list shared among all the workers.
       self.workers.put(
           SdkWorker(
               state_handler_factory=self._state_handler_factory,
               data_channel_factory=self._data_channel_factory,
-              fns=self._fns))
+              fns=self._fns,
+              profiler_factory=self._profiler_factory))
 
     def get_responses():
       while True:
         response = self._responses.get()
         if response is no_more_work:
           return
         yield response
@@ -170,14 +174,16 @@
         # Put the worker back in the free worker pool
         self.workers.put(worker)
 
     # Create a task for each process_bundle request and schedule it
     self._process_bundle_queue.put(request)
     self._unscheduled_process_bundle.add(request.instruction_id)
     self._process_thread_pool.submit(task)
+    logging.debug(
+        "Currently using %s threads." % len(self._process_thread_pool._threads))
 
   def _request_process_bundle_progress(self, request):
 
     def task():
       instruction_reference = getattr(
           request, request.WhichOneof('request')).instruction_reference
       if instruction_reference in self._instruction_id_vs_worker:
@@ -194,19 +200,22 @@
                     instruction_reference)), request)
 
     self._progress_thread_pool.submit(task)
 
 
 class SdkWorker(object):
 
-  def __init__(self, state_handler_factory, data_channel_factory, fns):
+  def __init__(self, state_handler_factory, data_channel_factory, fns,
+               profiler_factory=None):
     self.fns = fns
     self.state_handler_factory = state_handler_factory
     self.data_channel_factory = data_channel_factory
-    self.bundle_processors = {}
+    self.active_bundle_processors = {}
+    self.cached_bundle_processors = collections.defaultdict(list)
+    self.profiler_factory = profiler_factory
 
   def do_instruction(self, request):
     request_type = request.WhichOneof('request')
     if request_type:
       # E.g. if register is set, this will call self.register(request.register))
       return getattr(self, request_type)(getattr(request, request_type),
                                          request.instruction_id)
@@ -217,40 +226,69 @@
     for process_bundle_descriptor in request.process_bundle_descriptor:
       self.fns[process_bundle_descriptor.id] = process_bundle_descriptor
     return beam_fn_api_pb2.InstructionResponse(
         instruction_id=instruction_id,
         register=beam_fn_api_pb2.RegisterResponse())
 
   def process_bundle(self, request, instruction_id):
-    process_bundle_desc = self.fns[request.process_bundle_descriptor_reference]
-    state_handler = self.state_handler_factory.create_state_handler(
-        process_bundle_desc.state_api_service_descriptor)
-    self.bundle_processors[
-        instruction_id] = processor = bundle_processor.BundleProcessor(
-            process_bundle_desc,
-            state_handler,
-            self.data_channel_factory)
+    with self.get_bundle_processor(
+        instruction_id,
+        request.process_bundle_descriptor_reference) as bundle_processor:
+      with self.maybe_profile(instruction_id):
+        bundle_processor.process_bundle(instruction_id)
+      return beam_fn_api_pb2.InstructionResponse(
+          instruction_id=instruction_id,
+          process_bundle=beam_fn_api_pb2.ProcessBundleResponse(
+              metrics=bundle_processor.metrics(),
+              monitoring_infos=bundle_processor.monitoring_infos()))
+
+  @contextlib.contextmanager
+  def get_bundle_processor(self, instruction_id, bundle_descriptor_id):
+    try:
+      # pop() is threadsafe
+      processor = self.cached_bundle_processors[bundle_descriptor_id].pop()
+      state_handler = processor.state_handler
+    except IndexError:
+      process_bundle_desc = self.fns[bundle_descriptor_id]
+      state_handler = self.state_handler_factory.create_state_handler(
+          process_bundle_desc.state_api_service_descriptor)
+      processor = bundle_processor.BundleProcessor(
+          process_bundle_desc,
+          state_handler,
+          self.data_channel_factory)
     try:
+      self.active_bundle_processors[instruction_id] = processor
       with state_handler.process_instruction_id(instruction_id):
-        processor.process_bundle(instruction_id)
+        yield processor
     finally:
-      del self.bundle_processors[instruction_id]
-
-    return beam_fn_api_pb2.InstructionResponse(
-        instruction_id=instruction_id,
-        process_bundle=beam_fn_api_pb2.ProcessBundleResponse(
-            metrics=processor.metrics()))
+      del self.active_bundle_processors[instruction_id]
+    # Outside the finally block as we only want to re-use on success.
+    processor.reset()
+    self.cached_bundle_processors[bundle_descriptor_id].append(processor)
 
   def process_bundle_progress(self, request, instruction_id):
     # It is an error to get progress for a not-in-flight bundle.
-    processor = self.bundle_processors.get(request.instruction_reference)
+    processor = self.active_bundle_processors.get(request.instruction_reference)
     return beam_fn_api_pb2.InstructionResponse(
         instruction_id=instruction_id,
         process_bundle_progress=beam_fn_api_pb2.ProcessBundleProgressResponse(
-            metrics=processor.metrics() if processor else None))
+            metrics=processor.metrics() if processor else None,
+            monitoring_infos=processor.monitoring_infos() if processor else []))
+
+  @contextlib.contextmanager
+  def maybe_profile(self, instruction_id):
+    if self.profiler_factory:
+      profiler = self.profiler_factory(instruction_id)
+      if profiler:
+        with profiler:
+          yield
+      else:
+        yield
+    else:
+      yield
 
 
 class StateHandlerFactory(with_metaclass(abc.ABCMeta, object)):
   """An abstract factory for creating ``DataChannel``."""
 
   @abc.abstractmethod
   def create_state_handler(self, api_service_descriptor):
```

## Comparing `apache-beam-2.8.0/apache_beam/runners/worker/sideinputs.py` & `apache-beam-2.9.0/apache_beam/runners/worker/sideinputs.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/runners/worker/opcounters_test.py` & `apache-beam-2.9.0/apache_beam/runners/worker/opcounters_test.py`

 * *Files 5% similar despite different names*

```diff
@@ -105,64 +105,71 @@
         self.assertEqual(expected_size, opcounts.mean_byte_counter.value())
 
   def test_update_int(self):
     opcounts = OperationCounters(CounterFactory(), 'some-name',
                                  coders.PickleCoder(), 0)
     self.verify_counters(opcounts, 0)
     opcounts.update_from(GlobalWindows.windowed_value(1))
+    opcounts.update_collect()
     self.verify_counters(opcounts, 1)
 
   def test_update_str(self):
     coder = coders.PickleCoder()
     opcounts = OperationCounters(CounterFactory(), 'some-name',
                                  coder, 0)
     self.verify_counters(opcounts, 0, float('nan'))
     value = GlobalWindows.windowed_value('abcde')
     opcounts.update_from(value)
+    opcounts.update_collect()
     estimated_size = coder.estimate_size(value)
     self.verify_counters(opcounts, 1, estimated_size)
 
   def test_update_old_object(self):
     coder = coders.PickleCoder()
     opcounts = OperationCounters(CounterFactory(), 'some-name',
                                  coder, 0)
     self.verify_counters(opcounts, 0, float('nan'))
     obj = OldClassThatDoesNotImplementLen()
     value = GlobalWindows.windowed_value(obj)
     opcounts.update_from(value)
+    opcounts.update_collect()
     estimated_size = coder.estimate_size(value)
     self.verify_counters(opcounts, 1, estimated_size)
 
   def test_update_new_object(self):
     coder = coders.PickleCoder()
     opcounts = OperationCounters(CounterFactory(), 'some-name',
                                  coder, 0)
     self.verify_counters(opcounts, 0, float('nan'))
 
     obj = ObjectThatDoesNotImplementLen()
     value = GlobalWindows.windowed_value(obj)
     opcounts.update_from(value)
+    opcounts.update_collect()
     estimated_size = coder.estimate_size(value)
     self.verify_counters(opcounts, 1, estimated_size)
 
   def test_update_multiple(self):
     coder = coders.PickleCoder()
     total_size = 0
     opcounts = OperationCounters(CounterFactory(), 'some-name',
                                  coder, 0)
     self.verify_counters(opcounts, 0, float('nan'))
     value = GlobalWindows.windowed_value('abcde')
     opcounts.update_from(value)
+    opcounts.update_collect()
     total_size += coder.estimate_size(value)
     value = GlobalWindows.windowed_value('defghij')
     opcounts.update_from(value)
+    opcounts.update_collect()
     total_size += coder.estimate_size(value)
     self.verify_counters(opcounts, 2, (float(total_size) / 2))
     value = GlobalWindows.windowed_value('klmnop')
     opcounts.update_from(value)
+    opcounts.update_collect()
     total_size += coder.estimate_size(value)
     self.verify_counters(opcounts, 3, (float(total_size) / 3))
 
   @unittest.skipIf(sys.version_info[0] == 3 and
                    os.environ.get('RUN_SKIPPED_PY3_TESTS') != '1',
                    'This test still needs to be fixed on Python 3.')
   def test_should_sample(self):
```

## Comparing `apache-beam-2.8.0/apache_beam/runners/worker/operation_specs.py` & `apache-beam-2.9.0/apache_beam/runners/worker/operation_specs.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/runners/worker/opcounters.py` & `apache-beam-2.9.0/apache_beam/runners/worker/opcounters.py`

 * *Files 1% similar despite different names*

```diff
@@ -181,20 +181,20 @@
     self._counter_factory = counter_factory
     self.element_counter = counter_factory.get_counter(
         '%s-out%s-ElementCount' % (step_name, output_index), Counter.SUM)
     self.mean_byte_counter = counter_factory.get_counter(
         '%s-out%s-MeanByteCount' % (step_name, output_index), Counter.MEAN)
     self.coder_impl = coder.get_impl() if coder else None
     self.active_accumulator = None
+    self.current_size = None
     self._sample_counter = 0
     self._next_sample = 0
 
   def update_from(self, windowed_value):
     """Add one value to this counter."""
-    self.element_counter.update(1)
     if self._should_sample():
       self.do_sample(windowed_value)
 
   def _observable_callback(self, inner_coder_impl, accumulator):
     def _observable_callback_inner(value, is_encoded=False):
       # TODO(ccy): If this stream is large, sample it as well.
       # To do this, we'll need to compute the average size of elements
@@ -208,30 +208,34 @@
         accumulator.update(inner_coder_impl.estimate_size(value))
     return _observable_callback_inner
 
   def do_sample(self, windowed_value):
     size, observables = (
         self.coder_impl.get_estimated_size_and_observables(windowed_value))
     if not observables:
-      self.mean_byte_counter.update(size)
+      self.current_size = size
     else:
       self.active_accumulator = SumAccumulator()
       self.active_accumulator.update(size)
       for observable, inner_coder_impl in observables:
         observable.register_observer(
             self._observable_callback(
                 inner_coder_impl, self.active_accumulator))
 
   def update_collect(self):
     """Collects the accumulated size estimates.
 
     Now that the element has been processed, we ask our accumulator
     for the total and store the result in a counter.
     """
-    if self.active_accumulator is not None:
+    self.element_counter.update(1)
+    if self.current_size is not None:
+      self.mean_byte_counter.update(self.current_size)
+      self.current_size = None
+    elif self.active_accumulator is not None:
       self.mean_byte_counter.update(self.active_accumulator.value())
       self.active_accumulator = None
 
   def _compute_next_sample(self, i):
     # https://en.wikipedia.org/wiki/Reservoir_sampling#Fast_Approximation
     gap = math.log(1.0 - random.random()) / math.log(1.0 - (10.0 / i))
     return i + math.floor(gap)
```

## Comparing `apache-beam-2.8.0/apache_beam/runners/worker/worker_id_interceptor_test.py` & `apache-beam-2.9.0/apache_beam/runners/worker/worker_id_interceptor_test.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/runners/worker/data_plane_test.py` & `apache-beam-2.9.0/apache_beam/runners/worker/data_plane_test.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/runners/worker/opcounters.pxd` & `apache-beam-2.9.0/apache_beam/runners/worker/opcounters.pxd`

 * *Files 2% similar despite different names*

```diff
@@ -52,14 +52,15 @@
 
 cdef class OperationCounters(object):
   cdef public _counter_factory
   cdef public Counter element_counter
   cdef public Counter mean_byte_counter
   cdef public coder_impl
   cdef public SumAccumulator active_accumulator
+  cdef public object current_size
   cdef public libc.stdint.int64_t _sample_counter
   cdef public libc.stdint.int64_t _next_sample
 
   cpdef update_from(self, windowed_value)
   cdef inline do_sample(self, windowed_value)
   cpdef update_collect(self)
```

## Comparing `apache-beam-2.8.0/apache_beam/runners/worker/data_plane.py` & `apache-beam-2.9.0/apache_beam/runners/worker/data_plane.py`

 * *Files 7% similar despite different names*

```diff
@@ -133,18 +133,22 @@
     self._inputs = []
     self._inverse = inverse or InMemoryDataChannel(self)
 
   def inverse(self):
     return self._inverse
 
   def input_elements(self, instruction_id, unused_expected_targets=None):
+    other_inputs = []
     for data in self._inputs:
       if data.instruction_reference == instruction_id:
         if data.data:
           yield data
+      else:
+        other_inputs.append(data)
+    self._inputs = other_inputs
 
   def output_stream(self, instruction_id, target):
     def add_to_inverse_output(data):
       self._inverse._inputs.append(  # pylint: disable=protected-access
           beam_fn_api_pb2.Elements.Data(
               instruction_reference=instruction_id,
               target=target,
```

## Comparing `apache-beam-2.8.0/apache_beam/runners/worker/worker_id_interceptor.py` & `apache-beam-2.9.0/apache_beam/runners/worker/worker_id_interceptor.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/runners/worker/logger_test.py` & `apache-beam-2.9.0/apache_beam/runners/worker/logger_test.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/runners/worker/log_handler_test.py` & `apache-beam-2.9.0/apache_beam/runners/worker/log_handler_test.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/runners/worker/bundle_processor.py` & `apache-beam-2.9.0/apache_beam/runners/worker/bundle_processor.py`

 * *Files 2% similar despite different names*

```diff
@@ -33,14 +33,15 @@
 from future.utils import itervalues
 
 import apache_beam as beam
 from apache_beam.coders import WindowedValueCoder
 from apache_beam.coders import coder_impl
 from apache_beam.internal import pickler
 from apache_beam.io import iobase
+from apache_beam.metrics import monitoring_infos
 from apache_beam.portability import common_urns
 from apache_beam.portability import python_urns
 from apache_beam.portability.api import beam_fn_api_pb2
 from apache_beam.portability.api import beam_runner_api_pb2
 from apache_beam.runners import pipeline_context
 from apache_beam.runners.dataflow import dataflow_runner
 from apache_beam.runners.worker import operation_specs
@@ -146,15 +147,14 @@
     self._state_handler = state_handler
     self._transform_id = transform_id
     self._tag = tag
     self._side_input_data = side_input_data
     self._element_coder = coder.wrapped_value_coder
     self._target_window_coder = coder.window_coder
     # TODO(robertwb): Limit the cache size.
-    # TODO(robertwb): Cross-bundle caching respecting cache tokens.
     self._cache = {}
 
   def __getitem__(self, window):
     target_window = self._side_input_data.window_mapping_fn(window)
     if target_window not in self._cache:
       state_key = beam_fn_api_pb2.StateKey(
           multimap_side_input=beam_fn_api_pb2.StateKey.MultimapSideInput(
@@ -200,14 +200,18 @@
       self._cache[target_window] = self._side_input_data.view_fn(raw_view)
     return self._cache[target_window]
 
   def is_globally_windowed(self):
     return (self._side_input_data.window_mapping_fn
             == sideinputs._global_window_mapping_fn)
 
+  def reset(self):
+    # TODO(BEAM-5428): Cross-bundle caching respecting cache tokens.
+    self._cache = {}
+
 
 class CombiningValueRuntimeState(userstate.RuntimeState):
   def __init__(self, underlying_bag_state, combinefn):
     self._combinefn = combinefn
     self._underlying_bag_state = underlying_bag_state
 
   def _read_accumulator(self, rewrite=True):
@@ -305,14 +309,18 @@
       if isinstance(state_spec, userstate.BagStateSpec):
         return bag_state
       else:
         return CombiningValueRuntimeState(bag_state, state_spec.combine_fn)
     else:
       raise NotImplementedError(state_spec)
 
+  def reset(self):
+    # TODO(BEAM-5428): Implement cross-bundle state caching.
+    pass
+
 
 def memoize(func):
   cache = {}
   missing = object()
 
   def wrapper(*args):
     result = cache.get(args, missing)
@@ -337,14 +345,16 @@
     # TODO(robertwb): Figure out the correct prefix to use for output counters
     # from StateSampler.
     self.counter_factory = counters.CounterFactory()
     self.state_sampler = statesampler.StateSampler(
         'fnapi-step-%s' % self.process_bundle_descriptor.id,
         self.counter_factory)
     self.ops = self.create_execution_tree(self.process_bundle_descriptor)
+    for op in self.ops.values():
+      op.setup()
 
   def create_execution_tree(self, descriptor):
 
     transform_factory = BeamTransformFactory(
         descriptor, self.data_channel_factory, self.counter_factory,
         self.state_sampler, self.state_handler)
 
@@ -380,14 +390,21 @@
            for consumer in pcoll_consumers[pcoll]])
 
     return collections.OrderedDict([
         (transform_id, get_operation(transform_id))
         for transform_id in sorted(
             descriptor.transforms, key=topological_height, reverse=True)])
 
+  def reset(self):
+    self.counter_factory.reset()
+    self.state_sampler.reset()
+    # Side input caches.
+    for op in self.ops.values():
+      op.reset()
+
   def process_bundle(self, instruction_id):
     expected_inputs = []
     for op in self.ops.values():
       if isinstance(op, DataOutputOperation):
         # TODO(robertwb): Is there a better way to pass the instruction id to
         # the operation?
         op.set_output_stream(op.data_channel.output_stream(
@@ -422,41 +439,63 @@
       for op in self.ops.values():
         logging.debug('finish %s', op)
         op.finish()
     finally:
       self.state_sampler.stop_if_still_running()
 
   def metrics(self):
+    # DEPRECATED
     return beam_fn_api_pb2.Metrics(
         # TODO(robertwb): Rename to progress?
         ptransforms={
             transform_id:
             self._fix_output_tags(transform_id, op.progress_metrics())
             for transform_id, op in self.ops.items()})
 
   def _fix_output_tags(self, transform_id, metrics):
+    # DEPRECATED
+    actual_output_tags = list(
+        self.process_bundle_descriptor.transforms[transform_id].outputs.keys())
     # Outputs are still referred to by index, not by name, in many Operations.
     # However, if there is exactly one output, we can fix up the name here.
+
     def fix_only_output_tag(actual_output_tag, mapping):
       if len(mapping) == 1:
         fake_output_tag, count = only_element(list(mapping.items()))
         if fake_output_tag != actual_output_tag:
           del mapping[fake_output_tag]
           mapping[actual_output_tag] = count
-    actual_output_tags = list(
-        self.process_bundle_descriptor.transforms[transform_id].outputs.keys())
     if len(actual_output_tags) == 1:
       fix_only_output_tag(
           actual_output_tags[0],
           metrics.processed_elements.measured.output_element_counts)
       fix_only_output_tag(
           actual_output_tags[0],
           metrics.active_elements.measured.output_element_counts)
     return metrics
 
+  def monitoring_infos(self):
+    """Returns the list of MonitoringInfos collected processing this bundle."""
+    # Construct a new dict first to remove duplciates.
+    all_monitoring_infos_dict = {}
+    for transform_id, op in self.ops.items():
+      for mi in op.monitoring_infos(transform_id).values():
+        fixed_mi = self._fix_output_tags_monitoring_info(transform_id, mi)
+        all_monitoring_infos_dict[monitoring_infos.to_key(fixed_mi)] = fixed_mi
+    return list(all_monitoring_infos_dict.values())
+
+  def _fix_output_tags_monitoring_info(self, transform_id, monitoring_info):
+    actual_output_tags = list(
+        self.process_bundle_descriptor.transforms[transform_id].outputs.keys())
+    if ('TAG' in monitoring_info.labels and
+        monitoring_info.labels['TAG'] == 'ONLY_OUTPUT'):
+      if len(actual_output_tags) == 1:
+        monitoring_info.labels['TAG'] = actual_output_tags[0]
+    return monitoring_info
+
 
 class BeamTransformFactory(object):
   """Factory for turning transform_protos into executable operations."""
   def __init__(self, descriptor, data_channel_factory, counter_factory,
                state_sampler, state_handler):
     self.descriptor = descriptor
     self.data_channel_factory = data_channel_factory
@@ -472,14 +511,17 @@
     def wrapper(func):
       cls._known_urns[urn] = func, parameter_type
       return func
     return wrapper
 
   def create_operation(self, transform_id, consumers):
     transform_proto = self.descriptor.transforms[transform_id]
+    if not transform_proto.unique_name:
+      logging.warn("No unique name set for transform %s" % transform_id)
+      transform_proto.unique_name = transform_id
     creator, parameter_type = self._known_urns[transform_proto.spec.urn]
     payload = proto_utils.parse_Bytes(
         transform_proto.spec.payload, parameter_type)
     return creator(self, transform_id, transform_proto, payload, consumers)
 
   def get_coder(self, coder_id):
     if coder_id not in self.descriptor.coders:
@@ -555,42 +597,54 @@
       if pcoll_id == output_pcoll:
         output_consumers[:] = [TimerConsumer(tag, do_op)]
         break
 
   target = beam_fn_api_pb2.Target(
       primitive_transform_reference=transform_id,
       name=only_element(list(transform_proto.outputs.keys())))
+  if grpc_port.coder_id:
+    output_coder = factory.get_coder(grpc_port.coder_id)
+  else:
+    logging.error(
+        'Missing required coder_id on grpc_port for %s; '
+        'using deprecated fallback.',
+        transform_id)
+    output_coder = factory.get_only_output_coder(transform_proto)
   return DataInputOperation(
       transform_proto.unique_name,
       transform_proto.unique_name,
       consumers,
       factory.counter_factory,
       factory.state_sampler,
-      factory.get_coder(grpc_port.coder_id)
-      if grpc_port.coder_id
-      else factory.get_only_output_coder(transform_proto),
+      output_coder,
       input_target=target,
       data_channel=factory.data_channel_factory.create_data_channel(grpc_port))
 
 
 @BeamTransformFactory.register_urn(
     DATA_OUTPUT_URN, beam_fn_api_pb2.RemoteGrpcPort)
 def create(factory, transform_id, transform_proto, grpc_port, consumers):
   target = beam_fn_api_pb2.Target(
       primitive_transform_reference=transform_id,
       name=only_element(list(transform_proto.inputs.keys())))
+  if grpc_port.coder_id:
+    output_coder = factory.get_coder(grpc_port.coder_id)
+  else:
+    logging.error(
+        'Missing required coder_id on grpc_port for %s; '
+        'using deprecated fallback.',
+        transform_id)
+    output_coder = factory.get_only_input_coder(transform_proto)
   return DataOutputOperation(
       transform_proto.unique_name,
       transform_proto.unique_name,
       consumers,
       factory.counter_factory,
       factory.state_sampler,
-      factory.get_coder(grpc_port.coder_id)
-      if grpc_port.coder_id
-      else factory.get_only_input_coder(transform_proto),
+      output_coder,
       target=target,
       data_channel=factory.data_channel_factory.create_data_channel(grpc_port))
 
 
 @BeamTransformFactory.register_urn(OLD_DATAFLOW_RUNNER_HARNESS_READ_URN, None)
 def create(factory, transform_id, transform_proto, parameter, consumers):
   # The Dataflow runner harness strips the base64 encoding.
```

## Comparing `apache-beam-2.8.0/apache_beam/runners/worker/statesampler_test.py` & `apache-beam-2.9.0/apache_beam/runners/worker/statesampler_test.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/runners/worker/operations.pxd` & `apache-beam-2.9.0/apache_beam/runners/worker/operations.pxd`

 * *Files 10% similar despite different names*

```diff
@@ -16,15 +16,15 @@
 #
 
 cimport cython
 
 from apache_beam.runners.common cimport Receiver
 from apache_beam.runners.worker cimport opcounters
 from apache_beam.utils.windowed_value cimport WindowedValue
-
+#from libcpp.string cimport string
 
 cdef WindowedValue _globally_windowed_value
 cdef type _global_window_type
 
 
 cdef class ConsumerSet(Receiver):
   cdef list consumers
@@ -45,28 +45,33 @@
   cdef object consumers
   cdef readonly counter_factory
   cdef public metrics_container
   # Public for access by Fn harness operations.
   # TODO(robertwb): Cythonize FnHarness.
   cdef public list receivers
   cdef readonly bint debug_logging_enabled
+  # For legacy workers.
+  cdef bint setup_done
 
   cdef public step_name  # initialized lazily
 
   cdef readonly object state_sampler
 
   cdef readonly object scoped_start_state
   cdef readonly object scoped_process_state
   cdef readonly object scoped_finish_state
 
   cpdef start(self)
   cpdef process(self, WindowedValue windowed_value)
   cpdef finish(self)
   cpdef output(self, WindowedValue windowed_value, int output_index=*)
-  cpdef progress_metrics(self)
+  cpdef execution_time_monitoring_infos(self, transform_id)
+  cpdef user_monitoring_infos(self, transform_id)
+  cpdef element_count_monitoring_infos(self, transform_id)
+  cpdef monitoring_infos(self, transform_id)
 
 
 cdef class ReadOperation(Operation):
   @cython.locals(windowed_value=WindowedValue)
   cpdef start(self)
```

## Comparing `apache-beam-2.8.0/apache_beam/runners/worker/logger.py` & `apache-beam-2.9.0/apache_beam/runners/worker/logger.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/runners/worker/sideinputs_test.py` & `apache-beam-2.9.0/apache_beam/runners/worker/sideinputs_test.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/examples/wordcount_debugging.py` & `apache-beam-2.9.0/apache_beam/examples/wordcount_debugging.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/examples/wordcount_minimal_test.py` & `apache-beam-2.9.0/apache_beam/examples/wordcount_minimal_test.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/examples/streaming_wordcount_debugging.py` & `apache-beam-2.9.0/apache_beam/examples/streaming_wordcount_debugging.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/examples/__init__.py` & `apache-beam-2.9.0/apache_beam/io/gcp/tests/__init__.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/examples/wordcount_test.py` & `apache-beam-2.9.0/apache_beam/examples/wordcount_test.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/examples/wordcount_it_test.py` & `apache-beam-2.9.0/apache_beam/examples/wordcount_it_test.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/examples/fastavro_it_test.py` & `apache-beam-2.9.0/apache_beam/examples/fastavro_it_test.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/examples/windowed_wordcount.py` & `apache-beam-2.9.0/apache_beam/examples/windowed_wordcount.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/examples/wordcount.py` & `apache-beam-2.9.0/apache_beam/examples/wordcount.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/examples/streaming_wordcount_it_test.py` & `apache-beam-2.9.0/apache_beam/examples/streaming_wordcount_it_test.py`

 * *Files 11% similar despite different names*

```diff
@@ -48,59 +48,58 @@
   def setUp(self):
     self.test_pipeline = TestPipeline(is_integration_test=True)
     self.project = self.test_pipeline.get_option('project')
     self.uuid = str(uuid.uuid4())
 
     # Set up PubSub environment.
     from google.cloud import pubsub
-    self.pubsub_client = pubsub.Client(project=self.project)
-    self.input_topic = self.pubsub_client.topic(INPUT_TOPIC + self.uuid)
-    self.output_topic = self.pubsub_client.topic(OUTPUT_TOPIC + self.uuid)
-    self.input_sub = self.input_topic.subscription(INPUT_SUB + self.uuid)
-    self.output_sub = self.output_topic.subscription(OUTPUT_SUB + self.uuid)
-
-    self.input_topic.create()
-    self.output_topic.create()
-    test_utils.wait_for_topics_created([self.input_topic, self.output_topic])
-    self.input_sub.create()
-    self.output_sub.create()
+    self.pub_client = pubsub.PublisherClient()
+    self.input_topic = self.pub_client.create_topic(
+        self.pub_client.topic_path(self.project, INPUT_TOPIC + self.uuid))
+    self.output_topic = self.pub_client.create_topic(
+        self.pub_client.topic_path(self.project, OUTPUT_TOPIC + self.uuid))
+
+    self.sub_client = pubsub.SubscriberClient()
+    self.input_sub = self.sub_client.create_subscription(
+        self.sub_client.subscription_path(self.project, INPUT_SUB + self.uuid),
+        self.input_topic.name)
+    self.output_sub = self.sub_client.create_subscription(
+        self.sub_client.subscription_path(self.project, OUTPUT_SUB + self.uuid),
+        self.output_topic.name)
 
   def _inject_numbers(self, topic, num_messages):
     """Inject numbers as test data to PubSub."""
-    logging.debug('Injecting %d numbers to topic %s',
-                  num_messages, topic.full_name)
+    logging.debug('Injecting %d numbers to topic %s', num_messages, topic.name)
     for n in range(num_messages):
-      topic.publish(str(n))
-
-  def _cleanup_pubsub(self):
-    test_utils.cleanup_subscriptions([self.input_sub, self.output_sub])
-    test_utils.cleanup_topics([self.input_topic, self.output_topic])
+      self.pub_client.publish(self.input_topic.name, str(n))
 
   def tearDown(self):
-    self._cleanup_pubsub()
+    test_utils.cleanup_subscriptions(self.sub_client,
+                                     [self.input_sub, self.output_sub])
+    test_utils.cleanup_topics(self.pub_client,
+                              [self.input_topic, self.output_topic])
 
   @attr('IT')
   def test_streaming_wordcount_it(self):
     # Build expected dataset.
     expected_msg = [('%d: 1' % num) for num in range(DEFAULT_INPUT_NUMBERS)]
 
     # Set extra options to the pipeline for test purpose
     state_verifier = PipelineStateMatcher(PipelineState.RUNNING)
     pubsub_msg_verifier = PubSubMessageMatcher(self.project,
-                                               OUTPUT_SUB + self.uuid,
+                                               self.output_sub.name,
                                                expected_msg,
                                                timeout=400)
-    extra_opts = {'input_subscription': self.input_sub.full_name,
-                  'output_topic': self.output_topic.full_name,
+    extra_opts = {'input_subscription': self.input_sub.name,
+                  'output_topic': self.output_topic.name,
                   'wait_until_finish_duration': WAIT_UNTIL_FINISH_DURATION,
                   'on_success_matcher': all_of(state_verifier,
                                                pubsub_msg_verifier)}
 
     # Generate input data and inject to PubSub.
-    test_utils.wait_for_subscriptions_created([self.input_sub])
     self._inject_numbers(self.input_topic, DEFAULT_INPUT_NUMBERS)
 
     # Get pipeline options from command argument: --test-pipeline-options,
     # and start pipeline job by calling pipeline main function.
     streaming_wordcount.run(
         self.test_pipeline.get_full_options_as_args(**extra_opts))
```

## Comparing `apache-beam-2.8.0/apache_beam/examples/wordcount_debugging_test.py` & `apache-beam-2.9.0/apache_beam/examples/wordcount_debugging_test.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/examples/wordcount_minimal.py` & `apache-beam-2.9.0/apache_beam/examples/wordcount_minimal.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/examples/avro_bitcoin.py` & `apache-beam-2.9.0/apache_beam/examples/avro_bitcoin.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/examples/streaming_wordcount.py` & `apache-beam-2.9.0/apache_beam/examples/streaming_wordcount.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/examples/snippets/snippets_test.py` & `apache-beam-2.9.0/apache_beam/examples/snippets/snippets_test.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/examples/snippets/__init__.py` & `apache-beam-2.9.0/apache_beam/io/flink/__init__.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/examples/snippets/snippets.py` & `apache-beam-2.9.0/apache_beam/examples/snippets/snippets.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/examples/complete/estimate_pi_test.py` & `apache-beam-2.9.0/apache_beam/examples/complete/estimate_pi_test.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/examples/complete/autocomplete.py` & `apache-beam-2.9.0/apache_beam/examples/complete/autocomplete.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/examples/complete/distribopt.py` & `apache-beam-2.9.0/apache_beam/examples/complete/distribopt.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/examples/complete/tfidf_test.py` & `apache-beam-2.9.0/apache_beam/examples/complete/tfidf_test.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/examples/complete/__init__.py` & `apache-beam-2.9.0/apache_beam/examples/__init__.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/examples/complete/distribopt_test.py` & `apache-beam-2.9.0/apache_beam/examples/complete/distribopt_test.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/examples/complete/estimate_pi.py` & `apache-beam-2.9.0/apache_beam/examples/complete/estimate_pi.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/examples/complete/top_wikipedia_sessions.py` & `apache-beam-2.9.0/apache_beam/examples/complete/top_wikipedia_sessions.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/examples/complete/top_wikipedia_sessions_test.py` & `apache-beam-2.9.0/apache_beam/examples/complete/top_wikipedia_sessions_test.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/examples/complete/tfidf.py` & `apache-beam-2.9.0/apache_beam/examples/complete/tfidf.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/examples/complete/autocomplete_test.py` & `apache-beam-2.9.0/apache_beam/examples/complete/autocomplete_test.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/examples/complete/juliaset/__init__.py` & `apache-beam-2.9.0/apache_beam/examples/cookbook/__init__.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/examples/complete/juliaset/setup.py` & `apache-beam-2.9.0/apache_beam/examples/complete/juliaset/setup.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/examples/complete/juliaset/juliaset_main.py` & `apache-beam-2.9.0/apache_beam/examples/complete/juliaset/juliaset_main.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/examples/complete/juliaset/juliaset/__init__.py` & `apache-beam-2.9.0/apache_beam/examples/complete/__init__.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/examples/complete/juliaset/juliaset/juliaset_test.py` & `apache-beam-2.9.0/apache_beam/examples/complete/juliaset/juliaset/juliaset_test.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/examples/complete/juliaset/juliaset/juliaset.py` & `apache-beam-2.9.0/apache_beam/examples/complete/juliaset/juliaset/juliaset.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/examples/complete/game/game_stats_it_test.py` & `apache-beam-2.9.0/apache_beam/examples/complete/game/leader_board_it_test.py`

 * *Files 10% similar despite different names*

```diff
@@ -11,17 +11,17 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 #
 
-"""End-to-end test for the game stats example.
+"""End-to-end test for the leader board example.
 
-Code: beam/sdks/python/apache_beam/examples/complete/game/game_stats.py
+Code: beam/sdks/python/apache_beam/examples/complete/game/leader_board.py
 Usage:
 
   python setup.py nosetests --test-pipeline-options=" \
       --runner=TestDataflowRunner \
       --project=... \
       --staging_location=gs://... \
       --temp_location=gs://... \
@@ -32,127 +32,125 @@
 
 from __future__ import absolute_import
 
 import logging
 import time
 import unittest
 import uuid
+from builtins import range
 
 from hamcrest.core.core.allof import all_of
 from nose.plugins.attrib import attr
 
-from apache_beam.examples.complete.game import game_stats
+from apache_beam.examples.complete.game import leader_board
 from apache_beam.io.gcp.tests import utils
 from apache_beam.io.gcp.tests.bigquery_matcher import BigqueryMatcher
 from apache_beam.runners.runner import PipelineState
 from apache_beam.testing import test_utils
 from apache_beam.testing.pipeline_verifiers import PipelineStateMatcher
 from apache_beam.testing.test_pipeline import TestPipeline
 
 
-class GameStatsIT(unittest.TestCase):
+class LeaderBoardIT(unittest.TestCase):
 
-  # Input events containing user, team, score, processing time, window start.
+  # Input event containing user, team, score, processing time, window start.
   INPUT_EVENT = 'user1,teamA,10,%d,2015-11-02 09:09:28.224'
-  INPUT_TOPIC = 'game_stats_it_input_topic'
-  INPUT_SUB = 'game_stats_it_input_subscription'
+  INPUT_TOPIC = 'leader_board_it_input_topic'
+  INPUT_SUB = 'leader_board_it_input_subscription'
 
   # SHA-1 hash generated from sorted rows reading from BigQuery table
-  DEFAULT_EXPECTED_CHECKSUM = '5288ccaab77d347c8460d77c15a0db234ef5eb4f'
-  OUTPUT_DATASET = 'game_stats_it_dataset'
-  OUTPUT_TABLE_SESSIONS = 'game_stats_sessions'
-  OUTPUT_TABLE_TEAMS = 'game_stats_teams'
+  DEFAULT_EXPECTED_CHECKSUM = 'de00231fe6730b972c0ff60a99988438911cda53'
+  OUTPUT_DATASET = 'leader_board_it_dataset'
+  OUTPUT_TABLE_USERS = 'leader_board_users'
+  OUTPUT_TABLE_TEAMS = 'leader_board_teams'
   DEFAULT_INPUT_COUNT = 500
 
-  WAIT_UNTIL_FINISH_DURATION = 12 * 60 * 1000   # in milliseconds
+  WAIT_UNTIL_FINISH_DURATION = 10 * 60 * 1000   # in milliseconds
 
   def setUp(self):
     self.test_pipeline = TestPipeline(is_integration_test=True)
     self.project = self.test_pipeline.get_option('project')
     _unique_id = str(uuid.uuid4())
 
     # Set up PubSub environment.
     from google.cloud import pubsub
-    self.pubsub_client = pubsub.Client(project=self.project)
-    unique_topic_name = self.INPUT_TOPIC + _unique_id
-    unique_subscrition_name = self.INPUT_SUB + _unique_id
-    self.input_topic = self.pubsub_client.topic(unique_topic_name)
-    self.input_sub = self.input_topic.subscription(unique_subscrition_name)
-
-    self.input_topic.create()
-    test_utils.wait_for_topics_created([self.input_topic])
-    self.input_sub.create()
+
+    self.pub_client = pubsub.PublisherClient()
+    self.input_topic = self.pub_client.create_topic(
+        self.pub_client.topic_path(self.project, self.INPUT_TOPIC + _unique_id))
+
+    self.sub_client = pubsub.SubscriberClient()
+    self.input_sub = self.sub_client.create_subscription(
+        self.sub_client.subscription_path(self.project,
+                                          self.INPUT_SUB + _unique_id),
+        self.input_topic.name)
 
     # Set up BigQuery environment
-    from google.cloud import bigquery
-    client = bigquery.Client()
-    unique_dataset_name = self.OUTPUT_DATASET + str(int(time.time()))
-    self.dataset = client.dataset(unique_dataset_name, project=self.project)
-    self.dataset.create()
+    self.dataset_ref = utils.create_bq_dataset(self.project,
+                                               self.OUTPUT_DATASET)
 
     self._test_timestamp = int(time.time() * 1000)
 
   def _inject_pubsub_game_events(self, topic, message_count):
     """Inject game events as test data to PubSub."""
 
     logging.debug('Injecting %d game events to topic %s',
-                  message_count, topic.full_name)
+                  message_count, topic.name)
 
     for _ in range(message_count):
-      topic.publish(self.INPUT_EVENT % self._test_timestamp)
+      self.pub_client.publish(topic.name,
+                              self.INPUT_EVENT % self._test_timestamp)
 
   def _cleanup_pubsub(self):
-    test_utils.cleanup_subscriptions([self.input_sub])
-    test_utils.cleanup_topics([self.input_topic])
-
-  def _cleanup_dataset(self):
-    self.dataset.delete()
+    test_utils.cleanup_subscriptions(self.sub_client, [self.input_sub])
+    test_utils.cleanup_topics(self.pub_client, [self.input_topic])
 
   @attr('IT')
-  def test_game_stats_it(self):
+  def test_leader_board_it(self):
     state_verifier = PipelineStateMatcher(PipelineState.RUNNING)
 
-    success_condition = 'mean_duration=300 LIMIT 1'
-    sessions_query = ('SELECT mean_duration FROM [%s:%s.%s] '
-                      'WHERE %s' % (self.project,
-                                    self.dataset.name,
-                                    self.OUTPUT_TABLE_SESSIONS,
-                                    success_condition))
-    bq_sessions_verifier = BigqueryMatcher(self.project,
-                                           sessions_query,
-                                           self.DEFAULT_EXPECTED_CHECKSUM)
-
-    # TODO(mariagh): Add teams table verifier once game_stats.py is fixed.
-
-    extra_opts = {'subscription': self.input_sub.full_name,
-                  'dataset': self.dataset.name,
-                  'topic': self.input_topic.full_name,
-                  'fixed_window_duration': 1,
-                  'user_activity_window_duration': 1,
+    success_condition = 'total_score=5000 LIMIT 1'
+    users_query = ('SELECT total_score FROM `%s.%s.%s` '
+                   'WHERE %s' % (self.project,
+                                 self.dataset_ref.dataset_id,
+                                 self.OUTPUT_TABLE_USERS,
+                                 success_condition))
+    bq_users_verifier = BigqueryMatcher(self.project,
+                                        users_query,
+                                        self.DEFAULT_EXPECTED_CHECKSUM)
+
+    teams_query = ('SELECT total_score FROM `%s.%s.%s` '
+                   'WHERE %s' % (self.project,
+                                 self.dataset_ref.dataset_id,
+                                 self.OUTPUT_TABLE_TEAMS,
+                                 success_condition))
+    bq_teams_verifier = BigqueryMatcher(self.project,
+                                        teams_query,
+                                        self.DEFAULT_EXPECTED_CHECKSUM)
+
+    extra_opts = {'subscription': self.input_sub.name,
+                  'dataset': self.dataset_ref.dataset_id,
+                  'topic': self.input_topic.name,
+                  'team_window_duration': 1,
                   'wait_until_finish_duration':
                       self.WAIT_UNTIL_FINISH_DURATION,
                   'on_success_matcher': all_of(state_verifier,
-                                               bq_sessions_verifier)}
+                                               bq_users_verifier,
+                                               bq_teams_verifier)}
 
     # Register cleanup before pipeline execution.
     # Note that actual execution happens in reverse order.
     self.addCleanup(self._cleanup_pubsub)
-    self.addCleanup(self._cleanup_dataset)
-    self.addCleanup(utils.delete_bq_table, self.project,
-                    self.dataset.name, self.OUTPUT_TABLE_SESSIONS)
-    self.addCleanup(utils.delete_bq_table, self.project,
-                    self.dataset.name, self.OUTPUT_TABLE_TEAMS)
+    self.addCleanup(utils.delete_bq_dataset, self.project, self.dataset_ref)
 
     # Generate input data and inject to PubSub.
-    test_utils.wait_for_subscriptions_created([self.input_topic,
-                                               self.input_sub])
     self._inject_pubsub_game_events(self.input_topic, self.DEFAULT_INPUT_COUNT)
 
     # Get pipeline options from command argument: --test-pipeline-options,
     # and start pipeline job by calling pipeline main function.
-    game_stats.run(
+    leader_board.run(
         self.test_pipeline.get_full_options_as_args(**extra_opts))
 
 
 if __name__ == '__main__':
   logging.getLogger().setLevel(logging.DEBUG)
   unittest.main()
```

## Comparing `apache-beam-2.8.0/apache_beam/examples/complete/game/game_stats_test.py` & `apache-beam-2.9.0/apache_beam/examples/complete/game/game_stats_test.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/examples/complete/game/hourly_team_score_test.py` & `apache-beam-2.9.0/apache_beam/examples/complete/game/hourly_team_score_test.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/examples/complete/game/user_score_it_test.py` & `apache-beam-2.9.0/apache_beam/examples/complete/game/user_score_it_test.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/examples/complete/game/leader_board_it_test.py` & `apache-beam-2.9.0/apache_beam/examples/complete/game/game_stats_it_test.py`

 * *Files 14% similar despite different names*

```diff
@@ -11,17 +11,17 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 #
 
-"""End-to-end test for the leader board example.
+"""End-to-end test for the game stats example.
 
-Code: beam/sdks/python/apache_beam/examples/complete/game/leader_board.py
+Code: beam/sdks/python/apache_beam/examples/complete/game/game_stats.py
 Usage:
 
   python setup.py nosetests --test-pipeline-options=" \
       --runner=TestDataflowRunner \
       --project=... \
       --staging_location=gs://... \
       --temp_location=gs://... \
@@ -32,135 +32,116 @@
 
 from __future__ import absolute_import
 
 import logging
 import time
 import unittest
 import uuid
-from builtins import range
 
 from hamcrest.core.core.allof import all_of
 from nose.plugins.attrib import attr
 
-from apache_beam.examples.complete.game import leader_board
+from apache_beam.examples.complete.game import game_stats
 from apache_beam.io.gcp.tests import utils
 from apache_beam.io.gcp.tests.bigquery_matcher import BigqueryMatcher
 from apache_beam.runners.runner import PipelineState
 from apache_beam.testing import test_utils
 from apache_beam.testing.pipeline_verifiers import PipelineStateMatcher
 from apache_beam.testing.test_pipeline import TestPipeline
 
 
-class LeaderBoardIT(unittest.TestCase):
+class GameStatsIT(unittest.TestCase):
 
-  # Input event containing user, team, score, processing time, window start.
+  # Input events containing user, team, score, processing time, window start.
   INPUT_EVENT = 'user1,teamA,10,%d,2015-11-02 09:09:28.224'
-  INPUT_TOPIC = 'leader_board_it_input_topic'
-  INPUT_SUB = 'leader_board_it_input_subscription'
+  INPUT_TOPIC = 'game_stats_it_input_topic'
+  INPUT_SUB = 'game_stats_it_input_subscription'
 
   # SHA-1 hash generated from sorted rows reading from BigQuery table
-  DEFAULT_EXPECTED_CHECKSUM = 'de00231fe6730b972c0ff60a99988438911cda53'
-  OUTPUT_DATASET = 'leader_board_it_dataset'
-  OUTPUT_TABLE_USERS = 'leader_board_users'
-  OUTPUT_TABLE_TEAMS = 'leader_board_teams'
+  DEFAULT_EXPECTED_CHECKSUM = '5288ccaab77d347c8460d77c15a0db234ef5eb4f'
+  OUTPUT_DATASET = 'game_stats_it_dataset'
+  OUTPUT_TABLE_SESSIONS = 'game_stats_sessions'
+  OUTPUT_TABLE_TEAMS = 'game_stats_teams'
   DEFAULT_INPUT_COUNT = 500
 
-  WAIT_UNTIL_FINISH_DURATION = 10 * 60 * 1000   # in milliseconds
+  WAIT_UNTIL_FINISH_DURATION = 12 * 60 * 1000   # in milliseconds
 
   def setUp(self):
     self.test_pipeline = TestPipeline(is_integration_test=True)
     self.project = self.test_pipeline.get_option('project')
     _unique_id = str(uuid.uuid4())
 
     # Set up PubSub environment.
     from google.cloud import pubsub
-    self.pubsub_client = pubsub.Client(project=self.project)
-    unique_topic_name = self.INPUT_TOPIC + _unique_id
-    unique_subscrition_name = self.INPUT_SUB + _unique_id
-    self.input_topic = self.pubsub_client.topic(unique_topic_name)
-    self.input_sub = self.input_topic.subscription(unique_subscrition_name)
-
-    self.input_topic.create()
-    test_utils.wait_for_topics_created([self.input_topic])
-    self.input_sub.create()
+    self.pub_client = pubsub.PublisherClient()
+    self.input_topic = self.pub_client.create_topic(
+        self.pub_client.topic_path(self.project, self.INPUT_TOPIC + _unique_id))
+
+    self.sub_client = pubsub.SubscriberClient()
+    self.input_sub = self.sub_client.create_subscription(
+        self.sub_client.subscription_path(self.project,
+                                          self.INPUT_SUB + _unique_id),
+        self.input_topic.name)
 
     # Set up BigQuery environment
-    from google.cloud import bigquery
-    client = bigquery.Client()
-    unique_dataset_name = self.OUTPUT_DATASET + str(int(time.time()))
-    self.dataset = client.dataset(unique_dataset_name, project=self.project)
-    self.dataset.create()
+    self.dataset_ref = utils.create_bq_dataset(self.project,
+                                               self.OUTPUT_DATASET)
 
     self._test_timestamp = int(time.time() * 1000)
 
   def _inject_pubsub_game_events(self, topic, message_count):
     """Inject game events as test data to PubSub."""
 
     logging.debug('Injecting %d game events to topic %s',
-                  message_count, topic.full_name)
+                  message_count, topic.name)
 
     for _ in range(message_count):
-      topic.publish(self.INPUT_EVENT % self._test_timestamp)
+      self.pub_client.publish(topic.name,
+                              self.INPUT_EVENT % self._test_timestamp)
 
   def _cleanup_pubsub(self):
-    test_utils.cleanup_subscriptions([self.input_sub])
-    test_utils.cleanup_topics([self.input_topic])
-
-  def _cleanup_dataset(self):
-    self.dataset.delete()
+    test_utils.cleanup_subscriptions(self.sub_client, [self.input_sub])
+    test_utils.cleanup_topics(self.pub_client, [self.input_topic])
 
   @attr('IT')
-  def test_leader_board_it(self):
+  def test_game_stats_it(self):
     state_verifier = PipelineStateMatcher(PipelineState.RUNNING)
 
-    success_condition = 'total_score=5000 LIMIT 1'
-    users_query = ('SELECT total_score FROM [%s:%s.%s] '
-                   'WHERE %s' % (self.project,
-                                 self.dataset.name,
-                                 self.OUTPUT_TABLE_USERS,
-                                 success_condition))
-    bq_users_verifier = BigqueryMatcher(self.project,
-                                        users_query,
-                                        self.DEFAULT_EXPECTED_CHECKSUM)
-
-    teams_query = ('SELECT total_score FROM [%s:%s.%s] '
-                   'WHERE %s' % (self.project,
-                                 self.dataset.name,
-                                 self.OUTPUT_TABLE_TEAMS,
-                                 success_condition))
-    bq_teams_verifier = BigqueryMatcher(self.project,
-                                        teams_query,
-                                        self.DEFAULT_EXPECTED_CHECKSUM)
-
-    extra_opts = {'subscription': self.input_sub.full_name,
-                  'dataset': self.dataset.name,
-                  'topic': self.input_topic.full_name,
-                  'team_window_duration': 1,
+    success_condition = 'mean_duration=300 LIMIT 1'
+    sessions_query = ('SELECT mean_duration FROM `%s.%s.%s` '
+                      'WHERE %s' % (self.project,
+                                    self.dataset_ref.dataset_id,
+                                    self.OUTPUT_TABLE_SESSIONS,
+                                    success_condition))
+    bq_sessions_verifier = BigqueryMatcher(self.project,
+                                           sessions_query,
+                                           self.DEFAULT_EXPECTED_CHECKSUM)
+
+    # TODO(mariagh): Add teams table verifier once game_stats.py is fixed.
+
+    extra_opts = {'subscription': self.input_sub.name,
+                  'dataset': self.dataset_ref.dataset_id,
+                  'topic': self.input_topic.name,
+                  'fixed_window_duration': 1,
+                  'user_activity_window_duration': 1,
                   'wait_until_finish_duration':
                       self.WAIT_UNTIL_FINISH_DURATION,
                   'on_success_matcher': all_of(state_verifier,
-                                               bq_users_verifier,
-                                               bq_teams_verifier)}
+                                               bq_sessions_verifier)}
 
     # Register cleanup before pipeline execution.
     # Note that actual execution happens in reverse order.
     self.addCleanup(self._cleanup_pubsub)
-    self.addCleanup(self._cleanup_dataset)
-    self.addCleanup(utils.delete_bq_table, self.project,
-                    self.dataset.name, self.OUTPUT_TABLE_USERS)
-    self.addCleanup(utils.delete_bq_table, self.project,
-                    self.dataset.name, self.OUTPUT_TABLE_TEAMS)
+    self.addCleanup(utils.delete_bq_dataset, self.project, self.dataset_ref)
 
     # Generate input data and inject to PubSub.
-    test_utils.wait_for_subscriptions_created([self.input_topic,
-                                               self.input_sub])
     self._inject_pubsub_game_events(self.input_topic, self.DEFAULT_INPUT_COUNT)
 
     # Get pipeline options from command argument: --test-pipeline-options,
     # and start pipeline job by calling pipeline main function.
-    leader_board.run(
+    game_stats.run(
         self.test_pipeline.get_full_options_as_args(**extra_opts))
 
 
 if __name__ == '__main__':
   logging.getLogger().setLevel(logging.DEBUG)
   unittest.main()
```

## Comparing `apache-beam-2.8.0/apache_beam/examples/complete/game/game_stats.py` & `apache-beam-2.9.0/apache_beam/examples/complete/game/game_stats.py`

 * *Files 2% similar despite different names*

```diff
@@ -155,44 +155,45 @@
         'window_start': start,
         'processing_time': timestamp2str(int(time.time()))
     }
 
 
 class WriteToBigQuery(beam.PTransform):
   """Generate, format, and write BigQuery table row information."""
-  def __init__(self, table_name, dataset, schema):
+  def __init__(self, table_name, dataset, schema, project):
     """Initializes the transform.
     Args:
       table_name: Name of the BigQuery table to use.
       dataset: Name of the dataset to use.
       schema: Dictionary in the format {'column_name': 'bigquery_type'}
+      project: Name of the Cloud project containing BigQuery table.
     """
     super(WriteToBigQuery, self).__init__()
     self.table_name = table_name
     self.dataset = dataset
     self.schema = schema
+    self.project = project
 
   def get_schema(self):
     """Build the output table schema."""
     return ', '.join(
         '%s:%s' % (col, self.schema[col]) for col in self.schema)
 
   def get_schema(self):
     """Build the output table schema."""
     return ', '.join(
         '%s:%s' % (col, self.schema[col]) for col in self.schema)
 
   def expand(self, pcoll):
-    project = pcoll.pipeline.options.view_as(GoogleCloudOptions).project
     return (
         pcoll
         | 'ConvertToRow' >> beam.Map(
             lambda elem: {col: elem[col] for col in self.schema})
         | beam.io.WriteToBigQuery(
-            self.table_name, self.dataset, project, self.get_schema()))
+            self.table_name, self.dataset, self.project, self.get_schema()))
 
 
 # [START abuse_detect]
 class CalculateSpammyUsers(beam.PTransform):
   """Filter out all but those users with a high clickrate, which we will
   consider as 'spammy' uesrs.
 
@@ -350,15 +351,15 @@
      | 'TeamScoresDict' >> beam.ParDo(TeamScoresDict())
      | 'WriteTeamScoreSums' >> WriteToBigQuery(
          args.table_name + '_teams', args.dataset, {
              'team': 'STRING',
              'total_score': 'INTEGER',
              'window_start': 'STRING',
              'processing_time': 'STRING',
-         }))
+         }, options.view_as(GoogleCloudOptions).project))
 
     # [START session_calc]
     # Detect user sessions-- that is, a burst of activity separated by a gap
     # from further activity. Find and record the mean session lengths.
     # This information could help the game designers track the changing user
     # engagement as their set of game changes.
     (user_events  # pylint: disable=expression-not-assigned
@@ -384,14 +385,14 @@
      # Find the mean session duration in each window
      | beam.CombineGlobally(beam.combiners.MeanCombineFn()).without_defaults()
      | 'FormatAvgSessionLength' >> beam.Map(
          lambda elem: {'mean_duration': float(elem)})
      | 'WriteAvgSessionLength' >> WriteToBigQuery(
          args.table_name + '_sessions', args.dataset, {
              'mean_duration': 'FLOAT',
-         }))
+         }, options.view_as(GoogleCloudOptions).project))
      # [END rewindow]
 
 
 if __name__ == '__main__':
   logging.getLogger().setLevel(logging.INFO)
   run()
```

## Comparing `apache-beam-2.8.0/apache_beam/examples/complete/game/__init__.py` & `apache-beam-2.9.0/apache_beam/examples/complete/juliaset/__init__.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/examples/complete/game/hourly_team_score_it_test.py` & `apache-beam-2.9.0/apache_beam/examples/complete/game/hourly_team_score_it_test.py`

 * *Files 8% similar despite different names*

```diff
@@ -29,15 +29,14 @@
       --sdk_location=... \
 
 """
 
 from __future__ import absolute_import
 
 import logging
-import time
 import unittest
 
 from hamcrest.core.core.allof import all_of
 from nose.plugins.attrib import attr
 
 from apache_beam.examples.complete.game import hourly_team_score
 from apache_beam.io.gcp.tests import utils
@@ -56,45 +55,37 @@
   OUTPUT_TABLE = 'leader_board'
 
   def setUp(self):
     self.test_pipeline = TestPipeline(is_integration_test=True)
     self.project = self.test_pipeline.get_option('project')
 
     # Set up BigQuery environment
-    from google.cloud import bigquery
-    client = bigquery.Client()
-    unique_dataset_name = self.OUTPUT_DATASET + str(int(time.time()))
-    self.dataset = client.dataset(unique_dataset_name, project=self.project)
-    self.dataset.create()
-
-  def _cleanup_dataset(self):
-    self.dataset.delete()
+    self.dataset_ref = utils.create_bq_dataset(self.project,
+                                               self.OUTPUT_DATASET)
 
   @attr('IT')
   def test_hourly_team_score_it(self):
     state_verifier = PipelineStateMatcher(PipelineState.DONE)
-    query = ('SELECT COUNT(*) FROM [%s:%s.%s]' % (self.project,
-                                                  self.dataset.name,
+    query = ('SELECT COUNT(*) FROM `%s.%s.%s`' % (self.project,
+                                                  self.dataset_ref.dataset_id,
                                                   self.OUTPUT_TABLE))
 
     bigquery_verifier = BigqueryMatcher(self.project,
                                         query,
                                         self.DEFAULT_EXPECTED_CHECKSUM)
 
     extra_opts = {'input': self.DEFAULT_INPUT_FILE,
-                  'dataset': self.dataset.name,
+                  'dataset': self.dataset_ref.dataset_id,
                   'window_duration': 1,
                   'on_success_matcher': all_of(state_verifier,
                                                bigquery_verifier)}
 
     # Register clean up before pipeline execution
     # Note that actual execution happens in reverse order.
-    self.addCleanup(self._cleanup_dataset)
-    self.addCleanup(utils.delete_bq_table, self.project,
-                    self.dataset.name, self.OUTPUT_TABLE)
+    self.addCleanup(utils.delete_bq_dataset, self.project, self.dataset_ref)
 
     # Get pipeline options from command argument: --test-pipeline-options,
     # and start pipeline job by calling pipeline main function.
     hourly_team_score.run(
         self.test_pipeline.get_full_options_as_args(**extra_opts))
```

## Comparing `apache-beam-2.8.0/apache_beam/examples/complete/game/hourly_team_score.py` & `apache-beam-2.9.0/apache_beam/examples/complete/game/hourly_team_score.py`

 * *Files 1% similar despite different names*

```diff
@@ -155,39 +155,40 @@
         'window_start': start,
         'processing_time': timestamp2str(int(time.time()))
     }
 
 
 class WriteToBigQuery(beam.PTransform):
   """Generate, format, and write BigQuery table row information."""
-  def __init__(self, table_name, dataset, schema):
+  def __init__(self, table_name, dataset, schema, project):
     """Initializes the transform.
     Args:
       table_name: Name of the BigQuery table to use.
       dataset: Name of the dataset to use.
       schema: Dictionary in the format {'column_name': 'bigquery_type'}
+      project: Name of the Cloud project containing BigQuery table.
     """
     super(WriteToBigQuery, self).__init__()
     self.table_name = table_name
     self.dataset = dataset
     self.schema = schema
+    self.project = project
 
   def get_schema(self):
     """Build the output table schema."""
     return ', '.join(
         '%s:%s' % (col, self.schema[col]) for col in self.schema)
 
   def expand(self, pcoll):
-    project = pcoll.pipeline.options.view_as(GoogleCloudOptions).project
     return (
         pcoll
         | 'ConvertToRow' >> beam.Map(
             lambda elem: {col: elem[col] for col in self.schema})
         | beam.io.WriteToBigQuery(
-            self.table_name, self.dataset, project, self.get_schema()))
+            self.table_name, self.dataset, self.project, self.get_schema()))
 
 
 # [START main]
 class HourlyTeamScore(beam.PTransform):
   def __init__(self, start_min, stop_min, window_duration):
     super(HourlyTeamScore, self).__init__()
     self.start_timestamp = str2timestamp(start_min)
@@ -287,14 +288,14 @@
          args.start_min, args.stop_min, args.window_duration)
      | 'TeamScoresDict' >> beam.ParDo(TeamScoresDict())
      | 'WriteTeamScoreSums' >> WriteToBigQuery(
          args.table_name, args.dataset, {
              'team': 'STRING',
              'total_score': 'INTEGER',
              'window_start': 'STRING',
-         }))
+         }, options.view_as(GoogleCloudOptions).project))
 # [END main]
 
 
 if __name__ == '__main__':
   logging.getLogger().setLevel(logging.INFO)
   run()
```

## Comparing `apache-beam-2.8.0/apache_beam/examples/complete/game/leader_board.py` & `apache-beam-2.9.0/apache_beam/examples/complete/game/leader_board.py`

 * *Files 1% similar despite different names*

```diff
@@ -164,39 +164,40 @@
         'window_start': start,
         'processing_time': timestamp2str(int(time.time()))
     }
 
 
 class WriteToBigQuery(beam.PTransform):
   """Generate, format, and write BigQuery table row information."""
-  def __init__(self, table_name, dataset, schema):
+  def __init__(self, table_name, dataset, schema, project):
     """Initializes the transform.
     Args:
       table_name: Name of the BigQuery table to use.
       dataset: Name of the dataset to use.
       schema: Dictionary in the format {'column_name': 'bigquery_type'}
+      project: Name of the Cloud project containing BigQuery table.
     """
     super(WriteToBigQuery, self).__init__()
     self.table_name = table_name
     self.dataset = dataset
     self.schema = schema
+    self.project = project
 
   def get_schema(self):
     """Build the output table schema."""
     return ', '.join(
         '%s:%s' % (col, self.schema[col]) for col in self.schema)
 
   def expand(self, pcoll):
-    project = pcoll.pipeline.options.view_as(GoogleCloudOptions).project
     return (
         pcoll
         | 'ConvertToRow' >> beam.Map(
             lambda elem: {col: elem[col] for col in self.schema})
         | beam.io.WriteToBigQuery(
-            self.table_name, self.dataset, project, self.get_schema()))
+            self.table_name, self.dataset, self.project, self.get_schema()))
 
 
 # [START window_and_trigger]
 class CalculateTeamScores(beam.PTransform):
   """Calculates scores for each team within the configured window duration.
 
   Extract team/score pairs from the event stream, using hour-long windows by
@@ -325,27 +326,27 @@
      | 'TeamScoresDict' >> beam.ParDo(TeamScoresDict())
      | 'WriteTeamScoreSums' >> WriteToBigQuery(
          args.table_name + '_teams', args.dataset, {
              'team': 'STRING',
              'total_score': 'INTEGER',
              'window_start': 'STRING',
              'processing_time': 'STRING',
-         }))
+         }, options.view_as(GoogleCloudOptions).project))
 
     def format_user_score_sums(user_score):
       (user, score) = user_score
       return {'user': user, 'total_score': score}
 
     # Get user scores and write the results to BigQuery
     (events  # pylint: disable=expression-not-assigned
      | 'CalculateUserScores' >> CalculateUserScores(args.allowed_lateness)
      | 'FormatUserScoreSums' >> beam.Map(format_user_score_sums)
      | 'WriteUserScoreSums' >> WriteToBigQuery(
          args.table_name + '_users', args.dataset, {
              'user': 'STRING',
              'total_score': 'INTEGER',
-         }))
+         }, options.view_as(GoogleCloudOptions).project))
 
 
 if __name__ == '__main__':
   logging.getLogger().setLevel(logging.INFO)
   run()
```

## Comparing `apache-beam-2.8.0/apache_beam/examples/complete/game/user_score.py` & `apache-beam-2.9.0/apache_beam/examples/complete/game/user_score.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/examples/complete/game/leader_board_test.py` & `apache-beam-2.9.0/apache_beam/examples/complete/game/leader_board_test.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/examples/complete/game/user_score_test.py` & `apache-beam-2.9.0/apache_beam/examples/complete/game/user_score_test.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/examples/cookbook/group_with_coder_test.py` & `apache-beam-2.9.0/apache_beam/examples/cookbook/group_with_coder_test.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/examples/cookbook/filters.py` & `apache-beam-2.9.0/apache_beam/examples/cookbook/filters.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/examples/cookbook/datastore_wordcount_it_test.py` & `apache-beam-2.9.0/apache_beam/examples/cookbook/datastore_wordcount_it_test.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/examples/cookbook/bigquery_tornadoes_it_test.py` & `apache-beam-2.9.0/apache_beam/examples/cookbook/bigquery_tornadoes_it_test.py`

 * *Files 5% similar despite different names*

```diff
@@ -49,25 +49,26 @@
 
     # Set extra options to the pipeline for test purpose
     project = test_pipeline.get_option('project')
 
     dataset = 'BigQueryTornadoesIT'
     table = 'monthly_tornadoes_%s' % int(round(time.time() * 1000))
     output_table = '.'.join([dataset, table])
-    query = 'SELECT month, tornado_count FROM [%s]' % output_table
+    query = 'SELECT month, tornado_count FROM `%s`' % output_table
 
     pipeline_verifiers = [PipelineStateMatcher(),
                           BigqueryMatcher(
                               project=project,
                               query=query,
                               checksum=self.DEFAULT_CHECKSUM)]
     extra_opts = {'output': output_table,
                   'on_success_matcher': all_of(*pipeline_verifiers)}
 
     # Register cleanup before pipeline execution.
+    # Note that actual execution happens in reverse order.
     self.addCleanup(utils.delete_bq_table, project, dataset, table)
 
     # Get pipeline options from command argument: --test-pipeline-options,
     # and start pipeline job by calling pipeline main function.
     bigquery_tornadoes.run(
         test_pipeline.get_full_options_as_args(**extra_opts))
```

## Comparing `apache-beam-2.8.0/apache_beam/examples/cookbook/__init__.py` & `apache-beam-2.9.0/apache_beam/examples/complete/juliaset/juliaset/__init__.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/examples/cookbook/multiple_output_pardo_test.py` & `apache-beam-2.9.0/apache_beam/examples/cookbook/multiple_output_pardo_test.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/examples/cookbook/custom_ptransform.py` & `apache-beam-2.9.0/apache_beam/examples/cookbook/custom_ptransform.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/examples/cookbook/multiple_output_pardo.py` & `apache-beam-2.9.0/apache_beam/examples/cookbook/multiple_output_pardo.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/examples/cookbook/group_with_coder.py` & `apache-beam-2.9.0/apache_beam/examples/cookbook/group_with_coder.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/examples/cookbook/bigquery_tornadoes_test.py` & `apache-beam-2.9.0/apache_beam/examples/cookbook/bigquery_tornadoes_test.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/examples/cookbook/mergecontacts_test.py` & `apache-beam-2.9.0/apache_beam/examples/cookbook/mergecontacts_test.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/examples/cookbook/custom_ptransform_test.py` & `apache-beam-2.9.0/apache_beam/examples/cookbook/custom_ptransform_test.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/examples/cookbook/combiners_test.py` & `apache-beam-2.9.0/apache_beam/examples/cookbook/combiners_test.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/examples/cookbook/coders_test.py` & `apache-beam-2.9.0/apache_beam/examples/cookbook/coders_test.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/examples/cookbook/bigquery_side_input.py` & `apache-beam-2.9.0/apache_beam/examples/cookbook/bigquery_side_input.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/examples/cookbook/bigquery_tornadoes.py` & `apache-beam-2.9.0/apache_beam/examples/cookbook/bigquery_tornadoes.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/examples/cookbook/mergecontacts.py` & `apache-beam-2.9.0/apache_beam/examples/cookbook/mergecontacts.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/examples/cookbook/datastore_wordcount.py` & `apache-beam-2.9.0/apache_beam/examples/cookbook/datastore_wordcount.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/examples/cookbook/coders.py` & `apache-beam-2.9.0/apache_beam/examples/cookbook/coders.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/examples/cookbook/bigquery_side_input_test.py` & `apache-beam-2.9.0/apache_beam/examples/cookbook/bigquery_side_input_test.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/examples/cookbook/bigquery_schema.py` & `apache-beam-2.9.0/apache_beam/examples/cookbook/bigquery_schema.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/examples/cookbook/filters_test.py` & `apache-beam-2.9.0/apache_beam/examples/cookbook/filters_test.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/internal/__init__.py` & `apache-beam-2.9.0/apache_beam/internal/__init__.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/internal/util_test.py` & `apache-beam-2.9.0/apache_beam/internal/util_test.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/internal/pickler.py` & `apache-beam-2.9.0/apache_beam/internal/pickler.py`

 * *Files 2% similar despite different names*

```diff
@@ -51,15 +51,16 @@
   dill._dill = dill.dill
   sys.modules['dill._dill'] = dill.dill
 
 
 def _is_nested_class(cls):
   """Returns true if argument is a class object that appears to be nested."""
   return (isinstance(cls, type)
-          and cls.__module__ != '__builtin__'
+          and cls.__module__ != 'builtins'     # Python 3
+          and cls.__module__ != '__builtin__'  # Python 2
           and cls.__name__ not in sys.modules[cls.__module__].__dict__)
 
 
 def _find_containing_class(nested_class):
   """Finds containing class of a nested class passed as argument."""
 
   seen = set()
```

## Comparing `apache-beam-2.8.0/apache_beam/internal/module_test.py` & `apache-beam-2.9.0/apache_beam/internal/module_test.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/internal/pickler_test.py` & `apache-beam-2.9.0/apache_beam/internal/pickler_test.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/internal/util.py` & `apache-beam-2.9.0/apache_beam/internal/util.py`

 * *Files 3% similar despite different names*

```diff
@@ -50,14 +50,18 @@
       other: Another placeholder object to compare to.
 
     This method is used only for test code. All placeholder objects are
     equal to each other.
     """
     return isinstance(other, ArgumentPlaceholder)
 
+  def __ne__(self, other):
+    # TODO(BEAM-5949): Needed for Python 2 compatibility.
+    return not self == other
+
   def __hash__(self):
     return hash(type(self))
 
 
 def remove_objects_from_args(args, kwargs, pvalue_classes):
   """For internal use only; no backwards-compatibility guarantees.
```

## Comparing `apache-beam-2.8.0/apache_beam/internal/gcp/__init__.py` & `apache-beam-2.9.0/apache_beam/internal/gcp/__init__.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/internal/gcp/auth.py` & `apache-beam-2.9.0/apache_beam/internal/gcp/auth.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/internal/gcp/json_value.py` & `apache-beam-2.9.0/apache_beam/internal/gcp/json_value.py`

 * *Files 2% similar despite different names*

```diff
@@ -103,14 +103,16 @@
           extra_types.JsonObject.Property(
               key=k, value=to_json_value(v, with_type=with_type)))
     return extra_types.JsonValue(object_value=json_object)
   elif with_type:
     return to_json_value(get_typed_value_descriptor(obj), with_type=False)
   elif isinstance(obj, (str, unicode)):
     return extra_types.JsonValue(string_value=obj)
+  elif isinstance(obj, bytes):
+    return extra_types.JsonValue(string_value=obj.decode('utf8'))
   elif isinstance(obj, bool):
     return extra_types.JsonValue(boolean_value=obj)
   elif isinstance(obj, (int, long)):
     if _MININT64 <= obj <= _MAXINT64:
       return extra_types.JsonValue(integer_value=obj)
     else:
       raise TypeError('Can not encode {} as a 64-bit integer'.format(obj))
```

## Comparing `apache-beam-2.8.0/apache_beam/internal/gcp/json_value_test.py` & `apache-beam-2.9.0/apache_beam/internal/gcp/json_value_test.py`

 * *Files 2% similar despite different names*

```diff
@@ -37,14 +37,17 @@
 
 @unittest.skipIf(JsonValue is None, 'GCP dependencies are not installed')
 class JsonValueTest(unittest.TestCase):
 
   def test_string_to(self):
     self.assertEquals(JsonValue(string_value='abc'), to_json_value('abc'))
 
+  def test_bytes_to(self):
+    self.assertEquals(JsonValue(string_value='abc'), to_json_value(b'abc'))
+
   def test_true_to(self):
     self.assertEquals(JsonValue(boolean_value=True), to_json_value(True))
 
   def test_false_to(self):
     self.assertEquals(JsonValue(boolean_value=False), to_json_value(False))
 
   def test_int_to(self):
```

## Comparing `apache-beam-2.8.0/apache_beam/io/source_test_utils_test.py` & `apache-beam-2.9.0/apache_beam/io/source_test_utils_test.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/io/filebasedsink.py` & `apache-beam-2.9.0/apache_beam/io/filebasedsink.py`

 * *Files 2% similar despite different names*

```diff
@@ -370,14 +370,18 @@
     return FileBasedSink._template_replace_num_shards(shard_name_format)
 
   def __eq__(self, other):
     # TODO: Clean up workitem_test which uses this.
     # pylint: disable=unidiomatic-typecheck
     return type(self) == type(other) and self.__dict__ == other.__dict__
 
+  def __ne__(self, other):
+    # TODO(BEAM-5949): Needed for Python 2 compatibility.
+    return not self == other
+
 
 class FileBasedSinkWriter(iobase.Writer):
   """The writer for FileBasedSink.
   """
 
   def __init__(self, sink, temp_shard_path):
     self.sink = sink
```

## Comparing `apache-beam-2.8.0/apache_beam/io/localfilesystem_test.py` & `apache-beam-2.9.0/apache_beam/io/localfilesystem_test.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/io/filesystems_test.py` & `apache-beam-2.9.0/apache_beam/io/filesystems_test.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/io/filesystems.py` & `apache-beam-2.9.0/apache_beam/io/filesystems.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/io/concat_source.py` & `apache-beam-2.9.0/apache_beam/io/concat_source.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/io/filebasedsource_test.py` & `apache-beam-2.9.0/apache_beam/io/filebasedsource_test.py`

 * *Files 2% similar despite different names*

```diff
@@ -70,15 +70,15 @@
         line = f.readline()
         start += len(line)
       current = start
       line = f.readline()
       while line:
         if not range_tracker.try_claim(current):
           return
-        yield line.rstrip('\n')
+        yield line.rstrip(b'\n')
         current += len(line)
         line = f.readline()
     finally:
       f.close()
 
 
 class EOL(object):
@@ -182,26 +182,32 @@
           return
 
         yield value
 
     def estimate_size(self):
       return len(self._values)  # Assuming each value to be 1 byte.
 
+  @classmethod
+  def setUpClass(cls):
+    # Method has been renamed in Python 3
+    if sys.version_info[0] < 3:
+      cls.assertCountEqual = cls.assertItemsEqual
+
   def setUp(self):
     # Reducing the size of thread pools. Without this test execution may fail in
     # environments with limited amount of resources.
     filebasedsource.MAX_NUM_THREADS_FOR_SIZE_ESTIMATION = 2
 
   def test_read(self):
     sources = [TestConcatSource.DummySource(range(start, start + 10)) for start
                in [0, 10, 20]]
     concat = ConcatSource(sources)
     range_tracker = concat.get_range_tracker(None, None)
     read_data = [value for value in concat.read(range_tracker)]
-    self.assertItemsEqual(list(range(30)), read_data)
+    self.assertCountEqual(list(range(30)), read_data)
 
   def test_split(self):
     sources = [TestConcatSource.DummySource(list(range(start, start + 10)))
                for start in [0, 10, 20]]
     concat = ConcatSource(sources)
     splits = [split for split in concat.split()]
     self.assertEquals(6, len(splits))
@@ -210,25 +216,31 @@
     read_data = []
     for split in splits:
       range_tracker_for_split = split.source.get_range_tracker(
           split.start_position,
           split.stop_position)
       read_data.extend([value for value in split.source.read(
           range_tracker_for_split)])
-    self.assertItemsEqual(list(range(30)), read_data)
+    self.assertCountEqual(list(range(30)), read_data)
 
   def test_estimate_size(self):
     sources = [TestConcatSource.DummySource(range(start, start + 10)) for start
                in [0, 10, 20]]
     concat = ConcatSource(sources)
     self.assertEquals(30, concat.estimate_size())
 
 
 class TestFileBasedSource(unittest.TestCase):
 
+  @classmethod
+  def setUpClass(cls):
+    # Method has been renamed in Python 3
+    if sys.version_info[0] < 3:
+      cls.assertCountEqual = cls.assertItemsEqual
+
   def setUp(self):
     # Reducing the size of thread pools. Without this test execution may fail in
     # environments with limited amount of resources.
     filebasedsource.MAX_NUM_THREADS_FOR_SIZE_ESTIMATION = 2
 
   def test_string_or_value_provider_only(self):
     str_file_pattern = tempfile.NamedTemporaryFile(delete=False).name
@@ -273,15 +285,15 @@
 
   def test_fully_read_single_file(self):
     file_name, expected_data = write_data(10)
     assert len(expected_data) == 10
     fbs = LineSource(file_name)
     range_tracker = fbs.get_range_tracker(None, None)
     read_data = [record for record in fbs.read(range_tracker)]
-    self.assertItemsEqual(expected_data, read_data)
+    self.assertCountEqual(expected_data, read_data)
 
   def test_single_file_display_data(self):
     file_name, _ = write_data(10)
     fbs = LineSource(file_name)
     dd = DisplayData.create_from(fbs)
     expected_items = [
         DisplayDataItemMatcher('file_pattern', file_name),
@@ -291,23 +303,23 @@
 
   def test_fully_read_file_pattern(self):
     pattern, expected_data = write_pattern([5, 3, 12, 8, 8, 4])
     assert len(expected_data) == 40
     fbs = LineSource(pattern)
     range_tracker = fbs.get_range_tracker(None, None)
     read_data = [record for record in fbs.read(range_tracker)]
-    self.assertItemsEqual(expected_data, read_data)
+    self.assertCountEqual(expected_data, read_data)
 
   def test_fully_read_file_pattern_with_empty_files(self):
     pattern, expected_data = write_pattern([5, 0, 12, 0, 8, 0])
     assert len(expected_data) == 25
     fbs = LineSource(pattern)
     range_tracker = fbs.get_range_tracker(None, None)
     read_data = [record for record in fbs.read(range_tracker)]
-    self.assertItemsEqual(expected_data, read_data)
+    self.assertCountEqual(expected_data, read_data)
 
   def test_estimate_size_of_file(self):
     file_name, expected_data = write_data(10)
     assert len(expected_data) == 10
     fbs = LineSource(file_name)
     self.assertEquals(10 * 6, fbs.estimate_size())
 
@@ -371,15 +383,15 @@
     for split in splits:
       source = split.source
       range_tracker = source.get_range_tracker(split.start_position,
                                                split.stop_position)
       data_from_split = [data for data in source.read(range_tracker)]
       read_data.extend(data_from_split)
 
-    self.assertItemsEqual(expected_data, read_data)
+    self.assertCountEqual(expected_data, read_data)
 
   def test_read_splits_file_pattern(self):
     pattern, expected_data = write_pattern([34, 66, 40, 24, 24, 12])
     assert len(expected_data) == 200
     fbs = LineSource(pattern)
     splits = [split for split in fbs.split(desired_bundle_size=50)]
 
@@ -388,15 +400,15 @@
     for split in splits:
       source = split.source
       range_tracker = source.get_range_tracker(split.start_position,
                                                split.stop_position)
       data_from_split = [data for data in source.read(range_tracker)]
       read_data.extend(data_from_split)
 
-    self.assertItemsEqual(expected_data, read_data)
+    self.assertCountEqual(expected_data, read_data)
 
   def _run_source_test(self, pattern, expected_data, splittable=True):
     pipeline = TestPipeline()
     pcoll = pipeline | 'Read' >> beam.io.Read(LineSource(
         pattern, splittable=splittable))
     assert_that(pcoll, equal_to(expected_data))
     pipeline.run()
@@ -550,15 +562,15 @@
     for i, c in enumerate(chunks):
       if i%2 == 0:
         out = io.BytesIO()
         with gzip.GzipFile(fileobj=out, mode="wb") as f:
           f.write('\n'.join(c))
         chunks_to_write.append(out.getvalue())
       else:
-        chunks_to_write.append('\n'.join(c))
+        chunks_to_write.append(b'\n'.join(c))
     file_pattern = write_prepared_pattern(chunks_to_write,
                                           suffixes=(['.gz', '']*3))
     pipeline = TestPipeline()
     pcoll = pipeline | 'Read' >> beam.io.Read(LineSource(
         file_pattern,
         compression_type=CompressionTypes.AUTO))
     assert_that(pcoll, equal_to(lines))
```

## Comparing `apache-beam-2.8.0/apache_beam/io/restriction_trackers_test.py` & `apache-beam-2.9.0/apache_beam/io/restriction_trackers_test.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/io/filesystemio.py` & `apache-beam-2.9.0/apache_beam/io/filesystemio.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/io/range_trackers_test.py` & `apache-beam-2.9.0/apache_beam/io/range_trackers_test.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/io/source_test_utils.py` & `apache-beam-2.9.0/apache_beam/io/source_test_utils.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/io/tfrecordio_test.py` & `apache-beam-2.9.0/apache_beam/io/tfrecordio_test.py`

 * *Files 1% similar despite different names*

```diff
@@ -128,15 +128,15 @@
   def test_write_record(self):
     file_handle = io.BytesIO()
     _TFRecordUtil.write_record(file_handle, 'foo')
     self.assertEqual(self.record, file_handle.getvalue())
 
   def test_read_record(self):
     actual = _TFRecordUtil.read_record(self._as_file_handle(self.record))
-    self.assertEqual('foo', actual)
+    self.assertEqual(b'foo', actual)
 
   def test_read_record_invalid_record(self):
     self._test_error('bar', 'Not a valid TFRecord. Fewer than 12 bytes')
 
   def test_read_record_invalid_length_mask(self):
     record = self._increment_value_at_index(self.record, 9)
     self._test_error(record, 'Mismatch of length mask')
@@ -401,14 +401,17 @@
                   | Create([path])
                   | ReadAllFromTFRecord(
                       coder=coders.BytesCoder(),
                       compression_type=CompressionTypes.AUTO))
         assert_that(result, equal_to(['foo', 'bar']))
 
 
+@unittest.skipIf(sys.version_info[0] == 3,
+                 'This test still needs to be fixed on Python 3'
+                 'TODO: BEAM-5623 - several IO tests hang indefinitely')
 class TestEnd2EndWriteAndRead(unittest.TestCase):
 
   def create_inputs(self):
     input_array = [[random.random() - 0.5 for _ in range(15)]
                    for _ in range(12)]
     memfile = io.BytesIO()
     pickle.dump(input_array, memfile)
```

## Comparing `apache-beam-2.8.0/apache_beam/io/utils.py` & `apache-beam-2.9.0/apache_beam/io/utils.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/io/filebasedsource.py` & `apache-beam-2.9.0/apache_beam/io/filebasedsource.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/io/filesystem_test.py` & `apache-beam-2.9.0/apache_beam/io/filesystem_test.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/io/textio.py` & `apache-beam-2.9.0/apache_beam/io/textio.py`

 * *Files 0% similar despite different names*

```diff
@@ -379,29 +379,29 @@
     self._header = header
 
   def open(self, temp_path):
     file_handle = super(_TextSink, self).open(temp_path)
     if self._header is not None:
       file_handle.write(self._header)
       if self._append_trailing_newlines:
-        file_handle.write('\n')
+        file_handle.write(b'\n')
     return file_handle
 
   def display_data(self):
     dd_parent = super(_TextSink, self).display_data()
     dd_parent['append_newline'] = DisplayDataItem(
         self._append_trailing_newlines,
         label='Append Trailing New Lines')
     return dd_parent
 
   def write_encoded_record(self, file_handle, encoded_value):
     """Writes a single encoded record."""
     file_handle.write(encoded_value)
     if self._append_trailing_newlines:
-      file_handle.write('\n')
+      file_handle.write(b'\n')
 
 
 def _create_text_source(
     file_pattern=None, min_bundle_size=None, compression_type=None,
     strip_trailing_newlines=None, coder=None, skip_header_lines=None):
   return _TextSource(
       file_pattern=file_pattern, min_bundle_size=min_bundle_size,
```

## Comparing `apache-beam-2.8.0/apache_beam/io/__init__.py` & `apache-beam-2.9.0/apache_beam/io/__init__.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/io/filesystemio_test.py` & `apache-beam-2.9.0/apache_beam/io/filesystemio_test.py`

 * *Files 1% similar despite different names*

```diff
@@ -173,20 +173,20 @@
           self.assertFalse(seen_last_block)
         seen_last_block = True
       if not data:
         break
       data_list.append(data)
       bytes_read += len(data)
       self.assertEqual(stream.tell(), bytes_read)
-    self.assertEqual(''.join(data_list), expected)
+    self.assertEqual(b''.join(data_list), expected)
 
   def test_pipe_stream(self):
     block_sizes = list(4**i for i in range(0, 12))
     data_blocks = list(os.urandom(size) for size in block_sizes)
-    expected = ''.join(data_blocks)
+    expected = b''.join(data_blocks)
 
     buffer_sizes = [100001, 512 * 1024, 1024 * 1024]
 
     for buffer_size in buffer_sizes:
       parent_conn, child_conn = multiprocessing.Pipe()
       stream = filesystemio.PipeStream(child_conn)
       child_thread = threading.Thread(
```

## Comparing `apache-beam-2.8.0/apache_beam/io/restriction_trackers.py` & `apache-beam-2.9.0/apache_beam/io/restriction_trackers.py`

 * *Files 1% similar despite different names*

```diff
@@ -38,14 +38,18 @@
 
   def __eq__(self, other):
     if not isinstance(other, OffsetRange):
       return False
 
     return self.start == other.start and self.stop == other.stop
 
+  def __ne__(self, other):
+    # TODO(BEAM-5949): Needed for Python 2 compatibility.
+    return not self == other
+
   def __hash__(self):
     return hash((type(self), self.start, self.stop))
 
   def split(self, desired_num_offsets_per_split, min_num_offsets_per_split=1):
     current_split_start = self.start
     max_split_size = max(desired_num_offsets_per_split,
                          min_num_offsets_per_split)
```

## Comparing `apache-beam-2.8.0/apache_beam/io/localfilesystem.py` & `apache-beam-2.9.0/apache_beam/io/localfilesystem.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/io/hadoopfilesystem_test.py` & `apache-beam-2.9.0/apache_beam/io/hadoopfilesystem_test.py`

 * *Files 1% similar despite different names*

```diff
@@ -48,14 +48,18 @@
         'type': type,
     }
     self.saved_data = None
 
   def __eq__(self, other):
     return self.stat == other.stat and self.getvalue() == self.getvalue()
 
+  def __ne__(self, other):
+    # TODO(BEAM-5949): Needed for Python 2 compatibility.
+    return not self == other
+
   def close(self):
     self.saved_data = self.getvalue()
     io.BytesIO.close(self)
 
   def __enter__(self):
     return self
```

## Comparing `apache-beam-2.8.0/apache_beam/io/tfrecordio.py` & `apache-beam-2.9.0/apache_beam/io/tfrecordio.py`

 * *Files 2% similar despite different names*

```diff
@@ -35,20 +35,25 @@
 from apache_beam.io.iobase import Write
 from apache_beam.transforms import PTransform
 
 __all__ = ['ReadFromTFRecord', 'WriteToTFRecord']
 
 
 def _default_crc32c_fn(value):
-  """Calculates crc32c by either snappy or crcmod based on installation."""
+  """Calculates crc32c of a bytes object using either snappy or crcmod."""
 
   if not _default_crc32c_fn.fn:
     try:
       import snappy  # pylint: disable=import-error
-      _default_crc32c_fn.fn = snappy._snappy._crc32c  # pylint: disable=protected-access
+      # Support multiple versions of python-snappy:
+      # https://github.com/andrix/python-snappy/pull/53
+      if getattr(snappy, '_crc32c', None):
+        _default_crc32c_fn.fn = snappy._crc32c  # pylint: disable=protected-access
+      else:
+        _default_crc32c_fn.fn = snappy._snappy._crc32c  # pylint: disable=protected-access
     except ImportError:
       logging.warning('Couldn\'t find python-snappy so the implementation of '
                       '_TFRecordUtil._masked_crc32c is not as fast as it could '
                       'be.')
       _default_crc32c_fn.fn = crcmod.predefined.mkPredefinedCrcFun('crc-32c')
   return _default_crc32c_fn.fn(value)
 
@@ -66,15 +71,15 @@
   """
 
   @classmethod
   def _masked_crc32c(cls, value, crc32c_fn=_default_crc32c_fn):
     """Compute a masked crc32c checksum for a value.
 
     Args:
-      value: A string for which we compute the crc.
+      value: A bytes object for which we compute the crc.
       crc32c_fn: A function that can compute a crc32c.
         This is a performance hook that also helps with testing. Callers are
         not expected to make use of it directly.
     Returns:
       Masked crc32c checksum.
     """
 
@@ -89,22 +94,23 @@
 
   @classmethod
   def write_record(cls, file_handle, value):
     """Encode a value as a TFRecord.
 
     Args:
       file_handle: The file to write to.
-      value: A string content of the record.
+      value: A bytes object representing content of the record.
     """
-    encoded_length = struct.pack('<Q', len(value))
-    file_handle.write('{}{}{}{}'.format(
+    encoded_length = struct.pack(b'<Q', len(value))
+    file_handle.write(b''.join([
         encoded_length,
-        struct.pack('<I', cls._masked_crc32c(encoded_length)),  #
+        struct.pack(b'<I', cls._masked_crc32c(encoded_length)),
         value,
-        struct.pack('<I', cls._masked_crc32c(value))))
+        struct.pack(b'<I', cls._masked_crc32c(value))
+    ]))
 
   @classmethod
   def read_record(cls, file_handle):
     """Read a record from a TFRecords file.
 
     Args:
       file_handle: The file to read from.
```

## Comparing `apache-beam-2.8.0/apache_beam/io/filesystem.py` & `apache-beam-2.9.0/apache_beam/io/filesystem.py`

 * *Files 1% similar despite different names*

```diff
@@ -399,15 +399,16 @@
             self.path == other.path and
             self.size_in_bytes == other.size_in_bytes)
 
   def __hash__(self):
     return hash((self.path, self.size_in_bytes))
 
   def __ne__(self, other):
-    return not self.__eq__(other)
+    # TODO(BEAM-5949): Needed for Python 2 compatibility.
+    return not self == other
 
   def __repr__(self):
     return 'FileMetadata(%s, %s)' % (self.path, self.size_in_bytes)
 
 
 class MatchResult(object):
   """Result from the ``FileSystem`` match operation which contains the list
```

## Comparing `apache-beam-2.8.0/apache_beam/io/iobase.py` & `apache-beam-2.9.0/apache_beam/io/iobase.py`

 * *Files 0% similar despite different names*

```diff
@@ -47,14 +47,15 @@
 from apache_beam.pvalue import AsIter
 from apache_beam.pvalue import AsSingleton
 from apache_beam.transforms import core
 from apache_beam.transforms import ptransform
 from apache_beam.transforms import window
 from apache_beam.transforms.display import DisplayDataItem
 from apache_beam.transforms.display import HasDisplayData
+from apache_beam.utils import timestamp
 from apache_beam.utils import urns
 from apache_beam.utils.windowed_value import WindowedValue
 
 __all__ = ['BoundedSource', 'RangeTracker', 'Read', 'RestrictionTracker',
            'Sink', 'Write', 'Writer']
 
 
@@ -1048,15 +1049,15 @@
     return {'sink_dd': self.sink}
 
   def process(self, element, init_result):
     bundle = element
     writer = self.sink.open_writer(init_result, str(uuid.uuid4()))
     for e in bundle[1]:  # values
       writer.write(e)
-    return [window.TimestampedValue(writer.close(), window.MAX_TIMESTAMP)]
+    return [window.TimestampedValue(writer.close(), timestamp.MAX_TIMESTAMP)]
 
 
 def _pre_finalize(unused_element, sink, init_result, write_results):
   return sink.pre_finalize(init_result, write_results)
 
 
 def _finalize_write(unused_element, sink, init_result, write_results,
@@ -1068,15 +1069,16 @@
         'Creating %s empty shard(s).', min_shards - len(write_results))
     for _ in range(min_shards - len(write_results)):
       writer = sink.open_writer(init_result, str(uuid.uuid4()))
       extra_shards.append(writer.close())
   outputs = sink.finalize_write(init_result, write_results + extra_shards,
                                 pre_finalize_results)
   if outputs:
-    return (window.TimestampedValue(v, window.MAX_TIMESTAMP) for v in outputs)
+    return (
+        window.TimestampedValue(v, timestamp.MAX_TIMESTAMP) for v in outputs)
 
 
 class _RoundRobinKeyFn(core.DoFn):
 
   def __init__(self, count):
     self.count = count
```

## Comparing `apache-beam-2.8.0/apache_beam/io/vcfio_test.py` & `apache-beam-2.9.0/apache_beam/io/vcfio_test.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/io/textio_test.py` & `apache-beam-2.9.0/apache_beam/io/textio_test.py`

 * *Files 0% similar despite different names*

```diff
@@ -483,15 +483,15 @@
       pipeline.run()
 
   def test_read_corrupted_bzip2_fails(self):
     _, lines = write_data(15)
     with TempDir() as tempdir:
       file_name = tempdir.create_temp_file()
       with bz2.BZ2File(file_name, 'wb') as f:
-        f.write('\n'.join(lines))
+        f.write(b'\n'.join(lines))
 
       with open(file_name, 'wb') as f:
         f.write('corrupt')
 
       pipeline = TestPipeline()
       pcoll = pipeline | 'Read' >> ReadFromText(
           file_name,
@@ -562,15 +562,15 @@
       pipeline.run()
 
   def test_read_corrupted_gzip_fails(self):
     _, lines = write_data(15)
     with TempDir() as tempdir:
       file_name = tempdir.create_temp_file()
       with gzip.GzipFile(file_name, 'wb') as f:
-        f.write('\n'.join(lines))
+        f.write(b'\n'.join(lines))
 
       with open(file_name, 'wb') as f:
         f.write('corrupt')
 
       pipeline = TestPipeline()
       pcoll = pipeline | 'Read' >> ReadFromText(
           file_name,
```

## Comparing `apache-beam-2.8.0/apache_beam/io/hadoopfilesystem.py` & `apache-beam-2.9.0/apache_beam/io/hadoopfilesystem.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/io/concat_source_test.py` & `apache-beam-2.9.0/apache_beam/io/concat_source_test.py`

 * *Files 1% similar despite different names*

```diff
@@ -79,14 +79,15 @@
   def __eq__(self, other):
     return (type(self) == type(other)
             and self._start == other._start
             and self._end == other._end
             and self._split_freq == other._split_freq)
 
   def __ne__(self, other):
+    # TODO(BEAM-5949): Needed for Python 2 compatibility.
     return not self == other
 
 
 class ConcatSourceTest(unittest.TestCase):
 
   def test_range_source(self):
     source_test_utils.assert_split_at_fraction_exhaustive(RangeSource(0, 10, 3))
```

## Comparing `apache-beam-2.8.0/apache_beam/io/range_trackers.py` & `apache-beam-2.9.0/apache_beam/io/range_trackers.py`

 * *Files 2% similar despite different names*

```diff
@@ -85,25 +85,14 @@
 
     if record_start < self._last_record_start:
       raise ValueError(
           'Trying to return a record [starting at %d] which is before the '
           'last-returned record [starting at %d]' %
           (record_start, self._last_record_start))
 
-    if split_point:
-      if (self._offset_of_last_split_point != -1 and
-          record_start == self._offset_of_last_split_point):
-        raise ValueError(
-            'Record at a split point has same offset as the previous split '
-            'point: %d' % record_start)
-    elif self._last_record_start == -1:
-      raise ValueError(
-          'The first record [starting at %d] must be at a split point' %
-          record_start)
-
     if (split_point and self._offset_of_last_split_point != -1 and
         record_start == self._offset_of_last_split_point):
       raise ValueError(
           'Record at a split point has same offset as the previous split '
           'point: %d' % record_start)
 
     if not split_point and self._last_record_start == -1:
```

## Comparing `apache-beam-2.8.0/apache_beam/io/avroio.py` & `apache-beam-2.9.0/apache_beam/io/avroio.py`

 * *Files 2% similar despite different names*

```diff
@@ -326,28 +326,28 @@
     return self._size
 
   def offset(self):
     return self._offset
 
   @staticmethod
   def _decompress_bytes(data, codec):
-    if codec == 'null':
+    if codec == b'null':
       return data
-    elif codec == 'deflate':
+    elif codec == b'deflate':
       # zlib.MAX_WBITS is the window size. '-' sign indicates that this is
       # raw data (without headers). See zlib and Avro documentations for more
       # details.
       return zlib.decompress(data, -zlib.MAX_WBITS)
-    elif codec == 'snappy':
+    elif codec == b'snappy':
       # Snappy is an optional avro codec.
       # See Snappy and Avro documentation for more details.
       try:
         import snappy
       except ImportError:
-        raise ValueError('Snappy does not seem to be installed.')
+        raise ValueError('python-snappy does not seem to be installed.')
 
       # Compressed data includes a 4-byte CRC32 checksum which we verify.
       # We take care to avoid extra copies of data while slicing large objects
       # by use of a memoryview.
       result = snappy.decompress(memoryview(data)[:-4])
       avroio.BinaryDecoder(io.BytesIO(data[-4:])).check_crc32(result)
       return result
@@ -356,16 +356,18 @@
 
   def num_records(self):
     return self._num_records
 
   def records(self):
     decoder = avroio.BinaryDecoder(
         io.BytesIO(self._decompressed_block_bytes))
-    reader = avroio.DatumReader(
-        writers_schema=self._schema, readers_schema=self._schema)
+
+    writer_schema = self._schema
+    reader_schema = self._schema
+    reader = avroio.DatumReader(writer_schema, reader_schema)
 
     current_record = 0
     while current_record < self._num_records:
       yield reader.read(decoder)
       current_record += 1
```

## Comparing `apache-beam-2.8.0/apache_beam/io/filebasedsink_test.py` & `apache-beam-2.9.0/apache_beam/io/filebasedsink_test.py`

 * *Files 3% similar despite different names*

```diff
@@ -72,32 +72,28 @@
         delete=False, prefix=name,
         dir=self._new_tempdir(), suffix=suffix).name
     return file_name
 
 
 class MyFileBasedSink(filebasedsink.FileBasedSink):
 
-  @unittest.skipIf(sys.version_info[0] == 3 and
-                   os.environ.get('RUN_SKIPPED_PY3_TESTS') != '1',
-                   'This test still needs to be fixed on Python 3.'
-                   'TODO: BEAM-5627, TODO:5618')
   def open(self, temp_path):
     # TODO: Fix main session pickling.
     # file_handle = super(MyFileBasedSink, self).open(temp_path)
     file_handle = filebasedsink.FileBasedSink.open(self, temp_path)
-    file_handle.write('[start]')
+    file_handle.write(b'[start]')
     return file_handle
 
   def write_encoded_record(self, file_handle, encoded_value):
-    file_handle.write('[')
+    file_handle.write(b'[')
     file_handle.write(encoded_value)
-    file_handle.write(']')
+    file_handle.write(b']')
 
   def close(self, file_handle):
-    file_handle.write('[end]')
+    file_handle.write(b'[end]')
     # TODO: Fix main session pickling.
     # file_handle = super(MyFileBasedSink, self).close(file_handle)
     file_handle = filebasedsink.FileBasedSink.close(self, file_handle)
 
 
 class TestFileBasedSink(_TestCaseWithTempDirCleanUp):
```

## Comparing `apache-beam-2.8.0/apache_beam/io/vcfio.py` & `apache-beam-2.9.0/apache_beam/io/vcfio.py`

 * *Files 2% similar despite different names*

```diff
@@ -121,14 +121,18 @@
     self.info = info or {}
     self.calls = calls or []
 
   def __eq__(self, other):
     return (isinstance(other, Variant) and
             vars(self) == vars(other))
 
+  def __ne__(self, other):
+    # TODO(BEAM-5949): Needed for Python 2 compatibility.
+    return not self == other
+
   def __repr__(self):
     return ', '.join(
         [str(s) for s in [self.reference_name,
                           self.start,
                           self.end,
                           self.reference_bases,
                           self.alternate_bases,
@@ -215,14 +219,18 @@
     self.phaseset = phaseset
     self.info = info or {}
 
   def __eq__(self, other):
     return ((self.name, self.genotype, self.phaseset, self.info) ==
             (other.name, other.genotype, other.phaseset, other.info))
 
+  def __ne__(self, other):
+    # TODO(BEAM-5949): Needed for Python 2 compatibility.
+    return not self == other
+
   def __repr__(self):
     return ', '.join(
         [str(s) for s in [self.name, self.genotype, self.phaseset, self.info]])
 
 
 class _VcfSource(filebasedsource.FileBasedSource):
   """A source for reading VCF files.
```

## Comparing `apache-beam-2.8.0/apache_beam/io/sources_test.py` & `apache-beam-2.9.0/apache_beam/io/sources_test.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/io/avroio_test.py` & `apache-beam-2.9.0/apache_beam/io/avroio_test.py`

 * *Files 0% similar despite different names*

```diff
@@ -50,15 +50,15 @@
 from apache_beam.transforms.display_test import DisplayDataItemMatcher
 
 # Import snappy optionally; some tests will be skipped when import fails.
 try:
   import snappy  # pylint: disable=import-error
 except ImportError:
   snappy = None  # pylint: disable=invalid-name
-  logging.warning('snappy is not installed; some tests will be skipped.')
+  logging.warning('python-snappy is not installed; some tests will be skipped.')
 
 
 class TestAvro(unittest.TestCase):
 
   _temp_files = []
 
   def __init__(self, methodName='runTest'):
@@ -313,21 +313,21 @@
     self._run_avro_test(file_name, None, False, expected_result)
 
   def test_read_with_splitting_compressed_deflate(self):
     file_name = self._write_data(codec='deflate')
     expected_result = self.RECORDS
     self._run_avro_test(file_name, 100, True, expected_result)
 
-  @unittest.skipIf(snappy is None, 'snappy not installed.')
+  @unittest.skipIf(snappy is None, 'python-snappy not installed.')
   def test_read_without_splitting_compressed_snappy(self):
     file_name = self._write_data(codec='snappy')
     expected_result = self.RECORDS
     self._run_avro_test(file_name, None, False, expected_result)
 
-  @unittest.skipIf(snappy is None, 'snappy not installed.')
+  @unittest.skipIf(snappy is None, 'python-snappy not installed.')
   def test_read_with_splitting_compressed_snappy(self):
     file_name = self._write_data(codec='snappy')
     expected_result = self.RECORDS
     self._run_avro_test(file_name, 100, True, expected_result)
 
   def test_read_without_splitting_pattern(self):
     pattern = self._write_pattern(3)
@@ -359,15 +359,15 @@
     with open(file_name, 'rb') as f:
       data = f.read()
 
     # Corrupt the last character of the file which is also the last character of
     # the last sync_marker.
     last_char_index = len(data) - 1
     corrupted_data = data[:last_char_index]
-    corrupted_data += 'A' if data[last_char_index] == 'B' else 'B'
+    corrupted_data += b'A' if data[last_char_index] == b'B' else b'B'
     with tempfile.NamedTemporaryFile(
         delete=False, prefix=tempfile.template) as f:
       f.write(corrupted_data)
       corrupted_file_name = f.name
 
     source = _create_avro_source(
         corrupted_file_name, use_fastavro=self.use_fastavro)
@@ -434,15 +434,15 @@
         # json used for stable sortability
         readback = \
             p \
             | avroio.ReadFromAvro(path + '*', use_fastavro=self.use_fastavro) \
             | beam.Map(json.dumps)
         assert_that(readback, equal_to([json.dumps(r) for r in self.RECORDS]))
 
-  @unittest.skipIf(snappy is None, 'snappy not installed.')
+  @unittest.skipIf(snappy is None, 'python-snappy not installed.')
   def test_sink_transform_snappy(self):
     with tempfile.NamedTemporaryFile() as dst:
       path = dst.name
       with TestPipeline() as p:
         # pylint: disable=expression-not-assigned
         p \
         | beam.Create(self.RECORDS) \
```

## Comparing `apache-beam-2.8.0/apache_beam/io/gcp/bigquery_io_read_it_test.py` & `apache-beam-2.9.0/apache_beam/io/gcp/bigquery_io_read_it_test.py`

 * *Files 1% similar despite different names*

```diff
@@ -50,14 +50,14 @@
                                  self.DEFAULT_TABLE_PREFIX + input_size,
                   'num_records': self.NUM_RECORDS[input_size],
                   'on_success_matcher': all_of(*pipeline_verifiers)}
     bigquery_io_read_pipeline.run(test_pipeline.get_full_options_as_args(
         **extra_opts))
 
   @attr('IT')
-  def bigquery_read_1M_python(self):
+  def test_bigquery_read_1M_python(self):
     self.run_bigquery_io_read_pipeline('1M')
 
 
 if __name__ == '__main__':
   logging.getLogger().setLevel(logging.INFO)
   unittest.main()
```

## Comparing `apache-beam-2.8.0/apache_beam/io/gcp/big_query_query_to_table_it_test.py` & `apache-beam-2.9.0/apache_beam/io/gcp/big_query_query_to_table_it_test.py`

 * *Files 1% similar despite different names*

```diff
@@ -18,14 +18,15 @@
 Integration test for Google Cloud BigQuery.
 """
 
 from __future__ import absolute_import
 
 import datetime
 import logging
+import random
 import time
 import unittest
 
 from hamcrest.core.core.allof import all_of
 from nose.plugins.attrib import attr
 
 from apache_beam.io.gcp import big_query_query_to_table_pipeline
@@ -43,40 +44,41 @@
   pass
 
 BIG_QUERY_DATASET_ID = 'python_query_to_table_'
 NEW_TYPES_INPUT_TABLE = 'python_new_types_table'
 NEW_TYPES_OUTPUT_SCHEMA = (
     '{"fields": [{"name": "bytes","type": "BYTES"},'
     '{"name": "date","type": "DATE"},{"name": "time","type": "TIME"}]}')
-NEW_TYPES_OUTPUT_VERIFY_QUERY = ('SELECT date FROM [%s];')
+NEW_TYPES_OUTPUT_VERIFY_QUERY = ('SELECT date FROM `%s`;')
 # There are problems with query time and bytes with current version of bigquery.
 NEW_TYPES_OUTPUT_EXPECTED = [
     (datetime.date(2000, 1, 1),),
     (datetime.date(2011, 1, 1),),
     (datetime.date(3000, 12, 31),)]
 LEGACY_QUERY = (
     'SELECT * FROM (SELECT "apple" as fruit), (SELECT "orange" as fruit),')
 STANDARD_QUERY = (
     'SELECT * FROM (SELECT "apple" as fruit) '
     'UNION ALL (SELECT "orange" as fruit)')
 NEW_TYPES_QUERY = (
     'SELECT bytes, date, time FROM [%s.%s]')
 DIALECT_OUTPUT_SCHEMA = ('{"fields": [{"name": "fruit","type": "STRING"}]}')
-DIALECT_OUTPUT_VERIFY_QUERY = ('SELECT fruit from [%s];')
+DIALECT_OUTPUT_VERIFY_QUERY = ('SELECT fruit from `%s`;')
 DIALECT_OUTPUT_EXPECTED = [(u'apple',), (u'orange',)]
 
 
 class BigQueryQueryToTableIT(unittest.TestCase):
   def setUp(self):
     self.test_pipeline = TestPipeline(is_integration_test=True)
     self.runner_name = type(self.test_pipeline.runner).__name__
     self.project = self.test_pipeline.get_option('project')
 
     self.bigquery_client = BigQueryWrapper()
-    self.dataset_id = BIG_QUERY_DATASET_ID + str(int(time.time()))
+    self.dataset_id = '%s%s%d' % (BIG_QUERY_DATASET_ID, str(int(time.time())),
+                                  random.randint(0, 10000))
     self.bigquery_client.get_or_create_dataset(self.project, self.dataset_id)
     self.output_table = "%s.output_table" % (self.dataset_id)
 
   def tearDown(self):
     request = bigquery.BigqueryDatasetsDeleteRequest(
         projectId=self.project, datasetId=self.dataset_id,
         deleteContents=True)
```

## Comparing `apache-beam-2.8.0/apache_beam/io/gcp/bigquery_io_read_pipeline.py` & `apache-beam-2.9.0/apache_beam/io/gcp/bigquery_io_read_pipeline.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/io/gcp/datastore_write_it_test.py` & `apache-beam-2.9.0/apache_beam/io/gcp/datastore_write_it_test.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/io/gcp/big_query_query_to_table_pipeline.py` & `apache-beam-2.9.0/apache_beam/io/gcp/big_query_query_to_table_pipeline.py`

 * *Files 2% similar despite different names*

```diff
@@ -58,11 +58,11 @@
   # pylint: disable=bad-continuation
   (p | 'read' >> beam.io.Read(beam.io.BigQuerySource(
       query=known_args.query, use_standard_sql=known_args.use_standard_sql))
    | 'write' >> beam.io.Write(beam.io.BigQuerySink(
            known_args.output,
            schema=table_schema,
            create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED,
-           write_disposition=beam.io.BigQueryDisposition.WRITE_TRUNCATE)))
+           write_disposition=beam.io.BigQueryDisposition.WRITE_EMPTY)))
 
   result = p.run()
   result.wait_until_finish()
```

## Comparing `apache-beam-2.8.0/apache_beam/io/gcp/gcsfilesystem_test.py` & `apache-beam-2.9.0/apache_beam/io/gcp/gcsfilesystem_test.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/io/gcp/__init__.py` & `apache-beam-2.9.0/apache_beam/io/gcp/__init__.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/io/gcp/pubsub_test.py` & `apache-beam-2.9.0/apache_beam/io/gcp/pubsub_test.py`

 * *Files 15% similar despite different names*

```diff
@@ -16,20 +16,17 @@
 # limitations under the License.
 #
 
 """Unit tests for PubSub sources and sinks."""
 
 from __future__ import absolute_import
 
-import functools
 import logging
 import unittest
 from builtins import object
-from builtins import range
-from builtins import zip
 
 import hamcrest as hc
 import mock
 
 import apache_beam as beam
 from apache_beam.io.gcp.pubsub import PubsubMessage
 from apache_beam.io.gcp.pubsub import ReadFromPubSub
@@ -39,37 +36,30 @@
 from apache_beam.io.gcp.pubsub import _PubSubSink
 from apache_beam.io.gcp.pubsub import _PubSubSource
 from apache_beam.options.pipeline_options import StandardOptions
 from apache_beam.runners.direct import transform_evaluator
 from apache_beam.runners.direct.direct_runner import _DirectReadFromPubSub
 from apache_beam.runners.direct.direct_runner import _get_transform_overrides
 from apache_beam.runners.direct.transform_evaluator import _PubSubReadEvaluator
+from apache_beam.testing import test_utils
 from apache_beam.testing.test_pipeline import TestPipeline
 from apache_beam.testing.util import TestWindowedValue
 from apache_beam.testing.util import assert_that
 from apache_beam.testing.util import equal_to
 from apache_beam.transforms import window
 from apache_beam.transforms.core import Create
 from apache_beam.transforms.display import DisplayData
 from apache_beam.transforms.display_test import DisplayDataItemMatcher
 from apache_beam.utils import timestamp
 
 # Protect against environments where the PubSub library is not available.
-# pylint: disable=wrong-import-order, wrong-import-position
 try:
   from google.cloud import pubsub
 except ImportError:
   pubsub = None
-# pylint: enable=wrong-import-order, wrong-import-position
-
-# The protobuf library is only used for running on Dataflow.
-try:
-  from google.cloud.proto.pubsub.v1 import pubsub_pb2
-except ImportError:
-  pubsub_pb2 = None
 
 
 class TestPubsubMessage(unittest.TestCase):
 
   def test_payload_valid(self):
     _ = PubsubMessage('', None)
     _ = PubsubMessage('data', None)
@@ -77,16 +67,15 @@
 
   def test_payload_invalid(self):
     with self.assertRaisesRegexp(ValueError, r'data.*attributes.*must be set'):
       _ = PubsubMessage(None, None)
     with self.assertRaisesRegexp(ValueError, r'data.*attributes.*must be set'):
       _ = PubsubMessage(None, {})
 
-  @unittest.skipIf(pubsub_pb2 is None,
-                   'PubSub proto dependencies are not installed')
+  @unittest.skipIf(pubsub is None, 'GCP dependencies are not installed')
   def test_proto_conversion(self):
     data = 'data'
     attributes = {'k1': 'v1', 'k2': 'v2'}
     m = PubsubMessage(data, attributes)
     m_converted = PubsubMessage._from_proto_str(m._to_proto_str())
     self.assertEqual(m_converted.data, data)
     self.assertEqual(m_converted.attributes, attributes)
@@ -216,15 +205,15 @@
     p.replace_all(overrides)
 
     # Note that the direct output of ReadFromPubSub will be replaced
     # by a PTransformOverride, so we use a no-op Map.
     write_transform = pcoll.producer.inputs[0].producer.transform
 
     # Ensure that the properties passed through correctly
-    self.assertEqual('a_topic', write_transform.dofn.topic_name)
+    self.assertEqual('a_topic', write_transform.dofn.short_topic_name)
 
   def test_expand(self):
     p = TestPipeline()
     p.options.view_as(StandardOptions).streaming = True
     pcoll = (p
              | ReadFromPubSub('projects/fakeprj/topics/baz')
              | WriteToPubSub('projects/fakeprj/topics/a_topic',
@@ -236,15 +225,15 @@
     p.replace_all(overrides)
 
     # Note that the direct output of ReadFromPubSub will be replaced
     # by a PTransformOverride, so we use a no-op Map.
     write_transform = pcoll.producer.inputs[0].producer.transform
 
     # Ensure that the properties passed through correctly
-    self.assertEqual('a_topic', write_transform.dofn.topic_name)
+    self.assertEqual('a_topic', write_transform.dofn.short_topic_name)
     self.assertEqual(True, write_transform.dofn.with_attributes)
     # TODO(BEAM-4275): These properties aren't supported yet in direct runner.
     self.assertEqual(None, write_transform.dofn.id_label)
     self.assertEqual(None, write_transform.dofn.timestamp_attribute)
 
 
 @unittest.skipIf(pubsub is None, 'GCP dependencies are not installed')
@@ -329,375 +318,266 @@
 
 
 transform_evaluator.TransformEvaluatorRegistry._test_evaluators_overrides = {
     _DirectReadFromPubSub: TestPubSubReadEvaluator,
 }
 
 
-class FakePubsubTopic(object):
-
-  def __init__(self, name, client):
-    self.name = name
-    self.client = client
-
-  def subscription(self, name):
-    return FakePubsubSubscription(name, self.name, self.client)
-
-  def batch(self):
-    if self.client.batch is None:
-      self.client.batch = FakeBatch(self.client)
-    return self.client.batch
-
-
-class FakePubsubSubscription(object):
-
-  def __init__(self, name, topic, client):
-    self.name = name
-    self.topic = topic
-    self.client = client
-
-  def create(self):
-    pass
-
-
-class FakeAutoAck(object):
-
-  def __init__(self, sub, **unused_kwargs):
-    self.sub = sub
-
-  def __enter__(self):
-    messages = self.sub.client.messages_read
-    self.ack_id_to_msg = dict(zip(range(len(messages)), messages))
-    return self.ack_id_to_msg
-
-  def __exit__(self, exc_type, exc_val, exc_tb):
-    pass
-
-
-class FakeBatch(object):
-  """Context manager that accept Pubsub client writes via publish().
-
-  Verifies writes on exit.
-  """
-
-  def __init__(self, client):
-    self.client = client
-    self.published = []
-
-  def __enter__(self):
-    return self
-
-  def __exit__(self, exc_type, exc_val, exc_tb):
-    if exc_type is not None:
-      return  # Exception will be raised.
-    hc.assert_that(self.published,
-                   hc.only_contains(*self.client.messages_write))
-
-  def publish(self, message, **attrs):
-    self.published.append([message, attrs])
-
-
-class FakePubsubClient(object):
-
-  def __init__(self, messages_read=None, messages_write=None, project=None,
-               **unused_kwargs):
-    """Creates a Pubsub client fake.
-
-    Args:
-      messages_read: List of PubsubMessage objects to return.
-      messages_write: List of [data, attributes] pairs, corresponding to
-        messages expected to be written to the client.
-      project: Name of GCP project.
-    """
-    self.messages_read = messages_read
-    self.messages_write = messages_write
-    self.project = project
-    self.batch = None
-
-  def topic(self, name):
-    return FakePubsubTopic(name, self)
-
-
-def create_client_message(data, message_id, attributes, publish_time):
-  """Returns a message as it would be returned from Cloud Pub/Sub client.
-
-  This is what the reader sees.
-  """
-  msg = pubsub.message.Message(data, message_id, attributes)
-  msg._service_timestamp = publish_time
-  return msg
-
-
 @unittest.skipIf(pubsub is None, 'GCP dependencies are not installed')
+@mock.patch('google.cloud.pubsub.SubscriberClient')
 class TestReadFromPubSub(unittest.TestCase):
 
-  @mock.patch('google.cloud.pubsub')
   def test_read_messages_success(self, mock_pubsub):
     data = 'data'
-    message_id = 'message_id'
-    publish_time = '2018-03-12T13:37:01.234567Z'
+    publish_time_secs = 1520861821
+    publish_time_nanos = 234567000
     attributes = {'key': 'value'}
-    payloads = [create_client_message(
-        data, message_id, attributes, publish_time)]
+    ack_id = 'ack_id'
+    pull_response = test_utils.create_pull_response([
+        test_utils.PullResponseMessage(
+            data, attributes, publish_time_secs, publish_time_nanos, ack_id)
+    ])
     expected_elements = [
         TestWindowedValue(PubsubMessage(data, attributes),
                           timestamp.Timestamp(1520861821.234567),
                           [window.GlobalWindow()])]
-
-    mock_pubsub.Client = functools.partial(FakePubsubClient, payloads)
-    mock_pubsub.subscription.AutoAck = FakeAutoAck
+    mock_pubsub.return_value.pull.return_value = pull_response
 
     p = TestPipeline()
     p.options.view_as(StandardOptions).streaming = True
     pcoll = (p
              | ReadFromPubSub('projects/fakeprj/topics/a_topic',
                               None, None, with_attributes=True))
     assert_that(pcoll, equal_to(expected_elements), reify_windows=True)
     p.run()
+    mock_pubsub.return_value.acknowledge.assert_has_calls([
+        mock.call(mock.ANY, [ack_id])])
 
-  @mock.patch('google.cloud.pubsub')
   def test_read_strings_success(self, mock_pubsub):
     data = u' \\_()_/'
     data_encoded = data.encode('utf-8')
-    publish_time = '2018-03-12T13:37:01.234567Z'
-    payloads = [create_client_message(data_encoded, None, None, publish_time)]
+    ack_id = 'ack_id'
+    pull_response = test_utils.create_pull_response([
+        test_utils.PullResponseMessage(data_encoded, ack_id=ack_id)
+    ])
     expected_elements = [data]
-
-    mock_pubsub.Client = functools.partial(FakePubsubClient, payloads)
-    mock_pubsub.subscription.AutoAck = FakeAutoAck
+    mock_pubsub.return_value.pull.return_value = pull_response
 
     p = TestPipeline()
     p.options.view_as(StandardOptions).streaming = True
     pcoll = (p
              | ReadStringsFromPubSub('projects/fakeprj/topics/a_topic',
                                      None, None))
     assert_that(pcoll, equal_to(expected_elements))
     p.run()
+    mock_pubsub.return_value.acknowledge.assert_has_calls([
+        mock.call(mock.ANY, [ack_id])])
 
-  @mock.patch('google.cloud.pubsub')
   def test_read_data_success(self, mock_pubsub):
     data_encoded = u' \\_()_/'.encode('utf-8')
-    publish_time = '2018-03-12T13:37:01.234567Z'
-    payloads = [create_client_message(data_encoded, None, None, publish_time)]
+    ack_id = 'ack_id'
+    pull_response = test_utils.create_pull_response([
+        test_utils.PullResponseMessage(data_encoded, ack_id=ack_id)])
     expected_elements = [data_encoded]
-
-    mock_pubsub.Client = functools.partial(FakePubsubClient, payloads)
-    mock_pubsub.subscription.AutoAck = FakeAutoAck
+    mock_pubsub.return_value.pull.return_value = pull_response
 
     p = TestPipeline()
     p.options.view_as(StandardOptions).streaming = True
     pcoll = (p
              | ReadFromPubSub('projects/fakeprj/topics/a_topic', None, None))
     assert_that(pcoll, equal_to(expected_elements))
     p.run()
+    mock_pubsub.return_value.acknowledge.assert_has_calls([
+        mock.call(mock.ANY, [ack_id])])
 
-  @mock.patch('google.cloud.pubsub')
   def test_read_messages_timestamp_attribute_milli_success(self, mock_pubsub):
     data = 'data'
-    message_id = 'message_id'
     attributes = {'time': '1337'}
-    publish_time = '2018-03-12T13:37:01.234567Z'
-    payloads = [
-        create_client_message(data, message_id, attributes, publish_time)]
+    publish_time_secs = 1520861821
+    publish_time_nanos = 234567000
+    ack_id = 'ack_id'
+    pull_response = test_utils.create_pull_response([
+        test_utils.PullResponseMessage(
+            data, attributes, publish_time_secs, publish_time_nanos, ack_id)
+    ])
     expected_elements = [
         TestWindowedValue(
             PubsubMessage(data, attributes),
             timestamp.Timestamp(micros=int(attributes['time']) * 1000),
             [window.GlobalWindow()]),
     ]
-
-    mock_pubsub.Client = functools.partial(FakePubsubClient, payloads)
-    mock_pubsub.subscription.AutoAck = FakeAutoAck
+    mock_pubsub.return_value.pull.return_value = pull_response
 
     p = TestPipeline()
     p.options.view_as(StandardOptions).streaming = True
     pcoll = (p
              | ReadFromPubSub(
                  'projects/fakeprj/topics/a_topic', None, None,
                  with_attributes=True, timestamp_attribute='time'))
     assert_that(pcoll, equal_to(expected_elements), reify_windows=True)
     p.run()
+    mock_pubsub.return_value.acknowledge.assert_has_calls([
+        mock.call(mock.ANY, [ack_id])])
 
-  @mock.patch('google.cloud.pubsub')
   def test_read_messages_timestamp_attribute_rfc3339_success(self, mock_pubsub):
     data = 'data'
-    message_id = 'message_id'
     attributes = {'time': '2018-03-12T13:37:01.234567Z'}
-    publish_time = '2018-03-12T13:37:01.234567Z'
-    payloads = [
-        create_client_message(data, message_id, attributes, publish_time)]
+    publish_time_secs = 1337000000
+    publish_time_nanos = 133700000
+    ack_id = 'ack_id'
+    pull_response = test_utils.create_pull_response([
+        test_utils.PullResponseMessage(
+            data, attributes, publish_time_secs, publish_time_nanos, ack_id)
+    ])
     expected_elements = [
         TestWindowedValue(
             PubsubMessage(data, attributes),
             timestamp.Timestamp.from_rfc3339(attributes['time']),
             [window.GlobalWindow()]),
     ]
-
-    mock_pubsub.Client = functools.partial(FakePubsubClient, payloads)
-    mock_pubsub.subscription.AutoAck = FakeAutoAck
+    mock_pubsub.return_value.pull.return_value = pull_response
 
     p = TestPipeline()
     p.options.view_as(StandardOptions).streaming = True
     pcoll = (p
              | ReadFromPubSub(
                  'projects/fakeprj/topics/a_topic', None, None,
                  with_attributes=True, timestamp_attribute='time'))
     assert_that(pcoll, equal_to(expected_elements), reify_windows=True)
     p.run()
+    mock_pubsub.return_value.acknowledge.assert_has_calls([
+        mock.call(mock.ANY, [ack_id])])
 
-  @mock.patch('google.cloud.pubsub')
   def test_read_messages_timestamp_attribute_missing(self, mock_pubsub):
     data = 'data'
-    message_id = 'message_id'
     attributes = {}
+    publish_time_secs = 1520861821
+    publish_time_nanos = 234567000
     publish_time = '2018-03-12T13:37:01.234567Z'
-    payloads = [
-        create_client_message(data, message_id, attributes, publish_time)]
+    ack_id = 'ack_id'
+    pull_response = test_utils.create_pull_response([
+        test_utils.PullResponseMessage(
+            data, attributes, publish_time_secs, publish_time_nanos, ack_id)
+    ])
     expected_elements = [
         TestWindowedValue(
             PubsubMessage(data, attributes),
             timestamp.Timestamp.from_rfc3339(publish_time),
             [window.GlobalWindow()]),
     ]
-
-    mock_pubsub.Client = functools.partial(FakePubsubClient, payloads)
-    mock_pubsub.subscription.AutoAck = FakeAutoAck
+    mock_pubsub.return_value.pull.return_value = pull_response
 
     p = TestPipeline()
     p.options.view_as(StandardOptions).streaming = True
     pcoll = (p
              | ReadFromPubSub(
                  'projects/fakeprj/topics/a_topic', None, None,
                  with_attributes=True, timestamp_attribute='nonexistent'))
     assert_that(pcoll, equal_to(expected_elements), reify_windows=True)
     p.run()
+    mock_pubsub.return_value.acknowledge.assert_has_calls([
+        mock.call(mock.ANY, [ack_id])])
 
-  @mock.patch('google.cloud.pubsub')
   def test_read_messages_timestamp_attribute_fail_parse(self, mock_pubsub):
     data = 'data'
-    message_id = 'message_id'
     attributes = {'time': '1337 unparseable'}
-    publish_time = '2018-03-12T13:37:01.234567Z'
-    payloads = [
-        create_client_message(data, message_id, attributes, publish_time)]
-
-    mock_pubsub.Client = functools.partial(FakePubsubClient, payloads)
-    mock_pubsub.subscription.AutoAck = FakeAutoAck
+    publish_time_secs = 1520861821
+    publish_time_nanos = 234567000
+    ack_id = 'ack_id'
+    pull_response = test_utils.create_pull_response([
+        test_utils.PullResponseMessage(
+            data, attributes, publish_time_secs, publish_time_nanos, ack_id)
+    ])
+    mock_pubsub.return_value.pull.return_value = pull_response
 
     p = TestPipeline()
     p.options.view_as(StandardOptions).streaming = True
     _ = (p
          | ReadFromPubSub(
              'projects/fakeprj/topics/a_topic', None, None,
              with_attributes=True, timestamp_attribute='time'))
     with self.assertRaisesRegexp(ValueError, r'parse'):
       p.run()
+    mock_pubsub.return_value.acknowledge.assert_not_called()
 
-  @mock.patch('google.cloud.pubsub')
-  def test_read_message_id_label_unsupported(self, mock_pubsub):
+  def test_read_message_id_label_unsupported(self, unused_mock_pubsub):
     # id_label is unsupported in DirectRunner.
-    data = 'data'
-    message_id = 'message_id'
-    attributes = {'time': '1337 unparseable'}
-    publish_time = '2018-03-12T13:37:01.234567Z'
-    payloads = [
-        create_client_message(data, message_id, attributes, publish_time)]
-
-    mock_pubsub.Client = functools.partial(FakePubsubClient, payloads)
-    mock_pubsub.subscription.AutoAck = FakeAutoAck
-
     p = TestPipeline()
     p.options.view_as(StandardOptions).streaming = True
     _ = (p | ReadFromPubSub('projects/fakeprj/topics/a_topic', None, 'a_label'))
     with self.assertRaisesRegexp(NotImplementedError,
                                  r'id_label is not supported'):
       p.run()
 
 
 @unittest.skipIf(pubsub is None, 'GCP dependencies are not installed')
+@mock.patch('google.cloud.pubsub.PublisherClient')
 class TestWriteToPubSub(unittest.TestCase):
 
-  @mock.patch('google.cloud.pubsub')
   def test_write_messages_success(self, mock_pubsub):
     data = 'data'
     payloads = [data]
-    expected_payloads = [[data, {}]]
-
-    mock_pubsub.Client = functools.partial(FakePubsubClient,
-                                           messages_write=expected_payloads)
 
     p = TestPipeline()
     p.options.view_as(StandardOptions).streaming = True
     _ = (p
          | Create(payloads)
          | WriteToPubSub('projects/fakeprj/topics/a_topic',
                          with_attributes=False))
     p.run()
+    mock_pubsub.return_value.publish.assert_has_calls([
+        mock.call(mock.ANY, data)])
 
-  @mock.patch('google.cloud.pubsub')
   def test_write_messages_deprecated(self, mock_pubsub):
     data = 'data'
     payloads = [data]
-    expected_payloads = [[data, {}]]
-
-    mock_pubsub.Client = functools.partial(FakePubsubClient,
-                                           messages_write=expected_payloads)
 
     p = TestPipeline()
     p.options.view_as(StandardOptions).streaming = True
     _ = (p
          | Create(payloads)
          | WriteStringsToPubSub('projects/fakeprj/topics/a_topic'))
     p.run()
+    mock_pubsub.return_value.publish.assert_has_calls([
+        mock.call(mock.ANY, data)])
 
-  @mock.patch('google.cloud.pubsub')
   def test_write_messages_with_attributes_success(self, mock_pubsub):
     data = 'data'
     attributes = {'key': 'value'}
     payloads = [PubsubMessage(data, attributes)]
-    expected_payloads = [[data, attributes]]
-
-    mock_pubsub.Client = functools.partial(FakePubsubClient,
-                                           messages_write=expected_payloads)
 
     p = TestPipeline()
     p.options.view_as(StandardOptions).streaming = True
     _ = (p
          | Create(payloads)
          | WriteToPubSub('projects/fakeprj/topics/a_topic',
                          with_attributes=True))
     p.run()
+    mock_pubsub.return_value.publish.assert_has_calls([
+        mock.call(mock.ANY, data, **attributes)])
 
-  @mock.patch('google.cloud.pubsub')
   def test_write_messages_with_attributes_error(self, mock_pubsub):
     data = 'data'
     # Sending raw data when WriteToPubSub expects a PubsubMessage object.
     payloads = [data]
 
-    mock_pubsub.Client = functools.partial(FakePubsubClient)
-
     p = TestPipeline()
     p.options.view_as(StandardOptions).streaming = True
     _ = (p
          | Create(payloads)
          | WriteToPubSub('projects/fakeprj/topics/a_topic',
                          with_attributes=True))
     with self.assertRaisesRegexp(AttributeError,
                                  r'str.*has no attribute.*data'):
       p.run()
 
-  @mock.patch('google.cloud.pubsub')
   def test_write_messages_unsupported_features(self, mock_pubsub):
     data = 'data'
     attributes = {'key': 'value'}
     payloads = [PubsubMessage(data, attributes)]
-    expected_payloads = [[data, attributes]]
-
-    mock_pubsub.Client = functools.partial(FakePubsubClient,
-                                           messages_write=expected_payloads)
 
     p = TestPipeline()
     p.options.view_as(StandardOptions).streaming = True
     _ = (p
          | Create(payloads)
          | WriteToPubSub('projects/fakeprj/topics/a_topic',
                          id_label='a_label'))
```

## Comparing `apache-beam-2.8.0/apache_beam/io/gcp/pubsub_integration_test.py` & `apache-beam-2.9.0/apache_beam/io/gcp/pubsub_integration_test.py`

 * *Files 12% similar despite different names*

```diff
@@ -35,16 +35,19 @@
 from apache_beam.testing.test_pipeline import TestPipeline
 
 INPUT_TOPIC = 'psit_topic_input'
 OUTPUT_TOPIC = 'psit_topic_output'
 INPUT_SUB = 'psit_subscription_input'
 OUTPUT_SUB = 'psit_subscription_output'
 
-TEST_PIPELINE_DURATION_MS = 30 * 1000
-# Takes into account Dataflow pipelines startup time.
+# How long TestXXXRunner will wait for pubsub_it_pipeline to run before
+# cancelling it.
+TEST_PIPELINE_DURATION_MS = 3 * 60 * 1000
+# How long PubSubMessageMatcher will wait for the correct set of messages to
+# appear.
 MESSAGE_MATCHER_TIMEOUT_S = 5 * 60
 
 
 class PubSubIntegrationTest(unittest.TestCase):
 
   ID_LABEL = 'id'
   TIMESTAMP_ATTRIBUTE = 'timestamp'
@@ -96,29 +99,33 @@
     self.test_pipeline = TestPipeline(is_integration_test=True)
     self.runner_name = type(self.test_pipeline.runner).__name__
     self.project = self.test_pipeline.get_option('project')
     self.uuid = str(uuid.uuid4())
 
     # Set up PubSub environment.
     from google.cloud import pubsub
-    self.pubsub_client = pubsub.Client(project=self.project)
-    self.input_topic = self.pubsub_client.topic(INPUT_TOPIC + self.uuid)
-    self.output_topic = self.pubsub_client.topic(OUTPUT_TOPIC + self.uuid)
-    self.input_sub = self.input_topic.subscription(INPUT_SUB + self.uuid)
-    self.output_sub = self.output_topic.subscription(OUTPUT_SUB + self.uuid)
-
-    self.input_topic.create()
-    self.output_topic.create()
-    test_utils.wait_for_topics_created([self.input_topic, self.output_topic])
-    self.input_sub.create()
-    self.output_sub.create()
+    self.pub_client = pubsub.PublisherClient()
+    self.input_topic = self.pub_client.create_topic(
+        self.pub_client.topic_path(self.project, INPUT_TOPIC + self.uuid))
+    self.output_topic = self.pub_client.create_topic(
+        self.pub_client.topic_path(self.project, OUTPUT_TOPIC + self.uuid))
+
+    self.sub_client = pubsub.SubscriberClient()
+    self.input_sub = self.sub_client.create_subscription(
+        self.sub_client.subscription_path(self.project, INPUT_SUB + self.uuid),
+        self.input_topic.name)
+    self.output_sub = self.sub_client.create_subscription(
+        self.sub_client.subscription_path(self.project, OUTPUT_SUB + self.uuid),
+        self.output_topic.name)
 
   def tearDown(self):
-    test_utils.cleanup_subscriptions([self.input_sub, self.output_sub])
-    test_utils.cleanup_topics([self.input_topic, self.output_topic])
+    test_utils.cleanup_subscriptions(self.sub_client,
+                                     [self.input_sub, self.output_sub])
+    test_utils.cleanup_topics(self.pub_client,
+                              [self.input_topic, self.output_topic])
 
   def _test_streaming(self, with_attributes):
     """Runs IT pipeline with message verifier.
 
     Args:
       with_attributes: False - Reads and writes message data only.
         True - Reads and writes message data and attributes. Also verifies
@@ -135,29 +142,28 @@
       expected_messages = [pubsub_msg.data for pubsub_msg in expected_messages]
     if self.runner_name == 'TestDirectRunner':
       strip_attributes = None
     else:
       strip_attributes = [self.ID_LABEL, self.TIMESTAMP_ATTRIBUTE]
     pubsub_msg_verifier = PubSubMessageMatcher(
         self.project,
-        OUTPUT_SUB + self.uuid,
+        self.output_sub.name,
         expected_messages,
         timeout=MESSAGE_MATCHER_TIMEOUT_S,
         with_attributes=with_attributes,
         strip_attributes=strip_attributes)
-    extra_opts = {'input_subscription': self.input_sub.full_name,
-                  'output_topic': self.output_topic.full_name,
+    extra_opts = {'input_subscription': self.input_sub.name,
+                  'output_topic': self.output_topic.name,
                   'wait_until_finish_duration': TEST_PIPELINE_DURATION_MS,
                   'on_success_matcher': all_of(state_verifier,
                                                pubsub_msg_verifier)}
 
     # Generate input data and inject to PubSub.
-    test_utils.wait_for_subscriptions_created([self.input_sub])
     for msg in self.INPUT_MESSAGES[self.runner_name]:
-      self.input_topic.publish(msg.data, **msg.attributes)
+      self.pub_client.publish(self.input_topic.name, msg.data, **msg.attributes)
 
     # Get pipeline options from command argument: --test-pipeline-options,
     # and start pipeline job by calling pipeline main function.
     pubsub_it_pipeline.run_pipeline(
         argv=self.test_pipeline.get_full_options_as_args(**extra_opts),
         with_attributes=with_attributes,
         id_label=self.ID_LABEL,
```

## Comparing `apache-beam-2.8.0/apache_beam/io/gcp/pubsub_it_pipeline.py` & `apache-beam-2.9.0/apache_beam/io/gcp/pubsub_it_pipeline.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/io/gcp/bigquery.py` & `apache-beam-2.9.0/apache_beam/io/gcp/bigquery.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/io/gcp/datastore_write_it_pipeline.py` & `apache-beam-2.9.0/apache_beam/io/gcp/datastore_write_it_pipeline.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/io/gcp/bigquery_test.py` & `apache-beam-2.9.0/apache_beam/io/gcp/bigquery_test.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/io/gcp/gcsio_test.py` & `apache-beam-2.9.0/apache_beam/io/gcp/gcsio_test.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/io/gcp/gcsio.py` & `apache-beam-2.9.0/apache_beam/io/gcp/gcsio.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/io/gcp/gcsfilesystem.py` & `apache-beam-2.9.0/apache_beam/io/gcp/gcsfilesystem.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/io/gcp/pubsub.py` & `apache-beam-2.9.0/apache_beam/io/gcp/pubsub.py`

 * *Files 1% similar despite different names*

```diff
@@ -34,19 +34,18 @@
 from apache_beam.io.iobase import Write
 from apache_beam.runners.dataflow.native_io import iobase as dataflow_io
 from apache_beam.transforms import Map
 from apache_beam.transforms import PTransform
 from apache_beam.transforms.display import DisplayDataItem
 from apache_beam.utils.annotations import deprecated
 
-# The protobuf library is only used for running on Dataflow.
 try:
-  from google.cloud.proto.pubsub.v1 import pubsub_pb2
+  from google.cloud import pubsub
 except ImportError:
-  pubsub_pb2 = None
+  pubsub = None
 
 __all__ = ['PubsubMessage', 'ReadFromPubSub', 'ReadStringsFromPubSub',
            'WriteStringsToPubSub', 'WriteToPubSub']
 
 
 class PubsubMessage(object):
   """Represents a Cloud Pub/Sub message.
@@ -74,29 +73,33 @@
     return hash((self.data, frozenset(self.attributes.items())))
 
   def __eq__(self, other):
     return isinstance(other, PubsubMessage) and (
         self.data == other.data and
         self.attributes == other.attributes)
 
+  def __ne__(self, other):
+    # TODO(BEAM-5949): Needed for Python 2 compatibility.
+    return not self == other
+
   def __repr__(self):
     return 'PubsubMessage(%s, %s)' % (self.data, self.attributes)
 
   @staticmethod
   def _from_proto_str(proto_msg):
     """Construct from serialized form of ``PubsubMessage``.
 
     Args:
       proto_msg: String containing a serialized protobuf of type
       https://cloud.google.com/pubsub/docs/reference/rpc/google.pubsub.v1#google.pubsub.v1.PubsubMessage
 
     Returns:
       A new PubsubMessage object.
     """
-    msg = pubsub_pb2.PubsubMessage()
+    msg = pubsub.types.pubsub_pb2.PubsubMessage()
     msg.ParseFromString(proto_msg)
     # Convert ScalarMapContainer to dict.
     attributes = dict((key, msg.attributes[key]) for key in msg.attributes)
     return PubsubMessage(msg.data, attributes)
 
   def _to_proto_str(self):
     """Get serialized form of ``PubsubMessage``.
@@ -105,25 +108,25 @@
       proto_msg: str containing a serialized protobuf.
 
     Returns:
       A str containing a serialized protobuf of type
       https://cloud.google.com/pubsub/docs/reference/rpc/google.pubsub.v1#google.pubsub.v1.PubsubMessage
       containing the payload of this object.
     """
-    msg = pubsub_pb2.PubsubMessage()
+    msg = pubsub.types.pubsub_pb2.PubsubMessage()
     msg.data = self.data
     for key, value in self.attributes.iteritems():
       msg.attributes[key] = value
     return msg.SerializeToString()
 
   @staticmethod
   def _from_message(msg):
-    """Construct from ``google.cloud.pubsub.message.Message``.
+    """Construct from ``google.cloud.pubsub_v1.subscriber.message.Message``.
 
-    https://google-cloud-python.readthedocs.io/en/latest/pubsub/subscriber/api/message.html
+    https://googleapis.github.io/google-cloud-python/latest/pubsub/subscriber/api/message.html
     """
     # Convert ScalarMapContainer to dict.
     attributes = dict((key, msg.attributes[key]) for key in msg.attributes)
     return PubsubMessage(msg.data, attributes)
 
 
 class ReadFromPubSub(PTransform):
```

## Comparing `apache-beam-2.8.0/apache_beam/io/gcp/tests/utils.py` & `apache-beam-2.9.0/apache_beam/io/gcp/tests/utils.py`

 * *Files 20% similar despite different names*

```diff
@@ -17,49 +17,79 @@
 
 
 """Utility methods for testing on GCP."""
 
 from __future__ import absolute_import
 
 import logging
+import time
 
 from apache_beam.utils import retry
 
 # Protect against environments where bigquery library is not available.
 try:
   from google.cloud import bigquery
+  from google.cloud.exceptions import NotFound
 except ImportError:
   bigquery = None
+  NotFound = None
 
 
 class GcpTestIOError(retry.PermanentException):
   """Basic GCP IO error for testing. Function that raises this error should
   not be retried."""
   pass
 
 
 @retry.with_exponential_backoff(
     num_retries=3,
     retry_filter=retry.retry_on_server_errors_filter)
-def delete_bq_table(project, dataset, table):
-  """Delete a Biqquery table.
+def create_bq_dataset(project, dataset_base_name):
+  """Creates an empty BigQuery dataset.
+
+  Args:
+    project: Project to work in.
+    dataset_base_name: Prefix for dataset id.
+
+  Returns:
+    A ``google.cloud.bigquery.dataset.DatasetReference`` object pointing to the
+    new dataset.
+  """
+  client = bigquery.Client(project=project)
+  unique_dataset_name = dataset_base_name + str(int(time.time()))
+  dataset_ref = client.dataset(unique_dataset_name, project=project)
+  dataset = bigquery.Dataset(dataset_ref)
+  client.create_dataset(dataset)
+  return dataset_ref
+
+
+@retry.with_exponential_backoff(
+    num_retries=3,
+    retry_filter=retry.retry_on_server_errors_filter)
+def delete_bq_dataset(project, dataset_ref):
+  """Deletes a BigQuery dataset and its contents.
+
+  Args:
+    project: Project to work in.
+    dataset_ref: A ``google.cloud.bigquery.dataset.DatasetReference`` object
+      pointing to the dataset to delete.
+  """
+  client = bigquery.Client(project=project)
+  client.delete_dataset(dataset_ref, delete_contents=True)
+
+
+def delete_bq_table(project, dataset_id, table_id):
+  """Delete a BiqQuery table.
 
   Args:
     project: Name of the project.
-    dataset: Name of the dataset where table is.
-    table:   Name of the table.
+    dataset_id: Name of the dataset where table is.
+    table_id: Name of the table.
   """
-  logging.info('Clean up a Bigquery table with project: %s, dataset: %s, '
-               'table: %s.', project, dataset, table)
-  bq_dataset = bigquery.Client(project=project).dataset(dataset)
-  if not bq_dataset.exists():
-    raise GcpTestIOError('Failed to cleanup. Bigquery dataset %s doesn\'t '
-                         'exist in project %s.' % (dataset, project))
-  bq_table = bq_dataset.table(table)
-  if not bq_table.exists():
-    raise GcpTestIOError('Failed to cleanup. Bigquery table %s doesn\'t '
-                         'exist in project %s, dataset %s.' %
-                         (table, project, dataset))
-  bq_table.delete()
-  if bq_table.exists():
-    raise RuntimeError('Failed to cleanup. Bigquery table %s still exists '
-                       'after cleanup.' % table)
+  logging.info('Clean up a BigQuery table with project: %s, dataset: %s, '
+               'table: %s.', project, dataset_id, table_id)
+  client = bigquery.Client(project=project)
+  table_ref = client.dataset(dataset_id).table(table_id)
+  try:
+    client.delete_table(table_ref)
+  except NotFound:
+    raise GcpTestIOError('BigQuery table does not exist: %s' % table_ref)
```

## Comparing `apache-beam-2.8.0/apache_beam/io/gcp/tests/__init__.py` & `apache-beam-2.9.0/apache_beam/examples/complete/game/__init__.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/io/gcp/tests/pubsub_matcher.py` & `apache-beam-2.9.0/apache_beam/io/gcp/tests/pubsub_matcher.py`

 * *Files 12% similar despite different names*

```diff
@@ -27,44 +27,44 @@
 
 from apache_beam.io.gcp.pubsub import PubsubMessage
 
 __all__ = ['PubSubMessageMatcher']
 
 
 # Protect against environments where pubsub library is not available.
-# pylint: disable=wrong-import-order, wrong-import-position
 try:
   from google.cloud import pubsub
 except ImportError:
   pubsub = None
-# pylint: enable=wrong-import-order, wrong-import-position
 
 DEFAULT_TIMEOUT = 5 * 60
 MAX_MESSAGES_IN_ONE_PULL = 50
 
 
 class PubSubMessageMatcher(BaseMatcher):
   """Matcher that verifies messages from given subscription.
 
   This matcher can block the test and keep pulling messages from given
   subscription until all expected messages are shown or timeout.
   """
 
-  def __init__(self, project, sub_name, expected_msg, timeout=DEFAULT_TIMEOUT,
-               with_attributes=False, strip_attributes=None):
+  def __init__(self, project, sub_name, expected_msg,
+               timeout=DEFAULT_TIMEOUT, with_attributes=False,
+               strip_attributes=None):
     """Initialize PubSubMessageMatcher object.
 
     Args:
       project: A name string of project.
       sub_name: A name string of subscription which is attached to output.
       expected_msg: A string list that contains expected message data pulled
         from the subscription. See also: with_attributes.
       timeout: Timeout in seconds to wait for all expected messages appears.
-      with_attributes: Whether expected_msg is a list of
-        ``PubsubMessage`` objects.
+      with_attributes: If True, will match against both message data and
+        attributes. If True, expected_msg should be a list of ``PubsubMessage``
+        objects. Otherwise, it should be a list of ``bytes``.
       strip_attributes: List of strings. If with_attributes==True, strip the
         attributes keyed by these values from incoming messages.
         If a key is missing, will add an attribute with an error message as
         value to prevent a successful match.
     """
     if pubsub is None:
       raise ImportError(
@@ -82,51 +82,53 @@
     self.timeout = timeout
     self.messages = None
     self.with_attributes = with_attributes
     self.strip_attributes = strip_attributes
 
   def _matches(self, _):
     if self.messages is None:
-      self.messages = self._wait_for_messages(self._get_subscription(),
-                                              len(self.expected_msg),
+      self.messages = self._wait_for_messages(len(self.expected_msg),
                                               self.timeout)
     return Counter(self.messages) == Counter(self.expected_msg)
 
-  def _get_subscription(self):
-    return pubsub.Client(project=self.project).subscription(self.sub_name)
-
-  def _wait_for_messages(self, subscription, expected_num, timeout):
+  def _wait_for_messages(self, expected_num, timeout):
     """Wait for messages from given subscription."""
-    logging.debug('Start pulling messages from %s', subscription.full_name)
     total_messages = []
+
+    sub_client = pubsub.SubscriberClient()
     start_time = time.time()
     while time.time() - start_time <= timeout:
-      pulled = subscription.pull(max_messages=MAX_MESSAGES_IN_ONE_PULL)
-      for ack_id, message in pulled:
-        subscription.acknowledge([ack_id])
+      response = sub_client.pull(self.sub_name,
+                                 max_messages=MAX_MESSAGES_IN_ONE_PULL,
+                                 return_immediately=True)
+      for rm in response.received_messages:
+        msg = PubsubMessage._from_message(rm.message)
         if not self.with_attributes:
-          total_messages.append(message.data)
+          total_messages.append(msg.data)
           continue
 
-        msg = PubsubMessage._from_message(message)
         if self.strip_attributes:
           for attr in self.strip_attributes:
             try:
               del msg.attributes[attr]
             except KeyError:
               msg.attributes[attr] = ('PubSubMessageMatcher error: '
                                       'expected attribute not found.')
         total_messages.append(msg)
 
+      ack_ids = [rm.ack_id for rm in response.received_messages]
+      if ack_ids:
+        sub_client.acknowledge(self.sub_name, ack_ids)
       if len(total_messages) >= expected_num:
-        return total_messages
+        break
       time.sleep(1)
 
-    logging.error('Timeout after %d sec. Received %d messages from %s.',
-                  timeout, len(total_messages), subscription.full_name)
+    if time.time() - start_time > timeout:
+      logging.error('Timeout after %d sec. Received %d messages from %s.',
+                    timeout, len(total_messages), self.sub_name)
     return total_messages
 
   def describe_to(self, description):
     description.append_text(
         'Expected %d messages.' % len(self.expected_msg))
 
   def describe_mismatch(self, _, mismatch_description):
```

## Comparing `apache-beam-2.8.0/apache_beam/io/gcp/tests/bigquery_matcher.py` & `apache-beam-2.9.0/apache_beam/io/gcp/tests/bigquery_matcher.py`

 * *Files 11% similar despite different names*

```diff
@@ -90,27 +90,16 @@
     return self.checksum == self.expected_checksum
 
   @retry.with_exponential_backoff(
       num_retries=MAX_RETRIES,
       retry_filter=retry_on_http_and_value_error)
   def _query_with_retry(self, bigquery_client):
     """Run Bigquery query with retry if got error http response"""
-    query = bigquery_client.run_sync_query(self.query)
-    query.run()
-
-    # Fetch query data one page at a time.
-    page_token = None
-    results = []
-    while True:
-      for row in query.fetch_data(page_token=page_token):
-        results.append(row)
-      if results:
-        break
-
-    return results
+    query_job = bigquery_client.query(self.query)
+    return [row.values() for row in query_job]
 
   def describe_to(self, description):
     description \
       .append_text("Expected checksum is ") \
       .append_text(self.expected_checksum)
 
   def describe_mismatch(self, pipeline_result, mismatch_description):
```

## Comparing `apache-beam-2.8.0/apache_beam/io/gcp/datastore/__init__.py` & `apache-beam-2.9.0/apache_beam/io/gcp/internal/__init__.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/io/gcp/datastore/v1/helper_test.py` & `apache-beam-2.9.0/apache_beam/io/gcp/datastore/v1/helper_test.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/io/gcp/datastore/v1/fake_datastore.py` & `apache-beam-2.9.0/apache_beam/io/gcp/datastore/v1/fake_datastore.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/io/gcp/datastore/v1/adaptive_throttler.py` & `apache-beam-2.9.0/apache_beam/io/gcp/datastore/v1/adaptive_throttler.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/io/gcp/datastore/v1/__init__.py` & `apache-beam-2.9.0/apache_beam/io/gcp/internal/clients/__init__.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/io/gcp/datastore/v1/util_test.py` & `apache-beam-2.9.0/apache_beam/io/gcp/datastore/v1/util_test.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/io/gcp/datastore/v1/adaptive_throttler_test.py` & `apache-beam-2.9.0/apache_beam/io/gcp/datastore/v1/adaptive_throttler_test.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/io/gcp/datastore/v1/query_splitter_test.py` & `apache-beam-2.9.0/apache_beam/io/gcp/datastore/v1/query_splitter_test.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/io/gcp/datastore/v1/query_splitter.py` & `apache-beam-2.9.0/apache_beam/io/gcp/datastore/v1/query_splitter.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/io/gcp/datastore/v1/helper.py` & `apache-beam-2.9.0/apache_beam/io/gcp/datastore/v1/helper.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/io/gcp/datastore/v1/datastoreio_test.py` & `apache-beam-2.9.0/apache_beam/io/gcp/datastore/v1/datastoreio_test.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/io/gcp/datastore/v1/datastoreio.py` & `apache-beam-2.9.0/apache_beam/io/gcp/datastore/v1/datastoreio.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/io/gcp/datastore/v1/util.py` & `apache-beam-2.9.0/apache_beam/io/gcp/datastore/v1/util.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/io/gcp/internal/__init__.py` & `apache-beam-2.9.0/apache_beam/io/gcp/datastore/__init__.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/io/gcp/internal/clients/__init__.py` & `apache-beam-2.9.0/apache_beam/io/gcp/datastore/v1/__init__.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/io/gcp/internal/clients/bigquery/__init__.py` & `apache-beam-2.9.0/apache_beam/io/gcp/internal/clients/bigquery/__init__.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/io/gcp/internal/clients/bigquery/bigquery_v2_client.py` & `apache-beam-2.9.0/apache_beam/io/gcp/internal/clients/bigquery/bigquery_v2_client.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/io/gcp/internal/clients/bigquery/bigquery_v2_messages.py` & `apache-beam-2.9.0/apache_beam/io/gcp/internal/clients/bigquery/bigquery_v2_messages.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/io/gcp/internal/clients/storage/storage_v1_client.py` & `apache-beam-2.9.0/apache_beam/io/gcp/internal/clients/storage/storage_v1_client.py`

 * *Files 24% similar despite different names*

```diff
@@ -26,14 +26,15 @@
     storage_v1_messages as messages
 
 
 class StorageV1(base_api.BaseApiClient):
   """Generated client library for service storage version v1."""
 
   MESSAGES_MODULE = messages
+  BASE_URL = u'https://www.googleapis.com/storage/v1/'
 
   _PACKAGE = u'storage'
   _SCOPES = [u'https://www.googleapis.com/auth/cloud-platform', u'https://www.googleapis.com/auth/cloud-platform.read-only', u'https://www.googleapis.com/auth/devstorage.full_control', u'https://www.googleapis.com/auth/devstorage.read_only', u'https://www.googleapis.com/auth/devstorage.read_write']
   _VERSION = u'v1'
   _CLIENT_ID = '1042881264118.apps.googleusercontent.com'
   _CLIENT_SECRET = 'x_Tw5K8nnjoRAqULM9PFAC2b'
   _USER_AGENT = 'x_Tw5K8nnjoRAqULM9PFAC2b'
@@ -41,1003 +42,1354 @@
   _URL_VERSION = u'v1'
   _API_KEY = None
 
   def __init__(self, url='', credentials=None,
                get_credentials=True, http=None, model=None,
                log_request=False, log_response=False,
                credentials_args=None, default_global_params=None,
-               additional_http_headers=None):
+               additional_http_headers=None, response_encoding=None):
     """Create a new storage handle."""
-    url = url or u'https://www.googleapis.com/storage/v1/'
+    url = url or self.BASE_URL
     super(StorageV1, self).__init__(
         url, credentials=credentials,
         get_credentials=get_credentials, http=http, model=model,
         log_request=log_request, log_response=log_response,
         credentials_args=credentials_args,
         default_global_params=default_global_params,
-        additional_http_headers=additional_http_headers)
+        additional_http_headers=additional_http_headers,
+        response_encoding=response_encoding)
     self.bucketAccessControls = self.BucketAccessControlsService(self)
     self.buckets = self.BucketsService(self)
     self.channels = self.ChannelsService(self)
     self.defaultObjectAccessControls = self.DefaultObjectAccessControlsService(self)
+    self.notifications = self.NotificationsService(self)
     self.objectAccessControls = self.ObjectAccessControlsService(self)
     self.objects = self.ObjectsService(self)
+    self.projects_serviceAccount = self.ProjectsServiceAccountService(self)
+    self.projects = self.ProjectsService(self)
 
   class BucketAccessControlsService(base_api.BaseApiService):
     """Service class for the bucketAccessControls resource."""
 
     _NAME = u'bucketAccessControls'
 
     def __init__(self, client):
       super(StorageV1.BucketAccessControlsService, self).__init__(client)
-      self._method_configs = {
-          'Delete': base_api.ApiMethodInfo(
-              http_method=u'DELETE',
-              method_id=u'storage.bucketAccessControls.delete',
-              ordered_params=[u'bucket', u'entity'],
-              path_params=[u'bucket', u'entity'],
-              query_params=[],
-              relative_path=u'b/{bucket}/acl/{entity}',
-              request_field='',
-              request_type_name=u'StorageBucketAccessControlsDeleteRequest',
-              response_type_name=u'StorageBucketAccessControlsDeleteResponse',
-              supports_download=False,
-          ),
-          'Get': base_api.ApiMethodInfo(
-              http_method=u'GET',
-              method_id=u'storage.bucketAccessControls.get',
-              ordered_params=[u'bucket', u'entity'],
-              path_params=[u'bucket', u'entity'],
-              query_params=[],
-              relative_path=u'b/{bucket}/acl/{entity}',
-              request_field='',
-              request_type_name=u'StorageBucketAccessControlsGetRequest',
-              response_type_name=u'BucketAccessControl',
-              supports_download=False,
-          ),
-          'Insert': base_api.ApiMethodInfo(
-              http_method=u'POST',
-              method_id=u'storage.bucketAccessControls.insert',
-              ordered_params=[u'bucket'],
-              path_params=[u'bucket'],
-              query_params=[],
-              relative_path=u'b/{bucket}/acl',
-              request_field='<request>',
-              request_type_name=u'BucketAccessControl',
-              response_type_name=u'BucketAccessControl',
-              supports_download=False,
-          ),
-          'List': base_api.ApiMethodInfo(
-              http_method=u'GET',
-              method_id=u'storage.bucketAccessControls.list',
-              ordered_params=[u'bucket'],
-              path_params=[u'bucket'],
-              query_params=[],
-              relative_path=u'b/{bucket}/acl',
-              request_field='',
-              request_type_name=u'StorageBucketAccessControlsListRequest',
-              response_type_name=u'BucketAccessControls',
-              supports_download=False,
-          ),
-          'Patch': base_api.ApiMethodInfo(
-              http_method=u'PATCH',
-              method_id=u'storage.bucketAccessControls.patch',
-              ordered_params=[u'bucket', u'entity'],
-              path_params=[u'bucket', u'entity'],
-              query_params=[],
-              relative_path=u'b/{bucket}/acl/{entity}',
-              request_field='<request>',
-              request_type_name=u'BucketAccessControl',
-              response_type_name=u'BucketAccessControl',
-              supports_download=False,
-          ),
-          'Update': base_api.ApiMethodInfo(
-              http_method=u'PUT',
-              method_id=u'storage.bucketAccessControls.update',
-              ordered_params=[u'bucket', u'entity'],
-              path_params=[u'bucket', u'entity'],
-              query_params=[],
-              relative_path=u'b/{bucket}/acl/{entity}',
-              request_field='<request>',
-              request_type_name=u'BucketAccessControl',
-              response_type_name=u'BucketAccessControl',
-              supports_download=False,
-          ),
-          }
-
       self._upload_configs = {
           }
 
     def Delete(self, request, global_params=None):
-      """Permanently deletes the ACL entry for the specified entity on the specified bucket.
+      r"""Permanently deletes the ACL entry for the specified entity on the specified bucket.
 
       Args:
         request: (StorageBucketAccessControlsDeleteRequest) input message
         global_params: (StandardQueryParameters, default: None) global arguments
       Returns:
         (StorageBucketAccessControlsDeleteResponse) The response message.
       """
       config = self.GetMethodConfig('Delete')
       return self._RunMethod(
           config, request, global_params=global_params)
 
+    Delete.method_config = lambda: base_api.ApiMethodInfo(
+        http_method=u'DELETE',
+        method_id=u'storage.bucketAccessControls.delete',
+        ordered_params=[u'bucket', u'entity'],
+        path_params=[u'bucket', u'entity'],
+        query_params=[u'userProject'],
+        relative_path=u'b/{bucket}/acl/{entity}',
+        request_field='',
+        request_type_name=u'StorageBucketAccessControlsDeleteRequest',
+        response_type_name=u'StorageBucketAccessControlsDeleteResponse',
+        supports_download=False,
+    )
+
     def Get(self, request, global_params=None):
-      """Returns the ACL entry for the specified entity on the specified bucket.
+      r"""Returns the ACL entry for the specified entity on the specified bucket.
 
       Args:
         request: (StorageBucketAccessControlsGetRequest) input message
         global_params: (StandardQueryParameters, default: None) global arguments
       Returns:
         (BucketAccessControl) The response message.
       """
       config = self.GetMethodConfig('Get')
       return self._RunMethod(
           config, request, global_params=global_params)
 
+    Get.method_config = lambda: base_api.ApiMethodInfo(
+        http_method=u'GET',
+        method_id=u'storage.bucketAccessControls.get',
+        ordered_params=[u'bucket', u'entity'],
+        path_params=[u'bucket', u'entity'],
+        query_params=[u'userProject'],
+        relative_path=u'b/{bucket}/acl/{entity}',
+        request_field='',
+        request_type_name=u'StorageBucketAccessControlsGetRequest',
+        response_type_name=u'BucketAccessControl',
+        supports_download=False,
+    )
+
     def Insert(self, request, global_params=None):
-      """Creates a new ACL entry on the specified bucket.
+      r"""Creates a new ACL entry on the specified bucket.
 
       Args:
-        request: (BucketAccessControl) input message
+        request: (StorageBucketAccessControlsInsertRequest) input message
         global_params: (StandardQueryParameters, default: None) global arguments
       Returns:
         (BucketAccessControl) The response message.
       """
       config = self.GetMethodConfig('Insert')
       return self._RunMethod(
           config, request, global_params=global_params)
 
+    Insert.method_config = lambda: base_api.ApiMethodInfo(
+        http_method=u'POST',
+        method_id=u'storage.bucketAccessControls.insert',
+        ordered_params=[u'bucket'],
+        path_params=[u'bucket'],
+        query_params=[u'userProject'],
+        relative_path=u'b/{bucket}/acl',
+        request_field=u'bucketAccessControl',
+        request_type_name=u'StorageBucketAccessControlsInsertRequest',
+        response_type_name=u'BucketAccessControl',
+        supports_download=False,
+    )
+
     def List(self, request, global_params=None):
-      """Retrieves ACL entries on the specified bucket.
+      r"""Retrieves ACL entries on the specified bucket.
 
       Args:
         request: (StorageBucketAccessControlsListRequest) input message
         global_params: (StandardQueryParameters, default: None) global arguments
       Returns:
         (BucketAccessControls) The response message.
       """
       config = self.GetMethodConfig('List')
       return self._RunMethod(
           config, request, global_params=global_params)
 
+    List.method_config = lambda: base_api.ApiMethodInfo(
+        http_method=u'GET',
+        method_id=u'storage.bucketAccessControls.list',
+        ordered_params=[u'bucket'],
+        path_params=[u'bucket'],
+        query_params=[u'userProject'],
+        relative_path=u'b/{bucket}/acl',
+        request_field='',
+        request_type_name=u'StorageBucketAccessControlsListRequest',
+        response_type_name=u'BucketAccessControls',
+        supports_download=False,
+    )
+
     def Patch(self, request, global_params=None):
-      """Updates an ACL entry on the specified bucket. This method supports patch semantics.
+      r"""Patches an ACL entry on the specified bucket.
 
       Args:
-        request: (BucketAccessControl) input message
+        request: (StorageBucketAccessControlsPatchRequest) input message
         global_params: (StandardQueryParameters, default: None) global arguments
       Returns:
         (BucketAccessControl) The response message.
       """
       config = self.GetMethodConfig('Patch')
       return self._RunMethod(
           config, request, global_params=global_params)
 
+    Patch.method_config = lambda: base_api.ApiMethodInfo(
+        http_method=u'PATCH',
+        method_id=u'storage.bucketAccessControls.patch',
+        ordered_params=[u'bucket', u'entity'],
+        path_params=[u'bucket', u'entity'],
+        query_params=[u'userProject'],
+        relative_path=u'b/{bucket}/acl/{entity}',
+        request_field=u'bucketAccessControl',
+        request_type_name=u'StorageBucketAccessControlsPatchRequest',
+        response_type_name=u'BucketAccessControl',
+        supports_download=False,
+    )
+
     def Update(self, request, global_params=None):
-      """Updates an ACL entry on the specified bucket.
+      r"""Updates an ACL entry on the specified bucket.
 
       Args:
-        request: (BucketAccessControl) input message
+        request: (StorageBucketAccessControlsUpdateRequest) input message
         global_params: (StandardQueryParameters, default: None) global arguments
       Returns:
         (BucketAccessControl) The response message.
       """
       config = self.GetMethodConfig('Update')
       return self._RunMethod(
           config, request, global_params=global_params)
 
+    Update.method_config = lambda: base_api.ApiMethodInfo(
+        http_method=u'PUT',
+        method_id=u'storage.bucketAccessControls.update',
+        ordered_params=[u'bucket', u'entity'],
+        path_params=[u'bucket', u'entity'],
+        query_params=[u'userProject'],
+        relative_path=u'b/{bucket}/acl/{entity}',
+        request_field=u'bucketAccessControl',
+        request_type_name=u'StorageBucketAccessControlsUpdateRequest',
+        response_type_name=u'BucketAccessControl',
+        supports_download=False,
+    )
+
   class BucketsService(base_api.BaseApiService):
     """Service class for the buckets resource."""
 
     _NAME = u'buckets'
 
     def __init__(self, client):
       super(StorageV1.BucketsService, self).__init__(client)
-      self._method_configs = {
-          'Delete': base_api.ApiMethodInfo(
-              http_method=u'DELETE',
-              method_id=u'storage.buckets.delete',
-              ordered_params=[u'bucket'],
-              path_params=[u'bucket'],
-              query_params=[u'ifMetagenerationMatch', u'ifMetagenerationNotMatch'],
-              relative_path=u'b/{bucket}',
-              request_field='',
-              request_type_name=u'StorageBucketsDeleteRequest',
-              response_type_name=u'StorageBucketsDeleteResponse',
-              supports_download=False,
-          ),
-          'Get': base_api.ApiMethodInfo(
-              http_method=u'GET',
-              method_id=u'storage.buckets.get',
-              ordered_params=[u'bucket'],
-              path_params=[u'bucket'],
-              query_params=[u'ifMetagenerationMatch', u'ifMetagenerationNotMatch', u'projection'],
-              relative_path=u'b/{bucket}',
-              request_field='',
-              request_type_name=u'StorageBucketsGetRequest',
-              response_type_name=u'Bucket',
-              supports_download=False,
-          ),
-          'Insert': base_api.ApiMethodInfo(
-              http_method=u'POST',
-              method_id=u'storage.buckets.insert',
-              ordered_params=[u'project'],
-              path_params=[],
-              query_params=[u'predefinedAcl', u'predefinedDefaultObjectAcl', u'project', u'projection'],
-              relative_path=u'b',
-              request_field=u'bucket',
-              request_type_name=u'StorageBucketsInsertRequest',
-              response_type_name=u'Bucket',
-              supports_download=False,
-          ),
-          'List': base_api.ApiMethodInfo(
-              http_method=u'GET',
-              method_id=u'storage.buckets.list',
-              ordered_params=[u'project'],
-              path_params=[],
-              query_params=[u'maxResults', u'pageToken', u'prefix', u'project', u'projection'],
-              relative_path=u'b',
-              request_field='',
-              request_type_name=u'StorageBucketsListRequest',
-              response_type_name=u'Buckets',
-              supports_download=False,
-          ),
-          'Patch': base_api.ApiMethodInfo(
-              http_method=u'PATCH',
-              method_id=u'storage.buckets.patch',
-              ordered_params=[u'bucket'],
-              path_params=[u'bucket'],
-              query_params=[u'ifMetagenerationMatch', u'ifMetagenerationNotMatch', u'predefinedAcl', u'predefinedDefaultObjectAcl', u'projection'],
-              relative_path=u'b/{bucket}',
-              request_field=u'bucketResource',
-              request_type_name=u'StorageBucketsPatchRequest',
-              response_type_name=u'Bucket',
-              supports_download=False,
-          ),
-          'Update': base_api.ApiMethodInfo(
-              http_method=u'PUT',
-              method_id=u'storage.buckets.update',
-              ordered_params=[u'bucket'],
-              path_params=[u'bucket'],
-              query_params=[u'ifMetagenerationMatch', u'ifMetagenerationNotMatch', u'predefinedAcl', u'predefinedDefaultObjectAcl', u'projection'],
-              relative_path=u'b/{bucket}',
-              request_field=u'bucketResource',
-              request_type_name=u'StorageBucketsUpdateRequest',
-              response_type_name=u'Bucket',
-              supports_download=False,
-          ),
-          }
-
       self._upload_configs = {
           }
 
     def Delete(self, request, global_params=None):
-      """Permanently deletes an empty bucket.
+      r"""Permanently deletes an empty bucket.
 
       Args:
         request: (StorageBucketsDeleteRequest) input message
         global_params: (StandardQueryParameters, default: None) global arguments
       Returns:
         (StorageBucketsDeleteResponse) The response message.
       """
       config = self.GetMethodConfig('Delete')
       return self._RunMethod(
           config, request, global_params=global_params)
 
+    Delete.method_config = lambda: base_api.ApiMethodInfo(
+        http_method=u'DELETE',
+        method_id=u'storage.buckets.delete',
+        ordered_params=[u'bucket'],
+        path_params=[u'bucket'],
+        query_params=[u'ifMetagenerationMatch', u'ifMetagenerationNotMatch', u'userProject'],
+        relative_path=u'b/{bucket}',
+        request_field='',
+        request_type_name=u'StorageBucketsDeleteRequest',
+        response_type_name=u'StorageBucketsDeleteResponse',
+        supports_download=False,
+    )
+
     def Get(self, request, global_params=None):
-      """Returns metadata for the specified bucket.
+      r"""Returns metadata for the specified bucket.
 
       Args:
         request: (StorageBucketsGetRequest) input message
         global_params: (StandardQueryParameters, default: None) global arguments
       Returns:
         (Bucket) The response message.
       """
       config = self.GetMethodConfig('Get')
       return self._RunMethod(
           config, request, global_params=global_params)
 
+    Get.method_config = lambda: base_api.ApiMethodInfo(
+        http_method=u'GET',
+        method_id=u'storage.buckets.get',
+        ordered_params=[u'bucket'],
+        path_params=[u'bucket'],
+        query_params=[u'ifMetagenerationMatch', u'ifMetagenerationNotMatch', u'projection', u'userProject'],
+        relative_path=u'b/{bucket}',
+        request_field='',
+        request_type_name=u'StorageBucketsGetRequest',
+        response_type_name=u'Bucket',
+        supports_download=False,
+    )
+
+    def GetIamPolicy(self, request, global_params=None):
+      r"""Returns an IAM policy for the specified bucket.
+
+      Args:
+        request: (StorageBucketsGetIamPolicyRequest) input message
+        global_params: (StandardQueryParameters, default: None) global arguments
+      Returns:
+        (Policy) The response message.
+      """
+      config = self.GetMethodConfig('GetIamPolicy')
+      return self._RunMethod(
+          config, request, global_params=global_params)
+
+    GetIamPolicy.method_config = lambda: base_api.ApiMethodInfo(
+        http_method=u'GET',
+        method_id=u'storage.buckets.getIamPolicy',
+        ordered_params=[u'bucket'],
+        path_params=[u'bucket'],
+        query_params=[u'userProject'],
+        relative_path=u'b/{bucket}/iam',
+        request_field='',
+        request_type_name=u'StorageBucketsGetIamPolicyRequest',
+        response_type_name=u'Policy',
+        supports_download=False,
+    )
+
     def Insert(self, request, global_params=None):
-      """Creates a new bucket.
+      r"""Creates a new bucket.
 
       Args:
         request: (StorageBucketsInsertRequest) input message
         global_params: (StandardQueryParameters, default: None) global arguments
       Returns:
         (Bucket) The response message.
       """
       config = self.GetMethodConfig('Insert')
       return self._RunMethod(
           config, request, global_params=global_params)
 
+    Insert.method_config = lambda: base_api.ApiMethodInfo(
+        http_method=u'POST',
+        method_id=u'storage.buckets.insert',
+        ordered_params=[u'project'],
+        path_params=[],
+        query_params=[u'predefinedAcl', u'predefinedDefaultObjectAcl', u'project', u'projection', u'userProject'],
+        relative_path=u'b',
+        request_field=u'bucket',
+        request_type_name=u'StorageBucketsInsertRequest',
+        response_type_name=u'Bucket',
+        supports_download=False,
+    )
+
     def List(self, request, global_params=None):
-      """Retrieves a list of buckets for a given project.
+      r"""Retrieves a list of buckets for a given project.
 
       Args:
         request: (StorageBucketsListRequest) input message
         global_params: (StandardQueryParameters, default: None) global arguments
       Returns:
         (Buckets) The response message.
       """
       config = self.GetMethodConfig('List')
       return self._RunMethod(
           config, request, global_params=global_params)
 
+    List.method_config = lambda: base_api.ApiMethodInfo(
+        http_method=u'GET',
+        method_id=u'storage.buckets.list',
+        ordered_params=[u'project'],
+        path_params=[],
+        query_params=[u'maxResults', u'pageToken', u'prefix', u'project', u'projection', u'userProject'],
+        relative_path=u'b',
+        request_field='',
+        request_type_name=u'StorageBucketsListRequest',
+        response_type_name=u'Buckets',
+        supports_download=False,
+    )
+
+    def LockRetentionPolicy(self, request, global_params=None):
+      r"""Locks retention policy on a bucket.
+
+      Args:
+        request: (StorageBucketsLockRetentionPolicyRequest) input message
+        global_params: (StandardQueryParameters, default: None) global arguments
+      Returns:
+        (Bucket) The response message.
+      """
+      config = self.GetMethodConfig('LockRetentionPolicy')
+      return self._RunMethod(
+          config, request, global_params=global_params)
+
+    LockRetentionPolicy.method_config = lambda: base_api.ApiMethodInfo(
+        http_method=u'POST',
+        method_id=u'storage.buckets.lockRetentionPolicy',
+        ordered_params=[u'bucket', u'ifMetagenerationMatch'],
+        path_params=[u'bucket'],
+        query_params=[u'ifMetagenerationMatch', u'userProject'],
+        relative_path=u'b/{bucket}/lockRetentionPolicy',
+        request_field='',
+        request_type_name=u'StorageBucketsLockRetentionPolicyRequest',
+        response_type_name=u'Bucket',
+        supports_download=False,
+    )
+
     def Patch(self, request, global_params=None):
-      """Updates a bucket. This method supports patch semantics.
+      r"""Patches a bucket. Changes to the bucket will be readable immediately after writing, but configuration changes may take time to propagate.
 
       Args:
         request: (StorageBucketsPatchRequest) input message
         global_params: (StandardQueryParameters, default: None) global arguments
       Returns:
         (Bucket) The response message.
       """
       config = self.GetMethodConfig('Patch')
       return self._RunMethod(
           config, request, global_params=global_params)
 
+    Patch.method_config = lambda: base_api.ApiMethodInfo(
+        http_method=u'PATCH',
+        method_id=u'storage.buckets.patch',
+        ordered_params=[u'bucket'],
+        path_params=[u'bucket'],
+        query_params=[u'ifMetagenerationMatch', u'ifMetagenerationNotMatch', u'predefinedAcl', u'predefinedDefaultObjectAcl', u'projection', u'userProject'],
+        relative_path=u'b/{bucket}',
+        request_field=u'bucketResource',
+        request_type_name=u'StorageBucketsPatchRequest',
+        response_type_name=u'Bucket',
+        supports_download=False,
+    )
+
+    def SetIamPolicy(self, request, global_params=None):
+      r"""Updates an IAM policy for the specified bucket.
+
+      Args:
+        request: (StorageBucketsSetIamPolicyRequest) input message
+        global_params: (StandardQueryParameters, default: None) global arguments
+      Returns:
+        (Policy) The response message.
+      """
+      config = self.GetMethodConfig('SetIamPolicy')
+      return self._RunMethod(
+          config, request, global_params=global_params)
+
+    SetIamPolicy.method_config = lambda: base_api.ApiMethodInfo(
+        http_method=u'PUT',
+        method_id=u'storage.buckets.setIamPolicy',
+        ordered_params=[u'bucket'],
+        path_params=[u'bucket'],
+        query_params=[u'userProject'],
+        relative_path=u'b/{bucket}/iam',
+        request_field=u'policy',
+        request_type_name=u'StorageBucketsSetIamPolicyRequest',
+        response_type_name=u'Policy',
+        supports_download=False,
+    )
+
+    def TestIamPermissions(self, request, global_params=None):
+      r"""Tests a set of permissions on the given bucket to see which, if any, are held by the caller.
+
+      Args:
+        request: (StorageBucketsTestIamPermissionsRequest) input message
+        global_params: (StandardQueryParameters, default: None) global arguments
+      Returns:
+        (TestIamPermissionsResponse) The response message.
+      """
+      config = self.GetMethodConfig('TestIamPermissions')
+      return self._RunMethod(
+          config, request, global_params=global_params)
+
+    TestIamPermissions.method_config = lambda: base_api.ApiMethodInfo(
+        http_method=u'GET',
+        method_id=u'storage.buckets.testIamPermissions',
+        ordered_params=[u'bucket', u'permissions'],
+        path_params=[u'bucket'],
+        query_params=[u'permissions', u'userProject'],
+        relative_path=u'b/{bucket}/iam/testPermissions',
+        request_field='',
+        request_type_name=u'StorageBucketsTestIamPermissionsRequest',
+        response_type_name=u'TestIamPermissionsResponse',
+        supports_download=False,
+    )
+
     def Update(self, request, global_params=None):
-      """Updates a bucket.
+      r"""Updates a bucket. Changes to the bucket will be readable immediately after writing, but configuration changes may take time to propagate.
 
       Args:
         request: (StorageBucketsUpdateRequest) input message
         global_params: (StandardQueryParameters, default: None) global arguments
       Returns:
         (Bucket) The response message.
       """
       config = self.GetMethodConfig('Update')
       return self._RunMethod(
           config, request, global_params=global_params)
 
+    Update.method_config = lambda: base_api.ApiMethodInfo(
+        http_method=u'PUT',
+        method_id=u'storage.buckets.update',
+        ordered_params=[u'bucket'],
+        path_params=[u'bucket'],
+        query_params=[u'ifMetagenerationMatch', u'ifMetagenerationNotMatch', u'predefinedAcl', u'predefinedDefaultObjectAcl', u'projection', u'userProject'],
+        relative_path=u'b/{bucket}',
+        request_field=u'bucketResource',
+        request_type_name=u'StorageBucketsUpdateRequest',
+        response_type_name=u'Bucket',
+        supports_download=False,
+    )
+
   class ChannelsService(base_api.BaseApiService):
     """Service class for the channels resource."""
 
     _NAME = u'channels'
 
     def __init__(self, client):
       super(StorageV1.ChannelsService, self).__init__(client)
-      self._method_configs = {
-          'Stop': base_api.ApiMethodInfo(
-              http_method=u'POST',
-              method_id=u'storage.channels.stop',
-              ordered_params=[],
-              path_params=[],
-              query_params=[],
-              relative_path=u'channels/stop',
-              request_field='<request>',
-              request_type_name=u'Channel',
-              response_type_name=u'StorageChannelsStopResponse',
-              supports_download=False,
-          ),
-          }
-
       self._upload_configs = {
           }
 
     def Stop(self, request, global_params=None):
-      """Stop watching resources through this channel.
+      r"""Stop watching resources through this channel.
 
       Args:
         request: (Channel) input message
         global_params: (StandardQueryParameters, default: None) global arguments
       Returns:
         (StorageChannelsStopResponse) The response message.
       """
       config = self.GetMethodConfig('Stop')
       return self._RunMethod(
           config, request, global_params=global_params)
 
+    Stop.method_config = lambda: base_api.ApiMethodInfo(
+        http_method=u'POST',
+        method_id=u'storage.channels.stop',
+        ordered_params=[],
+        path_params=[],
+        query_params=[],
+        relative_path=u'channels/stop',
+        request_field='<request>',
+        request_type_name=u'Channel',
+        response_type_name=u'StorageChannelsStopResponse',
+        supports_download=False,
+    )
+
   class DefaultObjectAccessControlsService(base_api.BaseApiService):
     """Service class for the defaultObjectAccessControls resource."""
 
     _NAME = u'defaultObjectAccessControls'
 
     def __init__(self, client):
       super(StorageV1.DefaultObjectAccessControlsService, self).__init__(client)
-      self._method_configs = {
-          'Delete': base_api.ApiMethodInfo(
-              http_method=u'DELETE',
-              method_id=u'storage.defaultObjectAccessControls.delete',
-              ordered_params=[u'bucket', u'entity'],
-              path_params=[u'bucket', u'entity'],
-              query_params=[],
-              relative_path=u'b/{bucket}/defaultObjectAcl/{entity}',
-              request_field='',
-              request_type_name=u'StorageDefaultObjectAccessControlsDeleteRequest',
-              response_type_name=u'StorageDefaultObjectAccessControlsDeleteResponse',
-              supports_download=False,
-          ),
-          'Get': base_api.ApiMethodInfo(
-              http_method=u'GET',
-              method_id=u'storage.defaultObjectAccessControls.get',
-              ordered_params=[u'bucket', u'entity'],
-              path_params=[u'bucket', u'entity'],
-              query_params=[],
-              relative_path=u'b/{bucket}/defaultObjectAcl/{entity}',
-              request_field='',
-              request_type_name=u'StorageDefaultObjectAccessControlsGetRequest',
-              response_type_name=u'ObjectAccessControl',
-              supports_download=False,
-          ),
-          'Insert': base_api.ApiMethodInfo(
-              http_method=u'POST',
-              method_id=u'storage.defaultObjectAccessControls.insert',
-              ordered_params=[u'bucket'],
-              path_params=[u'bucket'],
-              query_params=[],
-              relative_path=u'b/{bucket}/defaultObjectAcl',
-              request_field='<request>',
-              request_type_name=u'ObjectAccessControl',
-              response_type_name=u'ObjectAccessControl',
-              supports_download=False,
-          ),
-          'List': base_api.ApiMethodInfo(
-              http_method=u'GET',
-              method_id=u'storage.defaultObjectAccessControls.list',
-              ordered_params=[u'bucket'],
-              path_params=[u'bucket'],
-              query_params=[u'ifMetagenerationMatch', u'ifMetagenerationNotMatch'],
-              relative_path=u'b/{bucket}/defaultObjectAcl',
-              request_field='',
-              request_type_name=u'StorageDefaultObjectAccessControlsListRequest',
-              response_type_name=u'ObjectAccessControls',
-              supports_download=False,
-          ),
-          'Patch': base_api.ApiMethodInfo(
-              http_method=u'PATCH',
-              method_id=u'storage.defaultObjectAccessControls.patch',
-              ordered_params=[u'bucket', u'entity'],
-              path_params=[u'bucket', u'entity'],
-              query_params=[],
-              relative_path=u'b/{bucket}/defaultObjectAcl/{entity}',
-              request_field='<request>',
-              request_type_name=u'ObjectAccessControl',
-              response_type_name=u'ObjectAccessControl',
-              supports_download=False,
-          ),
-          'Update': base_api.ApiMethodInfo(
-              http_method=u'PUT',
-              method_id=u'storage.defaultObjectAccessControls.update',
-              ordered_params=[u'bucket', u'entity'],
-              path_params=[u'bucket', u'entity'],
-              query_params=[],
-              relative_path=u'b/{bucket}/defaultObjectAcl/{entity}',
-              request_field='<request>',
-              request_type_name=u'ObjectAccessControl',
-              response_type_name=u'ObjectAccessControl',
-              supports_download=False,
-          ),
-          }
-
       self._upload_configs = {
           }
 
     def Delete(self, request, global_params=None):
-      """Permanently deletes the default object ACL entry for the specified entity on the specified bucket.
+      r"""Permanently deletes the default object ACL entry for the specified entity on the specified bucket.
 
       Args:
         request: (StorageDefaultObjectAccessControlsDeleteRequest) input message
         global_params: (StandardQueryParameters, default: None) global arguments
       Returns:
         (StorageDefaultObjectAccessControlsDeleteResponse) The response message.
       """
       config = self.GetMethodConfig('Delete')
       return self._RunMethod(
           config, request, global_params=global_params)
 
+    Delete.method_config = lambda: base_api.ApiMethodInfo(
+        http_method=u'DELETE',
+        method_id=u'storage.defaultObjectAccessControls.delete',
+        ordered_params=[u'bucket', u'entity'],
+        path_params=[u'bucket', u'entity'],
+        query_params=[u'userProject'],
+        relative_path=u'b/{bucket}/defaultObjectAcl/{entity}',
+        request_field='',
+        request_type_name=u'StorageDefaultObjectAccessControlsDeleteRequest',
+        response_type_name=u'StorageDefaultObjectAccessControlsDeleteResponse',
+        supports_download=False,
+    )
+
     def Get(self, request, global_params=None):
-      """Returns the default object ACL entry for the specified entity on the specified bucket.
+      r"""Returns the default object ACL entry for the specified entity on the specified bucket.
 
       Args:
         request: (StorageDefaultObjectAccessControlsGetRequest) input message
         global_params: (StandardQueryParameters, default: None) global arguments
       Returns:
         (ObjectAccessControl) The response message.
       """
       config = self.GetMethodConfig('Get')
       return self._RunMethod(
           config, request, global_params=global_params)
 
+    Get.method_config = lambda: base_api.ApiMethodInfo(
+        http_method=u'GET',
+        method_id=u'storage.defaultObjectAccessControls.get',
+        ordered_params=[u'bucket', u'entity'],
+        path_params=[u'bucket', u'entity'],
+        query_params=[u'userProject'],
+        relative_path=u'b/{bucket}/defaultObjectAcl/{entity}',
+        request_field='',
+        request_type_name=u'StorageDefaultObjectAccessControlsGetRequest',
+        response_type_name=u'ObjectAccessControl',
+        supports_download=False,
+    )
+
     def Insert(self, request, global_params=None):
-      """Creates a new default object ACL entry on the specified bucket.
+      r"""Creates a new default object ACL entry on the specified bucket.
 
       Args:
-        request: (ObjectAccessControl) input message
+        request: (StorageDefaultObjectAccessControlsInsertRequest) input message
         global_params: (StandardQueryParameters, default: None) global arguments
       Returns:
         (ObjectAccessControl) The response message.
       """
       config = self.GetMethodConfig('Insert')
       return self._RunMethod(
           config, request, global_params=global_params)
 
+    Insert.method_config = lambda: base_api.ApiMethodInfo(
+        http_method=u'POST',
+        method_id=u'storage.defaultObjectAccessControls.insert',
+        ordered_params=[u'bucket'],
+        path_params=[u'bucket'],
+        query_params=[u'userProject'],
+        relative_path=u'b/{bucket}/defaultObjectAcl',
+        request_field=u'objectAccessControl',
+        request_type_name=u'StorageDefaultObjectAccessControlsInsertRequest',
+        response_type_name=u'ObjectAccessControl',
+        supports_download=False,
+    )
+
     def List(self, request, global_params=None):
-      """Retrieves default object ACL entries on the specified bucket.
+      r"""Retrieves default object ACL entries on the specified bucket.
 
       Args:
         request: (StorageDefaultObjectAccessControlsListRequest) input message
         global_params: (StandardQueryParameters, default: None) global arguments
       Returns:
         (ObjectAccessControls) The response message.
       """
       config = self.GetMethodConfig('List')
       return self._RunMethod(
           config, request, global_params=global_params)
 
+    List.method_config = lambda: base_api.ApiMethodInfo(
+        http_method=u'GET',
+        method_id=u'storage.defaultObjectAccessControls.list',
+        ordered_params=[u'bucket'],
+        path_params=[u'bucket'],
+        query_params=[u'ifMetagenerationMatch', u'ifMetagenerationNotMatch', u'userProject'],
+        relative_path=u'b/{bucket}/defaultObjectAcl',
+        request_field='',
+        request_type_name=u'StorageDefaultObjectAccessControlsListRequest',
+        response_type_name=u'ObjectAccessControls',
+        supports_download=False,
+    )
+
     def Patch(self, request, global_params=None):
-      """Updates a default object ACL entry on the specified bucket. This method supports patch semantics.
+      r"""Patches a default object ACL entry on the specified bucket.
 
       Args:
-        request: (ObjectAccessControl) input message
+        request: (StorageDefaultObjectAccessControlsPatchRequest) input message
         global_params: (StandardQueryParameters, default: None) global arguments
       Returns:
         (ObjectAccessControl) The response message.
       """
       config = self.GetMethodConfig('Patch')
       return self._RunMethod(
           config, request, global_params=global_params)
 
+    Patch.method_config = lambda: base_api.ApiMethodInfo(
+        http_method=u'PATCH',
+        method_id=u'storage.defaultObjectAccessControls.patch',
+        ordered_params=[u'bucket', u'entity'],
+        path_params=[u'bucket', u'entity'],
+        query_params=[u'userProject'],
+        relative_path=u'b/{bucket}/defaultObjectAcl/{entity}',
+        request_field=u'objectAccessControl',
+        request_type_name=u'StorageDefaultObjectAccessControlsPatchRequest',
+        response_type_name=u'ObjectAccessControl',
+        supports_download=False,
+    )
+
     def Update(self, request, global_params=None):
-      """Updates a default object ACL entry on the specified bucket.
+      r"""Updates a default object ACL entry on the specified bucket.
 
       Args:
-        request: (ObjectAccessControl) input message
+        request: (StorageDefaultObjectAccessControlsUpdateRequest) input message
         global_params: (StandardQueryParameters, default: None) global arguments
       Returns:
         (ObjectAccessControl) The response message.
       """
       config = self.GetMethodConfig('Update')
       return self._RunMethod(
           config, request, global_params=global_params)
 
+    Update.method_config = lambda: base_api.ApiMethodInfo(
+        http_method=u'PUT',
+        method_id=u'storage.defaultObjectAccessControls.update',
+        ordered_params=[u'bucket', u'entity'],
+        path_params=[u'bucket', u'entity'],
+        query_params=[u'userProject'],
+        relative_path=u'b/{bucket}/defaultObjectAcl/{entity}',
+        request_field=u'objectAccessControl',
+        request_type_name=u'StorageDefaultObjectAccessControlsUpdateRequest',
+        response_type_name=u'ObjectAccessControl',
+        supports_download=False,
+    )
+
+  class NotificationsService(base_api.BaseApiService):
+    """Service class for the notifications resource."""
+
+    _NAME = u'notifications'
+
+    def __init__(self, client):
+      super(StorageV1.NotificationsService, self).__init__(client)
+      self._upload_configs = {
+          }
+
+    def Delete(self, request, global_params=None):
+      r"""Permanently deletes a notification subscription.
+
+      Args:
+        request: (StorageNotificationsDeleteRequest) input message
+        global_params: (StandardQueryParameters, default: None) global arguments
+      Returns:
+        (StorageNotificationsDeleteResponse) The response message.
+      """
+      config = self.GetMethodConfig('Delete')
+      return self._RunMethod(
+          config, request, global_params=global_params)
+
+    Delete.method_config = lambda: base_api.ApiMethodInfo(
+        http_method=u'DELETE',
+        method_id=u'storage.notifications.delete',
+        ordered_params=[u'bucket', u'notification'],
+        path_params=[u'bucket', u'notification'],
+        query_params=[u'userProject'],
+        relative_path=u'b/{bucket}/notificationConfigs/{notification}',
+        request_field='',
+        request_type_name=u'StorageNotificationsDeleteRequest',
+        response_type_name=u'StorageNotificationsDeleteResponse',
+        supports_download=False,
+    )
+
+    def Get(self, request, global_params=None):
+      r"""View a notification configuration.
+
+      Args:
+        request: (StorageNotificationsGetRequest) input message
+        global_params: (StandardQueryParameters, default: None) global arguments
+      Returns:
+        (Notification) The response message.
+      """
+      config = self.GetMethodConfig('Get')
+      return self._RunMethod(
+          config, request, global_params=global_params)
+
+    Get.method_config = lambda: base_api.ApiMethodInfo(
+        http_method=u'GET',
+        method_id=u'storage.notifications.get',
+        ordered_params=[u'bucket', u'notification'],
+        path_params=[u'bucket', u'notification'],
+        query_params=[u'userProject'],
+        relative_path=u'b/{bucket}/notificationConfigs/{notification}',
+        request_field='',
+        request_type_name=u'StorageNotificationsGetRequest',
+        response_type_name=u'Notification',
+        supports_download=False,
+    )
+
+    def Insert(self, request, global_params=None):
+      r"""Creates a notification subscription for a given bucket.
+
+      Args:
+        request: (StorageNotificationsInsertRequest) input message
+        global_params: (StandardQueryParameters, default: None) global arguments
+      Returns:
+        (Notification) The response message.
+      """
+      config = self.GetMethodConfig('Insert')
+      return self._RunMethod(
+          config, request, global_params=global_params)
+
+    Insert.method_config = lambda: base_api.ApiMethodInfo(
+        http_method=u'POST',
+        method_id=u'storage.notifications.insert',
+        ordered_params=[u'bucket'],
+        path_params=[u'bucket'],
+        query_params=[u'userProject'],
+        relative_path=u'b/{bucket}/notificationConfigs',
+        request_field=u'notification',
+        request_type_name=u'StorageNotificationsInsertRequest',
+        response_type_name=u'Notification',
+        supports_download=False,
+    )
+
+    def List(self, request, global_params=None):
+      r"""Retrieves a list of notification subscriptions for a given bucket.
+
+      Args:
+        request: (StorageNotificationsListRequest) input message
+        global_params: (StandardQueryParameters, default: None) global arguments
+      Returns:
+        (Notifications) The response message.
+      """
+      config = self.GetMethodConfig('List')
+      return self._RunMethod(
+          config, request, global_params=global_params)
+
+    List.method_config = lambda: base_api.ApiMethodInfo(
+        http_method=u'GET',
+        method_id=u'storage.notifications.list',
+        ordered_params=[u'bucket'],
+        path_params=[u'bucket'],
+        query_params=[u'userProject'],
+        relative_path=u'b/{bucket}/notificationConfigs',
+        request_field='',
+        request_type_name=u'StorageNotificationsListRequest',
+        response_type_name=u'Notifications',
+        supports_download=False,
+    )
+
   class ObjectAccessControlsService(base_api.BaseApiService):
     """Service class for the objectAccessControls resource."""
 
     _NAME = u'objectAccessControls'
 
     def __init__(self, client):
       super(StorageV1.ObjectAccessControlsService, self).__init__(client)
-      self._method_configs = {
-          'Delete': base_api.ApiMethodInfo(
-              http_method=u'DELETE',
-              method_id=u'storage.objectAccessControls.delete',
-              ordered_params=[u'bucket', u'object', u'entity'],
-              path_params=[u'bucket', u'entity', u'object'],
-              query_params=[u'generation'],
-              relative_path=u'b/{bucket}/o/{object}/acl/{entity}',
-              request_field='',
-              request_type_name=u'StorageObjectAccessControlsDeleteRequest',
-              response_type_name=u'StorageObjectAccessControlsDeleteResponse',
-              supports_download=False,
-          ),
-          'Get': base_api.ApiMethodInfo(
-              http_method=u'GET',
-              method_id=u'storage.objectAccessControls.get',
-              ordered_params=[u'bucket', u'object', u'entity'],
-              path_params=[u'bucket', u'entity', u'object'],
-              query_params=[u'generation'],
-              relative_path=u'b/{bucket}/o/{object}/acl/{entity}',
-              request_field='',
-              request_type_name=u'StorageObjectAccessControlsGetRequest',
-              response_type_name=u'ObjectAccessControl',
-              supports_download=False,
-          ),
-          'Insert': base_api.ApiMethodInfo(
-              http_method=u'POST',
-              method_id=u'storage.objectAccessControls.insert',
-              ordered_params=[u'bucket', u'object'],
-              path_params=[u'bucket', u'object'],
-              query_params=[u'generation'],
-              relative_path=u'b/{bucket}/o/{object}/acl',
-              request_field=u'objectAccessControl',
-              request_type_name=u'StorageObjectAccessControlsInsertRequest',
-              response_type_name=u'ObjectAccessControl',
-              supports_download=False,
-          ),
-          'List': base_api.ApiMethodInfo(
-              http_method=u'GET',
-              method_id=u'storage.objectAccessControls.list',
-              ordered_params=[u'bucket', u'object'],
-              path_params=[u'bucket', u'object'],
-              query_params=[u'generation'],
-              relative_path=u'b/{bucket}/o/{object}/acl',
-              request_field='',
-              request_type_name=u'StorageObjectAccessControlsListRequest',
-              response_type_name=u'ObjectAccessControls',
-              supports_download=False,
-          ),
-          'Patch': base_api.ApiMethodInfo(
-              http_method=u'PATCH',
-              method_id=u'storage.objectAccessControls.patch',
-              ordered_params=[u'bucket', u'object', u'entity'],
-              path_params=[u'bucket', u'entity', u'object'],
-              query_params=[u'generation'],
-              relative_path=u'b/{bucket}/o/{object}/acl/{entity}',
-              request_field=u'objectAccessControl',
-              request_type_name=u'StorageObjectAccessControlsPatchRequest',
-              response_type_name=u'ObjectAccessControl',
-              supports_download=False,
-          ),
-          'Update': base_api.ApiMethodInfo(
-              http_method=u'PUT',
-              method_id=u'storage.objectAccessControls.update',
-              ordered_params=[u'bucket', u'object', u'entity'],
-              path_params=[u'bucket', u'entity', u'object'],
-              query_params=[u'generation'],
-              relative_path=u'b/{bucket}/o/{object}/acl/{entity}',
-              request_field=u'objectAccessControl',
-              request_type_name=u'StorageObjectAccessControlsUpdateRequest',
-              response_type_name=u'ObjectAccessControl',
-              supports_download=False,
-          ),
-          }
-
       self._upload_configs = {
           }
 
     def Delete(self, request, global_params=None):
-      """Permanently deletes the ACL entry for the specified entity on the specified object.
+      r"""Permanently deletes the ACL entry for the specified entity on the specified object.
 
       Args:
         request: (StorageObjectAccessControlsDeleteRequest) input message
         global_params: (StandardQueryParameters, default: None) global arguments
       Returns:
         (StorageObjectAccessControlsDeleteResponse) The response message.
       """
       config = self.GetMethodConfig('Delete')
       return self._RunMethod(
           config, request, global_params=global_params)
 
+    Delete.method_config = lambda: base_api.ApiMethodInfo(
+        http_method=u'DELETE',
+        method_id=u'storage.objectAccessControls.delete',
+        ordered_params=[u'bucket', u'object', u'entity'],
+        path_params=[u'bucket', u'entity', u'object'],
+        query_params=[u'generation', u'userProject'],
+        relative_path=u'b/{bucket}/o/{object}/acl/{entity}',
+        request_field='',
+        request_type_name=u'StorageObjectAccessControlsDeleteRequest',
+        response_type_name=u'StorageObjectAccessControlsDeleteResponse',
+        supports_download=False,
+    )
+
     def Get(self, request, global_params=None):
-      """Returns the ACL entry for the specified entity on the specified object.
+      r"""Returns the ACL entry for the specified entity on the specified object.
 
       Args:
         request: (StorageObjectAccessControlsGetRequest) input message
         global_params: (StandardQueryParameters, default: None) global arguments
       Returns:
         (ObjectAccessControl) The response message.
       """
       config = self.GetMethodConfig('Get')
       return self._RunMethod(
           config, request, global_params=global_params)
 
+    Get.method_config = lambda: base_api.ApiMethodInfo(
+        http_method=u'GET',
+        method_id=u'storage.objectAccessControls.get',
+        ordered_params=[u'bucket', u'object', u'entity'],
+        path_params=[u'bucket', u'entity', u'object'],
+        query_params=[u'generation', u'userProject'],
+        relative_path=u'b/{bucket}/o/{object}/acl/{entity}',
+        request_field='',
+        request_type_name=u'StorageObjectAccessControlsGetRequest',
+        response_type_name=u'ObjectAccessControl',
+        supports_download=False,
+    )
+
     def Insert(self, request, global_params=None):
-      """Creates a new ACL entry on the specified object.
+      r"""Creates a new ACL entry on the specified object.
 
       Args:
         request: (StorageObjectAccessControlsInsertRequest) input message
         global_params: (StandardQueryParameters, default: None) global arguments
       Returns:
         (ObjectAccessControl) The response message.
       """
       config = self.GetMethodConfig('Insert')
       return self._RunMethod(
           config, request, global_params=global_params)
 
+    Insert.method_config = lambda: base_api.ApiMethodInfo(
+        http_method=u'POST',
+        method_id=u'storage.objectAccessControls.insert',
+        ordered_params=[u'bucket', u'object'],
+        path_params=[u'bucket', u'object'],
+        query_params=[u'generation', u'userProject'],
+        relative_path=u'b/{bucket}/o/{object}/acl',
+        request_field=u'objectAccessControl',
+        request_type_name=u'StorageObjectAccessControlsInsertRequest',
+        response_type_name=u'ObjectAccessControl',
+        supports_download=False,
+    )
+
     def List(self, request, global_params=None):
-      """Retrieves ACL entries on the specified object.
+      r"""Retrieves ACL entries on the specified object.
 
       Args:
         request: (StorageObjectAccessControlsListRequest) input message
         global_params: (StandardQueryParameters, default: None) global arguments
       Returns:
         (ObjectAccessControls) The response message.
       """
       config = self.GetMethodConfig('List')
       return self._RunMethod(
           config, request, global_params=global_params)
 
+    List.method_config = lambda: base_api.ApiMethodInfo(
+        http_method=u'GET',
+        method_id=u'storage.objectAccessControls.list',
+        ordered_params=[u'bucket', u'object'],
+        path_params=[u'bucket', u'object'],
+        query_params=[u'generation', u'userProject'],
+        relative_path=u'b/{bucket}/o/{object}/acl',
+        request_field='',
+        request_type_name=u'StorageObjectAccessControlsListRequest',
+        response_type_name=u'ObjectAccessControls',
+        supports_download=False,
+    )
+
     def Patch(self, request, global_params=None):
-      """Updates an ACL entry on the specified object. This method supports patch semantics.
+      r"""Patches an ACL entry on the specified object.
 
       Args:
         request: (StorageObjectAccessControlsPatchRequest) input message
         global_params: (StandardQueryParameters, default: None) global arguments
       Returns:
         (ObjectAccessControl) The response message.
       """
       config = self.GetMethodConfig('Patch')
       return self._RunMethod(
           config, request, global_params=global_params)
 
+    Patch.method_config = lambda: base_api.ApiMethodInfo(
+        http_method=u'PATCH',
+        method_id=u'storage.objectAccessControls.patch',
+        ordered_params=[u'bucket', u'object', u'entity'],
+        path_params=[u'bucket', u'entity', u'object'],
+        query_params=[u'generation', u'userProject'],
+        relative_path=u'b/{bucket}/o/{object}/acl/{entity}',
+        request_field=u'objectAccessControl',
+        request_type_name=u'StorageObjectAccessControlsPatchRequest',
+        response_type_name=u'ObjectAccessControl',
+        supports_download=False,
+    )
+
     def Update(self, request, global_params=None):
-      """Updates an ACL entry on the specified object.
+      r"""Updates an ACL entry on the specified object.
 
       Args:
         request: (StorageObjectAccessControlsUpdateRequest) input message
         global_params: (StandardQueryParameters, default: None) global arguments
       Returns:
         (ObjectAccessControl) The response message.
       """
       config = self.GetMethodConfig('Update')
       return self._RunMethod(
           config, request, global_params=global_params)
 
+    Update.method_config = lambda: base_api.ApiMethodInfo(
+        http_method=u'PUT',
+        method_id=u'storage.objectAccessControls.update',
+        ordered_params=[u'bucket', u'object', u'entity'],
+        path_params=[u'bucket', u'entity', u'object'],
+        query_params=[u'generation', u'userProject'],
+        relative_path=u'b/{bucket}/o/{object}/acl/{entity}',
+        request_field=u'objectAccessControl',
+        request_type_name=u'StorageObjectAccessControlsUpdateRequest',
+        response_type_name=u'ObjectAccessControl',
+        supports_download=False,
+    )
+
   class ObjectsService(base_api.BaseApiService):
     """Service class for the objects resource."""
 
     _NAME = u'objects'
 
     def __init__(self, client):
       super(StorageV1.ObjectsService, self).__init__(client)
-      self._method_configs = {
-          'Compose': base_api.ApiMethodInfo(
-              http_method=u'POST',
-              method_id=u'storage.objects.compose',
-              ordered_params=[u'destinationBucket', u'destinationObject'],
-              path_params=[u'destinationBucket', u'destinationObject'],
-              query_params=[u'destinationPredefinedAcl', u'ifGenerationMatch', u'ifMetagenerationMatch'],
-              relative_path=u'b/{destinationBucket}/o/{destinationObject}/compose',
-              request_field=u'composeRequest',
-              request_type_name=u'StorageObjectsComposeRequest',
-              response_type_name=u'Object',
-              supports_download=True,
-          ),
-          'Copy': base_api.ApiMethodInfo(
-              http_method=u'POST',
-              method_id=u'storage.objects.copy',
-              ordered_params=[u'sourceBucket', u'sourceObject', u'destinationBucket', u'destinationObject'],
-              path_params=[u'destinationBucket', u'destinationObject', u'sourceBucket', u'sourceObject'],
-              query_params=[u'destinationPredefinedAcl', u'ifGenerationMatch', u'ifGenerationNotMatch', u'ifMetagenerationMatch', u'ifMetagenerationNotMatch', u'ifSourceGenerationMatch', u'ifSourceGenerationNotMatch', u'ifSourceMetagenerationMatch', u'ifSourceMetagenerationNotMatch', u'projection', u'sourceGeneration'],
-              relative_path=u'b/{sourceBucket}/o/{sourceObject}/copyTo/b/{destinationBucket}/o/{destinationObject}',
-              request_field=u'object',
-              request_type_name=u'StorageObjectsCopyRequest',
-              response_type_name=u'Object',
-              supports_download=True,
-          ),
-          'Delete': base_api.ApiMethodInfo(
-              http_method=u'DELETE',
-              method_id=u'storage.objects.delete',
-              ordered_params=[u'bucket', u'object'],
-              path_params=[u'bucket', u'object'],
-              query_params=[u'generation', u'ifGenerationMatch', u'ifGenerationNotMatch', u'ifMetagenerationMatch', u'ifMetagenerationNotMatch'],
-              relative_path=u'b/{bucket}/o/{object}',
-              request_field='',
-              request_type_name=u'StorageObjectsDeleteRequest',
-              response_type_name=u'StorageObjectsDeleteResponse',
-              supports_download=False,
-          ),
-          'Get': base_api.ApiMethodInfo(
-              http_method=u'GET',
-              method_id=u'storage.objects.get',
-              ordered_params=[u'bucket', u'object'],
-              path_params=[u'bucket', u'object'],
-              query_params=[u'generation', u'ifGenerationMatch', u'ifGenerationNotMatch', u'ifMetagenerationMatch', u'ifMetagenerationNotMatch', u'projection'],
-              relative_path=u'b/{bucket}/o/{object}',
-              request_field='',
-              request_type_name=u'StorageObjectsGetRequest',
-              response_type_name=u'Object',
-              supports_download=True,
-          ),
-          'Insert': base_api.ApiMethodInfo(
-              http_method=u'POST',
-              method_id=u'storage.objects.insert',
-              ordered_params=[u'bucket'],
-              path_params=[u'bucket'],
-              query_params=[u'contentEncoding', u'ifGenerationMatch', u'ifGenerationNotMatch', u'ifMetagenerationMatch', u'ifMetagenerationNotMatch', u'name', u'predefinedAcl', u'projection'],
-              relative_path=u'b/{bucket}/o',
-              request_field=u'object',
-              request_type_name=u'StorageObjectsInsertRequest',
-              response_type_name=u'Object',
-              supports_download=True,
-          ),
-          'List': base_api.ApiMethodInfo(
-              http_method=u'GET',
-              method_id=u'storage.objects.list',
-              ordered_params=[u'bucket'],
-              path_params=[u'bucket'],
-              query_params=[u'delimiter', u'maxResults', u'pageToken', u'prefix', u'projection', u'versions'],
-              relative_path=u'b/{bucket}/o',
-              request_field='',
-              request_type_name=u'StorageObjectsListRequest',
-              response_type_name=u'Objects',
-              supports_download=False,
-          ),
-          'Patch': base_api.ApiMethodInfo(
-              http_method=u'PATCH',
-              method_id=u'storage.objects.patch',
-              ordered_params=[u'bucket', u'object'],
-              path_params=[u'bucket', u'object'],
-              query_params=[u'generation', u'ifGenerationMatch', u'ifGenerationNotMatch', u'ifMetagenerationMatch', u'ifMetagenerationNotMatch', u'predefinedAcl', u'projection'],
-              relative_path=u'b/{bucket}/o/{object}',
-              request_field=u'objectResource',
-              request_type_name=u'StorageObjectsPatchRequest',
-              response_type_name=u'Object',
-              supports_download=False,
-          ),
-          'Rewrite': base_api.ApiMethodInfo(
-              http_method=u'POST',
-              method_id=u'storage.objects.rewrite',
-              ordered_params=[u'sourceBucket', u'sourceObject', u'destinationBucket', u'destinationObject'],
-              path_params=[u'destinationBucket', u'destinationObject', u'sourceBucket', u'sourceObject'],
-              query_params=[u'destinationPredefinedAcl', u'ifGenerationMatch', u'ifGenerationNotMatch', u'ifMetagenerationMatch', u'ifMetagenerationNotMatch', u'ifSourceGenerationMatch', u'ifSourceGenerationNotMatch', u'ifSourceMetagenerationMatch', u'ifSourceMetagenerationNotMatch', u'maxBytesRewrittenPerCall', u'projection', u'rewriteToken', u'sourceGeneration'],
-              relative_path=u'b/{sourceBucket}/o/{sourceObject}/rewriteTo/b/{destinationBucket}/o/{destinationObject}',
-              request_field=u'object',
-              request_type_name=u'StorageObjectsRewriteRequest',
-              response_type_name=u'RewriteResponse',
-              supports_download=False,
-          ),
-          'Update': base_api.ApiMethodInfo(
-              http_method=u'PUT',
-              method_id=u'storage.objects.update',
-              ordered_params=[u'bucket', u'object'],
-              path_params=[u'bucket', u'object'],
-              query_params=[u'generation', u'ifGenerationMatch', u'ifGenerationNotMatch', u'ifMetagenerationMatch', u'ifMetagenerationNotMatch', u'predefinedAcl', u'projection'],
-              relative_path=u'b/{bucket}/o/{object}',
-              request_field=u'objectResource',
-              request_type_name=u'StorageObjectsUpdateRequest',
-              response_type_name=u'Object',
-              supports_download=True,
-          ),
-          'WatchAll': base_api.ApiMethodInfo(
-              http_method=u'POST',
-              method_id=u'storage.objects.watchAll',
-              ordered_params=[u'bucket'],
-              path_params=[u'bucket'],
-              query_params=[u'delimiter', u'maxResults', u'pageToken', u'prefix', u'projection', u'versions'],
-              relative_path=u'b/{bucket}/o/watch',
-              request_field=u'channel',
-              request_type_name=u'StorageObjectsWatchAllRequest',
-              response_type_name=u'Channel',
-              supports_download=False,
-          ),
-          }
-
       self._upload_configs = {
           'Insert': base_api.ApiUploadInfo(
               accept=['*/*'],
               max_size=None,
               resumable_multipart=True,
               resumable_path=u'/resumable/upload/storage/v1/b/{bucket}/o',
               simple_multipart=True,
               simple_path=u'/upload/storage/v1/b/{bucket}/o',
           ),
           }
 
-    def Compose(self, request, global_params=None, download=None):
-      """Concatenates a list of existing objects into a new object in the same bucket.
+    def Compose(self, request, global_params=None):
+      r"""Concatenates a list of existing objects into a new object in the same bucket.
 
       Args:
         request: (StorageObjectsComposeRequest) input message
         global_params: (StandardQueryParameters, default: None) global arguments
-        download: (Download, default: None) If present, download
-            data from the request via this stream.
       Returns:
         (Object) The response message.
       """
       config = self.GetMethodConfig('Compose')
       return self._RunMethod(
-          config, request, global_params=global_params,
-          download=download)
+          config, request, global_params=global_params)
 
-    def Copy(self, request, global_params=None, download=None):
-      """Copies a source object to a destination object. Optionally overrides metadata.
+    Compose.method_config = lambda: base_api.ApiMethodInfo(
+        http_method=u'POST',
+        method_id=u'storage.objects.compose',
+        ordered_params=[u'destinationBucket', u'destinationObject'],
+        path_params=[u'destinationBucket', u'destinationObject'],
+        query_params=[u'destinationPredefinedAcl', u'ifGenerationMatch', u'ifMetagenerationMatch', u'kmsKeyName', u'userProject'],
+        relative_path=u'b/{destinationBucket}/o/{destinationObject}/compose',
+        request_field=u'composeRequest',
+        request_type_name=u'StorageObjectsComposeRequest',
+        response_type_name=u'Object',
+        supports_download=False,
+    )
+
+    def Copy(self, request, global_params=None):
+      r"""Copies a source object to a destination object. Optionally overrides metadata.
 
       Args:
         request: (StorageObjectsCopyRequest) input message
         global_params: (StandardQueryParameters, default: None) global arguments
-        download: (Download, default: None) If present, download
-            data from the request via this stream.
       Returns:
         (Object) The response message.
       """
       config = self.GetMethodConfig('Copy')
       return self._RunMethod(
-          config, request, global_params=global_params,
-          download=download)
+          config, request, global_params=global_params)
+
+    Copy.method_config = lambda: base_api.ApiMethodInfo(
+        http_method=u'POST',
+        method_id=u'storage.objects.copy',
+        ordered_params=[u'sourceBucket', u'sourceObject', u'destinationBucket', u'destinationObject'],
+        path_params=[u'destinationBucket', u'destinationObject', u'sourceBucket', u'sourceObject'],
+        query_params=[u'destinationPredefinedAcl', u'ifGenerationMatch', u'ifGenerationNotMatch', u'ifMetagenerationMatch', u'ifMetagenerationNotMatch', u'ifSourceGenerationMatch', u'ifSourceGenerationNotMatch', u'ifSourceMetagenerationMatch', u'ifSourceMetagenerationNotMatch', u'projection', u'sourceGeneration', u'userProject'],
+        relative_path=u'b/{sourceBucket}/o/{sourceObject}/copyTo/b/{destinationBucket}/o/{destinationObject}',
+        request_field=u'object',
+        request_type_name=u'StorageObjectsCopyRequest',
+        response_type_name=u'Object',
+        supports_download=False,
+    )
 
     def Delete(self, request, global_params=None):
-      """Deletes an object and its metadata. Deletions are permanent if versioning is not enabled for the bucket, or if the generation parameter is used.
+      r"""Deletes an object and its metadata. Deletions are permanent if versioning is not enabled for the bucket, or if the generation parameter is used.
 
       Args:
         request: (StorageObjectsDeleteRequest) input message
         global_params: (StandardQueryParameters, default: None) global arguments
       Returns:
         (StorageObjectsDeleteResponse) The response message.
       """
       config = self.GetMethodConfig('Delete')
       return self._RunMethod(
           config, request, global_params=global_params)
 
+    Delete.method_config = lambda: base_api.ApiMethodInfo(
+        http_method=u'DELETE',
+        method_id=u'storage.objects.delete',
+        ordered_params=[u'bucket', u'object'],
+        path_params=[u'bucket', u'object'],
+        query_params=[u'generation', u'ifGenerationMatch', u'ifGenerationNotMatch', u'ifMetagenerationMatch', u'ifMetagenerationNotMatch', u'userProject'],
+        relative_path=u'b/{bucket}/o/{object}',
+        request_field='',
+        request_type_name=u'StorageObjectsDeleteRequest',
+        response_type_name=u'StorageObjectsDeleteResponse',
+        supports_download=False,
+    )
+
     def Get(self, request, global_params=None, download=None):
-      """Retrieves an object or its metadata.
+      r"""Retrieves an object or its metadata.
 
       Args:
         request: (StorageObjectsGetRequest) input message
         global_params: (StandardQueryParameters, default: None) global arguments
         download: (Download, default: None) If present, download
             data from the request via this stream.
       Returns:
         (Object) The response message.
       """
       config = self.GetMethodConfig('Get')
       return self._RunMethod(
           config, request, global_params=global_params,
           download=download)
 
-    def Insert(self, request, global_params=None, upload=None, download=None):
-      """Stores a new object and metadata.
+    Get.method_config = lambda: base_api.ApiMethodInfo(
+        http_method=u'GET',
+        method_id=u'storage.objects.get',
+        ordered_params=[u'bucket', u'object'],
+        path_params=[u'bucket', u'object'],
+        query_params=[u'generation', u'ifGenerationMatch', u'ifGenerationNotMatch', u'ifMetagenerationMatch', u'ifMetagenerationNotMatch', u'projection', u'userProject'],
+        relative_path=u'b/{bucket}/o/{object}',
+        request_field='',
+        request_type_name=u'StorageObjectsGetRequest',
+        response_type_name=u'Object',
+        supports_download=True,
+    )
+
+    def GetIamPolicy(self, request, global_params=None):
+      r"""Returns an IAM policy for the specified object.
+
+      Args:
+        request: (StorageObjectsGetIamPolicyRequest) input message
+        global_params: (StandardQueryParameters, default: None) global arguments
+      Returns:
+        (Policy) The response message.
+      """
+      config = self.GetMethodConfig('GetIamPolicy')
+      return self._RunMethod(
+          config, request, global_params=global_params)
+
+    GetIamPolicy.method_config = lambda: base_api.ApiMethodInfo(
+        http_method=u'GET',
+        method_id=u'storage.objects.getIamPolicy',
+        ordered_params=[u'bucket', u'object'],
+        path_params=[u'bucket', u'object'],
+        query_params=[u'generation', u'userProject'],
+        relative_path=u'b/{bucket}/o/{object}/iam',
+        request_field='',
+        request_type_name=u'StorageObjectsGetIamPolicyRequest',
+        response_type_name=u'Policy',
+        supports_download=False,
+    )
+
+    def Insert(self, request, global_params=None, upload=None):
+      r"""Stores a new object and metadata.
 
       Args:
         request: (StorageObjectsInsertRequest) input message
         global_params: (StandardQueryParameters, default: None) global arguments
         upload: (Upload, default: None) If present, upload
             this stream with the request.
-        download: (Download, default: None) If present, download
-            data from the request via this stream.
       Returns:
         (Object) The response message.
       """
       config = self.GetMethodConfig('Insert')
       upload_config = self.GetUploadConfig('Insert')
       return self._RunMethod(
           config, request, global_params=global_params,
-          upload=upload, upload_config=upload_config,
-          download=download)
+          upload=upload, upload_config=upload_config)
+
+    Insert.method_config = lambda: base_api.ApiMethodInfo(
+        http_method=u'POST',
+        method_id=u'storage.objects.insert',
+        ordered_params=[u'bucket'],
+        path_params=[u'bucket'],
+        query_params=[u'contentEncoding', u'ifGenerationMatch', u'ifGenerationNotMatch', u'ifMetagenerationMatch', u'ifMetagenerationNotMatch', u'kmsKeyName', u'name', u'predefinedAcl', u'projection', u'userProject'],
+        relative_path=u'b/{bucket}/o',
+        request_field=u'object',
+        request_type_name=u'StorageObjectsInsertRequest',
+        response_type_name=u'Object',
+        supports_download=False,
+    )
 
     def List(self, request, global_params=None):
-      """Retrieves a list of objects matching the criteria.
+      r"""Retrieves a list of objects matching the criteria.
 
       Args:
         request: (StorageObjectsListRequest) input message
         global_params: (StandardQueryParameters, default: None) global arguments
       Returns:
         (Objects) The response message.
       """
       config = self.GetMethodConfig('List')
       return self._RunMethod(
           config, request, global_params=global_params)
 
+    List.method_config = lambda: base_api.ApiMethodInfo(
+        http_method=u'GET',
+        method_id=u'storage.objects.list',
+        ordered_params=[u'bucket'],
+        path_params=[u'bucket'],
+        query_params=[u'delimiter', u'includeTrailingDelimiter', u'maxResults', u'pageToken', u'prefix', u'projection', u'userProject', u'versions'],
+        relative_path=u'b/{bucket}/o',
+        request_field='',
+        request_type_name=u'StorageObjectsListRequest',
+        response_type_name=u'Objects',
+        supports_download=False,
+    )
+
     def Patch(self, request, global_params=None):
-      """Updates an object's metadata. This method supports patch semantics.
+      r"""Patches an object's metadata.
 
       Args:
         request: (StorageObjectsPatchRequest) input message
         global_params: (StandardQueryParameters, default: None) global arguments
       Returns:
         (Object) The response message.
       """
       config = self.GetMethodConfig('Patch')
       return self._RunMethod(
           config, request, global_params=global_params)
 
+    Patch.method_config = lambda: base_api.ApiMethodInfo(
+        http_method=u'PATCH',
+        method_id=u'storage.objects.patch',
+        ordered_params=[u'bucket', u'object'],
+        path_params=[u'bucket', u'object'],
+        query_params=[u'generation', u'ifGenerationMatch', u'ifGenerationNotMatch', u'ifMetagenerationMatch', u'ifMetagenerationNotMatch', u'predefinedAcl', u'projection', u'userProject'],
+        relative_path=u'b/{bucket}/o/{object}',
+        request_field=u'objectResource',
+        request_type_name=u'StorageObjectsPatchRequest',
+        response_type_name=u'Object',
+        supports_download=False,
+    )
+
     def Rewrite(self, request, global_params=None):
-      """Rewrites a source object to a destination object. Optionally overrides metadata.
+      r"""Rewrites a source object to a destination object. Optionally overrides metadata.
 
       Args:
         request: (StorageObjectsRewriteRequest) input message
         global_params: (StandardQueryParameters, default: None) global arguments
       Returns:
         (RewriteResponse) The response message.
       """
       config = self.GetMethodConfig('Rewrite')
       return self._RunMethod(
           config, request, global_params=global_params)
 
-    def Update(self, request, global_params=None, download=None):
-      """Updates an object's metadata.
+    Rewrite.method_config = lambda: base_api.ApiMethodInfo(
+        http_method=u'POST',
+        method_id=u'storage.objects.rewrite',
+        ordered_params=[u'sourceBucket', u'sourceObject', u'destinationBucket', u'destinationObject'],
+        path_params=[u'destinationBucket', u'destinationObject', u'sourceBucket', u'sourceObject'],
+        query_params=[u'destinationKmsKeyName', u'destinationPredefinedAcl', u'ifGenerationMatch', u'ifGenerationNotMatch', u'ifMetagenerationMatch', u'ifMetagenerationNotMatch', u'ifSourceGenerationMatch', u'ifSourceGenerationNotMatch', u'ifSourceMetagenerationMatch', u'ifSourceMetagenerationNotMatch', u'maxBytesRewrittenPerCall', u'projection', u'rewriteToken', u'sourceGeneration', u'userProject'],
+        relative_path=u'b/{sourceBucket}/o/{sourceObject}/rewriteTo/b/{destinationBucket}/o/{destinationObject}',
+        request_field=u'object',
+        request_type_name=u'StorageObjectsRewriteRequest',
+        response_type_name=u'RewriteResponse',
+        supports_download=False,
+    )
+
+    def SetIamPolicy(self, request, global_params=None):
+      r"""Updates an IAM policy for the specified object.
+
+      Args:
+        request: (StorageObjectsSetIamPolicyRequest) input message
+        global_params: (StandardQueryParameters, default: None) global arguments
+      Returns:
+        (Policy) The response message.
+      """
+      config = self.GetMethodConfig('SetIamPolicy')
+      return self._RunMethod(
+          config, request, global_params=global_params)
+
+    SetIamPolicy.method_config = lambda: base_api.ApiMethodInfo(
+        http_method=u'PUT',
+        method_id=u'storage.objects.setIamPolicy',
+        ordered_params=[u'bucket', u'object'],
+        path_params=[u'bucket', u'object'],
+        query_params=[u'generation', u'userProject'],
+        relative_path=u'b/{bucket}/o/{object}/iam',
+        request_field=u'policy',
+        request_type_name=u'StorageObjectsSetIamPolicyRequest',
+        response_type_name=u'Policy',
+        supports_download=False,
+    )
+
+    def TestIamPermissions(self, request, global_params=None):
+      r"""Tests a set of permissions on the given object to see which, if any, are held by the caller.
+
+      Args:
+        request: (StorageObjectsTestIamPermissionsRequest) input message
+        global_params: (StandardQueryParameters, default: None) global arguments
+      Returns:
+        (TestIamPermissionsResponse) The response message.
+      """
+      config = self.GetMethodConfig('TestIamPermissions')
+      return self._RunMethod(
+          config, request, global_params=global_params)
+
+    TestIamPermissions.method_config = lambda: base_api.ApiMethodInfo(
+        http_method=u'GET',
+        method_id=u'storage.objects.testIamPermissions',
+        ordered_params=[u'bucket', u'object', u'permissions'],
+        path_params=[u'bucket', u'object'],
+        query_params=[u'generation', u'permissions', u'userProject'],
+        relative_path=u'b/{bucket}/o/{object}/iam/testPermissions',
+        request_field='',
+        request_type_name=u'StorageObjectsTestIamPermissionsRequest',
+        response_type_name=u'TestIamPermissionsResponse',
+        supports_download=False,
+    )
+
+    def Update(self, request, global_params=None):
+      r"""Updates an object's metadata.
 
       Args:
         request: (StorageObjectsUpdateRequest) input message
         global_params: (StandardQueryParameters, default: None) global arguments
-        download: (Download, default: None) If present, download
-            data from the request via this stream.
       Returns:
         (Object) The response message.
       """
       config = self.GetMethodConfig('Update')
       return self._RunMethod(
-          config, request, global_params=global_params,
-          download=download)
+          config, request, global_params=global_params)
+
+    Update.method_config = lambda: base_api.ApiMethodInfo(
+        http_method=u'PUT',
+        method_id=u'storage.objects.update',
+        ordered_params=[u'bucket', u'object'],
+        path_params=[u'bucket', u'object'],
+        query_params=[u'generation', u'ifGenerationMatch', u'ifGenerationNotMatch', u'ifMetagenerationMatch', u'ifMetagenerationNotMatch', u'predefinedAcl', u'projection', u'userProject'],
+        relative_path=u'b/{bucket}/o/{object}',
+        request_field=u'objectResource',
+        request_type_name=u'StorageObjectsUpdateRequest',
+        response_type_name=u'Object',
+        supports_download=False,
+    )
 
     def WatchAll(self, request, global_params=None):
-      """Watch for changes on all objects in a bucket.
+      r"""Watch for changes on all objects in a bucket.
 
       Args:
         request: (StorageObjectsWatchAllRequest) input message
         global_params: (StandardQueryParameters, default: None) global arguments
       Returns:
         (Channel) The response message.
       """
       config = self.GetMethodConfig('WatchAll')
       return self._RunMethod(
           config, request, global_params=global_params)
+
+    WatchAll.method_config = lambda: base_api.ApiMethodInfo(
+        http_method=u'POST',
+        method_id=u'storage.objects.watchAll',
+        ordered_params=[u'bucket'],
+        path_params=[u'bucket'],
+        query_params=[u'delimiter', u'includeTrailingDelimiter', u'maxResults', u'pageToken', u'prefix', u'projection', u'userProject', u'versions'],
+        relative_path=u'b/{bucket}/o/watch',
+        request_field=u'channel',
+        request_type_name=u'StorageObjectsWatchAllRequest',
+        response_type_name=u'Channel',
+        supports_download=False,
+    )
+
+  class ProjectsServiceAccountService(base_api.BaseApiService):
+    """Service class for the projects_serviceAccount resource."""
+
+    _NAME = u'projects_serviceAccount'
+
+    def __init__(self, client):
+      super(StorageV1.ProjectsServiceAccountService, self).__init__(client)
+      self._upload_configs = {
+          }
+
+    def Get(self, request, global_params=None):
+      r"""Get the email address of this project's Google Cloud Storage service account.
+
+      Args:
+        request: (StorageProjectsServiceAccountGetRequest) input message
+        global_params: (StandardQueryParameters, default: None) global arguments
+      Returns:
+        (ServiceAccount) The response message.
+      """
+      config = self.GetMethodConfig('Get')
+      return self._RunMethod(
+          config, request, global_params=global_params)
+
+    Get.method_config = lambda: base_api.ApiMethodInfo(
+        http_method=u'GET',
+        method_id=u'storage.projects.serviceAccount.get',
+        ordered_params=[u'projectId'],
+        path_params=[u'projectId'],
+        query_params=[u'userProject'],
+        relative_path=u'projects/{projectId}/serviceAccount',
+        request_field='',
+        request_type_name=u'StorageProjectsServiceAccountGetRequest',
+        response_type_name=u'ServiceAccount',
+        supports_download=False,
+    )
+
+  class ProjectsService(base_api.BaseApiService):
+    """Service class for the projects resource."""
+
+    _NAME = u'projects'
+
+    def __init__(self, client):
+      super(StorageV1.ProjectsService, self).__init__(client)
+      self._upload_configs = {
+          }
```

## Comparing `apache-beam-2.8.0/apache_beam/io/gcp/internal/clients/storage/storage_v1_messages.py` & `apache-beam-2.9.0/apache_beam/io/gcp/internal/clients/storage/storage_v1_messages.py`

 * *Files 18% similar despite different names*

```diff
@@ -27,62 +27,117 @@
 from apitools.base.protorpclite import messages as _messages
 from apitools.base.py import encoding, extra_types
 
 package = 'storage'
 
 
 class Bucket(_messages.Message):
-  """A bucket.
+  r"""A bucket.
 
   Messages:
+    BillingValue: The bucket's billing configuration.
     CorsValueListEntry: A CorsValueListEntry object.
+    EncryptionValue: Encryption configuration for a bucket.
+    LabelsValue: User-provided labels, in key/value pairs.
     LifecycleValue: The bucket's lifecycle configuration. See lifecycle
       management for more information.
     LoggingValue: The bucket's logging configuration, which defines the
       destination bucket and optional name prefix for the current bucket's
       logs.
     OwnerValue: The owner of the bucket. This is always the project team's
       owner group.
+    RetentionPolicyValue: The bucket's retention policy. The retention policy
+      enforces a minimum retention time for all objects contained in the
+      bucket, based on their creation time. Any attempt to overwrite or delete
+      objects younger than the retention period will result in a
+      PERMISSION_DENIED error. An unlocked retention policy can be modified or
+      removed from the bucket via a storage.buckets.update operation. A locked
+      retention policy cannot be removed or shortened in duration for the
+      lifetime of the bucket. Attempting to remove or decrease period of a
+      locked retention policy will result in a PERMISSION_DENIED error.
     VersioningValue: The bucket's versioning configuration.
-    WebsiteValue: The bucket's website configuration.
+    WebsiteValue: The bucket's website configuration, controlling how the
+      service behaves when accessing bucket contents as a web site. See the
+      Static Website Examples for more information.
 
   Fields:
     acl: Access controls on the bucket.
+    billing: The bucket's billing configuration.
     cors: The bucket's Cross-Origin Resource Sharing (CORS) configuration.
+    defaultEventBasedHold: The default value for event-based hold on newly
+      created objects in this bucket. Event-based hold is a way to retain
+      objects indefinitely until an event occurs, signified by the hold's
+      release. After being released, such objects will be subject to bucket-
+      level retention (if any). One sample use case of this flag is for banks
+      to hold loan documents for at least 3 years after loan is paid in full.
+      Here, bucket-level retention is 3 years and the event is loan being paid
+      in full. In this example, these objects will be held intact for any
+      number of years until the event has occurred (event-based hold on the
+      object is released) and then 3 more years after that. That means
+      retention duration of the objects begins from the moment event-based
+      hold transitioned from true to false. Objects under event-based hold
+      cannot be deleted, overwritten or archived until the hold is removed.
     defaultObjectAcl: Default access controls to apply to new objects when no
       ACL is provided.
+    encryption: Encryption configuration for a bucket.
     etag: HTTP 1.1 Entity tag for the bucket.
-    id: The ID of the bucket.
+    id: The ID of the bucket. For buckets, the id and name properties are the
+      same.
     kind: The kind of item this is. For buckets, this is always
       storage#bucket.
+    labels: User-provided labels, in key/value pairs.
     lifecycle: The bucket's lifecycle configuration. See lifecycle management
       for more information.
     location: The location of the bucket. Object data for objects in the
       bucket resides in physical storage within this region. Defaults to US.
       See the developer's guide for the authoritative list.
     logging: The bucket's logging configuration, which defines the destination
       bucket and optional name prefix for the current bucket's logs.
     metageneration: The metadata generation of this bucket.
     name: The name of the bucket.
     owner: The owner of the bucket. This is always the project team's owner
       group.
     projectNumber: The project number of the project the bucket belongs to.
+    retentionPolicy: The bucket's retention policy. The retention policy
+      enforces a minimum retention time for all objects contained in the
+      bucket, based on their creation time. Any attempt to overwrite or delete
+      objects younger than the retention period will result in a
+      PERMISSION_DENIED error. An unlocked retention policy can be modified or
+      removed from the bucket via a storage.buckets.update operation. A locked
+      retention policy cannot be removed or shortened in duration for the
+      lifetime of the bucket. Attempting to remove or decrease period of a
+      locked retention policy will result in a PERMISSION_DENIED error.
     selfLink: The URI of this bucket.
-    storageClass: The bucket's storage class. This defines how objects in the
-      bucket are stored and determines the SLA and the cost of storage. Values
-      include STANDARD, NEARLINE and DURABLE_REDUCED_AVAILABILITY. Defaults to
-      STANDARD. For more information, see storage classes.
+    storageClass: The bucket's default storage class, used whenever no
+      storageClass is specified for a newly-created object. This defines how
+      objects in the bucket are stored and determines the SLA and the cost of
+      storage. Values include MULTI_REGIONAL, REGIONAL, STANDARD, NEARLINE,
+      COLDLINE, and DURABLE_REDUCED_AVAILABILITY. If this value is not
+      specified when the bucket is created, it will default to STANDARD. For
+      more information, see storage classes.
     timeCreated: The creation time of the bucket in RFC 3339 format.
     updated: The modification time of the bucket in RFC 3339 format.
     versioning: The bucket's versioning configuration.
-    website: The bucket's website configuration.
+    website: The bucket's website configuration, controlling how the service
+      behaves when accessing bucket contents as a web site. See the Static
+      Website Examples for more information.
   """
 
+  class BillingValue(_messages.Message):
+    r"""The bucket's billing configuration.
+
+    Fields:
+      requesterPays: When set to true, Requester Pays is enabled for this
+        bucket.
+    """
+
+    requesterPays = _messages.BooleanField(1)
+
   class CorsValueListEntry(_messages.Message):
-    """A CorsValueListEntry object.
+    r"""A CorsValueListEntry object.
 
     Fields:
       maxAgeSeconds: The value, in seconds, to return in the  Access-Control-
         Max-Age header used in preflight responses.
       method: The list of HTTP methods on which to include CORS response
         headers, (GET, OPTIONS, POST, etc) Note: "*" is permitted in the list
         of methods, and means "any method".
@@ -93,144 +148,233 @@
     """
 
     maxAgeSeconds = _messages.IntegerField(1, variant=_messages.Variant.INT32)
     method = _messages.StringField(2, repeated=True)
     origin = _messages.StringField(3, repeated=True)
     responseHeader = _messages.StringField(4, repeated=True)
 
+  class EncryptionValue(_messages.Message):
+    r"""Encryption configuration for a bucket.
+
+    Fields:
+      defaultKmsKeyName: A Cloud KMS key that will be used to encrypt objects
+        inserted into this bucket, if no encryption method is specified.
+    """
+
+    defaultKmsKeyName = _messages.StringField(1)
+
+  @encoding.MapUnrecognizedFields('additionalProperties')
+  class LabelsValue(_messages.Message):
+    r"""User-provided labels, in key/value pairs.
+
+    Messages:
+      AdditionalProperty: An additional property for a LabelsValue object.
+
+    Fields:
+      additionalProperties: An individual label entry.
+    """
+
+    class AdditionalProperty(_messages.Message):
+      r"""An additional property for a LabelsValue object.
+
+      Fields:
+        key: Name of the additional property.
+        value: A string attribute.
+      """
+
+      key = _messages.StringField(1)
+      value = _messages.StringField(2)
+
+    additionalProperties = _messages.MessageField('AdditionalProperty', 1, repeated=True)
+
   class LifecycleValue(_messages.Message):
-    """The bucket's lifecycle configuration. See lifecycle management for more
-    information.
+    r"""The bucket's lifecycle configuration. See lifecycle management for
+    more information.
 
     Messages:
       RuleValueListEntry: A RuleValueListEntry object.
 
     Fields:
       rule: A lifecycle management rule, which is made of an action to take
         and the condition(s) under which the action will be taken.
     """
 
     class RuleValueListEntry(_messages.Message):
-      """A RuleValueListEntry object.
+      r"""A RuleValueListEntry object.
 
       Messages:
         ActionValue: The action to take.
         ConditionValue: The condition(s) under which the action will be taken.
 
       Fields:
         action: The action to take.
         condition: The condition(s) under which the action will be taken.
       """
 
       class ActionValue(_messages.Message):
-        """The action to take.
+        r"""The action to take.
 
         Fields:
-          type: Type of the action. Currently, only Delete is supported.
+          storageClass: Target storage class. Required iff the type of the
+            action is SetStorageClass.
+          type: Type of the action. Currently, only Delete and SetStorageClass
+            are supported.
         """
 
-        type = _messages.StringField(1)
+        storageClass = _messages.StringField(1)
+        type = _messages.StringField(2)
 
       class ConditionValue(_messages.Message):
-        """The condition(s) under which the action will be taken.
+        r"""The condition(s) under which the action will be taken.
 
         Fields:
           age: Age of an object (in days). This condition is satisfied when an
             object reaches the specified age.
           createdBefore: A date in RFC 3339 format with only the date part
             (for instance, "2013-01-15"). This condition is satisfied when an
             object is created before midnight of the specified date in UTC.
           isLive: Relevant only for versioned objects. If the value is true,
             this condition matches live objects; if the value is false, it
             matches archived objects.
+          matchesPattern: A regular expression that satisfies the RE2 syntax.
+            This condition is satisfied when the name of the object matches
+            the RE2 pattern. Note: This feature is currently in the "Early
+            Access" launch stage and is only available to a whitelisted set of
+            users; that means that this feature may be changed in backward-
+            incompatible ways and that it is not guaranteed to be released.
+          matchesStorageClass: Objects having any of the storage classes
+            specified by this condition will be matched. Values include
+            MULTI_REGIONAL, REGIONAL, NEARLINE, COLDLINE, STANDARD, and
+            DURABLE_REDUCED_AVAILABILITY.
           numNewerVersions: Relevant only for versioned objects. If the value
             is N, this condition is satisfied when there are at least N
             versions (including the live version) newer than this version of
             the object.
         """
 
         age = _messages.IntegerField(1, variant=_messages.Variant.INT32)
         createdBefore = extra_types.DateField(2)
         isLive = _messages.BooleanField(3)
-        numNewerVersions = _messages.IntegerField(4, variant=_messages.Variant.INT32)
+        matchesPattern = _messages.StringField(4)
+        matchesStorageClass = _messages.StringField(5, repeated=True)
+        numNewerVersions = _messages.IntegerField(6, variant=_messages.Variant.INT32)
 
       action = _messages.MessageField('ActionValue', 1)
       condition = _messages.MessageField('ConditionValue', 2)
 
     rule = _messages.MessageField('RuleValueListEntry', 1, repeated=True)
 
   class LoggingValue(_messages.Message):
-    """The bucket's logging configuration, which defines the destination
+    r"""The bucket's logging configuration, which defines the destination
     bucket and optional name prefix for the current bucket's logs.
 
     Fields:
       logBucket: The destination bucket where the current bucket's logs should
         be placed.
       logObjectPrefix: A prefix for log object names.
     """
 
     logBucket = _messages.StringField(1)
     logObjectPrefix = _messages.StringField(2)
 
   class OwnerValue(_messages.Message):
-    """The owner of the bucket. This is always the project team's owner group.
+    r"""The owner of the bucket. This is always the project team's owner
+    group.
 
     Fields:
       entity: The entity, in the form project-owner-projectId.
       entityId: The ID for the entity.
     """
 
     entity = _messages.StringField(1)
     entityId = _messages.StringField(2)
 
+  class RetentionPolicyValue(_messages.Message):
+    r"""The bucket's retention policy. The retention policy enforces a minimum
+    retention time for all objects contained in the bucket, based on their
+    creation time. Any attempt to overwrite or delete objects younger than the
+    retention period will result in a PERMISSION_DENIED error. An unlocked
+    retention policy can be modified or removed from the bucket via a
+    storage.buckets.update operation. A locked retention policy cannot be
+    removed or shortened in duration for the lifetime of the bucket.
+    Attempting to remove or decrease period of a locked retention policy will
+    result in a PERMISSION_DENIED error.
+
+    Fields:
+      effectiveTime: Server-determined value that indicates the time from
+        which policy was enforced and effective. This value is in RFC 3339
+        format.
+      isLocked: Once locked, an object retention policy cannot be modified.
+      retentionPeriod: The duration in seconds that objects need to be
+        retained. Retention duration must be greater than zero and less than
+        100 years. Note that enforcement of retention periods less than a day
+        is not guaranteed. Such periods should only be used for testing
+        purposes.
+    """
+
+    effectiveTime = _message_types.DateTimeField(1)
+    isLocked = _messages.BooleanField(2)
+    retentionPeriod = _messages.IntegerField(3)
+
   class VersioningValue(_messages.Message):
-    """The bucket's versioning configuration.
+    r"""The bucket's versioning configuration.
 
     Fields:
       enabled: While set to true, versioning is fully enabled for this bucket.
     """
 
     enabled = _messages.BooleanField(1)
 
   class WebsiteValue(_messages.Message):
-    """The bucket's website configuration.
+    r"""The bucket's website configuration, controlling how the service
+    behaves when accessing bucket contents as a web site. See the Static
+    Website Examples for more information.
 
     Fields:
-      mainPageSuffix: Behaves as the bucket's directory index where missing
-        objects are treated as potential directories.
-      notFoundPage: The custom object to return when a requested resource is
-        not found.
+      mainPageSuffix: If the requested object path is missing, the service
+        will ensure the path has a trailing '/', append this suffix, and
+        attempt to retrieve the resulting object. This allows the creation of
+        index.html objects to represent directory pages.
+      notFoundPage: If the requested object path is missing, and any
+        mainPageSuffix object is missing, if applicable, the service will
+        return the named object from this bucket as the content for a 404 Not
+        Found result.
     """
 
     mainPageSuffix = _messages.StringField(1)
     notFoundPage = _messages.StringField(2)
 
   acl = _messages.MessageField('BucketAccessControl', 1, repeated=True)
-  cors = _messages.MessageField('CorsValueListEntry', 2, repeated=True)
-  defaultObjectAcl = _messages.MessageField('ObjectAccessControl', 3, repeated=True)
-  etag = _messages.StringField(4)
-  id = _messages.StringField(5)
-  kind = _messages.StringField(6, default=u'storage#bucket')
-  lifecycle = _messages.MessageField('LifecycleValue', 7)
-  location = _messages.StringField(8)
-  logging = _messages.MessageField('LoggingValue', 9)
-  metageneration = _messages.IntegerField(10)
-  name = _messages.StringField(11)
-  owner = _messages.MessageField('OwnerValue', 12)
-  projectNumber = _messages.IntegerField(13, variant=_messages.Variant.UINT64)
-  selfLink = _messages.StringField(14)
-  storageClass = _messages.StringField(15)
-  timeCreated = _message_types.DateTimeField(16)
-  updated = _message_types.DateTimeField(17)
-  versioning = _messages.MessageField('VersioningValue', 18)
-  website = _messages.MessageField('WebsiteValue', 19)
+  billing = _messages.MessageField('BillingValue', 2)
+  cors = _messages.MessageField('CorsValueListEntry', 3, repeated=True)
+  defaultEventBasedHold = _messages.BooleanField(4)
+  defaultObjectAcl = _messages.MessageField('ObjectAccessControl', 5, repeated=True)
+  encryption = _messages.MessageField('EncryptionValue', 6)
+  etag = _messages.StringField(7)
+  id = _messages.StringField(8)
+  kind = _messages.StringField(9, default=u'storage#bucket')
+  labels = _messages.MessageField('LabelsValue', 10)
+  lifecycle = _messages.MessageField('LifecycleValue', 11)
+  location = _messages.StringField(12)
+  logging = _messages.MessageField('LoggingValue', 13)
+  metageneration = _messages.IntegerField(14)
+  name = _messages.StringField(15)
+  owner = _messages.MessageField('OwnerValue', 16)
+  projectNumber = _messages.IntegerField(17, variant=_messages.Variant.UINT64)
+  retentionPolicy = _messages.MessageField('RetentionPolicyValue', 18)
+  selfLink = _messages.StringField(19)
+  storageClass = _messages.StringField(20)
+  timeCreated = _message_types.DateTimeField(21)
+  updated = _message_types.DateTimeField(22)
+  versioning = _messages.MessageField('VersioningValue', 23)
+  website = _messages.MessageField('WebsiteValue', 24)
 
 
 class BucketAccessControl(_messages.Message):
-  """An access-control entry.
+  r"""An access-control entry.
 
   Messages:
     ProjectTeamValue: The project team associated with the entity, if any.
 
   Fields:
     bucket: The name of the bucket.
     domain: The domain associated with the entity, if any.
@@ -244,25 +388,24 @@
       for Business domain example.com, the entity would be domain-example.com.
     entityId: The ID for the entity, if any.
     etag: HTTP 1.1 Entity tag for the access-control entry.
     id: The ID of the access-control entry.
     kind: The kind of item this is. For bucket access control entries, this is
       always storage#bucketAccessControl.
     projectTeam: The project team associated with the entity, if any.
-    role: The access permission for the entity. Can be READER, WRITER, or
-      OWNER.
+    role: The access permission for the entity.
     selfLink: The link to this access-control entry.
   """
 
   class ProjectTeamValue(_messages.Message):
-    """The project team associated with the entity, if any.
+    r"""The project team associated with the entity, if any.
 
     Fields:
       projectNumber: The project number.
-      team: The team. Can be owners, editors, or viewers.
+      team: The team.
     """
 
     projectNumber = _messages.StringField(1)
     team = _messages.StringField(2)
 
   bucket = _messages.StringField(1)
   domain = _messages.StringField(2)
@@ -274,28 +417,28 @@
   kind = _messages.StringField(8, default=u'storage#bucketAccessControl')
   projectTeam = _messages.MessageField('ProjectTeamValue', 9)
   role = _messages.StringField(10)
   selfLink = _messages.StringField(11)
 
 
 class BucketAccessControls(_messages.Message):
-  """An access-control list.
+  r"""An access-control list.
 
   Fields:
     items: The list of items.
     kind: The kind of item this is. For lists of bucket access control
       entries, this is always storage#bucketAccessControls.
   """
 
   items = _messages.MessageField('BucketAccessControl', 1, repeated=True)
   kind = _messages.StringField(2, default=u'storage#bucketAccessControls')
 
 
 class Buckets(_messages.Message):
-  """A list of buckets.
+  r"""A list of buckets.
 
   Fields:
     items: The list of items.
     kind: The kind of item this is. For lists of buckets, this is always
       storage#buckets.
     nextPageToken: The continuation token, used to page through large result
       sets. Provide this value in a subsequent request to return the next page
@@ -304,15 +447,15 @@
 
   items = _messages.MessageField('Bucket', 1, repeated=True)
   kind = _messages.StringField(2, default=u'storage#buckets')
   nextPageToken = _messages.StringField(3)
 
 
 class Channel(_messages.Message):
-  """An notification channel used to watch for resource changes.
+  r"""An notification channel used to watch for resource changes.
 
   Messages:
     ParamsValue: Additional parameters controlling delivery channel behavior.
       Optional.
 
   Fields:
     address: The address where notifications are delivered for this channel.
@@ -330,25 +473,25 @@
     token: An arbitrary string delivered to the target address with each
       notification delivered over this channel. Optional.
     type: The type of delivery mechanism used for this channel.
   """
 
   @encoding.MapUnrecognizedFields('additionalProperties')
   class ParamsValue(_messages.Message):
-    """Additional parameters controlling delivery channel behavior. Optional.
+    r"""Additional parameters controlling delivery channel behavior. Optional.
 
     Messages:
       AdditionalProperty: An additional property for a ParamsValue object.
 
     Fields:
       additionalProperties: Declares a new parameter by name.
     """
 
     class AdditionalProperty(_messages.Message):
-      """An additional property for a ParamsValue object.
+      r"""An additional property for a ParamsValue object.
 
       Fields:
         key: Name of the additional property.
         value: A string attribute.
       """
 
       key = _messages.StringField(1)
@@ -365,43 +508,43 @@
   resourceId = _messages.StringField(7)
   resourceUri = _messages.StringField(8)
   token = _messages.StringField(9)
   type = _messages.StringField(10)
 
 
 class ComposeRequest(_messages.Message):
-  """A Compose request.
+  r"""A Compose request.
 
   Messages:
     SourceObjectsValueListEntry: A SourceObjectsValueListEntry object.
 
   Fields:
     destination: Properties of the resulting object.
     kind: The kind of item this is.
     sourceObjects: The list of source objects that will be concatenated into a
       single object.
   """
 
   class SourceObjectsValueListEntry(_messages.Message):
-    """A SourceObjectsValueListEntry object.
+    r"""A SourceObjectsValueListEntry object.
 
     Messages:
       ObjectPreconditionsValue: Conditions that must be met for this operation
         to execute.
 
     Fields:
       generation: The generation of this object to use as the source.
-      name: The source object's name. The source object's bucket is implicitly
-        the destination bucket.
+      name: The source object's name. All source objects must reside in the
+        same bucket.
       objectPreconditions: Conditions that must be met for this operation to
         execute.
     """
 
     class ObjectPreconditionsValue(_messages.Message):
-      """Conditions that must be met for this operation to execute.
+      r"""Conditions that must be met for this operation to execute.
 
       Fields:
         ifGenerationMatch: Only perform the composition if the generation of
           the source object that would be used matches this value. If this
           value and a generation are both specified, they must be the same
           value or the call will fail.
       """
@@ -413,103 +556,214 @@
     objectPreconditions = _messages.MessageField('ObjectPreconditionsValue', 3)
 
   destination = _messages.MessageField('Object', 1)
   kind = _messages.StringField(2, default=u'storage#composeRequest')
   sourceObjects = _messages.MessageField('SourceObjectsValueListEntry', 3, repeated=True)
 
 
+class Notification(_messages.Message):
+  r"""A subscription to receive Google PubSub notifications.
+
+  Messages:
+    CustomAttributesValue: An optional list of additional attributes to attach
+      to each Cloud PubSub message published for this notification
+      subscription.
+
+  Fields:
+    custom_attributes: An optional list of additional attributes to attach to
+      each Cloud PubSub message published for this notification subscription.
+    etag: HTTP 1.1 Entity tag for this subscription notification.
+    event_types: If present, only send notifications about listed event types.
+      If empty, sent notifications for all event types.
+    id: The ID of the notification.
+    kind: The kind of item this is. For notifications, this is always
+      storage#notification.
+    object_name_prefix: If present, only apply this notification configuration
+      to object names that begin with this prefix.
+    payload_format: The desired content of the Payload.
+    selfLink: The canonical URL of this notification.
+    topic: The Cloud PubSub topic to which this subscription publishes.
+      Formatted as: '//pubsub.googleapis.com/projects/{project-
+      identifier}/topics/{my-topic}'
+  """
+
+  @encoding.MapUnrecognizedFields('additionalProperties')
+  class CustomAttributesValue(_messages.Message):
+    r"""An optional list of additional attributes to attach to each Cloud
+    PubSub message published for this notification subscription.
+
+    Messages:
+      AdditionalProperty: An additional property for a CustomAttributesValue
+        object.
+
+    Fields:
+      additionalProperties: Additional properties of type
+        CustomAttributesValue
+    """
+
+    class AdditionalProperty(_messages.Message):
+      r"""An additional property for a CustomAttributesValue object.
+
+      Fields:
+        key: Name of the additional property.
+        value: A string attribute.
+      """
+
+      key = _messages.StringField(1)
+      value = _messages.StringField(2)
+
+    additionalProperties = _messages.MessageField('AdditionalProperty', 1, repeated=True)
+
+  custom_attributes = _messages.MessageField('CustomAttributesValue', 1)
+  etag = _messages.StringField(2)
+  event_types = _messages.StringField(3, repeated=True)
+  id = _messages.StringField(4)
+  kind = _messages.StringField(5, default=u'storage#notification')
+  object_name_prefix = _messages.StringField(6)
+  payload_format = _messages.StringField(7, default=u'JSON_API_V1')
+  selfLink = _messages.StringField(8)
+  topic = _messages.StringField(9)
+
+
+class Notifications(_messages.Message):
+  r"""A list of notification subscriptions.
+
+  Fields:
+    items: The list of items.
+    kind: The kind of item this is. For lists of notifications, this is always
+      storage#notifications.
+  """
+
+  items = _messages.MessageField('Notification', 1, repeated=True)
+  kind = _messages.StringField(2, default=u'storage#notifications')
+
+
 class Object(_messages.Message):
-  """An object.
+  r"""An object.
 
   Messages:
     CustomerEncryptionValue: Metadata of customer-supplied encryption key, if
       the object is encrypted by such a key.
     MetadataValue: User-provided metadata, in key/value pairs.
     OwnerValue: The owner of the object. This will always be the uploader of
       the object.
 
   Fields:
     acl: Access controls on the object.
     bucket: The name of the bucket containing this object.
-    cacheControl: Cache-Control directive for the object data.
+    cacheControl: Cache-Control directive for the object data. If omitted, and
+      the object is accessible to all anonymous users, the default will be
+      public, max-age=3600.
     componentCount: Number of underlying components that make up this object.
       Components are accumulated by compose operations.
     contentDisposition: Content-Disposition of the object data.
     contentEncoding: Content-Encoding of the object data.
     contentLanguage: Content-Language of the object data.
-    contentType: Content-Type of the object data.
+    contentType: Content-Type of the object data. If an object is stored
+      without a Content-Type, it is served as application/octet-stream.
     crc32c: CRC32c checksum, as described in RFC 4960, Appendix B; encoded
       using base64 in big-endian byte order. For more information about using
       the CRC32c checksum, see Hashes and ETags: Best Practices.
     customerEncryption: Metadata of customer-supplied encryption key, if the
       object is encrypted by such a key.
     etag: HTTP 1.1 Entity tag for the object.
+    eventBasedHold: Whether an object is under event-based hold. Event-based
+      hold is a way to retain objects until an event occurs, which is
+      signified by the hold's release (i.e. this value is set to false). After
+      being released (set to false), such objects will be subject to bucket-
+      level retention (if any). One sample use case of this flag is for banks
+      to hold loan documents for at least 3 years after loan is paid in full.
+      Here, bucket-level retention is 3 years and the event is the loan being
+      paid in full. In this example, these objects will be held intact for any
+      number of years until the event has occurred (event-based hold on the
+      object is released) and then 3 more years after that. That means
+      retention duration of the objects begins from the moment event-based
+      hold transitioned from true to false.
     generation: The content generation of this object. Used for object
       versioning.
-    id: The ID of the object.
+    id: The ID of the object, including the bucket name, object name, and
+      generation number.
     kind: The kind of item this is. For objects, this is always
       storage#object.
+    kmsKeyName: Cloud KMS Key used to encrypt this object, if the object is
+      encrypted by such a key.
     md5Hash: MD5 hash of the data; encoded using base64. For more information
       about using the MD5 hash, see Hashes and ETags: Best Practices.
     mediaLink: Media download link.
     metadata: User-provided metadata, in key/value pairs.
     metageneration: The version of the metadata for this object at this
       generation. Used for preconditions and for detecting changes in
       metadata. A metageneration number is only meaningful in the context of a
       particular generation of a particular object.
-    name: The name of this object. Required if not specified by URL parameter.
+    name: The name of the object. Required if not specified by URL parameter.
     owner: The owner of the object. This will always be the uploader of the
       object.
+    retentionExpirationTime: A server-determined value that specifies the
+      earliest time that the object's retention period expires. This value is
+      in RFC 3339 format. Note 1: This field is not provided for objects with
+      an active event-based hold, since retention expiration is unknown until
+      the hold is removed. Note 2: This value can be provided even when
+      temporary hold is set (so that the user can reason about policy without
+      having to first unset the temporary hold).
     selfLink: The link to this object.
     size: Content-Length of the data in bytes.
     storageClass: Storage class of the object.
+    temporaryHold: Whether an object is under temporary hold. While this flag
+      is set to true, the object is protected against deletion and overwrites.
+      A common use case of this flag is regulatory investigations where
+      objects need to be retained while the investigation is ongoing. Note
+      that unlike event-based hold, temporary hold does not impact retention
+      expiration time of an object.
     timeCreated: The creation time of the object in RFC 3339 format.
     timeDeleted: The deletion time of the object in RFC 3339 format. Will be
       returned if and only if this version of the object has been deleted.
+    timeStorageClassUpdated: The time at which the object's storage class was
+      last changed. When the object is initially created, it will be set to
+      timeCreated.
     updated: The modification time of the object metadata in RFC 3339 format.
   """
 
   class CustomerEncryptionValue(_messages.Message):
-    """Metadata of customer-supplied encryption key, if the object is
+    r"""Metadata of customer-supplied encryption key, if the object is
     encrypted by such a key.
 
     Fields:
       encryptionAlgorithm: The encryption algorithm.
       keySha256: SHA256 hash value of the encryption key.
     """
 
     encryptionAlgorithm = _messages.StringField(1)
     keySha256 = _messages.StringField(2)
 
   @encoding.MapUnrecognizedFields('additionalProperties')
   class MetadataValue(_messages.Message):
-    """User-provided metadata, in key/value pairs.
+    r"""User-provided metadata, in key/value pairs.
 
     Messages:
       AdditionalProperty: An additional property for a MetadataValue object.
 
     Fields:
       additionalProperties: An individual metadata entry.
     """
 
     class AdditionalProperty(_messages.Message):
-      """An additional property for a MetadataValue object.
+      r"""An additional property for a MetadataValue object.
 
       Fields:
         key: Name of the additional property.
         value: A string attribute.
       """
 
       key = _messages.StringField(1)
       value = _messages.StringField(2)
 
     additionalProperties = _messages.MessageField('AdditionalProperty', 1, repeated=True)
 
   class OwnerValue(_messages.Message):
-    """The owner of the object. This will always be the uploader of the
+    r"""The owner of the object. This will always be the uploader of the
     object.
 
     Fields:
       entity: The entity, in the form user-userId.
       entityId: The ID for the entity.
     """
 
@@ -523,33 +777,38 @@
   contentDisposition = _messages.StringField(5)
   contentEncoding = _messages.StringField(6)
   contentLanguage = _messages.StringField(7)
   contentType = _messages.StringField(8)
   crc32c = _messages.StringField(9)
   customerEncryption = _messages.MessageField('CustomerEncryptionValue', 10)
   etag = _messages.StringField(11)
-  generation = _messages.IntegerField(12)
-  id = _messages.StringField(13)
-  kind = _messages.StringField(14, default=u'storage#object')
-  md5Hash = _messages.StringField(15)
-  mediaLink = _messages.StringField(16)
-  metadata = _messages.MessageField('MetadataValue', 17)
-  metageneration = _messages.IntegerField(18)
-  name = _messages.StringField(19)
-  owner = _messages.MessageField('OwnerValue', 20)
-  selfLink = _messages.StringField(21)
-  size = _messages.IntegerField(22, variant=_messages.Variant.UINT64)
-  storageClass = _messages.StringField(23)
-  timeCreated = _message_types.DateTimeField(24)
-  timeDeleted = _message_types.DateTimeField(25)
-  updated = _message_types.DateTimeField(26)
+  eventBasedHold = _messages.BooleanField(12)
+  generation = _messages.IntegerField(13)
+  id = _messages.StringField(14)
+  kind = _messages.StringField(15, default=u'storage#object')
+  kmsKeyName = _messages.StringField(16)
+  md5Hash = _messages.StringField(17)
+  mediaLink = _messages.StringField(18)
+  metadata = _messages.MessageField('MetadataValue', 19)
+  metageneration = _messages.IntegerField(20)
+  name = _messages.StringField(21)
+  owner = _messages.MessageField('OwnerValue', 22)
+  retentionExpirationTime = _message_types.DateTimeField(23)
+  selfLink = _messages.StringField(24)
+  size = _messages.IntegerField(25, variant=_messages.Variant.UINT64)
+  storageClass = _messages.StringField(26)
+  temporaryHold = _messages.BooleanField(27)
+  timeCreated = _message_types.DateTimeField(28)
+  timeDeleted = _message_types.DateTimeField(29)
+  timeStorageClassUpdated = _message_types.DateTimeField(30)
+  updated = _message_types.DateTimeField(31)
 
 
 class ObjectAccessControl(_messages.Message):
-  """An access-control entry.
+  r"""An access-control entry.
 
   Messages:
     ProjectTeamValue: The project team associated with the entity, if any.
 
   Fields:
     bucket: The name of the bucket.
     domain: The domain associated with the entity, if any.
@@ -559,30 +818,30 @@
       domain  - project-team-projectId  - allUsers  - allAuthenticatedUsers
       Examples:  - The user liz@example.com would be user-liz@example.com.  -
       The group example@googlegroups.com would be group-
       example@googlegroups.com.  - To refer to all members of the Google Apps
       for Business domain example.com, the entity would be domain-example.com.
     entityId: The ID for the entity, if any.
     etag: HTTP 1.1 Entity tag for the access-control entry.
-    generation: The content generation of the object.
+    generation: The content generation of the object, if applied to an object.
     id: The ID of the access-control entry.
     kind: The kind of item this is. For object access control entries, this is
       always storage#objectAccessControl.
-    object: The name of the object.
+    object: The name of the object, if applied to an object.
     projectTeam: The project team associated with the entity, if any.
-    role: The access permission for the entity. Can be READER or OWNER.
+    role: The access permission for the entity.
     selfLink: The link to this access-control entry.
   """
 
   class ProjectTeamValue(_messages.Message):
-    """The project team associated with the entity, if any.
+    r"""The project team associated with the entity, if any.
 
     Fields:
       projectNumber: The project number.
-      team: The team. Can be owners, editors, or viewers.
+      team: The team.
     """
 
     projectNumber = _messages.StringField(1)
     team = _messages.StringField(2)
 
   bucket = _messages.StringField(1)
   domain = _messages.StringField(2)
@@ -596,28 +855,28 @@
   object = _messages.StringField(10)
   projectTeam = _messages.MessageField('ProjectTeamValue', 11)
   role = _messages.StringField(12)
   selfLink = _messages.StringField(13)
 
 
 class ObjectAccessControls(_messages.Message):
-  """An access-control list.
+  r"""An access-control list.
 
   Fields:
     items: The list of items.
     kind: The kind of item this is. For lists of object access control
       entries, this is always storage#objectAccessControls.
   """
 
-  items = _messages.MessageField('extra_types.JsonValue', 1, repeated=True)
+  items = _messages.MessageField('ObjectAccessControl', 1, repeated=True)
   kind = _messages.StringField(2, default=u'storage#objectAccessControls')
 
 
 class Objects(_messages.Message):
-  """A list of objects.
+  r"""A list of objects.
 
   Fields:
     items: The list of items.
     kind: The kind of item this is. For lists of objects, this is always
       storage#objects.
     nextPageToken: The continuation token, used to page through large result
       sets. Provide this value in a subsequent request to return the next page
@@ -628,16 +887,95 @@
 
   items = _messages.MessageField('Object', 1, repeated=True)
   kind = _messages.StringField(2, default=u'storage#objects')
   nextPageToken = _messages.StringField(3)
   prefixes = _messages.StringField(4, repeated=True)
 
 
+class Policy(_messages.Message):
+  r"""A bucket/object IAM policy.
+
+  Messages:
+    BindingsValueListEntry: A BindingsValueListEntry object.
+
+  Fields:
+    bindings: An association between a role, which comes with a set of
+      permissions, and members who may assume that role.
+    etag: HTTP 1.1  Entity tag for the policy.
+    kind: The kind of item this is. For policies, this is always
+      storage#policy. This field is ignored on input.
+    resourceId: The ID of the resource to which this policy belongs. Will be
+      of the form projects/_/buckets/bucket for buckets, and
+      projects/_/buckets/bucket/objects/object for objects. A specific
+      generation may be specified by appending #generationNumber to the end of
+      the object name, e.g. projects/_/buckets/my-bucket/objects/data.txt#17.
+      The current generation can be denoted with #0. This field is ignored on
+      input.
+  """
+
+  class BindingsValueListEntry(_messages.Message):
+    r"""A BindingsValueListEntry object.
+
+    Fields:
+      condition: A extra_types.JsonValue attribute.
+      members: A collection of identifiers for members who may assume the
+        provided role. Recognized identifiers are as follows:   - allUsers - A
+        special identifier that represents anyone on the internet; with or
+        without a Google account.   - allAuthenticatedUsers - A special
+        identifier that represents anyone who is authenticated with a Google
+        account or a service account.   - user:emailid - An email address that
+        represents a specific account. For example, user:alice@gmail.com or
+        user:joe@example.com.   - serviceAccount:emailid - An email address
+        that represents a service account. For example,  serviceAccount:my-
+        other-app@appspot.gserviceaccount.com .   - group:emailid - An email
+        address that represents a Google group. For example,
+        group:admins@example.com.   - domain:domain - A Google Apps domain
+        name that represents all the users of that domain. For example,
+        domain:google.com or domain:example.com.   - projectOwner:projectid -
+        Owners of the given project. For example, projectOwner:my-example-
+        project   - projectEditor:projectid - Editors of the given project.
+        For example, projectEditor:my-example-project   -
+        projectViewer:projectid - Viewers of the given project. For example,
+        projectViewer:my-example-project
+      role: The role to which members belong. Two types of roles are
+        supported: new IAM roles, which grant permissions that do not map
+        directly to those provided by ACLs, and legacy IAM roles, which do map
+        directly to ACL permissions. All roles are of the format
+        roles/storage.specificRole. The new IAM roles are:   -
+        roles/storage.admin - Full control of Google Cloud Storage resources.
+        - roles/storage.objectViewer - Read-Only access to Google Cloud
+        Storage objects.   - roles/storage.objectCreator - Access to create
+        objects in Google Cloud Storage.   - roles/storage.objectAdmin - Full
+        control of Google Cloud Storage objects.   The legacy IAM roles are:
+        - roles/storage.legacyObjectReader - Read-only access to objects
+        without listing. Equivalent to an ACL entry on an object with the
+        READER role.   - roles/storage.legacyObjectOwner - Read/write access
+        to existing objects without listing. Equivalent to an ACL entry on an
+        object with the OWNER role.   - roles/storage.legacyBucketReader -
+        Read access to buckets with object listing. Equivalent to an ACL entry
+        on a bucket with the READER role.   - roles/storage.legacyBucketWriter
+        - Read access to buckets with object listing/creation/deletion.
+        Equivalent to an ACL entry on a bucket with the WRITER role.   -
+        roles/storage.legacyBucketOwner - Read and write access to existing
+        buckets with object listing/creation/deletion. Equivalent to an ACL
+        entry on a bucket with the OWNER role.
+    """
+
+    condition = _messages.MessageField('extra_types.JsonValue', 1)
+    members = _messages.StringField(2, repeated=True)
+    role = _messages.StringField(3)
+
+  bindings = _messages.MessageField('BindingsValueListEntry', 1, repeated=True)
+  etag = _messages.BytesField(2)
+  kind = _messages.StringField(3, default=u'storage#policy')
+  resourceId = _messages.StringField(4)
+
+
 class RewriteResponse(_messages.Message):
-  """A rewrite response.
+  r"""A rewrite response.
 
   Fields:
     done: true if the copy is finished; otherwise, false if the copy is in
       progress. This property is always present in the response.
     kind: The kind of item this is.
     objectSize: The total size of the object being copied in bytes. This
       property is always present in the response.
@@ -649,45 +987,56 @@
     totalBytesRewritten: The total bytes written so far, which can be used to
       provide a waiting user with a progress indicator. This property is
       always present in the response.
   """
 
   done = _messages.BooleanField(1)
   kind = _messages.StringField(2, default=u'storage#rewriteResponse')
-  objectSize = _messages.IntegerField(3, variant=_messages.Variant.UINT64)
+  objectSize = _messages.IntegerField(3)
   resource = _messages.MessageField('Object', 4)
   rewriteToken = _messages.StringField(5)
-  totalBytesRewritten = _messages.IntegerField(6, variant=_messages.Variant.UINT64)
+  totalBytesRewritten = _messages.IntegerField(6)
+
+
+class ServiceAccount(_messages.Message):
+  r"""A subscription to receive Google PubSub notifications.
+
+  Fields:
+    email_address: The ID of the notification.
+    kind: The kind of item this is. For notifications, this is always
+      storage#notification.
+  """
+
+  email_address = _messages.StringField(1)
+  kind = _messages.StringField(2, default=u'storage#serviceAccount')
 
 
 class StandardQueryParameters(_messages.Message):
-  """Query parameters accepted by all methods.
+  r"""Query parameters accepted by all methods.
 
   Enums:
     AltValueValuesEnum: Data format for the response.
 
   Fields:
     alt: Data format for the response.
     fields: Selector specifying which fields to include in a partial response.
     key: API key. Your API key identifies your project and provides you with
       API access, quota, and reports. Required unless you provide an OAuth 2.0
       token.
     oauth_token: OAuth 2.0 token for the current user.
     prettyPrint: Returns response with indentations and line breaks.
-    quotaUser: Available to use for quota purposes for server-side
-      applications. Can be any arbitrary string assigned to a user, but should
-      not exceed 40 characters. Overrides userIp if both are provided.
+    quotaUser: An opaque string that represents a user for quota purposes.
+      Must not exceed 40 characters.
     trace: A tracing token of the form "token:<tokenid>" to include in api
       requests.
-    userIp: IP address of the site where the request originates. Use this if
-      you want to enforce per-user limits.
+    userIp: Deprecated. Please use quotaUser instead.
   """
 
   class AltValueValuesEnum(_messages.Enum):
-    """Data format for the response.
+    r"""Data format for the response.
 
     Values:
       json: Responses with Content-Type of application/json
     """
     json = 0
 
   alt = _messages.EnumField('AltValueValuesEnum', 1, default=u'json')
@@ -697,109 +1046,193 @@
   prettyPrint = _messages.BooleanField(5, default=True)
   quotaUser = _messages.StringField(6)
   trace = _messages.StringField(7)
   userIp = _messages.StringField(8)
 
 
 class StorageBucketAccessControlsDeleteRequest(_messages.Message):
-  """A StorageBucketAccessControlsDeleteRequest object.
+  r"""A StorageBucketAccessControlsDeleteRequest object.
 
   Fields:
     bucket: Name of a bucket.
     entity: The entity holding the permission. Can be user-userId, user-
       emailAddress, group-groupId, group-emailAddress, allUsers, or
       allAuthenticatedUsers.
+    userProject: The project to be billed for this request. Required for
+      Requester Pays buckets.
   """
 
   bucket = _messages.StringField(1, required=True)
   entity = _messages.StringField(2, required=True)
+  userProject = _messages.StringField(3)
 
 
 class StorageBucketAccessControlsDeleteResponse(_messages.Message):
-  """An empty StorageBucketAccessControlsDelete response."""
+  r"""An empty StorageBucketAccessControlsDelete response."""
 
 
 class StorageBucketAccessControlsGetRequest(_messages.Message):
-  """A StorageBucketAccessControlsGetRequest object.
+  r"""A StorageBucketAccessControlsGetRequest object.
 
   Fields:
     bucket: Name of a bucket.
     entity: The entity holding the permission. Can be user-userId, user-
       emailAddress, group-groupId, group-emailAddress, allUsers, or
       allAuthenticatedUsers.
+    userProject: The project to be billed for this request. Required for
+      Requester Pays buckets.
   """
 
   bucket = _messages.StringField(1, required=True)
   entity = _messages.StringField(2, required=True)
+  userProject = _messages.StringField(3)
+
+
+class StorageBucketAccessControlsInsertRequest(_messages.Message):
+  r"""A StorageBucketAccessControlsInsertRequest object.
+
+  Fields:
+    bucket: Name of a bucket.
+    bucketAccessControl: A BucketAccessControl resource to be passed as the
+      request body.
+    userProject: The project to be billed for this request. Required for
+      Requester Pays buckets.
+  """
+
+  bucket = _messages.StringField(1, required=True)
+  bucketAccessControl = _messages.MessageField('BucketAccessControl', 2)
+  userProject = _messages.StringField(3)
 
 
 class StorageBucketAccessControlsListRequest(_messages.Message):
-  """A StorageBucketAccessControlsListRequest object.
+  r"""A StorageBucketAccessControlsListRequest object.
+
+  Fields:
+    bucket: Name of a bucket.
+    userProject: The project to be billed for this request. Required for
+      Requester Pays buckets.
+  """
+
+  bucket = _messages.StringField(1, required=True)
+  userProject = _messages.StringField(2)
+
+
+class StorageBucketAccessControlsPatchRequest(_messages.Message):
+  r"""A StorageBucketAccessControlsPatchRequest object.
 
   Fields:
     bucket: Name of a bucket.
+    bucketAccessControl: A BucketAccessControl resource to be passed as the
+      request body.
+    entity: The entity holding the permission. Can be user-userId, user-
+      emailAddress, group-groupId, group-emailAddress, allUsers, or
+      allAuthenticatedUsers.
+    userProject: The project to be billed for this request. Required for
+      Requester Pays buckets.
   """
 
   bucket = _messages.StringField(1, required=True)
+  bucketAccessControl = _messages.MessageField('BucketAccessControl', 2)
+  entity = _messages.StringField(3, required=True)
+  userProject = _messages.StringField(4)
+
+
+class StorageBucketAccessControlsUpdateRequest(_messages.Message):
+  r"""A StorageBucketAccessControlsUpdateRequest object.
+
+  Fields:
+    bucket: Name of a bucket.
+    bucketAccessControl: A BucketAccessControl resource to be passed as the
+      request body.
+    entity: The entity holding the permission. Can be user-userId, user-
+      emailAddress, group-groupId, group-emailAddress, allUsers, or
+      allAuthenticatedUsers.
+    userProject: The project to be billed for this request. Required for
+      Requester Pays buckets.
+  """
+
+  bucket = _messages.StringField(1, required=True)
+  bucketAccessControl = _messages.MessageField('BucketAccessControl', 2)
+  entity = _messages.StringField(3, required=True)
+  userProject = _messages.StringField(4)
 
 
 class StorageBucketsDeleteRequest(_messages.Message):
-  """A StorageBucketsDeleteRequest object.
+  r"""A StorageBucketsDeleteRequest object.
 
   Fields:
     bucket: Name of a bucket.
     ifMetagenerationMatch: If set, only deletes the bucket if its
       metageneration matches this value.
     ifMetagenerationNotMatch: If set, only deletes the bucket if its
       metageneration does not match this value.
+    userProject: The project to be billed for this request. Required for
+      Requester Pays buckets.
   """
 
   bucket = _messages.StringField(1, required=True)
   ifMetagenerationMatch = _messages.IntegerField(2)
   ifMetagenerationNotMatch = _messages.IntegerField(3)
+  userProject = _messages.StringField(4)
 
 
 class StorageBucketsDeleteResponse(_messages.Message):
-  """An empty StorageBucketsDelete response."""
+  r"""An empty StorageBucketsDelete response."""
+
+
+class StorageBucketsGetIamPolicyRequest(_messages.Message):
+  r"""A StorageBucketsGetIamPolicyRequest object.
+
+  Fields:
+    bucket: Name of a bucket.
+    userProject: The project to be billed for this request. Required for
+      Requester Pays buckets.
+  """
+
+  bucket = _messages.StringField(1, required=True)
+  userProject = _messages.StringField(2)
 
 
 class StorageBucketsGetRequest(_messages.Message):
-  """A StorageBucketsGetRequest object.
+  r"""A StorageBucketsGetRequest object.
 
   Enums:
     ProjectionValueValuesEnum: Set of properties to return. Defaults to noAcl.
 
   Fields:
     bucket: Name of a bucket.
     ifMetagenerationMatch: Makes the return of the bucket metadata conditional
       on whether the bucket's current metageneration matches the given value.
     ifMetagenerationNotMatch: Makes the return of the bucket metadata
       conditional on whether the bucket's current metageneration does not
       match the given value.
     projection: Set of properties to return. Defaults to noAcl.
+    userProject: The project to be billed for this request. Required for
+      Requester Pays buckets.
   """
 
   class ProjectionValueValuesEnum(_messages.Enum):
-    """Set of properties to return. Defaults to noAcl.
+    r"""Set of properties to return. Defaults to noAcl.
 
     Values:
       full: Include all properties.
-      noAcl: Omit acl and defaultObjectAcl properties.
+      noAcl: Omit owner, acl and defaultObjectAcl properties.
     """
     full = 0
     noAcl = 1
 
   bucket = _messages.StringField(1, required=True)
   ifMetagenerationMatch = _messages.IntegerField(2)
   ifMetagenerationNotMatch = _messages.IntegerField(3)
   projection = _messages.EnumField('ProjectionValueValuesEnum', 4)
+  userProject = _messages.StringField(5)
 
 
 class StorageBucketsInsertRequest(_messages.Message):
-  """A StorageBucketsInsertRequest object.
+  r"""A StorageBucketsInsertRequest object.
 
   Enums:
     PredefinedAclValueValuesEnum: Apply a predefined set of access controls to
       this bucket.
     PredefinedDefaultObjectAclValueValuesEnum: Apply a predefined set of
       default object access controls to this bucket.
     ProjectionValueValuesEnum: Set of properties to return. Defaults to noAcl,
@@ -811,18 +1244,19 @@
     predefinedAcl: Apply a predefined set of access controls to this bucket.
     predefinedDefaultObjectAcl: Apply a predefined set of default object
       access controls to this bucket.
     project: A valid API project identifier.
     projection: Set of properties to return. Defaults to noAcl, unless the
       bucket resource specifies acl or defaultObjectAcl properties, when it
       defaults to full.
+    userProject: The project to be billed for this request.
   """
 
   class PredefinedAclValueValuesEnum(_messages.Enum):
-    """Apply a predefined set of access controls to this bucket.
+    r"""Apply a predefined set of access controls to this bucket.
 
     Values:
       authenticatedRead: Project team owners get OWNER access, and
         allAuthenticatedUsers get READER access.
       private: Project team owners get OWNER access.
       projectPrivate: Project team members get access according to their
         roles.
@@ -834,15 +1268,15 @@
     authenticatedRead = 0
     private = 1
     projectPrivate = 2
     publicRead = 3
     publicReadWrite = 4
 
   class PredefinedDefaultObjectAclValueValuesEnum(_messages.Enum):
-    """Apply a predefined set of default object access controls to this
+    r"""Apply a predefined set of default object access controls to this
     bucket.
 
     Values:
       authenticatedRead: Object owner gets OWNER access, and
         allAuthenticatedUsers get READER access.
       bucketOwnerFullControl: Object owner gets OWNER access, and project team
         owners get OWNER access.
@@ -858,66 +1292,86 @@
     bucketOwnerFullControl = 1
     bucketOwnerRead = 2
     private = 3
     projectPrivate = 4
     publicRead = 5
 
   class ProjectionValueValuesEnum(_messages.Enum):
-    """Set of properties to return. Defaults to noAcl, unless the bucket
+    r"""Set of properties to return. Defaults to noAcl, unless the bucket
     resource specifies acl or defaultObjectAcl properties, when it defaults to
     full.
 
     Values:
       full: Include all properties.
-      noAcl: Omit acl and defaultObjectAcl properties.
+      noAcl: Omit owner, acl and defaultObjectAcl properties.
     """
     full = 0
     noAcl = 1
 
   bucket = _messages.MessageField('Bucket', 1)
   predefinedAcl = _messages.EnumField('PredefinedAclValueValuesEnum', 2)
   predefinedDefaultObjectAcl = _messages.EnumField('PredefinedDefaultObjectAclValueValuesEnum', 3)
   project = _messages.StringField(4, required=True)
   projection = _messages.EnumField('ProjectionValueValuesEnum', 5)
+  userProject = _messages.StringField(6)
 
 
 class StorageBucketsListRequest(_messages.Message):
-  """A StorageBucketsListRequest object.
+  r"""A StorageBucketsListRequest object.
 
   Enums:
     ProjectionValueValuesEnum: Set of properties to return. Defaults to noAcl.
 
   Fields:
-    maxResults: Maximum number of buckets to return.
+    maxResults: Maximum number of buckets to return in a single response. The
+      service will use this parameter or 1,000 items, whichever is smaller.
     pageToken: A previously-returned page token representing part of the
       larger set of results to view.
     prefix: Filter results to buckets whose names begin with this prefix.
     project: A valid API project identifier.
     projection: Set of properties to return. Defaults to noAcl.
+    userProject: The project to be billed for this request.
   """
 
   class ProjectionValueValuesEnum(_messages.Enum):
-    """Set of properties to return. Defaults to noAcl.
+    r"""Set of properties to return. Defaults to noAcl.
 
     Values:
       full: Include all properties.
-      noAcl: Omit acl and defaultObjectAcl properties.
+      noAcl: Omit owner, acl and defaultObjectAcl properties.
     """
     full = 0
     noAcl = 1
 
-  maxResults = _messages.IntegerField(1, variant=_messages.Variant.UINT32)
+  maxResults = _messages.IntegerField(1, variant=_messages.Variant.UINT32, default=1000)
   pageToken = _messages.StringField(2)
   prefix = _messages.StringField(3)
   project = _messages.StringField(4, required=True)
   projection = _messages.EnumField('ProjectionValueValuesEnum', 5)
+  userProject = _messages.StringField(6)
+
+
+class StorageBucketsLockRetentionPolicyRequest(_messages.Message):
+  r"""A StorageBucketsLockRetentionPolicyRequest object.
+
+  Fields:
+    bucket: Name of a bucket.
+    ifMetagenerationMatch: Makes the operation conditional on whether bucket's
+      current metageneration matches the given value.
+    userProject: The project to be billed for this request. Required for
+      Requester Pays buckets.
+  """
+
+  bucket = _messages.StringField(1, required=True)
+  ifMetagenerationMatch = _messages.IntegerField(2, required=True)
+  userProject = _messages.StringField(3)
 
 
 class StorageBucketsPatchRequest(_messages.Message):
-  """A StorageBucketsPatchRequest object.
+  r"""A StorageBucketsPatchRequest object.
 
   Enums:
     PredefinedAclValueValuesEnum: Apply a predefined set of access controls to
       this bucket.
     PredefinedDefaultObjectAclValueValuesEnum: Apply a predefined set of
       default object access controls to this bucket.
     ProjectionValueValuesEnum: Set of properties to return. Defaults to full.
@@ -930,18 +1384,20 @@
     ifMetagenerationNotMatch: Makes the return of the bucket metadata
       conditional on whether the bucket's current metageneration does not
       match the given value.
     predefinedAcl: Apply a predefined set of access controls to this bucket.
     predefinedDefaultObjectAcl: Apply a predefined set of default object
       access controls to this bucket.
     projection: Set of properties to return. Defaults to full.
+    userProject: The project to be billed for this request. Required for
+      Requester Pays buckets.
   """
 
   class PredefinedAclValueValuesEnum(_messages.Enum):
-    """Apply a predefined set of access controls to this bucket.
+    r"""Apply a predefined set of access controls to this bucket.
 
     Values:
       authenticatedRead: Project team owners get OWNER access, and
         allAuthenticatedUsers get READER access.
       private: Project team owners get OWNER access.
       projectPrivate: Project team members get access according to their
         roles.
@@ -953,15 +1409,15 @@
     authenticatedRead = 0
     private = 1
     projectPrivate = 2
     publicRead = 3
     publicReadWrite = 4
 
   class PredefinedDefaultObjectAclValueValuesEnum(_messages.Enum):
-    """Apply a predefined set of default object access controls to this
+    r"""Apply a predefined set of default object access controls to this
     bucket.
 
     Values:
       authenticatedRead: Object owner gets OWNER access, and
         allAuthenticatedUsers get READER access.
       bucketOwnerFullControl: Object owner gets OWNER access, and project team
         owners get OWNER access.
@@ -977,34 +1433,65 @@
     bucketOwnerFullControl = 1
     bucketOwnerRead = 2
     private = 3
     projectPrivate = 4
     publicRead = 5
 
   class ProjectionValueValuesEnum(_messages.Enum):
-    """Set of properties to return. Defaults to full.
+    r"""Set of properties to return. Defaults to full.
 
     Values:
       full: Include all properties.
-      noAcl: Omit acl and defaultObjectAcl properties.
+      noAcl: Omit owner, acl and defaultObjectAcl properties.
     """
     full = 0
     noAcl = 1
 
   bucket = _messages.StringField(1, required=True)
   bucketResource = _messages.MessageField('Bucket', 2)
   ifMetagenerationMatch = _messages.IntegerField(3)
   ifMetagenerationNotMatch = _messages.IntegerField(4)
   predefinedAcl = _messages.EnumField('PredefinedAclValueValuesEnum', 5)
   predefinedDefaultObjectAcl = _messages.EnumField('PredefinedDefaultObjectAclValueValuesEnum', 6)
   projection = _messages.EnumField('ProjectionValueValuesEnum', 7)
+  userProject = _messages.StringField(8)
+
+
+class StorageBucketsSetIamPolicyRequest(_messages.Message):
+  r"""A StorageBucketsSetIamPolicyRequest object.
+
+  Fields:
+    bucket: Name of a bucket.
+    policy: A Policy resource to be passed as the request body.
+    userProject: The project to be billed for this request. Required for
+      Requester Pays buckets.
+  """
+
+  bucket = _messages.StringField(1, required=True)
+  policy = _messages.MessageField('Policy', 2)
+  userProject = _messages.StringField(3)
+
+
+class StorageBucketsTestIamPermissionsRequest(_messages.Message):
+  r"""A StorageBucketsTestIamPermissionsRequest object.
+
+  Fields:
+    bucket: Name of a bucket.
+    permissions: Permissions to test.
+    userProject: The project to be billed for this request. Required for
+      Requester Pays buckets.
+  """
+
+  bucket = _messages.StringField(1, required=True)
+  permissions = _messages.StringField(2, required=True)
+  userProject = _messages.StringField(3)
 
 
 class StorageBucketsUpdateRequest(_messages.Message):
-  """A StorageBucketsUpdateRequest object.
+  r"""A StorageBucketsUpdateRequest object.
 
   Enums:
     PredefinedAclValueValuesEnum: Apply a predefined set of access controls to
       this bucket.
     PredefinedDefaultObjectAclValueValuesEnum: Apply a predefined set of
       default object access controls to this bucket.
     ProjectionValueValuesEnum: Set of properties to return. Defaults to full.
@@ -1017,18 +1504,20 @@
     ifMetagenerationNotMatch: Makes the return of the bucket metadata
       conditional on whether the bucket's current metageneration does not
       match the given value.
     predefinedAcl: Apply a predefined set of access controls to this bucket.
     predefinedDefaultObjectAcl: Apply a predefined set of default object
       access controls to this bucket.
     projection: Set of properties to return. Defaults to full.
+    userProject: The project to be billed for this request. Required for
+      Requester Pays buckets.
   """
 
   class PredefinedAclValueValuesEnum(_messages.Enum):
-    """Apply a predefined set of access controls to this bucket.
+    r"""Apply a predefined set of access controls to this bucket.
 
     Values:
       authenticatedRead: Project team owners get OWNER access, and
         allAuthenticatedUsers get READER access.
       private: Project team owners get OWNER access.
       projectPrivate: Project team members get access according to their
         roles.
@@ -1040,15 +1529,15 @@
     authenticatedRead = 0
     private = 1
     projectPrivate = 2
     publicRead = 3
     publicReadWrite = 4
 
   class PredefinedDefaultObjectAclValueValuesEnum(_messages.Enum):
-    """Apply a predefined set of default object access controls to this
+    r"""Apply a predefined set of default object access controls to this
     bucket.
 
     Values:
       authenticatedRead: Object owner gets OWNER access, and
         allAuthenticatedUsers get READER access.
       bucketOwnerFullControl: Object owner gets OWNER access, and project team
         owners get OWNER access.
@@ -1064,232 +1553,386 @@
     bucketOwnerFullControl = 1
     bucketOwnerRead = 2
     private = 3
     projectPrivate = 4
     publicRead = 5
 
   class ProjectionValueValuesEnum(_messages.Enum):
-    """Set of properties to return. Defaults to full.
+    r"""Set of properties to return. Defaults to full.
 
     Values:
       full: Include all properties.
-      noAcl: Omit acl and defaultObjectAcl properties.
+      noAcl: Omit owner, acl and defaultObjectAcl properties.
     """
     full = 0
     noAcl = 1
 
   bucket = _messages.StringField(1, required=True)
   bucketResource = _messages.MessageField('Bucket', 2)
   ifMetagenerationMatch = _messages.IntegerField(3)
   ifMetagenerationNotMatch = _messages.IntegerField(4)
   predefinedAcl = _messages.EnumField('PredefinedAclValueValuesEnum', 5)
   predefinedDefaultObjectAcl = _messages.EnumField('PredefinedDefaultObjectAclValueValuesEnum', 6)
   projection = _messages.EnumField('ProjectionValueValuesEnum', 7)
+  userProject = _messages.StringField(8)
 
 
 class StorageChannelsStopResponse(_messages.Message):
-  """An empty StorageChannelsStop response."""
+  r"""An empty StorageChannelsStop response."""
 
 
 class StorageDefaultObjectAccessControlsDeleteRequest(_messages.Message):
-  """A StorageDefaultObjectAccessControlsDeleteRequest object.
+  r"""A StorageDefaultObjectAccessControlsDeleteRequest object.
 
   Fields:
     bucket: Name of a bucket.
     entity: The entity holding the permission. Can be user-userId, user-
       emailAddress, group-groupId, group-emailAddress, allUsers, or
       allAuthenticatedUsers.
+    userProject: The project to be billed for this request. Required for
+      Requester Pays buckets.
   """
 
   bucket = _messages.StringField(1, required=True)
   entity = _messages.StringField(2, required=True)
+  userProject = _messages.StringField(3)
 
 
 class StorageDefaultObjectAccessControlsDeleteResponse(_messages.Message):
-  """An empty StorageDefaultObjectAccessControlsDelete response."""
+  r"""An empty StorageDefaultObjectAccessControlsDelete response."""
 
 
 class StorageDefaultObjectAccessControlsGetRequest(_messages.Message):
-  """A StorageDefaultObjectAccessControlsGetRequest object.
+  r"""A StorageDefaultObjectAccessControlsGetRequest object.
 
   Fields:
     bucket: Name of a bucket.
     entity: The entity holding the permission. Can be user-userId, user-
       emailAddress, group-groupId, group-emailAddress, allUsers, or
       allAuthenticatedUsers.
+    userProject: The project to be billed for this request. Required for
+      Requester Pays buckets.
   """
 
   bucket = _messages.StringField(1, required=True)
   entity = _messages.StringField(2, required=True)
+  userProject = _messages.StringField(3)
+
+
+class StorageDefaultObjectAccessControlsInsertRequest(_messages.Message):
+  r"""A StorageDefaultObjectAccessControlsInsertRequest object.
+
+  Fields:
+    bucket: Name of a bucket.
+    objectAccessControl: A ObjectAccessControl resource to be passed as the
+      request body.
+    userProject: The project to be billed for this request. Required for
+      Requester Pays buckets.
+  """
+
+  bucket = _messages.StringField(1, required=True)
+  objectAccessControl = _messages.MessageField('ObjectAccessControl', 2)
+  userProject = _messages.StringField(3)
 
 
 class StorageDefaultObjectAccessControlsListRequest(_messages.Message):
-  """A StorageDefaultObjectAccessControlsListRequest object.
+  r"""A StorageDefaultObjectAccessControlsListRequest object.
 
   Fields:
     bucket: Name of a bucket.
     ifMetagenerationMatch: If present, only return default ACL listing if the
       bucket's current metageneration matches this value.
     ifMetagenerationNotMatch: If present, only return default ACL listing if
       the bucket's current metageneration does not match the given value.
+    userProject: The project to be billed for this request. Required for
+      Requester Pays buckets.
   """
 
   bucket = _messages.StringField(1, required=True)
   ifMetagenerationMatch = _messages.IntegerField(2)
   ifMetagenerationNotMatch = _messages.IntegerField(3)
+  userProject = _messages.StringField(4)
+
+
+class StorageDefaultObjectAccessControlsPatchRequest(_messages.Message):
+  r"""A StorageDefaultObjectAccessControlsPatchRequest object.
+
+  Fields:
+    bucket: Name of a bucket.
+    entity: The entity holding the permission. Can be user-userId, user-
+      emailAddress, group-groupId, group-emailAddress, allUsers, or
+      allAuthenticatedUsers.
+    objectAccessControl: A ObjectAccessControl resource to be passed as the
+      request body.
+    userProject: The project to be billed for this request. Required for
+      Requester Pays buckets.
+  """
+
+  bucket = _messages.StringField(1, required=True)
+  entity = _messages.StringField(2, required=True)
+  objectAccessControl = _messages.MessageField('ObjectAccessControl', 3)
+  userProject = _messages.StringField(4)
+
+
+class StorageDefaultObjectAccessControlsUpdateRequest(_messages.Message):
+  r"""A StorageDefaultObjectAccessControlsUpdateRequest object.
+
+  Fields:
+    bucket: Name of a bucket.
+    entity: The entity holding the permission. Can be user-userId, user-
+      emailAddress, group-groupId, group-emailAddress, allUsers, or
+      allAuthenticatedUsers.
+    objectAccessControl: A ObjectAccessControl resource to be passed as the
+      request body.
+    userProject: The project to be billed for this request. Required for
+      Requester Pays buckets.
+  """
+
+  bucket = _messages.StringField(1, required=True)
+  entity = _messages.StringField(2, required=True)
+  objectAccessControl = _messages.MessageField('ObjectAccessControl', 3)
+  userProject = _messages.StringField(4)
+
+
+class StorageNotificationsDeleteRequest(_messages.Message):
+  r"""A StorageNotificationsDeleteRequest object.
+
+  Fields:
+    bucket: The parent bucket of the notification.
+    notification: ID of the notification to delete.
+    userProject: The project to be billed for this request. Required for
+      Requester Pays buckets.
+  """
+
+  bucket = _messages.StringField(1, required=True)
+  notification = _messages.StringField(2, required=True)
+  userProject = _messages.StringField(3)
+
+
+class StorageNotificationsDeleteResponse(_messages.Message):
+  r"""An empty StorageNotificationsDelete response."""
+
+
+class StorageNotificationsGetRequest(_messages.Message):
+  r"""A StorageNotificationsGetRequest object.
+
+  Fields:
+    bucket: The parent bucket of the notification.
+    notification: Notification ID
+    userProject: The project to be billed for this request. Required for
+      Requester Pays buckets.
+  """
+
+  bucket = _messages.StringField(1, required=True)
+  notification = _messages.StringField(2, required=True)
+  userProject = _messages.StringField(3)
+
+
+class StorageNotificationsInsertRequest(_messages.Message):
+  r"""A StorageNotificationsInsertRequest object.
+
+  Fields:
+    bucket: The parent bucket of the notification.
+    notification: A Notification resource to be passed as the request body.
+    userProject: The project to be billed for this request. Required for
+      Requester Pays buckets.
+  """
+
+  bucket = _messages.StringField(1, required=True)
+  notification = _messages.MessageField('Notification', 2)
+  userProject = _messages.StringField(3)
+
+
+class StorageNotificationsListRequest(_messages.Message):
+  r"""A StorageNotificationsListRequest object.
+
+  Fields:
+    bucket: Name of a Google Cloud Storage bucket.
+    userProject: The project to be billed for this request. Required for
+      Requester Pays buckets.
+  """
+
+  bucket = _messages.StringField(1, required=True)
+  userProject = _messages.StringField(2)
 
 
 class StorageObjectAccessControlsDeleteRequest(_messages.Message):
-  """A StorageObjectAccessControlsDeleteRequest object.
+  r"""A StorageObjectAccessControlsDeleteRequest object.
 
   Fields:
     bucket: Name of a bucket.
     entity: The entity holding the permission. Can be user-userId, user-
       emailAddress, group-groupId, group-emailAddress, allUsers, or
       allAuthenticatedUsers.
     generation: If present, selects a specific revision of this object (as
       opposed to the latest version, the default).
     object: Name of the object. For information about how to URL encode object
       names to be path safe, see Encoding URI Path Parts.
+    userProject: The project to be billed for this request. Required for
+      Requester Pays buckets.
   """
 
   bucket = _messages.StringField(1, required=True)
   entity = _messages.StringField(2, required=True)
   generation = _messages.IntegerField(3)
   object = _messages.StringField(4, required=True)
+  userProject = _messages.StringField(5)
 
 
 class StorageObjectAccessControlsDeleteResponse(_messages.Message):
-  """An empty StorageObjectAccessControlsDelete response."""
+  r"""An empty StorageObjectAccessControlsDelete response."""
 
 
 class StorageObjectAccessControlsGetRequest(_messages.Message):
-  """A StorageObjectAccessControlsGetRequest object.
+  r"""A StorageObjectAccessControlsGetRequest object.
 
   Fields:
     bucket: Name of a bucket.
     entity: The entity holding the permission. Can be user-userId, user-
       emailAddress, group-groupId, group-emailAddress, allUsers, or
       allAuthenticatedUsers.
     generation: If present, selects a specific revision of this object (as
       opposed to the latest version, the default).
     object: Name of the object. For information about how to URL encode object
       names to be path safe, see Encoding URI Path Parts.
+    userProject: The project to be billed for this request. Required for
+      Requester Pays buckets.
   """
 
   bucket = _messages.StringField(1, required=True)
   entity = _messages.StringField(2, required=True)
   generation = _messages.IntegerField(3)
   object = _messages.StringField(4, required=True)
+  userProject = _messages.StringField(5)
 
 
 class StorageObjectAccessControlsInsertRequest(_messages.Message):
-  """A StorageObjectAccessControlsInsertRequest object.
+  r"""A StorageObjectAccessControlsInsertRequest object.
 
   Fields:
     bucket: Name of a bucket.
     generation: If present, selects a specific revision of this object (as
       opposed to the latest version, the default).
     object: Name of the object. For information about how to URL encode object
       names to be path safe, see Encoding URI Path Parts.
     objectAccessControl: A ObjectAccessControl resource to be passed as the
       request body.
+    userProject: The project to be billed for this request. Required for
+      Requester Pays buckets.
   """
 
   bucket = _messages.StringField(1, required=True)
   generation = _messages.IntegerField(2)
   object = _messages.StringField(3, required=True)
   objectAccessControl = _messages.MessageField('ObjectAccessControl', 4)
+  userProject = _messages.StringField(5)
 
 
 class StorageObjectAccessControlsListRequest(_messages.Message):
-  """A StorageObjectAccessControlsListRequest object.
+  r"""A StorageObjectAccessControlsListRequest object.
 
   Fields:
     bucket: Name of a bucket.
     generation: If present, selects a specific revision of this object (as
       opposed to the latest version, the default).
     object: Name of the object. For information about how to URL encode object
       names to be path safe, see Encoding URI Path Parts.
+    userProject: The project to be billed for this request. Required for
+      Requester Pays buckets.
   """
 
   bucket = _messages.StringField(1, required=True)
   generation = _messages.IntegerField(2)
   object = _messages.StringField(3, required=True)
+  userProject = _messages.StringField(4)
 
 
 class StorageObjectAccessControlsPatchRequest(_messages.Message):
-  """A StorageObjectAccessControlsPatchRequest object.
+  r"""A StorageObjectAccessControlsPatchRequest object.
 
   Fields:
     bucket: Name of a bucket.
     entity: The entity holding the permission. Can be user-userId, user-
       emailAddress, group-groupId, group-emailAddress, allUsers, or
       allAuthenticatedUsers.
     generation: If present, selects a specific revision of this object (as
       opposed to the latest version, the default).
     object: Name of the object. For information about how to URL encode object
       names to be path safe, see Encoding URI Path Parts.
     objectAccessControl: A ObjectAccessControl resource to be passed as the
       request body.
+    userProject: The project to be billed for this request. Required for
+      Requester Pays buckets.
   """
 
   bucket = _messages.StringField(1, required=True)
   entity = _messages.StringField(2, required=True)
   generation = _messages.IntegerField(3)
   object = _messages.StringField(4, required=True)
   objectAccessControl = _messages.MessageField('ObjectAccessControl', 5)
+  userProject = _messages.StringField(6)
 
 
 class StorageObjectAccessControlsUpdateRequest(_messages.Message):
-  """A StorageObjectAccessControlsUpdateRequest object.
+  r"""A StorageObjectAccessControlsUpdateRequest object.
 
   Fields:
     bucket: Name of a bucket.
     entity: The entity holding the permission. Can be user-userId, user-
       emailAddress, group-groupId, group-emailAddress, allUsers, or
       allAuthenticatedUsers.
     generation: If present, selects a specific revision of this object (as
       opposed to the latest version, the default).
     object: Name of the object. For information about how to URL encode object
       names to be path safe, see Encoding URI Path Parts.
     objectAccessControl: A ObjectAccessControl resource to be passed as the
       request body.
+    userProject: The project to be billed for this request. Required for
+      Requester Pays buckets.
   """
 
   bucket = _messages.StringField(1, required=True)
   entity = _messages.StringField(2, required=True)
   generation = _messages.IntegerField(3)
   object = _messages.StringField(4, required=True)
   objectAccessControl = _messages.MessageField('ObjectAccessControl', 5)
+  userProject = _messages.StringField(6)
 
 
 class StorageObjectsComposeRequest(_messages.Message):
-  """A StorageObjectsComposeRequest object.
+  r"""A StorageObjectsComposeRequest object.
 
   Enums:
     DestinationPredefinedAclValueValuesEnum: Apply a predefined set of access
       controls to the destination object.
 
   Fields:
     composeRequest: A ComposeRequest resource to be passed as the request
       body.
-    destinationBucket: Name of the bucket in which to store the new object.
+    destinationBucket: Name of the bucket containing the source objects. The
+      destination object is stored in this bucket.
     destinationObject: Name of the new object. For information about how to
       URL encode object names to be path safe, see Encoding URI Path Parts.
     destinationPredefinedAcl: Apply a predefined set of access controls to the
       destination object.
     ifGenerationMatch: Makes the operation conditional on whether the object's
-      current generation matches the given value.
+      current generation matches the given value. Setting to 0 makes the
+      operation succeed only if there are no live versions of the object.
     ifMetagenerationMatch: Makes the operation conditional on whether the
       object's current metageneration matches the given value.
+    kmsKeyName: Resource name of the Cloud KMS key, of the form projects/my-
+      project/locations/global/keyRings/my-kr/cryptoKeys/my-key, that will be
+      used to encrypt the object. Overrides the object metadata's kms_key_name
+      value, if any.
+    userProject: The project to be billed for this request. Required for
+      Requester Pays buckets.
   """
 
   class DestinationPredefinedAclValueValuesEnum(_messages.Enum):
-    """Apply a predefined set of access controls to the destination object.
+    r"""Apply a predefined set of access controls to the destination object.
 
     Values:
       authenticatedRead: Object owner gets OWNER access, and
         allAuthenticatedUsers get READER access.
       bucketOwnerFullControl: Object owner gets OWNER access, and project team
         owners get OWNER access.
       bucketOwnerRead: Object owner gets OWNER access, and project team owners
@@ -1309,18 +1952,20 @@
 
   composeRequest = _messages.MessageField('ComposeRequest', 1)
   destinationBucket = _messages.StringField(2, required=True)
   destinationObject = _messages.StringField(3, required=True)
   destinationPredefinedAcl = _messages.EnumField('DestinationPredefinedAclValueValuesEnum', 4)
   ifGenerationMatch = _messages.IntegerField(5)
   ifMetagenerationMatch = _messages.IntegerField(6)
+  kmsKeyName = _messages.StringField(7)
+  userProject = _messages.StringField(8)
 
 
 class StorageObjectsCopyRequest(_messages.Message):
-  """A StorageObjectsCopyRequest object.
+  r"""A StorageObjectsCopyRequest object.
 
   Enums:
     DestinationPredefinedAclValueValuesEnum: Apply a predefined set of access
       controls to the destination object.
     ProjectionValueValuesEnum: Set of properties to return. Defaults to noAcl,
       unless the object resource specifies the acl property, when it defaults
       to full.
@@ -1332,43 +1977,49 @@
       Encoding URI Path Parts.
     destinationObject: Name of the new object. Required when the object
       metadata is not otherwise provided. Overrides the object metadata's name
       value, if any.
     destinationPredefinedAcl: Apply a predefined set of access controls to the
       destination object.
     ifGenerationMatch: Makes the operation conditional on whether the
-      destination object's current generation matches the given value.
+      destination object's current generation matches the given value. Setting
+      to 0 makes the operation succeed only if there are no live versions of
+      the object.
     ifGenerationNotMatch: Makes the operation conditional on whether the
       destination object's current generation does not match the given value.
+      If no live object exists, the precondition fails. Setting to 0 makes the
+      operation succeed only if there is a live version of the object.
     ifMetagenerationMatch: Makes the operation conditional on whether the
       destination object's current metageneration matches the given value.
     ifMetagenerationNotMatch: Makes the operation conditional on whether the
       destination object's current metageneration does not match the given
       value.
     ifSourceGenerationMatch: Makes the operation conditional on whether the
-      source object's generation matches the given value.
+      source object's current generation matches the given value.
     ifSourceGenerationNotMatch: Makes the operation conditional on whether the
-      source object's generation does not match the given value.
+      source object's current generation does not match the given value.
     ifSourceMetagenerationMatch: Makes the operation conditional on whether
       the source object's current metageneration matches the given value.
     ifSourceMetagenerationNotMatch: Makes the operation conditional on whether
       the source object's current metageneration does not match the given
       value.
     object: A Object resource to be passed as the request body.
     projection: Set of properties to return. Defaults to noAcl, unless the
       object resource specifies the acl property, when it defaults to full.
     sourceBucket: Name of the bucket in which to find the source object.
     sourceGeneration: If present, selects a specific revision of the source
       object (as opposed to the latest version, the default).
     sourceObject: Name of the source object. For information about how to URL
       encode object names to be path safe, see Encoding URI Path Parts.
+    userProject: The project to be billed for this request. Required for
+      Requester Pays buckets.
   """
 
   class DestinationPredefinedAclValueValuesEnum(_messages.Enum):
-    """Apply a predefined set of access controls to the destination object.
+    r"""Apply a predefined set of access controls to the destination object.
 
     Values:
       authenticatedRead: Object owner gets OWNER access, and
         allAuthenticatedUsers get READER access.
       bucketOwnerFullControl: Object owner gets OWNER access, and project team
         owners get OWNER access.
       bucketOwnerRead: Object owner gets OWNER access, and project team owners
@@ -1383,20 +2034,20 @@
     bucketOwnerFullControl = 1
     bucketOwnerRead = 2
     private = 3
     projectPrivate = 4
     publicRead = 5
 
   class ProjectionValueValuesEnum(_messages.Enum):
-    """Set of properties to return. Defaults to noAcl, unless the object
+    r"""Set of properties to return. Defaults to noAcl, unless the object
     resource specifies the acl property, when it defaults to full.
 
     Values:
       full: Include all properties.
-      noAcl: Omit the acl property.
+      noAcl: Omit the owner, acl property.
     """
     full = 0
     noAcl = 1
 
   destinationBucket = _messages.StringField(1, required=True)
   destinationObject = _messages.StringField(2, required=True)
   destinationPredefinedAcl = _messages.EnumField('DestinationPredefinedAclValueValuesEnum', 3)
@@ -1409,93 +2060,125 @@
   ifSourceMetagenerationMatch = _messages.IntegerField(10)
   ifSourceMetagenerationNotMatch = _messages.IntegerField(11)
   object = _messages.MessageField('Object', 12)
   projection = _messages.EnumField('ProjectionValueValuesEnum', 13)
   sourceBucket = _messages.StringField(14, required=True)
   sourceGeneration = _messages.IntegerField(15)
   sourceObject = _messages.StringField(16, required=True)
+  userProject = _messages.StringField(17)
 
 
 class StorageObjectsDeleteRequest(_messages.Message):
-  """A StorageObjectsDeleteRequest object.
+  r"""A StorageObjectsDeleteRequest object.
 
   Fields:
     bucket: Name of the bucket in which the object resides.
     generation: If present, permanently deletes a specific revision of this
       object (as opposed to the latest version, the default).
     ifGenerationMatch: Makes the operation conditional on whether the object's
-      current generation matches the given value.
+      current generation matches the given value. Setting to 0 makes the
+      operation succeed only if there are no live versions of the object.
     ifGenerationNotMatch: Makes the operation conditional on whether the
-      object's current generation does not match the given value.
+      object's current generation does not match the given value. If no live
+      object exists, the precondition fails. Setting to 0 makes the operation
+      succeed only if there is a live version of the object.
     ifMetagenerationMatch: Makes the operation conditional on whether the
       object's current metageneration matches the given value.
     ifMetagenerationNotMatch: Makes the operation conditional on whether the
       object's current metageneration does not match the given value.
     object: Name of the object. For information about how to URL encode object
       names to be path safe, see Encoding URI Path Parts.
+    userProject: The project to be billed for this request. Required for
+      Requester Pays buckets.
   """
 
   bucket = _messages.StringField(1, required=True)
   generation = _messages.IntegerField(2)
   ifGenerationMatch = _messages.IntegerField(3)
   ifGenerationNotMatch = _messages.IntegerField(4)
   ifMetagenerationMatch = _messages.IntegerField(5)
   ifMetagenerationNotMatch = _messages.IntegerField(6)
   object = _messages.StringField(7, required=True)
+  userProject = _messages.StringField(8)
 
 
 class StorageObjectsDeleteResponse(_messages.Message):
-  """An empty StorageObjectsDelete response."""
+  r"""An empty StorageObjectsDelete response."""
+
+
+class StorageObjectsGetIamPolicyRequest(_messages.Message):
+  r"""A StorageObjectsGetIamPolicyRequest object.
+
+  Fields:
+    bucket: Name of the bucket in which the object resides.
+    generation: If present, selects a specific revision of this object (as
+      opposed to the latest version, the default).
+    object: Name of the object. For information about how to URL encode object
+      names to be path safe, see Encoding URI Path Parts.
+    userProject: The project to be billed for this request. Required for
+      Requester Pays buckets.
+  """
+
+  bucket = _messages.StringField(1, required=True)
+  generation = _messages.IntegerField(2)
+  object = _messages.StringField(3, required=True)
+  userProject = _messages.StringField(4)
 
 
 class StorageObjectsGetRequest(_messages.Message):
-  """A StorageObjectsGetRequest object.
+  r"""A StorageObjectsGetRequest object.
 
   Enums:
     ProjectionValueValuesEnum: Set of properties to return. Defaults to noAcl.
 
   Fields:
     bucket: Name of the bucket in which the object resides.
     generation: If present, selects a specific revision of this object (as
       opposed to the latest version, the default).
     ifGenerationMatch: Makes the operation conditional on whether the object's
-      generation matches the given value.
+      current generation matches the given value. Setting to 0 makes the
+      operation succeed only if there are no live versions of the object.
     ifGenerationNotMatch: Makes the operation conditional on whether the
-      object's generation does not match the given value.
+      object's current generation does not match the given value. If no live
+      object exists, the precondition fails. Setting to 0 makes the operation
+      succeed only if there is a live version of the object.
     ifMetagenerationMatch: Makes the operation conditional on whether the
       object's current metageneration matches the given value.
     ifMetagenerationNotMatch: Makes the operation conditional on whether the
       object's current metageneration does not match the given value.
     object: Name of the object. For information about how to URL encode object
       names to be path safe, see Encoding URI Path Parts.
     projection: Set of properties to return. Defaults to noAcl.
+    userProject: The project to be billed for this request. Required for
+      Requester Pays buckets.
   """
 
   class ProjectionValueValuesEnum(_messages.Enum):
-    """Set of properties to return. Defaults to noAcl.
+    r"""Set of properties to return. Defaults to noAcl.
 
     Values:
       full: Include all properties.
-      noAcl: Omit the acl property.
+      noAcl: Omit the owner, acl property.
     """
     full = 0
     noAcl = 1
 
   bucket = _messages.StringField(1, required=True)
   generation = _messages.IntegerField(2)
   ifGenerationMatch = _messages.IntegerField(3)
   ifGenerationNotMatch = _messages.IntegerField(4)
   ifMetagenerationMatch = _messages.IntegerField(5)
   ifMetagenerationNotMatch = _messages.IntegerField(6)
   object = _messages.StringField(7, required=True)
   projection = _messages.EnumField('ProjectionValueValuesEnum', 8)
+  userProject = _messages.StringField(9)
 
 
 class StorageObjectsInsertRequest(_messages.Message):
-  """A StorageObjectsInsertRequest object.
+  r"""A StorageObjectsInsertRequest object.
 
   Enums:
     PredefinedAclValueValuesEnum: Apply a predefined set of access controls to
       this object.
     ProjectionValueValuesEnum: Set of properties to return. Defaults to noAcl,
       unless the object resource specifies the acl property, when it defaults
       to full.
@@ -1505,33 +2188,42 @@
       provided object metadata's bucket value, if any.
     contentEncoding: If set, sets the contentEncoding property of the final
       object to this value. Setting this parameter is equivalent to setting
       the contentEncoding metadata property. This can be useful when uploading
       an object with uploadType=media to indicate the encoding of the content
       being uploaded.
     ifGenerationMatch: Makes the operation conditional on whether the object's
-      current generation matches the given value.
+      current generation matches the given value. Setting to 0 makes the
+      operation succeed only if there are no live versions of the object.
     ifGenerationNotMatch: Makes the operation conditional on whether the
-      object's current generation does not match the given value.
+      object's current generation does not match the given value. If no live
+      object exists, the precondition fails. Setting to 0 makes the operation
+      succeed only if there is a live version of the object.
     ifMetagenerationMatch: Makes the operation conditional on whether the
       object's current metageneration matches the given value.
     ifMetagenerationNotMatch: Makes the operation conditional on whether the
       object's current metageneration does not match the given value.
+    kmsKeyName: Resource name of the Cloud KMS key, of the form projects/my-
+      project/locations/global/keyRings/my-kr/cryptoKeys/my-key, that will be
+      used to encrypt the object. Overrides the object metadata's kms_key_name
+      value, if any.
     name: Name of the object. Required when the object metadata is not
       otherwise provided. Overrides the object metadata's name value, if any.
       For information about how to URL encode object names to be path safe,
       see Encoding URI Path Parts.
     object: A Object resource to be passed as the request body.
     predefinedAcl: Apply a predefined set of access controls to this object.
     projection: Set of properties to return. Defaults to noAcl, unless the
       object resource specifies the acl property, when it defaults to full.
+    userProject: The project to be billed for this request. Required for
+      Requester Pays buckets.
   """
 
   class PredefinedAclValueValuesEnum(_messages.Enum):
-    """Apply a predefined set of access controls to this object.
+    r"""Apply a predefined set of access controls to this object.
 
     Values:
       authenticatedRead: Object owner gets OWNER access, and
         allAuthenticatedUsers get READER access.
       bucketOwnerFullControl: Object owner gets OWNER access, and project team
         owners get OWNER access.
       bucketOwnerRead: Object owner gets OWNER access, and project team owners
@@ -1546,108 +2238,123 @@
     bucketOwnerFullControl = 1
     bucketOwnerRead = 2
     private = 3
     projectPrivate = 4
     publicRead = 5
 
   class ProjectionValueValuesEnum(_messages.Enum):
-    """Set of properties to return. Defaults to noAcl, unless the object
+    r"""Set of properties to return. Defaults to noAcl, unless the object
     resource specifies the acl property, when it defaults to full.
 
     Values:
       full: Include all properties.
-      noAcl: Omit the acl property.
+      noAcl: Omit the owner, acl property.
     """
     full = 0
     noAcl = 1
 
   bucket = _messages.StringField(1, required=True)
   contentEncoding = _messages.StringField(2)
   ifGenerationMatch = _messages.IntegerField(3)
   ifGenerationNotMatch = _messages.IntegerField(4)
   ifMetagenerationMatch = _messages.IntegerField(5)
   ifMetagenerationNotMatch = _messages.IntegerField(6)
-  name = _messages.StringField(7)
-  object = _messages.MessageField('Object', 8)
-  predefinedAcl = _messages.EnumField('PredefinedAclValueValuesEnum', 9)
-  projection = _messages.EnumField('ProjectionValueValuesEnum', 10)
+  kmsKeyName = _messages.StringField(7)
+  name = _messages.StringField(8)
+  object = _messages.MessageField('Object', 9)
+  predefinedAcl = _messages.EnumField('PredefinedAclValueValuesEnum', 10)
+  projection = _messages.EnumField('ProjectionValueValuesEnum', 11)
+  userProject = _messages.StringField(12)
 
 
 class StorageObjectsListRequest(_messages.Message):
-  """A StorageObjectsListRequest object.
+  r"""A StorageObjectsListRequest object.
 
   Enums:
     ProjectionValueValuesEnum: Set of properties to return. Defaults to noAcl.
 
   Fields:
     bucket: Name of the bucket in which to look for objects.
     delimiter: Returns results in a directory-like mode. items will contain
       only objects whose names, aside from the prefix, do not contain
       delimiter. Objects whose names, aside from the prefix, contain delimiter
       will have their name, truncated after the delimiter, returned in
       prefixes. Duplicate prefixes are omitted.
-    maxResults: Maximum number of items plus prefixes to return. As duplicate
-      prefixes are omitted, fewer total results may be returned than
-      requested. The default value of this parameter is 1,000 items.
+    includeTrailingDelimiter: If true, objects that end in exactly one
+      instance of delimiter will have their metadata included in items in
+      addition to prefixes.
+    maxResults: Maximum number of items plus prefixes to return in a single
+      page of responses. As duplicate prefixes are omitted, fewer total
+      results may be returned than requested. The service will use this
+      parameter or 1,000 items, whichever is smaller.
     pageToken: A previously-returned page token representing part of the
       larger set of results to view.
     prefix: Filter results to objects whose names begin with this prefix.
     projection: Set of properties to return. Defaults to noAcl.
+    userProject: The project to be billed for this request. Required for
+      Requester Pays buckets.
     versions: If true, lists all versions of an object as distinct results.
       The default is false. For more information, see Object Versioning.
   """
 
   class ProjectionValueValuesEnum(_messages.Enum):
-    """Set of properties to return. Defaults to noAcl.
+    r"""Set of properties to return. Defaults to noAcl.
 
     Values:
       full: Include all properties.
-      noAcl: Omit the acl property.
+      noAcl: Omit the owner, acl property.
     """
     full = 0
     noAcl = 1
 
   bucket = _messages.StringField(1, required=True)
   delimiter = _messages.StringField(2)
-  maxResults = _messages.IntegerField(3, variant=_messages.Variant.UINT32)
-  pageToken = _messages.StringField(4)
-  prefix = _messages.StringField(5)
-  projection = _messages.EnumField('ProjectionValueValuesEnum', 6)
-  versions = _messages.BooleanField(7)
+  includeTrailingDelimiter = _messages.BooleanField(3)
+  maxResults = _messages.IntegerField(4, variant=_messages.Variant.UINT32, default=1000)
+  pageToken = _messages.StringField(5)
+  prefix = _messages.StringField(6)
+  projection = _messages.EnumField('ProjectionValueValuesEnum', 7)
+  userProject = _messages.StringField(8)
+  versions = _messages.BooleanField(9)
 
 
 class StorageObjectsPatchRequest(_messages.Message):
-  """A StorageObjectsPatchRequest object.
+  r"""A StorageObjectsPatchRequest object.
 
   Enums:
     PredefinedAclValueValuesEnum: Apply a predefined set of access controls to
       this object.
     ProjectionValueValuesEnum: Set of properties to return. Defaults to full.
 
   Fields:
     bucket: Name of the bucket in which the object resides.
     generation: If present, selects a specific revision of this object (as
       opposed to the latest version, the default).
     ifGenerationMatch: Makes the operation conditional on whether the object's
-      current generation matches the given value.
+      current generation matches the given value. Setting to 0 makes the
+      operation succeed only if there are no live versions of the object.
     ifGenerationNotMatch: Makes the operation conditional on whether the
-      object's current generation does not match the given value.
+      object's current generation does not match the given value. If no live
+      object exists, the precondition fails. Setting to 0 makes the operation
+      succeed only if there is a live version of the object.
     ifMetagenerationMatch: Makes the operation conditional on whether the
       object's current metageneration matches the given value.
     ifMetagenerationNotMatch: Makes the operation conditional on whether the
       object's current metageneration does not match the given value.
     object: Name of the object. For information about how to URL encode object
       names to be path safe, see Encoding URI Path Parts.
     objectResource: A Object resource to be passed as the request body.
     predefinedAcl: Apply a predefined set of access controls to this object.
     projection: Set of properties to return. Defaults to full.
+    userProject: The project to be billed for this request, for Requester Pays
+      buckets.
   """
 
   class PredefinedAclValueValuesEnum(_messages.Enum):
-    """Apply a predefined set of access controls to this object.
+    r"""Apply a predefined set of access controls to this object.
 
     Values:
       authenticatedRead: Object owner gets OWNER access, and
         allAuthenticatedUsers get READER access.
       bucketOwnerFullControl: Object owner gets OWNER access, and project team
         owners get OWNER access.
       bucketOwnerRead: Object owner gets OWNER access, and project team owners
@@ -1662,67 +2369,75 @@
     bucketOwnerFullControl = 1
     bucketOwnerRead = 2
     private = 3
     projectPrivate = 4
     publicRead = 5
 
   class ProjectionValueValuesEnum(_messages.Enum):
-    """Set of properties to return. Defaults to full.
+    r"""Set of properties to return. Defaults to full.
 
     Values:
       full: Include all properties.
-      noAcl: Omit the acl property.
+      noAcl: Omit the owner, acl property.
     """
     full = 0
     noAcl = 1
 
   bucket = _messages.StringField(1, required=True)
   generation = _messages.IntegerField(2)
   ifGenerationMatch = _messages.IntegerField(3)
   ifGenerationNotMatch = _messages.IntegerField(4)
   ifMetagenerationMatch = _messages.IntegerField(5)
   ifMetagenerationNotMatch = _messages.IntegerField(6)
   object = _messages.StringField(7, required=True)
   objectResource = _messages.MessageField('Object', 8)
   predefinedAcl = _messages.EnumField('PredefinedAclValueValuesEnum', 9)
   projection = _messages.EnumField('ProjectionValueValuesEnum', 10)
+  userProject = _messages.StringField(11)
 
 
 class StorageObjectsRewriteRequest(_messages.Message):
-  """A StorageObjectsRewriteRequest object.
+  r"""A StorageObjectsRewriteRequest object.
 
   Enums:
     DestinationPredefinedAclValueValuesEnum: Apply a predefined set of access
       controls to the destination object.
     ProjectionValueValuesEnum: Set of properties to return. Defaults to noAcl,
       unless the object resource specifies the acl property, when it defaults
       to full.
 
   Fields:
     destinationBucket: Name of the bucket in which to store the new object.
       Overrides the provided object metadata's bucket value, if any.
+    destinationKmsKeyName: Resource name of the Cloud KMS key, of the form
+      projects/my-project/locations/global/keyRings/my-kr/cryptoKeys/my-key,
+      that will be used to encrypt the object. Overrides the object metadata's
+      kms_key_name value, if any.
     destinationObject: Name of the new object. Required when the object
       metadata is not otherwise provided. Overrides the object metadata's name
       value, if any. For information about how to URL encode object names to
       be path safe, see Encoding URI Path Parts.
     destinationPredefinedAcl: Apply a predefined set of access controls to the
       destination object.
-    ifGenerationMatch: Makes the operation conditional on whether the
-      destination object's current generation matches the given value.
+    ifGenerationMatch: Makes the operation conditional on whether the object's
+      current generation matches the given value. Setting to 0 makes the
+      operation succeed only if there are no live versions of the object.
     ifGenerationNotMatch: Makes the operation conditional on whether the
-      destination object's current generation does not match the given value.
+      object's current generation does not match the given value. If no live
+      object exists, the precondition fails. Setting to 0 makes the operation
+      succeed only if there is a live version of the object.
     ifMetagenerationMatch: Makes the operation conditional on whether the
       destination object's current metageneration matches the given value.
     ifMetagenerationNotMatch: Makes the operation conditional on whether the
       destination object's current metageneration does not match the given
       value.
     ifSourceGenerationMatch: Makes the operation conditional on whether the
-      source object's generation matches the given value.
+      source object's current generation matches the given value.
     ifSourceGenerationNotMatch: Makes the operation conditional on whether the
-      source object's generation does not match the given value.
+      source object's current generation does not match the given value.
     ifSourceMetagenerationMatch: Makes the operation conditional on whether
       the source object's current metageneration matches the given value.
     ifSourceMetagenerationNotMatch: Makes the operation conditional on whether
       the source object's current metageneration does not match the given
       value.
     maxBytesRewrittenPerCall: The maximum number of bytes that will be
       rewritten per rewrite request. Most callers shouldn't need to specify
@@ -1741,18 +2456,20 @@
       other request fields, but if included those fields must match the values
       provided in the first rewrite request.
     sourceBucket: Name of the bucket in which to find the source object.
     sourceGeneration: If present, selects a specific revision of the source
       object (as opposed to the latest version, the default).
     sourceObject: Name of the source object. For information about how to URL
       encode object names to be path safe, see Encoding URI Path Parts.
+    userProject: The project to be billed for this request. Required for
+      Requester Pays buckets.
   """
 
   class DestinationPredefinedAclValueValuesEnum(_messages.Enum):
-    """Apply a predefined set of access controls to the destination object.
+    r"""Apply a predefined set of access controls to the destination object.
 
     Values:
       authenticatedRead: Object owner gets OWNER access, and
         allAuthenticatedUsers get READER access.
       bucketOwnerFullControl: Object owner gets OWNER access, and project team
         owners get OWNER access.
       bucketOwnerRead: Object owner gets OWNER access, and project team owners
@@ -1767,73 +2484,122 @@
     bucketOwnerFullControl = 1
     bucketOwnerRead = 2
     private = 3
     projectPrivate = 4
     publicRead = 5
 
   class ProjectionValueValuesEnum(_messages.Enum):
-    """Set of properties to return. Defaults to noAcl, unless the object
+    r"""Set of properties to return. Defaults to noAcl, unless the object
     resource specifies the acl property, when it defaults to full.
 
     Values:
       full: Include all properties.
-      noAcl: Omit the acl property.
+      noAcl: Omit the owner, acl property.
     """
     full = 0
     noAcl = 1
 
   destinationBucket = _messages.StringField(1, required=True)
-  destinationObject = _messages.StringField(2, required=True)
-  destinationPredefinedAcl = _messages.EnumField('DestinationPredefinedAclValueValuesEnum', 3)
-  ifGenerationMatch = _messages.IntegerField(4)
-  ifGenerationNotMatch = _messages.IntegerField(5)
-  ifMetagenerationMatch = _messages.IntegerField(6)
-  ifMetagenerationNotMatch = _messages.IntegerField(7)
-  ifSourceGenerationMatch = _messages.IntegerField(8)
-  ifSourceGenerationNotMatch = _messages.IntegerField(9)
-  ifSourceMetagenerationMatch = _messages.IntegerField(10)
-  ifSourceMetagenerationNotMatch = _messages.IntegerField(11)
-  maxBytesRewrittenPerCall = _messages.IntegerField(12)
-  object = _messages.MessageField('Object', 13)
-  projection = _messages.EnumField('ProjectionValueValuesEnum', 14)
-  rewriteToken = _messages.StringField(15)
-  sourceBucket = _messages.StringField(16, required=True)
-  sourceGeneration = _messages.IntegerField(17)
-  sourceObject = _messages.StringField(18, required=True)
+  destinationKmsKeyName = _messages.StringField(2)
+  destinationObject = _messages.StringField(3, required=True)
+  destinationPredefinedAcl = _messages.EnumField('DestinationPredefinedAclValueValuesEnum', 4)
+  ifGenerationMatch = _messages.IntegerField(5)
+  ifGenerationNotMatch = _messages.IntegerField(6)
+  ifMetagenerationMatch = _messages.IntegerField(7)
+  ifMetagenerationNotMatch = _messages.IntegerField(8)
+  ifSourceGenerationMatch = _messages.IntegerField(9)
+  ifSourceGenerationNotMatch = _messages.IntegerField(10)
+  ifSourceMetagenerationMatch = _messages.IntegerField(11)
+  ifSourceMetagenerationNotMatch = _messages.IntegerField(12)
+  maxBytesRewrittenPerCall = _messages.IntegerField(13)
+  object = _messages.MessageField('Object', 14)
+  projection = _messages.EnumField('ProjectionValueValuesEnum', 15)
+  rewriteToken = _messages.StringField(16)
+  sourceBucket = _messages.StringField(17, required=True)
+  sourceGeneration = _messages.IntegerField(18)
+  sourceObject = _messages.StringField(19, required=True)
+  userProject = _messages.StringField(20)
+
+
+class StorageObjectsSetIamPolicyRequest(_messages.Message):
+  r"""A StorageObjectsSetIamPolicyRequest object.
+
+  Fields:
+    bucket: Name of the bucket in which the object resides.
+    generation: If present, selects a specific revision of this object (as
+      opposed to the latest version, the default).
+    object: Name of the object. For information about how to URL encode object
+      names to be path safe, see Encoding URI Path Parts.
+    policy: A Policy resource to be passed as the request body.
+    userProject: The project to be billed for this request. Required for
+      Requester Pays buckets.
+  """
+
+  bucket = _messages.StringField(1, required=True)
+  generation = _messages.IntegerField(2)
+  object = _messages.StringField(3, required=True)
+  policy = _messages.MessageField('Policy', 4)
+  userProject = _messages.StringField(5)
+
+
+class StorageObjectsTestIamPermissionsRequest(_messages.Message):
+  r"""A StorageObjectsTestIamPermissionsRequest object.
+
+  Fields:
+    bucket: Name of the bucket in which the object resides.
+    generation: If present, selects a specific revision of this object (as
+      opposed to the latest version, the default).
+    object: Name of the object. For information about how to URL encode object
+      names to be path safe, see Encoding URI Path Parts.
+    permissions: Permissions to test.
+    userProject: The project to be billed for this request. Required for
+      Requester Pays buckets.
+  """
+
+  bucket = _messages.StringField(1, required=True)
+  generation = _messages.IntegerField(2)
+  object = _messages.StringField(3, required=True)
+  permissions = _messages.StringField(4, required=True)
+  userProject = _messages.StringField(5)
 
 
 class StorageObjectsUpdateRequest(_messages.Message):
-  """A StorageObjectsUpdateRequest object.
+  r"""A StorageObjectsUpdateRequest object.
 
   Enums:
     PredefinedAclValueValuesEnum: Apply a predefined set of access controls to
       this object.
     ProjectionValueValuesEnum: Set of properties to return. Defaults to full.
 
   Fields:
     bucket: Name of the bucket in which the object resides.
     generation: If present, selects a specific revision of this object (as
       opposed to the latest version, the default).
     ifGenerationMatch: Makes the operation conditional on whether the object's
-      current generation matches the given value.
+      current generation matches the given value. Setting to 0 makes the
+      operation succeed only if there are no live versions of the object.
     ifGenerationNotMatch: Makes the operation conditional on whether the
-      object's current generation does not match the given value.
+      object's current generation does not match the given value. If no live
+      object exists, the precondition fails. Setting to 0 makes the operation
+      succeed only if there is a live version of the object.
     ifMetagenerationMatch: Makes the operation conditional on whether the
       object's current metageneration matches the given value.
     ifMetagenerationNotMatch: Makes the operation conditional on whether the
       object's current metageneration does not match the given value.
     object: Name of the object. For information about how to URL encode object
       names to be path safe, see Encoding URI Path Parts.
     objectResource: A Object resource to be passed as the request body.
     predefinedAcl: Apply a predefined set of access controls to this object.
     projection: Set of properties to return. Defaults to full.
+    userProject: The project to be billed for this request. Required for
+      Requester Pays buckets.
   """
 
   class PredefinedAclValueValuesEnum(_messages.Enum):
-    """Apply a predefined set of access controls to this object.
+    r"""Apply a predefined set of access controls to this object.
 
     Values:
       authenticatedRead: Object owner gets OWNER access, and
         allAuthenticatedUsers get READER access.
       bucketOwnerFullControl: Object owner gets OWNER access, and project team
         owners get OWNER access.
       bucketOwnerRead: Object owner gets OWNER access, and project team owners
@@ -1848,71 +2614,117 @@
     bucketOwnerFullControl = 1
     bucketOwnerRead = 2
     private = 3
     projectPrivate = 4
     publicRead = 5
 
   class ProjectionValueValuesEnum(_messages.Enum):
-    """Set of properties to return. Defaults to full.
+    r"""Set of properties to return. Defaults to full.
 
     Values:
       full: Include all properties.
-      noAcl: Omit the acl property.
+      noAcl: Omit the owner, acl property.
     """
     full = 0
     noAcl = 1
 
   bucket = _messages.StringField(1, required=True)
   generation = _messages.IntegerField(2)
   ifGenerationMatch = _messages.IntegerField(3)
   ifGenerationNotMatch = _messages.IntegerField(4)
   ifMetagenerationMatch = _messages.IntegerField(5)
   ifMetagenerationNotMatch = _messages.IntegerField(6)
   object = _messages.StringField(7, required=True)
   objectResource = _messages.MessageField('Object', 8)
   predefinedAcl = _messages.EnumField('PredefinedAclValueValuesEnum', 9)
   projection = _messages.EnumField('ProjectionValueValuesEnum', 10)
+  userProject = _messages.StringField(11)
 
 
 class StorageObjectsWatchAllRequest(_messages.Message):
-  """A StorageObjectsWatchAllRequest object.
+  r"""A StorageObjectsWatchAllRequest object.
 
   Enums:
     ProjectionValueValuesEnum: Set of properties to return. Defaults to noAcl.
 
   Fields:
     bucket: Name of the bucket in which to look for objects.
     channel: A Channel resource to be passed as the request body.
     delimiter: Returns results in a directory-like mode. items will contain
       only objects whose names, aside from the prefix, do not contain
       delimiter. Objects whose names, aside from the prefix, contain delimiter
       will have their name, truncated after the delimiter, returned in
       prefixes. Duplicate prefixes are omitted.
-    maxResults: Maximum number of items plus prefixes to return. As duplicate
-      prefixes are omitted, fewer total results may be returned than
-      requested. The default value of this parameter is 1,000 items.
+    includeTrailingDelimiter: If true, objects that end in exactly one
+      instance of delimiter will have their metadata included in items in
+      addition to prefixes.
+    maxResults: Maximum number of items plus prefixes to return in a single
+      page of responses. As duplicate prefixes are omitted, fewer total
+      results may be returned than requested. The service will use this
+      parameter or 1,000 items, whichever is smaller.
     pageToken: A previously-returned page token representing part of the
       larger set of results to view.
     prefix: Filter results to objects whose names begin with this prefix.
     projection: Set of properties to return. Defaults to noAcl.
+    userProject: The project to be billed for this request. Required for
+      Requester Pays buckets.
     versions: If true, lists all versions of an object as distinct results.
       The default is false. For more information, see Object Versioning.
   """
 
   class ProjectionValueValuesEnum(_messages.Enum):
-    """Set of properties to return. Defaults to noAcl.
+    r"""Set of properties to return. Defaults to noAcl.
 
     Values:
       full: Include all properties.
-      noAcl: Omit the acl property.
+      noAcl: Omit the owner, acl property.
     """
     full = 0
     noAcl = 1
 
   bucket = _messages.StringField(1, required=True)
   channel = _messages.MessageField('Channel', 2)
   delimiter = _messages.StringField(3)
-  maxResults = _messages.IntegerField(4, variant=_messages.Variant.UINT32)
-  pageToken = _messages.StringField(5)
-  prefix = _messages.StringField(6)
-  projection = _messages.EnumField('ProjectionValueValuesEnum', 7)
-  versions = _messages.BooleanField(8)
+  includeTrailingDelimiter = _messages.BooleanField(4)
+  maxResults = _messages.IntegerField(5, variant=_messages.Variant.UINT32, default=1000)
+  pageToken = _messages.StringField(6)
+  prefix = _messages.StringField(7)
+  projection = _messages.EnumField('ProjectionValueValuesEnum', 8)
+  userProject = _messages.StringField(9)
+  versions = _messages.BooleanField(10)
+
+
+class StorageProjectsServiceAccountGetRequest(_messages.Message):
+  r"""A StorageProjectsServiceAccountGetRequest object.
+
+  Fields:
+    projectId: Project ID
+    userProject: The project to be billed for this request.
+  """
+
+  projectId = _messages.StringField(1, required=True)
+  userProject = _messages.StringField(2)
+
+
+class TestIamPermissionsResponse(_messages.Message):
+  r"""A storage.(buckets|objects).testIamPermissions response.
+
+  Fields:
+    kind: The kind of item this is.
+    permissions: The permissions held by the caller. Permissions are always of
+      the format storage.resource.capability, where resource is one of buckets
+      or objects. The supported permissions are as follows:   -
+      storage.buckets.delete - Delete bucket.   - storage.buckets.get - Read
+      bucket metadata.   - storage.buckets.getIamPolicy - Read bucket IAM
+      policy.   - storage.buckets.create - Create bucket.   -
+      storage.buckets.list - List buckets.   - storage.buckets.setIamPolicy -
+      Update bucket IAM policy.   - storage.buckets.update - Update bucket
+      metadata.   - storage.objects.delete - Delete object.   -
+      storage.objects.get - Read object data and metadata.   -
+      storage.objects.getIamPolicy - Read object IAM policy.   -
+      storage.objects.create - Create object.   - storage.objects.list - List
+      objects.   - storage.objects.setIamPolicy - Update object IAM policy.
+      - storage.objects.update - Update object metadata.
+  """
+
+  kind = _messages.StringField(1, default=u'storage#testIamPermissionsResponse')
+  permissions = _messages.StringField(2, repeated=True)
```

## Comparing `apache-beam-2.8.0/apache_beam/io/gcp/internal/clients/storage/__init__.py` & `apache-beam-2.9.0/apache_beam/io/gcp/internal/clients/storage/__init__.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/testing/pipeline_verifiers_test.py` & `apache-beam-2.9.0/apache_beam/testing/pipeline_verifiers_test.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/testing/synthetic_pipeline.py` & `apache-beam-2.9.0/apache_beam/testing/synthetic_pipeline.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/testing/__init__.py` & `apache-beam-2.9.0/apache_beam/testing/__init__.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/testing/util_test.py` & `apache-beam-2.9.0/apache_beam/testing/util_test.py`

 * *Files 7% similar despite different names*

```diff
@@ -34,14 +34,22 @@
 
 class UtilTest(unittest.TestCase):
 
   def test_assert_that_passes(self):
     with TestPipeline() as p:
       assert_that(p | Create([1, 2, 3]), equal_to([1, 2, 3]))
 
+  def test_assert_that_passes_order_does_not_matter(self):
+    with TestPipeline() as p:
+      assert_that(p | Create([1, 2, 3]), equal_to([2, 1, 3]))
+
+  def test_assert_that_passes_order_does_not_matter_with_negatives(self):
+    with TestPipeline() as p:
+      assert_that(p | Create([1, -2, 3]), equal_to([-2, 1, 3]))
+
   def test_assert_that_passes_empty_equal_to(self):
     with TestPipeline() as p:
       assert_that(p | Create([]), equal_to([]))
 
   def test_assert_that_passes_empty_is_empty(self):
     with TestPipeline() as p:
       assert_that(p | Create([]), is_empty())
```

## Comparing `apache-beam-2.8.0/apache_beam/testing/test_pipeline.py` & `apache-beam-2.9.0/apache_beam/testing/test_pipeline.py`

 * *Files 4% similar despite different names*

```diff
@@ -90,22 +90,25 @@
         completed.
 
     Raises:
       ~exceptions.ValueError: if either the runner or options argument is not
         of the expected type.
     """
     self.is_integration_test = is_integration_test
+    self.not_use_test_runner_api = False
     self.options_list = self._parse_test_option_args(argv)
     self.blocking = blocking
     if options is None:
       options = PipelineOptions(self.options_list)
     super(TestPipeline, self).__init__(runner, options)
 
   def run(self, test_runner_api=True):
-    result = super(TestPipeline, self).run(test_runner_api)
+    result = super(TestPipeline, self).run(
+        test_runner_api=(False if self.not_use_test_runner_api
+                         else test_runner_api))
     if self.blocking:
       state = result.wait_until_finish()
       assert state in (PipelineState.DONE, PipelineState.CANCELLED), \
           "Pipeline execution failed."
 
     return result
 
@@ -122,23 +125,28 @@
       build a pipeline option.
     """
     parser = argparse.ArgumentParser()
     parser.add_argument('--test-pipeline-options',
                         type=str,
                         action='store',
                         help='only run tests providing service options')
+    parser.add_argument('--not-use-test-runner-api',
+                        action='store_true',
+                        default=False,
+                        help='whether not to use test-runner-api')
     known, unused_argv = parser.parse_known_args(argv)
 
     if self.is_integration_test and not known.test_pipeline_options:
       # Skip integration test when argument '--test-pipeline-options' is not
       # specified since nose calls integration tests when runs unit test by
       # 'setup.py test'.
       raise SkipTest('IT is skipped because --test-pipeline-options '
                      'is not specified')
 
+    self.not_use_test_runner_api = known.not_use_test_runner_api
     return shlex.split(known.test_pipeline_options) \
       if known.test_pipeline_options else []
 
   def get_full_options_as_args(self, **extra_opts):
     """Get full pipeline options as an argument list.
 
     Append extra pipeline options to existing option list if provided.
```

## Comparing `apache-beam-2.8.0/apache_beam/testing/test_stream_test.py` & `apache-beam-2.9.0/apache_beam/testing/test_stream_test.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/testing/test_stream.py` & `apache-beam-2.9.0/apache_beam/testing/test_stream.py`

 * *Files 2% similar despite different names*

```diff
@@ -58,27 +58,28 @@
   def __hash__(self):
     raise NotImplementedError
 
   @abstractmethod
   def __lt__(self, other):
     raise NotImplementedError
 
+  def __ne__(self, other):
+    # TODO(BEAM-5949): Needed for Python 2 compatibility.
+    return not self == other
+
 
 class ElementEvent(Event):
   """Element-producing test stream event."""
 
   def __init__(self, timestamped_values):
     self.timestamped_values = timestamped_values
 
   def __eq__(self, other):
     return self.timestamped_values == other.timestamped_values
 
-  def __ne__(self, other):
-    return not self == other
-
   def __hash__(self):
     return hash(self.timestamped_values)
 
   def __lt__(self, other):
     return self.timestamped_values < other.timestamped_values
 
 
@@ -87,17 +88,14 @@
 
   def __init__(self, new_watermark):
     self.new_watermark = timestamp.Timestamp.of(new_watermark)
 
   def __eq__(self, other):
     return self.new_watermark == other.new_watermark
 
-  def __ne__(self, other):
-    return not self == other
-
   def __hash__(self):
     return hash(self.new_watermark)
 
   def __lt__(self, other):
     return self.new_watermark < other.new_watermark
 
 
@@ -106,17 +104,14 @@
 
   def __init__(self, advance_by):
     self.advance_by = timestamp.Duration.of(advance_by)
 
   def __eq__(self, other):
     return self.advance_by == other.advance_by
 
-  def __ne__(self, other):
-    return not self == other
-
   def __hash__(self):
     return hash(self.advance_by)
 
   def __lt__(self, other):
     return self.advance_by < other.advance_by
```

## Comparing `apache-beam-2.8.0/apache_beam/testing/pipeline_verifiers.py` & `apache-beam-2.9.0/apache_beam/testing/pipeline_verifiers.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/testing/test_pipeline_test.py` & `apache-beam-2.9.0/apache_beam/testing/test_pipeline_test.py`

 * *Files 7% similar despite different names*

```diff
@@ -18,14 +18,15 @@
 """Unit test for the TestPipeline class"""
 
 from __future__ import absolute_import
 
 import logging
 import unittest
 
+import mock
 from hamcrest.core.assert_that import assert_that as hc_assert_that
 from hamcrest.core.base_matcher import BaseMatcher
 
 from apache_beam.internal import pickler
 from apache_beam.options.pipeline_options import PipelineOptions
 from apache_beam.testing.test_pipeline import TestPipeline
 
@@ -104,11 +105,18 @@
 
   def test_skip_IT(self):
     test_pipeline = TestPipeline(is_integration_test=True)
     test_pipeline.run()
     # Note that this will never be reached since it should be skipped above.
     self.fail()
 
+  @mock.patch('apache_beam.testing.test_pipeline.Pipeline.run', autospec=True)
+  def test_not_use_test_runner_api(self, mock_run):
+    test_pipeline = TestPipeline(argv=['--not-use-test-runner-api'],
+                                 blocking=False)
+    test_pipeline.run()
+    mock_run.assert_called_once_with(test_pipeline, test_runner_api=False)
+
 
 if __name__ == '__main__':
   logging.getLogger().setLevel(logging.INFO)
   unittest.main()
```

## Comparing `apache-beam-2.8.0/apache_beam/testing/test_utils.py` & `apache-beam-2.9.0/apache_beam/testing/test_utils.py`

 * *Files 27% similar despite different names*

```diff
@@ -20,19 +20,17 @@
 For internal use only; no backwards-compatibility guarantees.
 """
 
 from __future__ import absolute_import
 
 import hashlib
 import imp
-import logging
 import os
 import shutil
 import tempfile
-import time
 from builtins import object
 
 from mock import Mock
 from mock import patch
 
 from apache_beam.io.filesystems import FileSystems
 from apache_beam.utils import retry
@@ -132,50 +130,65 @@
   """
   if len(file_paths) == 0:
     raise RuntimeError('Clean up failed. Invalid file path: %s.' %
                        file_paths)
   FileSystems.delete(file_paths)
 
 
-def wait_for_subscriptions_created(subs, timeout=60):
-  """Wait for all PubSub subscriptions are created."""
-  return _wait_until_all_exist(subs, timeout)
+def cleanup_subscriptions(sub_client, subs):
+  """Cleanup PubSub subscriptions if exist."""
+  for sub in subs:
+    sub_client.delete_subscription(sub.name)
 
 
-def wait_for_topics_created(topics, timeout=60):
-  """Wait for all PubSub topics are created."""
-  return _wait_until_all_exist(topics, timeout)
+def cleanup_topics(pub_client, topics):
+  """Cleanup PubSub topics if exist."""
+  for topic in topics:
+    pub_client.delete_topic(topic.name)
 
 
-def _wait_until_all_exist(components, timeout):
-  unchecked_components = set(components)
-  start_time = time.time()
-  while time.time() - start_time <= timeout:
-    unchecked_components = set(
-        [c for c in unchecked_components if not c.exists()])
-    if len(unchecked_components) == 0:
-      return True
-    time.sleep(2)
+class PullResponseMessage(object):
+  """Data representing a pull request response.
 
-  raise RuntimeError(
-      'Timeout after %d seconds. %d of %d topics/subscriptions not exist. '
-      'They are %s.' % (timeout, len(unchecked_components),
-                        len(components), list(unchecked_components)))
+  Utility class for ``create_pull_response``.
+  """
+  def __init__(self, data, attributes=None,
+               publish_time_secs=None, publish_time_nanos=None, ack_id=None):
+    self.data = data
+    self.attributes = attributes
+    self.publish_time_secs = publish_time_secs
+    self.publish_time_nanos = publish_time_nanos
+    self.ack_id = ack_id
 
 
-def cleanup_subscriptions(subs):
-  """Cleanup PubSub subscriptions if exist."""
-  _cleanup_pubsub(subs)
+def create_pull_response(responses):
+  """Create an instance of ``google.cloud.pubsub.types.ReceivedMessage``.
 
+  Used to simulate the response from pubsub.SubscriberClient().pull().
 
-def cleanup_topics(topics):
-  """Cleanup PubSub topics if exist."""
-  _cleanup_pubsub(topics)
+  Args:
+    responses: list of ``PullResponseMessage``
+
+  Returns:
+    An instance of ``google.cloud.pubsub.types.PullResponse`` populated with
+    responses.
+  """
+  from google.cloud import pubsub
+
+  res = pubsub.types.PullResponse()
+  for response in responses:
+    received_message = res.received_messages.add()
+
+    message = received_message.message
+    message.data = response.data
+    if response.attributes is not None:
+      for k, v in response.attributes.items():
+        message.attributes[k] = v
+    if response.publish_time_secs is not None:
+      message.publish_time.seconds = response.publish_time_secs
+    if response.publish_time_nanos is not None:
+      message.publish_time.nanos = response.publish_time_nanos
 
+    if response.ack_id is not None:
+      received_message.ack_id = response.ack_id
 
-def _cleanup_pubsub(components):
-  for c in components:
-    if c.exists():
-      c.delete()
-    else:
-      logging.debug('Cannot delete topic/subscription. %s does not exist.',
-                    c.full_name)
+  return res
```

## Comparing `apache-beam-2.8.0/apache_beam/testing/synthetic_pipeline_test.py` & `apache-beam-2.9.0/apache_beam/testing/synthetic_pipeline_test.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/testing/util.py` & `apache-beam-2.9.0/apache_beam/testing/util.py`

 * *Files 4% similar despite different names*

```diff
@@ -66,14 +66,18 @@
   class InAnyOrder(object):
     def __init__(self, iterable):
       self._counter = collections.Counter(iterable)
 
     def __eq__(self, other):
       return self._counter == collections.Counter(other)
 
+    def __ne__(self, other):
+      # TODO(BEAM-5949): Needed for Python 2 compatibility.
+      return not self == other
+
     def __hash__(self):
       return hash(self._counter)
 
     def __repr__(self):
       return "InAnyOrder(%s)" % self._counter
 
   return InAnyOrder(iterable)
@@ -100,27 +104,43 @@
         # For example, in the presence of early triggers.
         if all(elem in sorted_expected for elem in sorted_actual) is False:
           raise BeamAssertException(
               'Failed assert: %r not in %r' % (sorted_actual, sorted_expected))
   return matcher
 
 
-# Note that equal_to always sorts the expected and actual since what we
-# compare are PCollections for which there is no guaranteed order.
-# However the sorting does not go beyond top level therefore [1,2] and [2,1]
-# are considered equal and [[1,2]] and [[2,1]] are not.
+# Note that equal_to checks if expected and actual are permutations of each
+# other. However, only permutations of the top level are checked. Therefore
+# [1,2] and [2,1] are considered equal and [[1,2]] and [[2,1]] are not.
 def equal_to(expected):
-  expected = list(expected)
 
   def _equal(actual):
-    sorted_expected = sorted(expected)
-    sorted_actual = sorted(actual)
-    if sorted_expected != sorted_actual:
-      raise BeamAssertException(
-          'Failed assert: %r == %r' % (sorted_expected, sorted_actual))
+    expected_list = list(expected)
+
+    # Try to compare actual and expected by sorting. This fails with a
+    # TypeError in Python 3 if different types are present in the same
+    # collection.
+    try:
+      sorted_expected = sorted(expected)
+      sorted_actual = sorted(actual)
+      if sorted_expected != sorted_actual:
+        raise BeamAssertException(
+            'Failed assert: %r == %r' % (sorted_expected, sorted_actual))
+    # Fall back to slower method which works for different types on Python 3.
+    except TypeError:
+      for element in actual:
+        try:
+          expected_list.remove(element)
+        except ValueError:
+          raise BeamAssertException(
+              'Failed assert: %r == %r' % (expected, actual))
+      if expected_list:
+        raise BeamAssertException(
+            'Failed assert: %r == %r' % (expected, actual))
+
   return _equal
 
 
 def is_empty():
   def _empty(actual):
     actual = list(actual)
     if actual:
```

## Comparing `apache-beam-2.8.0/apache_beam/testing/data/trigger_transcripts.yaml` & `apache-beam-2.9.0/apache_beam/testing/data/trigger_transcripts.yaml`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/testing/data/standard_coders.yaml` & `apache-beam-2.9.0/apache_beam/testing/data/standard_coders.yaml`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/testing/benchmarks/__init__.py` & `apache-beam-2.9.0/apache_beam/testing/benchmarks/__init__.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/testing/benchmarks/nexmark/nexmark_launcher.py` & `apache-beam-2.9.0/apache_beam/testing/benchmarks/nexmark/nexmark_launcher.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/testing/benchmarks/nexmark/__init__.py` & `apache-beam-2.9.0/apache_beam/testing/benchmarks/nexmark/__init__.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/testing/benchmarks/nexmark/nexmark_util.py` & `apache-beam-2.9.0/apache_beam/testing/benchmarks/nexmark/nexmark_util.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/testing/benchmarks/nexmark/queries/query2.py` & `apache-beam-2.9.0/apache_beam/testing/benchmarks/nexmark/queries/query2.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/testing/benchmarks/nexmark/queries/query0.py` & `apache-beam-2.9.0/apache_beam/testing/benchmarks/nexmark/queries/query0.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/testing/benchmarks/nexmark/queries/query1.py` & `apache-beam-2.9.0/apache_beam/testing/benchmarks/nexmark/queries/query1.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/testing/benchmarks/nexmark/queries/__init__.py` & `apache-beam-2.9.0/apache_beam/runners/interactive/display/__init__.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/testing/benchmarks/nexmark/models/__init__.py` & `apache-beam-2.9.0/apache_beam/testing/benchmarks/nexmark/models/__init__.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/testing/benchmarks/nexmark/models/nexmark_model.py` & `apache-beam-2.9.0/apache_beam/testing/benchmarks/nexmark/models/nexmark_model.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/metrics/execution.py` & `apache-beam-2.9.0/apache_beam/metrics/execution.py`

 * *Files 17% similar despite different names*

```diff
@@ -32,17 +32,19 @@
 
 from __future__ import absolute_import
 
 import threading
 from builtins import object
 from collections import defaultdict
 
+from apache_beam.metrics import monitoring_infos
 from apache_beam.metrics.cells import CounterCell
 from apache_beam.metrics.cells import DistributionCell
 from apache_beam.metrics.cells import GaugeCell
+from apache_beam.metrics.monitoring_infos import user_metric_urn
 from apache_beam.portability.api import beam_fn_api_pb2
 from apache_beam.runners.worker import statesampler
 
 
 class MetricKey(object):
   """Key used to identify instance of metric cell.
 
@@ -59,14 +61,18 @@
     self.step = step
     self.metric = metric
 
   def __eq__(self, other):
     return (self.step == other.step and
             self.metric == other.metric)
 
+  def __ne__(self, other):
+    # TODO(BEAM-5949): Needed for Python 2 compatibility.
+    return not self == other
+
   def __hash__(self):
     return hash((self.step, self.metric))
 
   def __repr__(self):
     return 'MetricKey(step={}, metric={})'.format(
         self.step, self.metric)
 
@@ -101,14 +107,18 @@
     self.attempted = attempted
 
   def __eq__(self, other):
     return (self.key == other.key and
             self.committed == other.committed and
             self.attempted == other.attempted)
 
+  def __ne__(self, other):
+    # TODO(BEAM-5949): Needed for Python 2 compatibility.
+    return not self == other
+
   def __hash__(self):
     return hash((self.key, self.committed, self.attempted))
 
   def __repr__(self):
     return 'MetricResult(key={}, committed={}, attempted={})'.format(
         self.key, str(self.committed), str(self.attempted))
 
@@ -207,14 +217,47 @@
          for k, v in self.distributions.items()] +
         [beam_fn_api_pb2.Metrics.User(
             metric_name=k.to_runner_api(),
             gauge_data=v.get_cumulative().to_runner_api())
          for k, v in self.gauges.items()]
     )
 
+  def to_runner_api_monitoring_infos(self, transform_id):
+    """Returns a list of MonitoringInfos for the metrics in this container."""
+    all_user_metrics = []
+    for k, v in self.counters.items():
+      all_user_metrics.append(monitoring_infos.int64_counter(
+          user_metric_urn(k.namespace, k.name),
+          v.to_runner_api_monitoring_info(),
+          ptransform=transform_id
+      ))
+
+    for k, v in self.distributions.items():
+      all_user_metrics.append(monitoring_infos.int64_distribution(
+          user_metric_urn(k.namespace, k.name),
+          v.get_cumulative().to_runner_api_monitoring_info(),
+          ptransform=transform_id
+      ))
+
+    for k, v in self.gauges.items():
+      all_user_metrics.append(monitoring_infos.int64_gauge(
+          user_metric_urn(k.namespace, k.name),
+          v.get_cumulative().to_runner_api_monitoring_info(),
+          ptransform=transform_id
+      ))
+    return {monitoring_infos.to_key(mi) : mi for mi in all_user_metrics}
+
+  def reset(self):
+    for counter in self.counters.values():
+      counter.reset()
+    for distribution in self.distributions.values():
+      distribution.reset()
+    for gauge in self.gauges.values():
+      gauge.reset()
+
 
 class MetricUpdates(object):
   """Contains updates for several metrics.
 
   A metric update is an object containing information to update a metric.
   For Distribution metrics, it is DistributionData, and for Counter metrics,
   it's an int.
```

## Comparing `apache-beam-2.8.0/apache_beam/metrics/execution.pxd` & `apache-beam-2.9.0/apache_beam/metrics/execution.pxd`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/metrics/__init__.py` & `apache-beam-2.9.0/apache_beam/metrics/__init__.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/metrics/cells.py` & `apache-beam-2.9.0/apache_beam/metrics/cells.py`

 * *Files 16% similar despite different names*

```diff
@@ -143,28 +143,44 @@
 
   This class is thread safe.
   """
   def __init__(self, *args):
     super(CounterCell, self).__init__(*args)
     self.value = CounterAggregator.identity_element()
 
+  def reset(self):
+    self.commit = CellCommitState()
+    self.value = CounterAggregator.identity_element()
+
   def combine(self, other):
     result = CounterCell()
     result.inc(self.value + other.value)
     return result
 
   def inc(self, n=1):
     with self._lock:
       self.value += n
       self.commit.after_modification()
 
   def get_cumulative(self):
     with self._lock:
       return self.value
 
+  def to_runner_api_monitoring_info(self):
+    """Returns a Metric with this counter value for use in a MonitoringInfo."""
+    # TODO(ajamato): Update this code to be consisten with Gauges
+    # and Distributions. Since there is no CounterData class this method
+    # was added to CounterCell. Consider adding a CounterData class or
+    # removing the GaugeData and DistributionData classes.
+    return beam_fn_api_pb2.Metric(
+        counter_data=beam_fn_api_pb2.CounterData(
+            int64_value=self.get_cumulative()
+        )
+    )
+
 
 class DistributionCell(Distribution, MetricCell):
   """For internal use only; no backwards-compatibility guarantees.
 
   Tracks the current value and delta for a distribution metric.
 
   Each cell tracks the state of a metric independently per context per bundle.
@@ -173,14 +189,18 @@
 
   This class is thread safe.
   """
   def __init__(self, *args):
     super(DistributionCell, self).__init__(*args)
     self.data = DistributionAggregator.identity_element()
 
+  def reset(self):
+    self.commit = CellCommitState()
+    self.data = DistributionAggregator.identity_element()
+
   def combine(self, other):
     result = DistributionCell()
     result.data = self.data.combine(other.data)
     return result
 
   def update(self, value):
     with self._lock:
@@ -214,14 +234,18 @@
 
   This class is thread safe.
   """
   def __init__(self, *args):
     super(GaugeCell, self).__init__(*args)
     self.data = GaugeAggregator.identity_element()
 
+  def reset(self):
+    self.commit = CellCommitState()
+    self.data = GaugeAggregator.identity_element()
+
   def combine(self, other):
     result = GaugeCell()
     result.data = self.data.combine(other.data)
     return result
 
   def set(self, value):
     value = int(value)
@@ -248,15 +272,16 @@
     else:
       return False
 
   def __hash__(self):
     return hash(self.data)
 
   def __ne__(self, other):
-    return not self.__eq__(other)
+    # TODO(BEAM-5949): Needed for Python 2 compatibility.
+    return not self == other
 
   def __repr__(self):
     return '<DistributionResult(sum={}, count={}, min={}, max={})>'.format(
         self.sum,
         self.count,
         self.min,
         self.max)
@@ -298,15 +323,16 @@
     else:
       return False
 
   def __hash__(self):
     return hash(self.data)
 
   def __ne__(self, other):
-    return not self.__eq__(other)
+    # TODO(BEAM-5949): Needed for Python 2 compatibility.
+    return not self == other
 
   def __repr__(self):
     return '<GaugeResult(value={}, timestamp={})>'.format(
         self.value,
         self.timestamp)
 
   @property
@@ -335,15 +361,16 @@
   def __eq__(self, other):
     return self.value == other.value and self.timestamp == other.timestamp
 
   def __hash__(self):
     return hash((self.value, self.timestamp))
 
   def __ne__(self, other):
-    return not self.__eq__(other)
+    # TODO(BEAM-5949): Needed for Python 2 compatibility.
+    return not self == other
 
   def __repr__(self):
     return '<GaugeData(value={}, timestamp={})>'.format(
         self.value,
         self.timestamp)
 
   def get_cumulative(self):
@@ -371,14 +398,22 @@
 
   @staticmethod
   def from_runner_api(proto):
     gauge_timestamp = (proto.timestamp.seconds +
                        float(proto.timestamp.nanos) / 10**9)
     return GaugeData(proto.value, timestamp=gauge_timestamp)
 
+  def to_runner_api_monitoring_info(self):
+    """Returns a Metric with this value for use in a MonitoringInfo."""
+    return beam_fn_api_pb2.Metric(
+        counter_data=beam_fn_api_pb2.CounterData(
+            int64_value=self.value
+        )
+    )
+
 
 class DistributionData(object):
   """For internal use only; no backwards-compatibility guarantees.
 
   The data structure that holds data about a distribution metric.
 
   Distribution metrics are restricted to distributions of integers only.
@@ -398,15 +433,16 @@
             self.min == other.min and
             self.max == other.max)
 
   def __hash__(self):
     return hash((self.sum, self.count, self.min, self.max))
 
   def __ne__(self, other):
-    return not self.__eq__(other)
+    # TODO(BEAM-5949): Needed for Python 2 compatibility.
+    return not self == other
 
   def __repr__(self):
     return '<DistributionData(sum={}, count={}, min={}, max={})>'.format(
         self.sum,
         self.count,
         self.min,
         self.max)
@@ -436,14 +472,21 @@
     return beam_fn_api_pb2.Metrics.User.DistributionData(
         count=self.count, sum=self.sum, min=self.min, max=self.max)
 
   @staticmethod
   def from_runner_api(proto):
     return DistributionData(proto.sum, proto.count, proto.min, proto.max)
 
+  def to_runner_api_monitoring_info(self):
+    """Returns a Metric with this value for use in a MonitoringInfo."""
+    return beam_fn_api_pb2.Metric(
+        distribution_data=beam_fn_api_pb2.DistributionData(
+            int_distribution_data=beam_fn_api_pb2.IntDistributionData(
+                count=self.count, sum=self.sum, min=self.min, max=self.max)))
+
 
 class MetricAggregator(object):
   """For internal use only; no backwards-compatibility guarantees.
 
   Base interface for aggregating metric data during pipeline execution."""
 
   def identity_element(self):
```

## Comparing `apache-beam-2.8.0/apache_beam/metrics/cells_test.py` & `apache-beam-2.9.0/apache_beam/metrics/cells_test.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/metrics/metric.py` & `apache-beam-2.9.0/apache_beam/metrics/metric.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/metrics/execution_test.py` & `apache-beam-2.9.0/apache_beam/metrics/execution_test.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/metrics/metricbase.py` & `apache-beam-2.9.0/apache_beam/metrics/metricbase.py`

 * *Files 2% similar despite different names*

```diff
@@ -62,14 +62,18 @@
     self.namespace = namespace
     self.name = name
 
   def __eq__(self, other):
     return (self.namespace == other.namespace and
             self.name == other.name)
 
+  def __ne__(self, other):
+    # TODO(BEAM-5949): Needed for Python 2 compatibility.
+    return not self == other
+
   def __str__(self):
     return 'MetricName(namespace={}, name={})'.format(
         self.namespace, self.name)
 
   def __hash__(self):
     return hash((self.namespace, self.name))
```

## Comparing `apache-beam-2.8.0/apache_beam/metrics/metric_test.py` & `apache-beam-2.9.0/apache_beam/metrics/metric_test.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/typehints/decorators.py` & `apache-beam-2.9.0/apache_beam/typehints/decorators.py`

 * *Files 4% similar despite different names*

```diff
@@ -108,46 +108,50 @@
 
 # This is missing in the builtin types module.  str.upper is arbitrary, any
 # method on a C-implemented type will do.
 # pylint: disable=invalid-name
 _MethodDescriptorType = type(str.upper)
 # pylint: enable=invalid-name
 
+try:
+  _original_getfullargspec = inspect.getfullargspec
+  _use_full_argspec = True
+except AttributeError:  # Python 2
+  _original_getfullargspec = inspect.getargspec
+  _use_full_argspec = False
 
-# Monkeypatch inspect.getargspec to allow passing non-function objects.
-# This is needed to use higher-level functions such as getcallargs.
-_original_getargspec = inspect.getargspec
 
-
-def getargspec(func):
+def getfullargspec(func):
   try:
-    return _original_getargspec(func)
+    return _original_getfullargspec(func)
   except TypeError:
     if isinstance(func, type):
-      argspec = getargspec(func.__init__)
+      argspec = getfullargspec(func.__init__)
       del argspec.args[0]
       return argspec
     elif callable(func):
       try:
-        return _original_getargspec(func.__call__)
+        return _original_getfullargspec(func.__call__)
       except TypeError:
         # Return an ArgSpec with at least one positional argument,
         # and any number of other (positional or keyword) arguments
-        # whose name won't match any real agument.
+        # whose name won't match any real argument.
         # Arguments with the %unknown% prefix will be ignored in the type
         # checking code.
-        return inspect.ArgSpec(
-            ['_'], '__unknown__varargs', '__unknown__keywords', ())
+        if _use_full_argspec:
+          return inspect.FullArgSpec(
+              ['_'], '__unknown__varargs', '__unknown__keywords', (),
+              [], {}, {})
+        else:  # Python 2
+          return inspect.ArgSpec(
+              ['_'], '__unknown__varargs', '__unknown__keywords', ())
     else:
       raise
 
 
-inspect.getargspec = getargspec
-
-
 class IOTypeHints(object):
   """Encapsulates all type hint information about a Dataflow construct.
 
   This should primarily be used via the WithTypeHints mixin class, though
   may also be attached to other objects (such as Python functions).
   """
   __slots__ = ('input_types', 'output_types')
@@ -255,38 +259,62 @@
     return (typehints.Any,) * len(arg)
   return hint
 
 
 def getcallargs_forhints(func, *typeargs, **typekwargs):
   """Like inspect.getcallargs, but understands that Tuple[] and an Any unpack.
   """
-  argspec = inspect.getargspec(func)
+  argspec = getfullargspec(func)
   # Turn Tuple[x, y] into (x, y) so getcallargs can do the proper unpacking.
   packed_typeargs = [_unpack_positional_arg_hints(arg, hint)
                      for (arg, hint) in zip(argspec.args, typeargs)]
   packed_typeargs += list(typeargs[len(packed_typeargs):])
+
+  # Monkeypatch inspect.getfullargspec to allow passing non-function objects.
+  # getfullargspec (getargspec on Python 2) are used by inspect.getcallargs.
+  # TODO(BEAM-5490): Reimplement getcallargs and stop relying on monkeypatch.
+  if _use_full_argspec:
+    inspect.getfullargspec = getfullargspec
+  else:  # Python 2
+    inspect.getargspec = getfullargspec
+
   try:
     callargs = inspect.getcallargs(func, *packed_typeargs, **typekwargs)
   except TypeError as e:
     raise TypeCheckError(e)
+  finally:
+    # Revert monkey-patch.
+    if _use_full_argspec:
+      inspect.getfullargspec = _original_getfullargspec
+    else:
+      inspect.getargspec = _original_getfullargspec
+
   if argspec.defaults:
     # Declare any default arguments to be Any.
     for k, var in enumerate(reversed(argspec.args)):
       if k >= len(argspec.defaults):
         break
       if callargs.get(var, None) is argspec.defaults[-k-1]:
         callargs[var] = typehints.Any
   # Patch up varargs and keywords
   if argspec.varargs:
     callargs[argspec.varargs] = typekwargs.get(
         argspec.varargs, typehints.Tuple[typehints.Any, ...])
-  if argspec.keywords:
+  if _use_full_argspec:
+    varkw = argspec.varkw
+  else:  # Python 2
+    varkw = argspec.keywords
+
+  if varkw:
     # TODO(robertwb): Consider taking the union of key and value types.
-    callargs[argspec.keywords] = typekwargs.get(
-        argspec.keywords, typehints.Dict[typehints.Any, typehints.Any])
+    callargs[varkw] = typekwargs.get(
+        varkw, typehints.Dict[typehints.Any, typehints.Any])
+
+  # TODO(BEAM-5878) Support kwonlyargs.
+
   return callargs
 
 
 def get_type_hints(fn):
   """Gets the type hint associated with an arbitrary object fn.
 
   Always returns a valid IOTypeHints object, creating one if necissary.
```

## Comparing `apache-beam-2.8.0/apache_beam/typehints/typehints_test.py` & `apache-beam-2.9.0/apache_beam/typehints/typehints_test.py`

 * *Files 0% similar despite different names*

```diff
@@ -34,14 +34,15 @@
 from apache_beam.typehints import with_output_types
 from apache_beam.typehints.decorators import GeneratorWrapper
 from apache_beam.typehints.decorators import _check_instance_type
 from apache_beam.typehints.decorators import _interleave_type_check
 from apache_beam.typehints.decorators import _positional_arg_hints
 from apache_beam.typehints.decorators import get_type_hints
 from apache_beam.typehints.decorators import getcallargs_forhints
+from apache_beam.typehints.decorators import getfullargspec
 from apache_beam.typehints.typehints import is_consistent_with
 
 
 def check_or_interleave(hint, value, var):
   if hint is None:
     return value
   elif isinstance(hint, typehints.IteratorHint.IteratorTypeConstraint):
@@ -62,15 +63,15 @@
         value = inputs[var]
         new_value = check_or_interleave(hint, value, var)
         if new_value is not value:
           if var in kwargs:
             kwargs[var] = new_value
           else:
             args = list(args)
-            for ix, pvar in enumerate(inspect.getargspec(f).args):
+            for ix, pvar in enumerate(getfullargspec(f).args):
               if pvar == var:
                 args[ix] = new_value
                 break
             else:
               raise NotImplementedError('Iterable in nested argument %s' % var)
     res = f(*args, **kwargs)
     return check_or_interleave(hints.simple_output_type('typecheck'), res, None)
```

## Comparing `apache-beam-2.8.0/apache_beam/typehints/native_type_compatibility.py` & `apache-beam-2.9.0/apache_beam/typehints/native_type_compatibility.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/typehints/native_type_compatibility_test.py` & `apache-beam-2.9.0/apache_beam/typehints/native_type_compatibility_test.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/typehints/__init__.py` & `apache-beam-2.9.0/apache_beam/typehints/__init__.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/typehints/typehints.py` & `apache-beam-2.9.0/apache_beam/typehints/typehints.py`

 * *Files 2% similar despite different names*

```diff
@@ -174,14 +174,18 @@
     visitor(self, visitor_arg)
     for t in self._inner_types():
       if isinstance(t, TypeConstraint):
         t.visit(visitor, visitor_arg)
       else:
         visitor(t, visitor_arg)
 
+  def __ne__(self, other):
+    # TODO(BEAM-5949): Needed for Python 2 compatibility.
+    return not self == other
+
 
 def match_type_variables(type_constraint, concrete_type):
   if isinstance(type_constraint, TypeConstraint):
     return type_constraint.match_type_variables(concrete_type)
   return {}
```

## Comparing `apache-beam-2.8.0/apache_beam/typehints/trivial_inference_test.py` & `apache-beam-2.9.0/apache_beam/typehints/trivial_inference_test.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/typehints/typed_pipeline_test.py` & `apache-beam-2.9.0/apache_beam/typehints/typed_pipeline_test.py`

 * *Files 1% similar despite different names*

```diff
@@ -15,28 +15,28 @@
 # limitations under the License.
 #
 
 """Unit tests for the type-hint objects and decorators."""
 
 from __future__ import absolute_import
 
-import inspect
 import os
 import sys
 import typing
 import unittest
 
 import apache_beam as beam
 from apache_beam import pvalue
 from apache_beam import typehints
 from apache_beam.options.pipeline_options import OptionsContext
 from apache_beam.testing.test_pipeline import TestPipeline
 from apache_beam.testing.util import assert_that
 from apache_beam.testing.util import equal_to
 from apache_beam.typehints import WithTypeHints
+from apache_beam.typehints.decorators import getfullargspec
 
 # These test often construct a pipeline as value | PTransform to test side
 # effects (e.g. errors).
 # pylint: disable=expression-not-assigned
 
 
 class MainInputTest(unittest.TestCase):
@@ -158,15 +158,15 @@
     # Various mismatches.
     with self.assertRaises(typehints.TypeCheckError):
       ['a', 'bb', 'c'] | beam.Map(repeat, 'z')
     with self.assertRaises(typehints.TypeCheckError):
       ['a', 'bb', 'c'] | beam.Map(repeat, times='z')
     with self.assertRaises(typehints.TypeCheckError):
       ['a', 'bb', 'c'] | beam.Map(repeat, 3, 4)
-    if not inspect.getargspec(repeat).defaults:
+    if not getfullargspec(repeat).defaults:
       with self.assertRaises(typehints.TypeCheckError):
         ['a', 'bb', 'c'] | beam.Map(repeat)
 
   def test_basic_side_input_hint(self):
     @typehints.with_input_types(str, int)
     def repeat(s, times):
       return s * times
```

## Comparing `apache-beam-2.8.0/apache_beam/typehints/opcodes.py` & `apache-beam-2.9.0/apache_beam/typehints/opcodes.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/typehints/trivial_inference.py` & `apache-beam-2.9.0/apache_beam/typehints/trivial_inference.py`

 * *Files 2% similar despite different names*

```diff
@@ -90,14 +90,18 @@
   def __init__(self, value):
     self.value = value
     self.type = instance_to_type(value)
 
   def __eq__(self, other):
     return isinstance(other, Const) and self.value == other.value
 
+  def __ne__(self, other):
+    # TODO(BEAM-5949): Needed for Python 2 compatibility.
+    return not self == other
+
   def __hash__(self):
     return hash(self.value)
 
   def __repr__(self):
     return 'Const[%s]' % str(self.value)[:100]
 
   @staticmethod
@@ -120,14 +124,18 @@
     self.co = f.__code__
     self.vars = list(local_vars)
     self.stack = list(stack)
 
   def __eq__(self, other):
     return isinstance(other, FrameState) and self.__dict__ == other.__dict__
 
+  def __ne__(self, other):
+    # TODO(BEAM-5949): Needed for Python 2 compatibility.
+    return not self == other
+
   def __hash__(self):
     return hash(tuple(sorted(self.__dict__.items())))
 
   def copy(self):
     return FrameState(self.f, self.vars, self.stack)
 
   def const_type(self, i):
```

## Comparing `apache-beam-2.8.0/apache_beam/typehints/typecheck.py` & `apache-beam-2.9.0/apache_beam/typehints/typecheck.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/tools/map_fn_microbenchmark.py` & `apache-beam-2.9.0/apache_beam/tools/map_fn_microbenchmark.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/tools/utils.py` & `apache-beam-2.9.0/apache_beam/tools/utils.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/tools/__init__.py` & `apache-beam-2.9.0/apache_beam/tools/__init__.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/tools/distribution_counter_microbenchmark.py` & `apache-beam-2.9.0/apache_beam/tools/distribution_counter_microbenchmark.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/tools/coders_microbenchmark.py` & `apache-beam-2.9.0/apache_beam/tools/coders_microbenchmark.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/tools/microbenchmarks_test.py` & `apache-beam-2.9.0/apache_beam/tools/microbenchmarks_test.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/tools/sideinput_microbenchmark.py` & `apache-beam-2.9.0/apache_beam/tools/sideinput_microbenchmark.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/coders/coder_impl.pxd` & `apache-beam-2.9.0/apache_beam/coders/coder_impl.pxd`

 * *Files 5% similar despite different names*

```diff
@@ -65,20 +65,21 @@
 cdef class DeterministicFastPrimitivesCoderImpl(CoderImpl):
   cdef CoderImpl _underlying_coder
   cdef object _step_label
   cdef bint _check_safe(self, value) except -1
 
 
 cdef object NoneType
-cdef char UNKNOWN_TYPE, NONE_TYPE, INT_TYPE, FLOAT_TYPE, BOOL_TYPE
-cdef char BYTES_TYPE, UNICODE_TYPE, LIST_TYPE, TUPLE_TYPE, DICT_TYPE, SET_TYPE
+cdef unsigned char UNKNOWN_TYPE, NONE_TYPE, INT_TYPE, FLOAT_TYPE, BOOL_TYPE
+cdef unsigned char BYTES_TYPE, UNICODE_TYPE, LIST_TYPE, TUPLE_TYPE, DICT_TYPE
+cdef unsigned char SET_TYPE
 
 cdef class FastPrimitivesCoderImpl(StreamCoderImpl):
   cdef CoderImpl fallback_coder_impl
-  @cython.locals(dict_value=dict)
+  @cython.locals(dict_value=dict, int_value=libc.stdint.int64_t)
   cpdef encode_to_stream(self, value, OutputStream stream, bint nested)
 
 
 cdef class BytesCoderImpl(CoderImpl):
   pass
```

## Comparing `apache-beam-2.8.0/apache_beam/coders/stream_test.py` & `apache-beam-2.9.0/apache_beam/coders/stream_test.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/coders/__init__.py` & `apache-beam-2.9.0/apache_beam/coders/__init__.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/coders/slow_stream.py` & `apache-beam-2.9.0/apache_beam/coders/slow_stream.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/coders/stream.pxd` & `apache-beam-2.9.0/apache_beam/coders/stream.pxd`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/coders/coders_test_common.py` & `apache-beam-2.9.0/apache_beam/coders/coders_test_common.py`

 * *Files 1% similar despite different names*

```diff
@@ -139,14 +139,18 @@
     self.check_coder(coder, [], [1, 2, 3])
     self.check_coder(coder, dict(), {'a': 'b'}, {0: dict(), 1: len})
     self.check_coder(coder, set(), {'a', 'b'})
     self.check_coder(coder, True, False)
     self.check_coder(coder, len)
     self.check_coder(coders.TupleCoder((coder,)), ('a',), (1,))
 
+  def test_fast_primitives_coder_large_int(self):
+    coder = coders.FastPrimitivesCoder()
+    self.check_coder(coder, 10 ** 100)
+
   def test_bytes_coder(self):
     self.check_coder(coders.BytesCoder(), b'a', b'\0', b'z' * 1000)
 
   def test_varint_coder(self):
     # Small ints.
     self.check_coder(coders.VarIntCoder(), *range(-10, 10))
     # Multi-byte encoding starts at 128
```

## Comparing `apache-beam-2.8.0/apache_beam/coders/proto2_coder_test_messages_pb2.py` & `apache-beam-2.9.0/apache_beam/coders/proto2_coder_test_messages_pb2.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/coders/observable_test.py` & `apache-beam-2.9.0/apache_beam/coders/observable_test.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/coders/standard_coders_test.py` & `apache-beam-2.9.0/apache_beam/coders/standard_coders_test.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/coders/coders_test.py` & `apache-beam-2.9.0/apache_beam/coders/coders_test.py`

 * *Files 2% similar despite different names*

```diff
@@ -90,14 +90,18 @@
     pass
 
   def __eq__(self, other):
     if isinstance(other, self.__class__):
       return True
     return False
 
+  def __ne__(self, other):
+    # TODO(BEAM-5949): Needed for Python 2 compatibility.
+    return not self == other
+
   def __hash__(self):
     return hash(type(self))
 
 
 class FallbackCoderTest(unittest.TestCase):
 
   def test_default_fallback_path(self):
```

## Comparing `apache-beam-2.8.0/apache_beam/coders/typecoders.py` & `apache-beam-2.9.0/apache_beam/coders/typecoders.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/coders/stream.pyx` & `apache-beam-2.9.0/apache_beam/coders/stream.pyx`

 * *Files 0% similar despite different names*

```diff
@@ -170,15 +170,15 @@
     # unsigned char here.
     return <long>(<unsigned char> self.allc[self.pos - 1])
 
   cpdef ssize_t size(self) except? -1:
     return len(self.all) - self.pos
 
   cpdef bytes read_all(self, bint nested=False):
-    return self.read(self.read_var_int64() if nested else self.size())
+    return self.read(<ssize_t>self.read_var_int64() if nested else self.size())
 
   cpdef libc.stdint.int64_t read_var_int64(self) except? -1:
     """Decode a variable-length encoded long from a stream."""
     cdef long byte
     cdef long bits
     cdef long shift = 0
     cdef libc.stdint.int64_t result = 0
```

## Comparing `apache-beam-2.8.0/apache_beam/coders/typecoders_test.py` & `apache-beam-2.9.0/apache_beam/coders/typecoders_test.py`

 * *Files 2% similar despite different names*

```diff
@@ -31,14 +31,18 @@
 
   def __init__(self, n):
     self.number = n
 
   def __eq__(self, other):
     return self.number == other.number
 
+  def __ne__(self, other):
+    # TODO(BEAM-5949): Needed for Python 2 compatibility.
+    return not self == other
+
   def __hash__(self):
     return self.number
 
 
 class CustomCoder(coders.Coder):
 
   def encode(self, value):
```

## Comparing `apache-beam-2.8.0/apache_beam/coders/slow_coders_test.py` & `apache-beam-2.9.0/apache_beam/coders/slow_coders_test.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/coders/coder_impl.py` & `apache-beam-2.9.0/apache_beam/coders/coder_impl.py`

 * *Files 1% similar despite different names*

```diff
@@ -58,14 +58,20 @@
   globals()['create_OutputStream'] = create_OutputStream
   globals()['ByteCountingOutputStream'] = ByteCountingOutputStream
 except ImportError:
   from .slow_stream import InputStream as create_InputStream
   from .slow_stream import OutputStream as create_OutputStream
   from .slow_stream import ByteCountingOutputStream
   from .slow_stream import get_varint_size
+  if False:  # pylint: disable=using-constant-test
+    # This clause is interpreted by the compiler.
+    from cython import compiled as is_compiled
+  else:
+    is_compiled = False
+    fits_in_64_bits = lambda x: -(1 << 63) <= x <= (1 << 63) - 1
 # pylint: enable=wrong-import-order, wrong-import-position, ungrouped-imports
 
 
 class CoderImpl(object):
   """For internal use only; no backwards-compatibility guarantees."""
 
   def encode_to_stream(self, value, stream, nested):
@@ -289,16 +295,30 @@
     return out.get_count(), []
 
   def encode_to_stream(self, value, stream, nested):
     t = type(value)
     if value is None:
       stream.write_byte(NONE_TYPE)
     elif t is int:
-      stream.write_byte(INT_TYPE)
-      stream.write_var_int64(value)
+      # In Python 3, an int may be larger than 64 bits.
+      # We need to check whether value fits into a 64 bit integer before
+      # writing the marker byte.
+      try:
+        # In Cython-compiled code this will throw an overflow error
+        # when value does not fit into int64.
+        int_value = value
+        # If Cython is not used, we must do a (slower) check ourselves.
+        if not is_compiled:
+          if not fits_in_64_bits(value):
+            raise OverflowError()
+        stream.write_byte(INT_TYPE)
+        stream.write_var_int64(int_value)
+      except OverflowError:
+        stream.write_byte(UNKNOWN_TYPE)
+        self.fallback_coder_impl.encode_to_stream(value, stream, nested)
     elif t is float:
       stream.write_byte(FLOAT_TYPE)
       stream.write_bigendian_double(value)
     elif t is bytes:
       stream.write_byte(BYTES_TYPE)
       stream.write(value, nested)
     elif t is unicode:
@@ -350,16 +370,18 @@
       v = {}
       for _ in range(vlen):
         k = self.decode_from_stream(stream, True)
         v[k] = self.decode_from_stream(stream, True)
       return v
     elif t == BOOL_TYPE:
       return not not stream.read_byte()
-
-    return self.fallback_coder_impl.decode_from_stream(stream, nested)
+    elif t == UNKNOWN_TYPE:
+      return self.fallback_coder_impl.decode_from_stream(stream, nested)
+    else:
+      raise ValueError('Unknown type tag %x' % t)
 
 
 class BytesCoderImpl(CoderImpl):
   """For internal use only; no backwards-compatibility guarantees.
 
   A coder for bytes/str objects."""
```

## Comparing `apache-beam-2.8.0/apache_beam/coders/fast_coders_test.py` & `apache-beam-2.9.0/apache_beam/coders/fast_coders_test.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/coders/coders.py` & `apache-beam-2.9.0/apache_beam/coders/coders.py`

 * *Files 0% similar despite different names*

```diff
@@ -214,14 +214,18 @@
 
   # pylint: disable=protected-access
   def __eq__(self, other):
     return (self.__class__ == other.__class__
             and self._dict_without_impl() == other._dict_without_impl())
   # pylint: enable=protected-access
 
+  def __ne__(self, other):
+    # TODO(BEAM-5949): Needed for Python 2 compatibility.
+    return not self == other
+
   def __hash__(self):
     return hash(type(self))
 
   _known_urns = {}
 
   @classmethod
   def register_urn(cls, urn, parameter_type, fn=None):
```

## Comparing `apache-beam-2.8.0/apache_beam/coders/observable.py` & `apache-beam-2.9.0/apache_beam/coders/observable.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/transforms/window_test.py` & `apache-beam-2.9.0/apache_beam/transforms/window_test.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/transforms/userstate.py` & `apache-beam-2.9.0/apache_beam/transforms/userstate.py`

 * *Files 2% similar despite different names*

```diff
@@ -23,14 +23,15 @@
 from __future__ import absolute_import
 
 import itertools
 import types
 from builtins import object
 
 from apache_beam.coders import Coder
+from apache_beam.coders import coders
 from apache_beam.portability.api import beam_runner_api_pb2
 from apache_beam.transforms.timeutil import TimeDomain
 
 
 class StateSpec(object):
   """Specification for a user DoFn state cell."""
 
@@ -91,15 +92,17 @@
     self._attached_callback = None
 
   def __repr__(self):
     return '%s(%s)' % (self.__class__.__name__, self.name)
 
   def to_runner_api(self, context):
     return beam_runner_api_pb2.TimerSpec(
-        time_domain=TimeDomain.to_runner_api(self.time_domain))
+        time_domain=TimeDomain.to_runner_api(self.time_domain),
+        timer_coder_id=context.coders.get_id(
+            coders._TimerCoder(coders.SingletonCoder(None))))
 
 
 def on_timer(timer_spec):
   """Decorator for timer firing DoFn method.
 
   This decorator allows a user to specify an on_timer processing method
   in a stateful DoFn.  Sample usage::
```

## Comparing `apache-beam-2.8.0/apache_beam/transforms/create_source.py` & `apache-beam-2.9.0/apache_beam/transforms/create_source.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/transforms/userstate_test.py` & `apache-beam-2.9.0/apache_beam/transforms/userstate_test.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/transforms/window.py` & `apache-beam-2.9.0/apache_beam/transforms/window.py`

 * *Files 1% similar despite different names*

```diff
@@ -62,15 +62,14 @@
 from apache_beam.portability import common_urns
 from apache_beam.portability import python_urns
 from apache_beam.portability.api import beam_runner_api_pb2
 from apache_beam.portability.api import standard_window_fns_pb2
 from apache_beam.transforms import timeutil
 from apache_beam.utils import proto_utils
 from apache_beam.utils import urns
-from apache_beam.utils.timestamp import MAX_TIMESTAMP
 from apache_beam.utils.timestamp import MIN_TIMESTAMP
 from apache_beam.utils.timestamp import Duration
 from apache_beam.utils.timestamp import Timestamp
 from apache_beam.utils.windowed_value import WindowedValue
 
 __all__ = [
     'TimestampCombiner',
@@ -291,27 +290,22 @@
       return self.value < other.value
     return self.timestamp < other.timestamp
 
 
 class GlobalWindow(BoundedWindow):
   """The default window into which all data is placed (via GlobalWindows)."""
   _instance = None
-  # The maximum timestamp for global windows is MAX_TIMESTAMP - 1 day.
-  # This is due to timers triggering when the watermark passes the trigger
-  # time, which is only possible for timestamps < MAX_TIMESTAMP.
-  # See also GlobalWindow in the Java SDK.
-  _END_OF_GLOBAL_WINDOW = MAX_TIMESTAMP - (24 * 60 * 60)
 
   def __new__(cls):
     if cls._instance is None:
       cls._instance = super(GlobalWindow, cls).__new__(cls)
     return cls._instance
 
   def __init__(self):
-    super(GlobalWindow, self).__init__(GlobalWindow._END_OF_GLOBAL_WINDOW)
+    super(GlobalWindow, self).__init__(GlobalWindow._getTimestampFromProto())
     self.start = MIN_TIMESTAMP
 
   def __repr__(self):
     return 'GlobalWindow'
 
   def __hash__(self):
     return hash(type(self))
@@ -319,14 +313,20 @@
   def __eq__(self, other):
     # Global windows are always and only equal to each other.
     return self is other or type(self) is type(other)
 
   def __ne__(self, other):
     return not self == other
 
+  @staticmethod
+  def _getTimestampFromProto():
+    ts_millis = int(
+        common_urns.constants.GLOBAL_WINDOW_MAX_TIMESTAMP_MILLIS.constant)
+    return Timestamp(micros=ts_millis*1000)
+
 
 class NonMergingWindowFn(WindowFn):
 
   def is_merging(self):
     return False
 
   def merge(self, merge_context):
```

## Comparing `apache-beam-2.8.0/apache_beam/transforms/cy_dataflow_distribution_counter.pyx` & `apache-beam-2.9.0/apache_beam/transforms/cy_dataflow_distribution_counter.pyx`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/transforms/timeutil.py` & `apache-beam-2.9.0/apache_beam/transforms/timeutil.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/transforms/display_test.py` & `apache-beam-2.9.0/apache_beam/transforms/display_test.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/transforms/__init__.py` & `apache-beam-2.9.0/apache_beam/transforms/__init__.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/transforms/cy_dataflow_distribution_counter.pxd` & `apache-beam-2.9.0/apache_beam/transforms/cy_dataflow_distribution_counter.pxd`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/transforms/combiners.py` & `apache-beam-2.9.0/apache_beam/transforms/combiners.py`

 * *Files 12% similar despite different names*

```diff
@@ -16,29 +16,32 @@
 #
 
 """A library of basic combiner PTransform subclasses."""
 
 from __future__ import absolute_import
 from __future__ import division
 
+import heapq
 import operator
 import random
 from builtins import object
 from builtins import zip
 from functools import cmp_to_key
 
 from past.builtins import long
 
 from apache_beam.transforms import core
 from apache_beam.transforms import cy_combiners
 from apache_beam.transforms import ptransform
+from apache_beam.transforms import window
 from apache_beam.transforms.display import DisplayDataItem
 from apache_beam.typehints import KV
 from apache_beam.typehints import Any
 from apache_beam.typehints import Dict
+from apache_beam.typehints import Iterable
 from apache_beam.typehints import List
 from apache_beam.typehints import Tuple
 from apache_beam.typehints import TypeVariable
 from apache_beam.typehints import Union
 from apache_beam.typehints import with_input_types
 from apache_beam.typehints import with_output_types
 
@@ -152,76 +155,124 @@
     return accumulator
 
 
 class Top(object):
   """Combiners for obtaining extremal elements."""
   # pylint: disable=no-self-argument
 
-  @staticmethod
-  @ptransform.ptransform_fn
-  def Of(pcoll, n, compare=None, *args, **kwargs):
+  class Of(ptransform.PTransform):
     """Obtain a list of the compare-most N elements in a PCollection.
 
     This transform will retrieve the n greatest elements in the PCollection
     to which it is applied, where "greatest" is determined by the comparator
     function supplied as the compare argument.
-
-    compare should be an implementation of "a < b" taking at least two arguments
-    (a and b). Additional arguments and side inputs specified in the apply call
-    become additional arguments to the comparator.  Defaults to the natural
-    ordering of the elements.
-
-    The arguments 'key' and 'reverse' may instead be passed as keyword
-    arguments, and have the same meaning as for Python's sort functions.
-
-    Args:
-      pcoll: PCollection to process.
-      n: number of elements to extract from pcoll.
-      compare: as described above.
-      *args: as described above.
-      **kwargs: as described above.
     """
-    key = kwargs.pop('key', None)
-    reverse = kwargs.pop('reverse', False)
-    return pcoll | core.CombineGlobally(
-        TopCombineFn(n, compare, key, reverse), *args, **kwargs)
 
-  @staticmethod
-  @ptransform.ptransform_fn
-  def PerKey(pcoll, n, compare=None, *args, **kwargs):
+    def __init__(self, n, compare=None, *args, **kwargs):
+      """Initializer.
+
+      compare should be an implementation of "a < b" taking at least two
+      arguments (a and b). Additional arguments and side inputs specified in
+      the apply call become additional arguments to the comparator. Defaults to
+      the natural ordering of the elements.
+      The arguments 'key' and 'reverse' may instead be passed as keyword
+      arguments, and have the same meaning as for Python's sort functions.
+
+      Args:
+        pcoll: PCollection to process.
+        n: number of elements to extract from pcoll.
+        compare: as described above.
+        *args: as described above.
+        **kwargs: as described above.
+      """
+      self._n = n
+      self._compare = compare
+      self._key = kwargs.pop('key', None)
+      self._reverse = kwargs.pop('reverse', False)
+      self._args = args
+      self._kwargs = kwargs
+
+    def default_label(self):
+      return 'Top(%d)' % self._n
+
+    def expand(self, pcoll):
+      compare = self._compare
+      if (not self._args and not self._kwargs and
+          not self._key and pcoll.windowing.is_default()):
+        if self._reverse:
+          if compare is None or compare is operator.lt:
+            compare = operator.gt
+          else:
+            original_compare = compare
+            compare = lambda a, b: original_compare(b, a)
+        # This is a more efficient global algorithm.
+        top_per_bundle = pcoll | core.ParDo(_TopPerBundle(self._n, compare))
+        # If pcoll is empty, we can't guerentee that top_per_bundle
+        # won't be empty, so inject at least one empty accumulator
+        # so that downstream is guerenteed to produce non-empty output.
+        empty_bundle = pcoll.pipeline | core.Create([(None, [])])
+        return (
+            (top_per_bundle, empty_bundle) | core.Flatten()
+            | core.GroupByKey()
+            | core.ParDo(_MergeTopPerBundle(self._n, compare)))
+      else:
+        return pcoll | core.CombineGlobally(
+            TopCombineFn(self._n, compare, self._key, self._reverse),
+            *self._args, **self._kwargs)
+
+  class PerKey(ptransform.PTransform):
     """Identifies the compare-most N elements associated with each key.
 
     This transform will produce a PCollection mapping unique keys in the input
     PCollection to the n greatest elements with which they are associated, where
     "greatest" is determined by the comparator function supplied as the compare
-    argument.
-
-    compare should be an implementation of "a < b" taking at least two arguments
-    (a and b). Additional arguments and side inputs specified in the apply call
-    become additional arguments to the comparator.  Defaults to the natural
-    ordering of the elements.
-
-    The arguments 'key' and 'reverse' may instead be passed as keyword
-    arguments, and have the same meaning as for Python's sort functions.
-
-    Args:
-      pcoll: PCollection to process.
-      n: number of elements to extract from pcoll.
-      compare: as described above.
-      *args: as described above.
-      **kwargs: as described above.
-
-    Raises:
-      TypeCheckError: If the output type of the input PCollection is not
-        compatible with KV[A, B].
+    argument in the initializer.
     """
-    key = kwargs.pop('key', None)
-    reverse = kwargs.pop('reverse', False)
-    return pcoll | core.CombinePerKey(
-        TopCombineFn(n, compare, key, reverse), *args, **kwargs)
+    def __init__(self, n, compare=None, *args, **kwargs):
+      """Initializer.
+
+      compare should be an implementation of "a < b" taking at least two
+      arguments (a and b). Additional arguments and side inputs specified in
+      the apply call become additional arguments to the comparator.  Defaults to
+      the natural ordering of the elements.
+
+      The arguments 'key' and 'reverse' may instead be passed as keyword
+      arguments, and have the same meaning as for Python's sort functions.
+
+      Args:
+        n: number of elements to extract from input.
+        compare: as described above.
+        *args: as described above.
+        **kwargs: as described above.
+      """
+      self._n = n
+      self._compare = compare
+      self._key = kwargs.pop('key', None)
+      self._reverse = kwargs.pop('reverse', False)
+      self._args = args
+      self._kwargs = kwargs
+
+    def default_label(self):
+      return 'TopPerKey(%d)' % self._n
+
+    def expand(self, pcoll):
+      """Expands the transform.
+
+      Raises TypeCheckError: If the output type of the input PCollection is not
+      compatible with KV[A, B].
+
+      Args:
+        pcoll: PCollection to process
+
+      Returns:
+        the PCollection containing the result.
+      """
+      return pcoll | core.CombinePerKey(
+          TopCombineFn(self._n, self._compare, self._key, self._reverse),
+          *self._args, **self._kwargs)
 
   @staticmethod
   @ptransform.ptransform_fn
   def Largest(pcoll, n):
     """Obtain a list of the greatest N elements in a PCollection."""
     return pcoll | Top.Of(n)
 
@@ -240,14 +291,100 @@
   @staticmethod
   @ptransform.ptransform_fn
   def SmallestPerKey(pcoll, n, reverse=True):
     """Identifies the N least elements associated with each key."""
     return pcoll | Top.PerKey(n, reverse=True)
 
 
+class _ComparableValue(object):
+
+  __slots__ = ('value', 'less_than')
+
+  def __init__(self, value, less_than):
+    self.value = value
+    self.less_than = less_than
+
+  def __lt__(self, other):
+    return self.less_than(self.value, other.value)
+
+  def __repr__(self):
+    return "_ComparableValue[%s]" % self.value
+
+
+@with_input_types(T)
+@with_output_types(KV[None, List[T]])
+class _TopPerBundle(core.DoFn):
+  def __init__(self, n, less_than):
+    self._n = n
+    self._less_than = None if less_than is operator.le else less_than
+
+  def start_bundle(self):
+    self._heap = []
+
+  def process(self, element):
+    if self._less_than is not None:
+      element = _ComparableValue(element, self._less_than)
+    if len(self._heap) < self._n:
+      heapq.heappush(self._heap, element)
+    else:
+      heapq.heappushpop(self._heap, element)
+
+  def finish_bundle(self):
+    # Though sorting here results in more total work, this allows us to
+    # skip most elements in the reducer.
+    # Essentially, given s map bundles, we are trading about O(sn) compares in
+    # the (single) reducer for O(sn log n) compares across all mappers.
+    self._heap.sort()
+
+    # Unwrap to avoid serialization via pickle.
+    if self._less_than:
+      yield window.GlobalWindows.windowed_value(
+          (None, [wrapper.value for wrapper in self._heap]))
+    else:
+      yield window.GlobalWindows.windowed_value(
+          (None, self._heap))
+
+
+@with_input_types(KV[None, Iterable[List[T]]])
+@with_output_types(List[T])
+class _MergeTopPerBundle(core.DoFn):
+  def __init__(self, n, less_than):
+    self._n = n
+    self._less_than = None if less_than is operator.le else less_than
+
+  def process(self, key_and_bundles):
+    _, bundles = key_and_bundles
+    heap = []
+    for bundle in bundles:
+      if not heap:
+        if self._less_than:
+          heap = [
+              _ComparableValue(element, self._less_than) for element in bundle]
+        else:
+          heap = bundle
+        continue
+      for element in reversed(bundle):
+        if self._less_than is not None:
+          element = _ComparableValue(element, self._less_than)
+        if len(heap) < self._n:
+          heapq.heappush(heap, element)
+        elif element <= heap[0]:
+          # Because _TopPerBundle returns sorted lists, all other elements
+          # will also be smaller.
+          break
+        else:
+          heapq.heappushpop(heap, element)
+
+    heap.sort()
+    if self._less_than:
+      yield [wrapper.value for wrapper in reversed(heap)]
+    else:
+      yield heap[::-1]
+
+
 @with_input_types(T)
 @with_output_types(List[T])
 class TopCombineFn(core.CombineFn):
   """CombineFn doing the combining for all of the Top transforms.
 
   This CombineFn uses a key or comparison operator to rank the elements.
```

## Comparing `apache-beam-2.8.0/apache_beam/transforms/sideinputs.py` & `apache-beam-2.9.0/apache_beam/transforms/sideinputs.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/transforms/util_test.py` & `apache-beam-2.9.0/apache_beam/transforms/util_test.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/transforms/dataflow_distribution_counter_test.py` & `apache-beam-2.9.0/apache_beam/transforms/dataflow_distribution_counter_test.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/transforms/display.py` & `apache-beam-2.9.0/apache_beam/transforms/display.py`

 * *Files 1% similar despite different names*

```diff
@@ -276,14 +276,15 @@
 
   def __eq__(self, other):
     if isinstance(other, self.__class__):
       return self._get_dict() == other._get_dict()
     return False
 
   def __ne__(self, other):
+    # TODO(BEAM-5949): Needed for Python 2 compatibility.
     return not self == other
 
   def __hash__(self):
     return hash(tuple(sorted(self._get_dict().items())))
 
   @classmethod
   def _format_value(cls, value, type_):
```

## Comparing `apache-beam-2.8.0/apache_beam/transforms/cy_combiners.pxd` & `apache-beam-2.9.0/apache_beam/transforms/cy_combiners.pxd`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/transforms/combiners_test.py` & `apache-beam-2.9.0/apache_beam/transforms/combiners_test.py`

 * *Files 1% similar despite different names*

```diff
@@ -110,14 +110,30 @@
                 label='key:top')
     assert_that(result_key_bot, equal_to([('a', [0, 1, 1, 1])]),
                 label='key:bot')
     assert_that(result_key_cmp, equal_to([('a', [9, 6, 6, 5, 3, 2])]),
                 label='key:cmp')
     pipeline.run()
 
+  def test_empty_global_top(self):
+    with TestPipeline() as p:
+      assert_that(p | beam.Create([]) | combine.Top.Largest(10),
+                  equal_to([[]]))
+
+  def test_sharded_top(self):
+    elements = list(range(100))
+    random.shuffle(elements)
+
+    pipeline = TestPipeline()
+    shards = [pipeline | 'Shard%s' % shard >> beam.Create(elements[shard::7])
+              for shard in range(7)]
+    assert_that(shards | beam.Flatten() | combine.Top.Largest(10),
+                equal_to([[99, 98, 97, 96, 95, 94, 93, 92, 91, 90]]))
+    pipeline.run()
+
   def test_top_key(self):
     self.assertEqual(
         ['aa', 'bbb', 'c', 'dddd'] | combine.Top.Of(3, key=len),
         [['dddd', 'bbb', 'aa']])
     self.assertEqual(
         ['aa', 'bbb', 'c', 'dddd'] | combine.Top.Of(3, key=len, reverse=True),
         [['c', 'aa', 'bbb']])
```

## Comparing `apache-beam-2.8.0/apache_beam/transforms/create_test.py` & `apache-beam-2.9.0/apache_beam/transforms/create_test.py`

 * *Files 2% similar despite different names*

```diff
@@ -33,14 +33,16 @@
 
 class CreateTest(unittest.TestCase):
   def setUp(self):
     self.coder = FastPrimitivesCoder()
 
   def test_create_transform(self):
     with TestPipeline() as p:
+      assert_that(p | 'Empty' >> Create([]), equal_to([]), label='empty')
+      assert_that(p | 'One' >> Create([None]), equal_to([None]), label='one')
       assert_that(p | Create(list(range(10))), equal_to(list(range(10))))
 
   def test_create_source_read(self):
     self.check_read([], self.coder)
     self.check_read([1], self.coder)
     # multiple values.
     self.check_read(list(range(10)), self.coder)
```

## Comparing `apache-beam-2.8.0/apache_beam/transforms/cy_combiners.py` & `apache-beam-2.9.0/apache_beam/transforms/cy_combiners.py`

 * *Files 0% similar despite different names*

```diff
@@ -54,14 +54,18 @@
   def extract_output(accumulator):
     return accumulator.extract_output()
 
   def __eq__(self, other):
     return (isinstance(other, AccumulatorCombineFn)
             and self._accumulator_type is other._accumulator_type)
 
+  def __ne__(self, other):
+    # TODO(BEAM-5949): Needed for Python 2 compatibility.
+    return not self == other
+
   def __hash__(self):
     return hash(self._accumulator_type)
 
 
 _63 = 63  # Avoid large literals in C source code.
 globals()['INT64_MAX'] = 2**_63 - 1
 globals()['INT64_MIN'] = -2**_63
```

## Comparing `apache-beam-2.8.0/apache_beam/transforms/ptransform.py` & `apache-beam-2.9.0/apache_beam/transforms/ptransform.py`

 * *Files 0% similar despite different names*

```diff
@@ -33,15 +33,14 @@
 class and wrapper class that allows lambda functions to be used as
 FlatMap processing functions.
 """
 
 from __future__ import absolute_import
 
 import copy
-import inspect
 import itertools
 import operator
 import os
 import sys
 import threading
 from builtins import hex
 from builtins import object
@@ -57,14 +56,15 @@
 from apache_beam.portability import python_urns
 from apache_beam.transforms.display import DisplayDataItem
 from apache_beam.transforms.display import HasDisplayData
 from apache_beam.typehints import typehints
 from apache_beam.typehints.decorators import TypeCheckError
 from apache_beam.typehints.decorators import WithTypeHints
 from apache_beam.typehints.decorators import getcallargs_forhints
+from apache_beam.typehints.decorators import getfullargspec
 from apache_beam.typehints.trivial_inference import instance_to_type
 from apache_beam.typehints.typehints import validate_composite_type_param
 from apache_beam.utils import proto_utils
 
 __all__ = [
     'PTransform',
     'ptransform_fn',
@@ -567,14 +567,17 @@
     return (python_urns.GENERIC_COMPOSITE_TRANSFORM,
             getattr(self, '_fn_api_payload', str(self)))
 
   def to_runner_api_pickled(self, unused_context):
     return (python_urns.PICKLED_TRANSFORM,
             pickler.dumps(self))
 
+  def runner_api_requires_keyed_input(self):
+    return False
+
 
 @PTransform.register_urn(python_urns.GENERIC_COMPOSITE_TRANSFORM, None)
 def _create_transform(payload, unused_context):
   empty_transform = PTransform()
   empty_transform._fn_api_payload = payload
   return empty_transform
 
@@ -746,16 +749,18 @@
   def expand(self, pcoll):
     # Since the PTransform will be implemented entirely as a function
     # (once called), we need to pass through any type-hinting information that
     # may have been annotated via the .with_input_types() and
     # .with_output_types() methods.
     kwargs = dict(self._kwargs)
     args = tuple(self._args)
+
+    # TODO(BEAM-5878) Support keyword-only arguments.
     try:
-      if 'type_hints' in inspect.getargspec(self._fn).args:
+      if 'type_hints' in getfullargspec(self._fn).args:
         args = (self.get_type_hints(),) + args
     except TypeError:
       # Might not be a function.
       pass
     return self._fn(pcoll, *args, **kwargs)
 
   def default_label(self):
```

## Comparing `apache-beam-2.8.0/apache_beam/transforms/core.py` & `apache-beam-2.9.0/apache_beam/transforms/core.py`

 * *Files 2% similar despite different names*

```diff
@@ -16,15 +16,14 @@
 #
 
 """Core PTransform subclasses, such as FlatMap, GroupByKey, and Map."""
 
 from __future__ import absolute_import
 
 import copy
-import inspect
 import logging
 import random
 import re
 import types
 from builtins import map
 from builtins import object
 from builtins import range
@@ -34,14 +33,15 @@
 
 from apache_beam import coders
 from apache_beam import pvalue
 from apache_beam import typehints
 from apache_beam.coders import typecoders
 from apache_beam.internal import pickler
 from apache_beam.internal import util
+from apache_beam.options.pipeline_options import DebugOptions
 from apache_beam.options.pipeline_options import TypeOptions
 from apache_beam.portability import common_urns
 from apache_beam.portability import python_urns
 from apache_beam.portability.api import beam_runner_api_pb2
 from apache_beam.transforms import ptransform
 from apache_beam.transforms import userstate
 from apache_beam.transforms.display import DisplayDataItem
@@ -59,14 +59,15 @@
 from apache_beam.typehints import Any
 from apache_beam.typehints import Iterable
 from apache_beam.typehints import Union
 from apache_beam.typehints import trivial_inference
 from apache_beam.typehints.decorators import TypeCheckError
 from apache_beam.typehints.decorators import WithTypeHints
 from apache_beam.typehints.decorators import get_type_hints
+from apache_beam.typehints.decorators import getfullargspec
 from apache_beam.typehints.trivial_inference import element_type
 from apache_beam.typehints.typehints import is_consistent_with
 from apache_beam.utils import urns
 
 __all__ = [
     'DoFn',
     'CombineFn',
@@ -270,35 +271,39 @@
     """
     return coders.registry.get_coder(object)
 
 
 def get_function_arguments(obj, func):
   """Return the function arguments based on the name provided. If they have
   a _inspect_function attached to the class then use that otherwise default
-  to the python inspect library.
+  to the modified version of python inspect library.
   """
   func_name = '_inspect_%s' % func
   if hasattr(obj, func_name):
     f = getattr(obj, func_name)
     return f()
   f = getattr(obj, func)
-  return inspect.getargspec(f)
+  return getfullargspec(f)
 
 
 class _DoFnParam(object):
   """DoFn parameter."""
 
   def __init__(self, param_id):
     self.param_id = param_id
 
   def __eq__(self, other):
     if type(self) == type(other):
       return self.param_id == other.param_id
     return False
 
+  def __ne__(self, other):
+    # TODO(BEAM-5949): Needed for Python 2 compatibility.
+    return not self == other
+
   def __hash__(self):
     return hash(self.param_id)
 
   def __repr__(self):
     return self.param_id
 
 
@@ -437,20 +442,29 @@
     return True
 
   urns.RunnerApiFn.register_pickle_urn(python_urns.PICKLED_DOFN)
 
 
 def _fn_takes_side_inputs(fn):
   try:
-    argspec = inspect.getargspec(fn)
+    argspec = getfullargspec(fn)
   except TypeError:
     # We can't tell; maybe it does.
     return True
   is_bound = isinstance(fn, types.MethodType) and fn.__self__ is not None
-  return len(argspec.args) > 1 + is_bound or argspec.varargs or argspec.keywords
+
+  try:
+    varkw = argspec.varkw
+    kwonlyargs = argspec.kwonlyargs
+  except AttributeError:  # Python 2
+    varkw = argspec.keywords
+    kwonlyargs = []
+
+  return (len(argspec.args) + len(kwonlyargs) > 1 + is_bound or
+          argspec.varargs or varkw)
 
 
 class CallableWrapperDoFn(DoFn):
   """For internal use only; no backwards-compatibility guarantees.
 
   A DoFn (function) object wrapping a callable object.
 
@@ -510,15 +524,15 @@
     return self._strip_output_annotations(
         trivial_inference.infer_return_type(self._fn, [input_type]))
 
   def _process_argspec_fn(self):
     return getattr(self._fn, '_argspec_fn', self._fn)
 
   def _inspect_process(self):
-    return inspect.getargspec(self._process_argspec_fn())
+    return getfullargspec(self._process_argspec_fn())
 
 
 class CombineFn(WithTypeHints, HasDisplayData, urns.RunnerApiFn):
   """A function object used by a Combine transform with custom processing.
 
   A CombineFn specifies how multiple values in all or part of a PCollection can
   be merged into a single value---essentially providing the same kind of
@@ -986,14 +1000,17 @@
     indexed_side_inputs = [
         (int(re.match('side([0-9]+)(-.*)?$', tag).group(1)),
          pvalue.AsSideInput.from_runner_api(si, context))
         for tag, si in pardo_payload.side_inputs.items()]
     result.side_inputs = [si for _, si in sorted(indexed_side_inputs)]
     return result
 
+  def runner_api_requires_keyed_input(self):
+    return userstate.is_stateful_dofn(self.fn)
+
 
 class _MultiParDo(PTransform):
 
   def __init__(self, do_transform, tags, main_tag):
     super(_MultiParDo, self).__init__(do_transform.label)
     self._do_transform = do_transform
     self._tags = tags
@@ -1370,14 +1387,17 @@
   @PTransform.register_urn(
       common_urns.composites.COMBINE_PER_KEY.urn,
       beam_runner_api_pb2.CombinePayload)
   def from_runner_api_parameter(combine_payload, context):
     return CombinePerKey(
         CombineFn.from_runner_api(combine_payload.combine_fn, context))
 
+  def runner_api_requires_keyed_input(self):
+    return True
+
 
 # TODO(robertwb): Rename to CombineGroupedValues?
 class CombineValues(PTransformWithSideInputs):
 
   def make_fn(self, fn):
     return fn if isinstance(fn, CombineFn) else CombineFn.from_callable(fn)
 
@@ -1601,21 +1621,28 @@
     else:
       # The input_type is None, run the default
       return (pcoll
               | 'ReifyWindows' >> ParDo(self.ReifyWindows())
               | 'GroupByKey' >> _GroupByKeyOnly()
               | 'GroupByWindow' >> _GroupAlsoByWindow(pcoll.windowing))
 
+  def infer_output_type(self, input_type):
+    key_type, value_type = trivial_inference.key_value_types(input_type)
+    return KV[key_type, Iterable[value_type]]
+
   def to_runner_api_parameter(self, unused_context):
     return common_urns.primitives.GROUP_BY_KEY.urn, None
 
   @PTransform.register_urn(common_urns.primitives.GROUP_BY_KEY.urn, None)
   def from_runner_api_parameter(unused_payload, unused_context):
     return GroupByKey()
 
+  def runner_api_requires_keyed_input(self):
+    return True
+
 
 @typehints.with_input_types(typehints.KV[K, V])
 @typehints.with_output_types(typehints.KV[K, typehints.Iterable[V]])
 class _GroupByKeyOnly(PTransform):
   """A group by key transform, ignoring windows."""
   def infer_output_type(self, input_type):
     key_type, value_type = trivial_inference.key_value_types(input_type)
@@ -1746,14 +1773,18 @@
       return (
           self.windowfn == other.windowfn
           and self.triggerfn == other.triggerfn
           and self.accumulation_mode == other.accumulation_mode
           and self.timestamp_combiner == other.timestamp_combiner)
     return False
 
+  def __ne__(self, other):
+    # TODO(BEAM-5949): Needed for Python 2 compatibility.
+    return not self == other
+
   def __hash__(self):
     return hash((self.windowfn, self.accumulation_mode,
                  self.timestamp_combiner))
 
   def is_default(self):
     return self._is_default
 
@@ -1938,50 +1969,75 @@
 PTransform.register_urn(
     common_urns.primitives.FLATTEN.urn, None, Flatten.from_runner_api_parameter)
 
 
 class Create(PTransform):
   """A transform that creates a PCollection from an iterable."""
 
-  def __init__(self, value):
+  def __init__(self, values):
     """Initializes a Create transform.
 
     Args:
-      value: An object of values for the PCollection
+      values: An object of values for the PCollection
     """
     super(Create, self).__init__()
-    if isinstance(value, (unicode, str, bytes)):
+    if isinstance(values, (unicode, str, bytes)):
       raise TypeError('PTransform Create: Refusing to treat string as '
-                      'an iterable. (string=%r)' % value)
-    elif isinstance(value, dict):
-      value = value.items()
-    self.value = tuple(value)
+                      'an iterable. (string=%r)' % values)
+    elif isinstance(values, dict):
+      values = values.items()
+    self.values = tuple(values)
 
   def to_runner_api_parameter(self, context):
     # Required as this is identified by type in PTransformOverrides.
     # TODO(BEAM-3812): Use an actual URN here.
     return self.to_runner_api_pickled(context)
 
   def infer_output_type(self, unused_input_type):
-    if not self.value:
+    if not self.values:
       return Any
-    return Union[[trivial_inference.instance_to_type(v) for v in self.value]]
+    return Union[[trivial_inference.instance_to_type(v) for v in self.values]]
 
   def get_output_type(self):
     return (self.get_type_hints().simple_output_type(self.label) or
             self.infer_output_type(None))
 
   def expand(self, pbegin):
-    from apache_beam.io import iobase
     assert isinstance(pbegin, pvalue.PBegin)
-    self.pipeline = pbegin.pipeline
-    coder = typecoders.registry.get_coder(self.get_output_type())
-    source = self._create_source_from_iterable(self.value, coder)
-    return (pbegin.pipeline
-            | iobase.Read(source).with_output_types(self.get_output_type()))
+    # Must guard against this as some legacy runners don't implement impulse.
+    debug_options = pbegin.pipeline._options.view_as(DebugOptions)
+    fn_api = (debug_options.experiments
+              and 'beam_fn_api' in debug_options.experiments)
+    if fn_api:
+      coder = typecoders.registry.get_coder(self.get_output_type())
+      serialized_values = [coder.encode(v) for v in self.values]
+      # Avoid the "redistributing" reshuffle for 0 and 1 element Creates.
+      # These special cases are often used in building up more complex
+      # transforms (e.g. Write).
+
+      class MaybeReshuffle(PTransform):
+        def expand(self, pcoll):
+          if len(serialized_values) > 1:
+            from apache_beam.transforms.util import Reshuffle
+            return pcoll | Reshuffle()
+          else:
+            return pcoll
+      return (
+          pbegin
+          | Impulse()
+          | FlatMap(lambda _: serialized_values)
+          | MaybeReshuffle()
+          | Map(coder.decode).with_output_types(self.get_output_type()))
+    else:
+      self.pipeline = pbegin.pipeline
+      from apache_beam.io import iobase
+      coder = typecoders.registry.get_coder(self.get_output_type())
+      source = self._create_source_from_iterable(self.values, coder)
+      return (pbegin.pipeline
+              | iobase.Read(source).with_output_types(self.get_output_type()))
 
   def get_windowing(self, unused_inputs):
     return Windowing(GlobalWindows())
 
   @staticmethod
   def _create_source_from_iterable(values, coder):
     return Create._create_source(list(map(coder.encode, values)), coder)
```

## Comparing `apache-beam-2.8.0/apache_beam/transforms/trigger_test.py` & `apache-beam-2.9.0/apache_beam/transforms/trigger_test.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/transforms/write_ptransform_test.py` & `apache-beam-2.9.0/apache_beam/transforms/write_ptransform_test.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/transforms/trigger.py` & `apache-beam-2.9.0/apache_beam/transforms/trigger.py`

 * *Files 0% similar despite different names*

```diff
@@ -229,14 +229,18 @@
         'repeat': Repeatedly,
     }[proto.WhichOneof('trigger')].from_runner_api(proto, context)
 
   @abstractmethod
   def to_runner_api(self, unused_context):
     pass
 
+  def __ne__(self, other):
+    # TODO(BEAM-5949): Needed for Python 2 compatibility.
+    return not self == other
+
 
 class DefaultTrigger(TriggerFn):
   """Semantically Repeatedly(AfterWatermark()), but more optimized."""
 
   def __init__(self):
     pass
 
@@ -989,14 +993,15 @@
     else:
       return NotImplemented
 
   def __hash__(self):
     return hash(tuple(self))
 
   def __ne__(self, other):
+    # TODO(BEAM-5949): Needed for Python 2 compatibility.
     return not self == other
 
 
 class DiscardingGlobalTriggerDriver(TriggerDriver):
   """Groups all received values together.
   """
   GLOBAL_WINDOW_TUPLE = (GlobalWindow(),)
```

## Comparing `apache-beam-2.8.0/apache_beam/transforms/ptransform_test.py` & `apache-beam-2.9.0/apache_beam/transforms/ptransform_test.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/transforms/util.py` & `apache-beam-2.9.0/apache_beam/transforms/util.py`

 * *Files 0% similar despite different names*

```diff
@@ -219,15 +219,15 @@
           min_batch_size, max_batch_size))
     if target_batch_overhead and not 0 < target_batch_overhead <= 1:
       raise ValueError("target_batch_overhead (%s) must be between 0 and 1" % (
           target_batch_overhead))
     if target_batch_duration_secs and target_batch_duration_secs <= 0:
       raise ValueError("target_batch_duration_secs (%s) must be positive" % (
           target_batch_duration_secs))
-    if max(0, target_batch_overhead, target_batch_duration_secs) == 0:
+    if not (target_batch_overhead or target_batch_duration_secs):
       raise ValueError("At least one of target_batch_overhead or "
                        "target_batch_duration_secs must be positive.")
     self._min_batch_size = min_batch_size
     self._max_batch_size = max_batch_size
     self._target_batch_overhead = target_batch_overhead
     self._target_batch_duration_secs = target_batch_duration_secs
     self._variance = variance
```

## Comparing `apache-beam-2.8.0/apache_beam/transforms/sideinputs_test.py` & `apache-beam-2.9.0/apache_beam/transforms/sideinputs_test.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/transforms/py_dataflow_distribution_counter.py` & `apache-beam-2.9.0/apache_beam/transforms/py_dataflow_distribution_counter.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/utils/windowed_value_test.py` & `apache-beam-2.9.0/apache_beam/utils/windowed_value_test.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/utils/retry.py` & `apache-beam-2.9.0/apache_beam/utils/retry.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/utils/counters.py` & `apache-beam-2.9.0/apache_beam/utils/counters.py`

 * *Files 6% similar despite different names*

```diff
@@ -151,14 +151,17 @@
     self.combine_fn = combine_fn
     self.accumulator = combine_fn.create_accumulator()
     self._add_input = self.combine_fn.add_input
 
   def update(self, value):
     self.accumulator = self._add_input(self.accumulator, value)
 
+  def reset(self, value):
+    self.accumulator = self.combine_fn.create_accumulator()
+
   def value(self):
     return self.combine_fn.extract_output(self.accumulator)
 
   def __str__(self):
     return '<%s>' % self._str_internal()
 
   def __repr__(self):
@@ -171,19 +174,23 @@
 
 class AccumulatorCombineFnCounter(Counter):
   """Counter optimized for a mutating accumulator that holds all the logic."""
 
   def __init__(self, name, combine_fn):
     assert isinstance(combine_fn, cy_combiners.AccumulatorCombineFn)
     super(AccumulatorCombineFnCounter, self).__init__(name, combine_fn)
-    self._fast_add_input = self.accumulator.add_input
+    self.reset()
 
   def update(self, value):
     self._fast_add_input(value)
 
+  def reset(self):
+    self.accumulator = self.combine_fn.create_accumulator()
+    self._fast_add_input = self.accumulator.add_input
+
 
 class CounterFactory(object):
   """Keeps track of unique counters."""
 
   def __init__(self):
     self.counters = {}
 
@@ -211,14 +218,20 @@
         if isinstance(combine_fn, cy_combiners.AccumulatorCombineFn):
           counter = AccumulatorCombineFnCounter(name, combine_fn)
         else:
           counter = Counter(name, combine_fn)
         self.counters[name] = counter
       return counter
 
+  def reset(self):
+    # Counters are cached in state sampler states.
+    with self._lock:
+      for counter in self.counters.values():
+        counter.reset()
+
   def get_counters(self):
     """Returns the current set of counters.
 
     Returns:
       An iterable that contains the current set of counters. To make sure that
       multiple threads can iterate over the set of counters, we return a new
       iterable here. Note that the actual set of counters may get modified after
```

## Comparing `apache-beam-2.8.0/apache_beam/utils/counters_test.py` & `apache-beam-2.9.0/apache_beam/utils/counters_test.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/utils/plugin.py` & `apache-beam-2.9.0/apache_beam/utils/plugin.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/utils/retry_test.py` & `apache-beam-2.9.0/apache_beam/utils/retry_test.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/utils/__init__.py` & `apache-beam-2.9.0/apache_beam/utils/__init__.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/utils/windowed_value.py` & `apache-beam-2.9.0/apache_beam/utils/windowed_value.py`

 * *Files 2% similar despite different names*

```diff
@@ -109,14 +109,18 @@
       return True
     return (self.is_first == other.is_first and
             self.is_last == other.is_last and
             self.timing == other.timing and
             self.index == other.index and
             self.nonspeculative_index == other.nonspeculative_index)
 
+  def __ne__(self, other):
+    # TODO(BEAM-5949): Needed for Python 2 compatibility.
+    return not self == other
+
   def __hash__(self):
     return hash((self.is_first, self.is_last, self.timing, self.index,
                  self.nonspeculative_index))
 
 
 def _construct_well_known_pane_infos():
   pane_infos = []
@@ -185,14 +189,18 @@
   def __eq__(self, other):
     return (type(self) == type(other)
             and self.timestamp_micros == other.timestamp_micros
             and self.value == other.value
             and self.windows == other.windows
             and self.pane_info == other.pane_info)
 
+  def __ne__(self, other):
+    # TODO(BEAM-5949): Needed for Python 2 compatibility.
+    return not self == other
+
   def __hash__(self):
     return (hash(self.value) +
             3 * self.timestamp_micros +
             7 * hash(self.windows) +
             11 * hash(self.pane_info))
 
   def with_value(self, new_value):
```

## Comparing `apache-beam-2.8.0/apache_beam/utils/processes.py` & `apache-beam-2.9.0/apache_beam/utils/processes.py`

 * *Files 6% similar despite different names*

```diff
@@ -28,14 +28,15 @@
 # On Windows, we need to use shell=True when creating subprocesses for binary
 # paths to be resolved correctly.
 force_shell = platform.system() == 'Windows'
 
 # We mimic the interface of the standard Python subprocess module.
 PIPE = subprocess.PIPE
 STDOUT = subprocess.STDOUT
+CalledProcessError = subprocess.CalledProcessError
 
 
 def call(*args, **kwargs):
   if force_shell:
     kwargs['shell'] = True
   return subprocess.call(*args, **kwargs)
```

## Comparing `apache-beam-2.8.0/apache_beam/utils/profiler.py` & `apache-beam-2.9.0/apache_beam/utils/profiler.py`

 * *Files 10% similar despite different names*

```diff
@@ -23,62 +23,88 @@
 from __future__ import absolute_import
 
 import cProfile  # pylint: disable=bad-python3-import
 import io
 import logging
 import os
 import pstats
+import random
 import tempfile
 import time
 import warnings
 from builtins import object
 from threading import Timer
 
+from apache_beam.io import filesystems
+
 
 class Profile(object):
   """cProfile wrapper context for saving and logging profiler results."""
 
   SORTBY = 'cumulative'
 
   def __init__(self, profile_id, profile_location=None, log_results=False,
-               file_copy_fn=None):
+               file_copy_fn=None, time_prefix='%Y-%m-%d_%H_%M_%S-'):
     self.stats = None
     self.profile_id = str(profile_id)
     self.profile_location = profile_location
     self.log_results = log_results
-    self.file_copy_fn = file_copy_fn
+    self.file_copy_fn = file_copy_fn or self.default_file_copy_fn
+    self.time_prefix = time_prefix
+    self.profile_output = None
 
   def __enter__(self):
     logging.info('Start profiling: %s', self.profile_id)
     self.profile = cProfile.Profile()
     self.profile.enable()
     return self
 
   def __exit__(self, *args):
     self.profile.disable()
     logging.info('Stop profiling: %s', self.profile_id)
 
-    if self.profile_location and self.file_copy_fn:
+    if self.profile_location:
       dump_location = os.path.join(
-          self.profile_location, 'profile',
-          ('%s-%s' % (time.strftime('%Y-%m-%d_%H_%M_%S'), self.profile_id)))
+          self.profile_location,
+          time.strftime(self.time_prefix + self.profile_id))
       fd, filename = tempfile.mkstemp()
-      self.profile.dump_stats(filename)
-      logging.info('Copying profiler data to: [%s]', dump_location)
-      self.file_copy_fn(filename, dump_location)  # pylint: disable=protected-access
-      os.close(fd)
-      os.remove(filename)
+      try:
+        os.close(fd)
+        self.profile.dump_stats(filename)
+        logging.info('Copying profiler data to: [%s]', dump_location)
+        self.file_copy_fn(filename, dump_location)
+      finally:
+        os.remove(filename)
+      self.profile_output = dump_location
 
     if self.log_results:
       s = io.StringIO()
       self.stats = pstats.Stats(
           self.profile, stream=s).sort_stats(Profile.SORTBY)
       self.stats.print_stats()
       logging.info('Profiler data: [%s]', s.getvalue())
 
+  @staticmethod
+  def default_file_copy_fn(src, dest):
+    dest_handle = filesystems.FileSystems.create(dest + '.tmp')
+    try:
+      with open(src, 'rb') as src_handle:
+        dest_handle.write(src_handle.read())
+    finally:
+      dest_handle.close()
+    filesystems.FileSystems.rename([dest + '.tmp'], [dest])
+
+  @staticmethod
+  def factory_from_options(options):
+    if options.profile_cpu:
+      def create_profiler(profile_id, **kwargs):
+        if random.random() < options.profile_sample_rate:
+          return Profile(profile_id, options.profile_location, **kwargs)
+      return create_profiler
+
 
 class MemoryReporter(object):
   """A memory reporter that reports the memory usage and heap profile.
   Usage:::
 
     mr = MemoryReporter(interval_second=30.0)
     mr.start()
```

## Comparing `apache-beam-2.8.0/apache_beam/utils/urns.py` & `apache-beam-2.9.0/apache_beam/utils/urns.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/utils/windowed_value.pxd` & `apache-beam-2.9.0/apache_beam/utils/windowed_value.pxd`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/utils/proto_utils.py` & `apache-beam-2.9.0/apache_beam/utils/proto_utils.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/utils/timestamp.py` & `apache-beam-2.9.0/apache_beam/utils/timestamp.py`

 * *Files 3% similar despite different names*

```diff
@@ -27,14 +27,16 @@
 import functools
 import re
 from builtins import object
 
 import pytz
 from past.builtins import long
 
+from apache_beam.portability import common_urns
+
 
 @functools.total_ordering
 class Timestamp(object):
   """Represents a Unix second timestamp with microsecond granularity.
 
   Can be treated in common timestamp arithmetic operations as a numeric type.
 
@@ -148,14 +150,18 @@
 
   def __eq__(self, other):
     # Allow comparisons between Duration and Timestamp values.
     if not isinstance(other, Duration):
       other = Timestamp.of(other)
     return self.micros == other.micros
 
+  def __ne__(self, other):
+    # TODO(BEAM-5949): Needed for Python 2 compatibility.
+    return not self == other
+
   def __lt__(self, other):
     # Allow comparisons between Duration and Timestamp values.
     if not isinstance(other, Duration):
       other = Timestamp.of(other)
     return self.micros < other.micros
 
   def __hash__(self):
@@ -173,16 +179,18 @@
     return Timestamp(micros=self.micros - other.micros)
 
   def __mod__(self, other):
     other = Duration.of(other)
     return Duration(micros=self.micros % other.micros)
 
 
-MIN_TIMESTAMP = Timestamp(micros=-0x7fffffffffffffff - 1)
-MAX_TIMESTAMP = Timestamp(micros=0x7fffffffffffffff)
+MIN_TIMESTAMP = Timestamp(micros=int(
+    common_urns.constants.MIN_TIMESTAMP_MILLIS.constant)*1000)
+MAX_TIMESTAMP = Timestamp(micros=int(
+    common_urns.constants.MAX_TIMESTAMP_MILLIS.constant)*1000)
 
 
 @functools.total_ordering
 class Duration(object):
   """Represents a second duration with microsecond granularity.
 
   Can be treated in common arithmetic operations as a numeric type.
@@ -233,14 +241,18 @@
 
   def __eq__(self, other):
     # Allow comparisons between Duration and Timestamp values.
     if not isinstance(other, Timestamp):
       other = Duration.of(other)
     return self.micros == other.micros
 
+  def __ne__(self, other):
+    # TODO(BEAM-5949): Needed for Python 2 compatibility.
+    return not self == other
+
   def __lt__(self, other):
     # Allow comparisons between Duration and Timestamp values.
     if not isinstance(other, Timestamp):
       other = Duration.of(other)
     return self.micros < other.micros
 
   def __hash__(self):
```

## Comparing `apache-beam-2.8.0/apache_beam/utils/counters.pxd` & `apache-beam-2.9.0/apache_beam/utils/counters.pxd`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/utils/timestamp_test.py` & `apache-beam-2.9.0/apache_beam/utils/timestamp_test.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/utils/processes_test.py` & `apache-beam-2.9.0/apache_beam/utils/processes_test.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/portability/python_urns.py` & `apache-beam-2.9.0/apache_beam/portability/python_urns.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/portability/common_urns.py` & `apache-beam-2.9.0/apache_beam/portability/common_urns.py`

 * *Files 15% similar despite different names*

```diff
@@ -17,22 +17,26 @@
 
 """ Accessors for URNs of common Beam entities. """
 
 from __future__ import absolute_import
 
 from builtins import object
 
+from apache_beam.portability.api import beam_fn_api_pb2
 from apache_beam.portability.api import beam_runner_api_pb2
 from apache_beam.portability.api import standard_window_fns_pb2
 
 
 class PropertiesFromEnumValue(object):
   def __init__(self, value_descriptor):
     self.urn = (
         value_descriptor.GetOptions().Extensions[beam_runner_api_pb2.beam_urn])
+    self.constant = (
+        value_descriptor.GetOptions().Extensions[
+            beam_runner_api_pb2.beam_constant])
 
 
 class PropertiesFromEnumType(object):
   def __init__(self, enum_type):
     for v in enum_type.DESCRIPTOR.values:
       setattr(self, v.name, PropertiesFromEnumValue(v))
 
@@ -47,14 +51,17 @@
     beam_runner_api_pb2.StandardPTransforms.CombineComponents)
 
 side_inputs = PropertiesFromEnumType(
     beam_runner_api_pb2.StandardSideInputTypes.Enum)
 
 coders = PropertiesFromEnumType(beam_runner_api_pb2.StandardCoders.Enum)
 
+constants = PropertiesFromEnumType(
+    beam_runner_api_pb2.BeamConstants.Constants)
+
 environments = PropertiesFromEnumType(
     beam_runner_api_pb2.StandardEnvironments.Environments)
 
 
 def PropertiesFromPayloadType(payload_type):
   return PropertiesFromEnumType(payload_type.Enum).PROPERTIES
 
@@ -63,7 +70,12 @@
     standard_window_fns_pb2.GlobalWindowsPayload)
 fixed_windows = PropertiesFromPayloadType(
     standard_window_fns_pb2.FixedWindowsPayload)
 sliding_windows = PropertiesFromPayloadType(
     standard_window_fns_pb2.SlidingWindowsPayload)
 session_windows = PropertiesFromPayloadType(
     standard_window_fns_pb2.SessionsPayload)
+
+monitoring_infos = PropertiesFromEnumType(
+    beam_fn_api_pb2.MonitoringInfoUrns.Enum)
+monitoring_info_types = PropertiesFromEnumType(
+    beam_fn_api_pb2.MonitoringInfoTypeUrns.Enum)
```

## Comparing `apache-beam-2.8.0/apache_beam/portability/__init__.py` & `apache-beam-2.9.0/apache_beam/portability/__init__.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/portability/api/standard_window_fns_pb2.py` & `apache-beam-2.9.0/apache_beam/portability/api/standard_window_fns_pb2.py`

 * *Files 1% similar despite different names*

```diff
@@ -22,14 +22,15 @@
 DESCRIPTOR = _descriptor.FileDescriptor(
   name='standard_window_fns.proto',
   package='org.apache.beam.model.pipeline.v1',
   syntax='proto3',
   serialized_pb=_b('\n\x19standard_window_fns.proto\x12!org.apache.beam.model.pipeline.v1\x1a\x15\x62\x65\x61m_runner_api.proto\x1a\x1egoogle/protobuf/duration.proto\x1a\x1fgoogle/protobuf/timestamp.proto\"W\n\x14GlobalWindowsPayload\"?\n\x04\x45num\x12\x37\n\nPROPERTIES\x10\x00\x1a\'\xa2\xb4\xfa\xc2\x05!beam:windowfn:global_windows:v0.1\"\xaa\x01\n\x13\x46ixedWindowsPayload\x12\'\n\x04size\x18\x01 \x01(\x0b\x32\x19.google.protobuf.Duration\x12*\n\x06offset\x18\x02 \x01(\x0b\x32\x1a.google.protobuf.Timestamp\">\n\x04\x45num\x12\x36\n\nPROPERTIES\x10\x00\x1a&\xa2\xb4\xfa\xc2\x05 beam:windowfn:fixed_windows:v0.1\"\xd9\x01\n\x15SlidingWindowsPayload\x12\'\n\x04size\x18\x01 \x01(\x0b\x32\x19.google.protobuf.Duration\x12*\n\x06offset\x18\x02 \x01(\x0b\x32\x1a.google.protobuf.Timestamp\x12)\n\x06period\x18\x03 \x01(\x0b\x32\x19.google.protobuf.Duration\"@\n\x04\x45num\x12\x38\n\nPROPERTIES\x10\x00\x1a(\xa2\xb4\xfa\xc2\x05\"beam:windowfn:sliding_windows:v0.1\"\x80\x01\n\x0fSessionsPayload\x12+\n\x08gap_size\x18\x01 \x01(\x0b\x32\x19.google.protobuf.Duration\"@\n\x04\x45num\x12\x38\n\nPROPERTIES\x10\x00\x1a(\xa2\xb4\xfa\xc2\x05\"beam:windowfn:session_windows:v0.1BC\n!org.apache.beam.model.pipeline.v1B\x11StandardWindowFnsZ\x0bpipeline_v1b\x06proto3')
   ,
   dependencies=[beam__runner__api__pb2.DESCRIPTOR,google_dot_protobuf_dot_duration__pb2.DESCRIPTOR,google_dot_protobuf_dot_timestamp__pb2.DESCRIPTOR,])
+_sym_db.RegisterFileDescriptor(DESCRIPTOR)
 
 
 
 _GLOBALWINDOWSPAYLOAD_ENUM = _descriptor.EnumDescriptor(
   name='Enum',
   full_name='org.apache.beam.model.pipeline.v1.GlobalWindowsPayload.Enum',
   filename=None,
@@ -136,22 +137,22 @@
   fields=[
     _descriptor.FieldDescriptor(
       name='size', full_name='org.apache.beam.model.pipeline.v1.FixedWindowsPayload.size', index=0,
       number=1, type=11, cpp_type=10, label=1,
       has_default_value=False, default_value=None,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='offset', full_name='org.apache.beam.model.pipeline.v1.FixedWindowsPayload.offset', index=1,
       number=2, type=11, cpp_type=10, label=1,
       has_default_value=False, default_value=None,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
   ],
   extensions=[
   ],
   nested_types=[],
   enum_types=[
     _FIXEDWINDOWSPAYLOAD_ENUM,
   ],
@@ -175,29 +176,29 @@
   fields=[
     _descriptor.FieldDescriptor(
       name='size', full_name='org.apache.beam.model.pipeline.v1.SlidingWindowsPayload.size', index=0,
       number=1, type=11, cpp_type=10, label=1,
       has_default_value=False, default_value=None,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='offset', full_name='org.apache.beam.model.pipeline.v1.SlidingWindowsPayload.offset', index=1,
       number=2, type=11, cpp_type=10, label=1,
       has_default_value=False, default_value=None,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='period', full_name='org.apache.beam.model.pipeline.v1.SlidingWindowsPayload.period', index=2,
       number=3, type=11, cpp_type=10, label=1,
       has_default_value=False, default_value=None,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
   ],
   extensions=[
   ],
   nested_types=[],
   enum_types=[
     _SLIDINGWINDOWSPAYLOAD_ENUM,
   ],
@@ -221,15 +222,15 @@
   fields=[
     _descriptor.FieldDescriptor(
       name='gap_size', full_name='org.apache.beam.model.pipeline.v1.SessionsPayload.gap_size', index=0,
       number=1, type=11, cpp_type=10, label=1,
       has_default_value=False, default_value=None,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
   ],
   extensions=[
   ],
   nested_types=[],
   enum_types=[
     _SESSIONSPAYLOAD_ENUM,
   ],
@@ -253,15 +254,14 @@
 _SLIDINGWINDOWSPAYLOAD_ENUM.containing_type = _SLIDINGWINDOWSPAYLOAD
 _SESSIONSPAYLOAD.fields_by_name['gap_size'].message_type = google_dot_protobuf_dot_duration__pb2._DURATION
 _SESSIONSPAYLOAD_ENUM.containing_type = _SESSIONSPAYLOAD
 DESCRIPTOR.message_types_by_name['GlobalWindowsPayload'] = _GLOBALWINDOWSPAYLOAD
 DESCRIPTOR.message_types_by_name['FixedWindowsPayload'] = _FIXEDWINDOWSPAYLOAD
 DESCRIPTOR.message_types_by_name['SlidingWindowsPayload'] = _SLIDINGWINDOWSPAYLOAD
 DESCRIPTOR.message_types_by_name['SessionsPayload'] = _SESSIONSPAYLOAD
-_sym_db.RegisterFileDescriptor(DESCRIPTOR)
 
 GlobalWindowsPayload = _reflection.GeneratedProtocolMessageType('GlobalWindowsPayload', (_message.Message,), dict(
   DESCRIPTOR = _GLOBALWINDOWSPAYLOAD,
   __module__ = 'standard_window_fns_pb2'
   # @@protoc_insertion_point(class_scope:org.apache.beam.model.pipeline.v1.GlobalWindowsPayload)
   ))
 _sym_db.RegisterMessage(GlobalWindowsPayload)
```

## Comparing `apache-beam-2.8.0/apache_beam/portability/api/beam_fn_api_pb2_grpc.py` & `apache-beam-2.9.0/apache_beam/portability/api/beam_fn_api_pb2_grpc.py`

 * *Files 1% similar despite different names*

```diff
@@ -102,16 +102,14 @@
   }
   generic_handler = grpc.method_handlers_generic_handler(
       'org.apache.beam.model.fn_execution.v1.BeamFnData', rpc_method_handlers)
   server.add_generic_rpc_handlers((generic_handler,))
 
 
 class BeamFnStateStub(object):
-  # missing associated documentation comment in .proto file
-  pass
 
   def __init__(self, channel):
     """Constructor.
 
     Args:
       channel: A grpc.Channel.
     """
@@ -119,16 +117,14 @@
         '/org.apache.beam.model.fn_execution.v1.BeamFnState/State',
         request_serializer=beam__fn__api__pb2.StateRequest.SerializeToString,
         response_deserializer=beam__fn__api__pb2.StateResponse.FromString,
         )
 
 
 class BeamFnStateServicer(object):
-  # missing associated documentation comment in .proto file
-  pass
 
   def State(self, request_iterator, context):
     """Used to get/append/clear state stored by the runner on behalf of the SDK.
     """
     context.set_code(grpc.StatusCode.UNIMPLEMENTED)
     context.set_details('Method not implemented!')
     raise NotImplementedError('Method not implemented!')
```

## Comparing `apache-beam-2.8.0/apache_beam/portability/api/beam_runner_api_pb2.py` & `apache-beam-2.9.0/apache_beam/portability/api/beam_runner_api_pb2.py`

 * *Files 4% similar despite different names*

```diff
@@ -17,27 +17,62 @@
 from google.protobuf import descriptor_pb2 as google_dot_protobuf_dot_descriptor__pb2
 
 
 DESCRIPTOR = _descriptor.FileDescriptor(
   name='beam_runner_api.proto',
   package='org.apache.beam.model.pipeline.v1',
   syntax='proto3',
-  serialized_pb=_b('\n\x15\x62\x65\x61m_runner_api.proto\x12!org.apache.beam.model.pipeline.v1\x1a\x19google/protobuf/any.proto\x1a google/protobuf/descriptor.proto\"\xb5\x07\n\nComponents\x12Q\n\ntransforms\x18\x01 \x03(\x0b\x32=.org.apache.beam.model.pipeline.v1.Components.TransformsEntry\x12U\n\x0cpcollections\x18\x02 \x03(\x0b\x32?.org.apache.beam.model.pipeline.v1.Components.PcollectionsEntry\x12\x64\n\x14windowing_strategies\x18\x03 \x03(\x0b\x32\x46.org.apache.beam.model.pipeline.v1.Components.WindowingStrategiesEntry\x12I\n\x06\x63oders\x18\x04 \x03(\x0b\x32\x39.org.apache.beam.model.pipeline.v1.Components.CodersEntry\x12U\n\x0c\x65nvironments\x18\x05 \x03(\x0b\x32?.org.apache.beam.model.pipeline.v1.Components.EnvironmentsEntry\x1a`\n\x0fTransformsEntry\x12\x0b\n\x03key\x18\x01 \x01(\t\x12<\n\x05value\x18\x02 \x01(\x0b\x32-.org.apache.beam.model.pipeline.v1.PTransform:\x02\x38\x01\x1a\x63\n\x11PcollectionsEntry\x12\x0b\n\x03key\x18\x01 \x01(\t\x12=\n\x05value\x18\x02 \x01(\x0b\x32..org.apache.beam.model.pipeline.v1.PCollection:\x02\x38\x01\x1ap\n\x18WindowingStrategiesEntry\x12\x0b\n\x03key\x18\x01 \x01(\t\x12\x43\n\x05value\x18\x02 \x01(\x0b\x32\x34.org.apache.beam.model.pipeline.v1.WindowingStrategy:\x02\x38\x01\x1aW\n\x0b\x43odersEntry\x12\x0b\n\x03key\x18\x01 \x01(\t\x12\x37\n\x05value\x18\x02 \x01(\x0b\x32(.org.apache.beam.model.pipeline.v1.Coder:\x02\x38\x01\x1a\x63\n\x11\x45nvironmentsEntry\x12\x0b\n\x03key\x18\x01 \x01(\t\x12=\n\x05value\x18\x02 \x01(\x0b\x32..org.apache.beam.model.pipeline.v1.Environment:\x02\x38\x01\"\x92\x07\n\x15MessageWithComponents\x12\x41\n\ncomponents\x18\x01 \x01(\x0b\x32-.org.apache.beam.model.pipeline.v1.Components\x12\x39\n\x05\x63oder\x18\x02 \x01(\x0b\x32(.org.apache.beam.model.pipeline.v1.CoderH\x00\x12L\n\x0f\x63ombine_payload\x18\x03 \x01(\x0b\x32\x31.org.apache.beam.model.pipeline.v1.CombinePayloadH\x00\x12O\n\x11sdk_function_spec\x18\x04 \x01(\x0b\x32\x32.org.apache.beam.model.pipeline.v1.SdkFunctionSpecH\x00\x12I\n\x0epar_do_payload\x18\x06 \x01(\x0b\x32/.org.apache.beam.model.pipeline.v1.ParDoPayloadH\x00\x12\x43\n\nptransform\x18\x07 \x01(\x0b\x32-.org.apache.beam.model.pipeline.v1.PTransformH\x00\x12\x45\n\x0bpcollection\x18\x08 \x01(\x0b\x32..org.apache.beam.model.pipeline.v1.PCollectionH\x00\x12\x46\n\x0cread_payload\x18\t \x01(\x0b\x32..org.apache.beam.model.pipeline.v1.ReadPayloadH\x00\x12\x42\n\nside_input\x18\x0b \x01(\x0b\x32,.org.apache.beam.model.pipeline.v1.SideInputH\x00\x12S\n\x13window_into_payload\x18\x0c \x01(\x0b\x32\x34.org.apache.beam.model.pipeline.v1.WindowIntoPayloadH\x00\x12R\n\x12windowing_strategy\x18\r \x01(\x0b\x32\x34.org.apache.beam.model.pipeline.v1.WindowingStrategyH\x00\x12H\n\rfunction_spec\x18\x0e \x01(\x0b\x32/.org.apache.beam.model.pipeline.v1.FunctionSpecH\x00\x42\x06\n\x04root\"\xaf\x01\n\x08Pipeline\x12\x41\n\ncomponents\x18\x01 \x01(\x0b\x32-.org.apache.beam.model.pipeline.v1.Components\x12\x1a\n\x12root_transform_ids\x18\x02 \x03(\t\x12\x44\n\x0c\x64isplay_data\x18\x03 \x01(\x0b\x32..org.apache.beam.model.pipeline.v1.DisplayData\"\xb4\x03\n\nPTransform\x12\x13\n\x0bunique_name\x18\x05 \x01(\t\x12=\n\x04spec\x18\x01 \x01(\x0b\x32/.org.apache.beam.model.pipeline.v1.FunctionSpec\x12\x15\n\rsubtransforms\x18\x02 \x03(\t\x12I\n\x06inputs\x18\x03 \x03(\x0b\x32\x39.org.apache.beam.model.pipeline.v1.PTransform.InputsEntry\x12K\n\x07outputs\x18\x04 \x03(\x0b\x32:.org.apache.beam.model.pipeline.v1.PTransform.OutputsEntry\x12\x44\n\x0c\x64isplay_data\x18\x06 \x01(\x0b\x32..org.apache.beam.model.pipeline.v1.DisplayData\x1a-\n\x0bInputsEntry\x12\x0b\n\x03key\x18\x01 \x01(\t\x12\r\n\x05value\x18\x02 \x01(\t:\x02\x38\x01\x1a.\n\x0cOutputsEntry\x12\x0b\n\x03key\x18\x01 \x01(\t\x12\r\n\x05value\x18\x02 \x01(\t:\x02\x38\x01\"\xd0\r\n\x13StandardPTransforms\"\xb1\x03\n\nPrimitives\x12-\n\x06PAR_DO\x10\x00\x1a!\xa2\xb4\xfa\xc2\x05\x1burn:beam:transform:pardo:v1\x12,\n\x07\x46LATTEN\x10\x01\x1a\x1f\xa2\xb4\xfa\xc2\x05\x19\x62\x65\x61m:transform:flatten:v1\x12\x36\n\x0cGROUP_BY_KEY\x10\x02\x1a$\xa2\xb4\xfa\xc2\x05\x1e\x62\x65\x61m:transform:group_by_key:v1\x12,\n\x07IMPULSE\x10\x03\x1a\x1f\xa2\xb4\xfa\xc2\x05\x19\x62\x65\x61m:transform:impulse:v1\x12\x37\n\x0e\x41SSIGN_WINDOWS\x10\x04\x1a#\xa2\xb4\xfa\xc2\x05\x1d\x62\x65\x61m:transform:window_into:v1\x12\x37\n\x0bTEST_STREAM\x10\x05\x1a&\xa2\xb4\xfa\xc2\x05 urn:beam:transform:teststream:v1\x12\x34\n\x0bMAP_WINDOWS\x10\x06\x1a#\xa2\xb4\xfa\xc2\x05\x1d\x62\x65\x61m:transform:map_windows:v1\x12\x38\n\rMERGE_WINDOWS\x10\x07\x1a%\xa2\xb4\xfa\xc2\x05\x1f\x62\x65\x61m:transform:merge_windows:v1\"t\n\x14\x44\x65precatedPrimitives\x12&\n\x04READ\x10\x00\x1a\x1c\xa2\xb4\xfa\xc2\x05\x16\x62\x65\x61m:transform:read:v1\x12\x34\n\x0b\x43REATE_VIEW\x10\x01\x1a#\xa2\xb4\xfa\xc2\x05\x1d\x62\x65\x61m:transform:create_view:v1\"\xf2\x01\n\nComposites\x12<\n\x0f\x43OMBINE_PER_KEY\x10\x00\x1a\'\xa2\xb4\xfa\xc2\x05!beam:transform:combine_per_key:v1\x12>\n\x10\x43OMBINE_GLOBALLY\x10\x01\x1a(\xa2\xb4\xfa\xc2\x05\"beam:transform:combine_globally:v1\x12\x30\n\tRESHUFFLE\x10\x02\x1a!\xa2\xb4\xfa\xc2\x05\x1b\x62\x65\x61m:transform:reshuffle:v1\x12\x34\n\x0bWRITE_FILES\x10\x03\x1a#\xa2\xb4\xfa\xc2\x05\x1d\x62\x65\x61m:transform:write_files:v1\"\xd3\x04\n\x11\x43ombineComponents\x12:\n\x0e\x43OMBINE_PGBKCV\x10\x00\x1a&\xa2\xb4\xfa\xc2\x05 beam:transform:combine_pgbkcv:v1\x12R\n\x1a\x43OMBINE_MERGE_ACCUMULATORS\x10\x01\x1a\x32\xa2\xb4\xfa\xc2\x05,beam:transform:combine_merge_accumulators:v1\x12L\n\x17\x43OMBINE_EXTRACT_OUTPUTS\x10\x02\x1a/\xa2\xb4\xfa\xc2\x05)beam:transform:combine_extract_outputs:v1\x12R\n\x1a\x43OMBINE_PER_KEY_PRECOMBINE\x10\x03\x1a\x32\xa2\xb4\xfa\xc2\x05,beam:transform:combine_per_key_precombine:v1\x12\x62\n\"COMBINE_PER_KEY_MERGE_ACCUMULATORS\x10\x04\x1a:\xa2\xb4\xfa\xc2\x05\x34\x62\x65\x61m:transform:combine_per_key_merge_accumulators:v1\x12\\\n\x1f\x43OMBINE_PER_KEY_EXTRACT_OUTPUTS\x10\x05\x1a\x37\xa2\xb4\xfa\xc2\x05\x31\x62\x65\x61m:transform:combine_per_key_extract_outputs:v1\x12J\n\x16\x43OMBINE_GROUPED_VALUES\x10\x06\x1a.\xa2\xb4\xfa\xc2\x05(beam:transform:combine_grouped_values:v1\"\xc3\x02\n\x19SplittableParDoComponents\x12L\n\x15PAIR_WITH_RESTRICTION\x10\x00\x1a\x31\xa2\xb4\xfa\xc2\x05+beam:transform:sdf_pair_with_restriction:v1\x12\x44\n\x11SPLIT_RESTRICTION\x10\x01\x1a-\xa2\xb4\xfa\xc2\x05\'beam:transform:sdf_split_restriction:v1\x12N\n\x16PROCESS_KEYED_ELEMENTS\x10\x02\x1a\x32\xa2\xb4\xfa\xc2\x05,beam:transform:sdf_process_keyed_elements:v1\x12\x42\n\x10PROCESS_ELEMENTS\x10\x03\x1a,\xa2\xb4\xfa\xc2\x05&beam:transform:sdf_process_elements:v1\"\x82\x01\n\x16StandardSideInputTypes\"h\n\x04\x45num\x12/\n\x08ITERABLE\x10\x00\x1a!\xa2\xb4\xfa\xc2\x05\x1b\x62\x65\x61m:side_input:iterable:v1\x12/\n\x08MULTIMAP\x10\x01\x1a!\xa2\xb4\xfa\xc2\x05\x1b\x62\x65\x61m:side_input:multimap:v1\"\xe0\x01\n\x0bPCollection\x12\x13\n\x0bunique_name\x18\x01 \x01(\t\x12\x10\n\x08\x63oder_id\x18\x02 \x01(\t\x12\x45\n\nis_bounded\x18\x03 \x01(\x0e\x32\x31.org.apache.beam.model.pipeline.v1.IsBounded.Enum\x12\x1d\n\x15windowing_strategy_id\x18\x04 \x01(\t\x12\x44\n\x0c\x64isplay_data\x18\x05 \x01(\x0b\x32..org.apache.beam.model.pipeline.v1.DisplayData\"\x86\x05\n\x16\x45xecutableStagePayload\x12\x43\n\x0b\x65nvironment\x18\x01 \x01(\x0b\x32..org.apache.beam.model.pipeline.v1.Environment\x12\r\n\x05input\x18\x02 \x01(\t\x12Z\n\x0bside_inputs\x18\x03 \x03(\x0b\x32\x45.org.apache.beam.model.pipeline.v1.ExecutableStagePayload.SideInputId\x12\x12\n\ntransforms\x18\x04 \x03(\t\x12\x0f\n\x07outputs\x18\x05 \x03(\t\x12\x41\n\ncomponents\x18\x06 \x01(\x0b\x32-.org.apache.beam.model.pipeline.v1.Components\x12Z\n\x0buser_states\x18\x07 \x03(\x0b\x32\x45.org.apache.beam.model.pipeline.v1.ExecutableStagePayload.UserStateId\x12Q\n\x06timers\x18\x08 \x03(\x0b\x32\x41.org.apache.beam.model.pipeline.v1.ExecutableStagePayload.TimerId\x1a\x37\n\x0bSideInputId\x12\x14\n\x0ctransform_id\x18\x01 \x01(\t\x12\x12\n\nlocal_name\x18\x02 \x01(\t\x1a\x37\n\x0bUserStateId\x12\x14\n\x0ctransform_id\x18\x01 \x01(\t\x12\x12\n\nlocal_name\x18\x02 \x01(\t\x1a\x33\n\x07TimerId\x12\x14\n\x0ctransform_id\x18\x01 \x01(\t\x12\x12\n\nlocal_name\x18\x02 \x01(\t\"\xea\x05\n\x0cParDoPayload\x12\x41\n\x05\x64o_fn\x18\x01 \x01(\x0b\x32\x32.org.apache.beam.model.pipeline.v1.SdkFunctionSpec\x12@\n\nparameters\x18\x02 \x03(\x0b\x32,.org.apache.beam.model.pipeline.v1.Parameter\x12T\n\x0bside_inputs\x18\x03 \x03(\x0b\x32?.org.apache.beam.model.pipeline.v1.ParDoPayload.SideInputsEntry\x12T\n\x0bstate_specs\x18\x04 \x03(\x0b\x32?.org.apache.beam.model.pipeline.v1.ParDoPayload.StateSpecsEntry\x12T\n\x0btimer_specs\x18\x05 \x03(\x0b\x32?.org.apache.beam.model.pipeline.v1.ParDoPayload.TimerSpecsEntry\x12\x12\n\nsplittable\x18\x06 \x01(\x08\x12\x1c\n\x14restriction_coder_id\x18\x07 \x01(\t\x1a_\n\x0fSideInputsEntry\x12\x0b\n\x03key\x18\x01 \x01(\t\x12;\n\x05value\x18\x02 \x01(\x0b\x32,.org.apache.beam.model.pipeline.v1.SideInput:\x02\x38\x01\x1a_\n\x0fStateSpecsEntry\x12\x0b\n\x03key\x18\x01 \x01(\t\x12;\n\x05value\x18\x02 \x01(\x0b\x32,.org.apache.beam.model.pipeline.v1.StateSpec:\x02\x38\x01\x1a_\n\x0fTimerSpecsEntry\x12\x0b\n\x03key\x18\x01 \x01(\t\x12;\n\x05value\x18\x02 \x01(\x0b\x32,.org.apache.beam.model.pipeline.v1.TimerSpec:\x02\x38\x01\"\xad\x01\n\tParameter\x12\x44\n\x04type\x18\x01 \x01(\x0e\x32\x36.org.apache.beam.model.pipeline.v1.Parameter.Type.Enum\x1aZ\n\x04Type\"R\n\x04\x45num\x12\x0f\n\x0bUNSPECIFIED\x10\x00\x12\n\n\x06WINDOW\x10\x01\x12\x14\n\x10PIPELINE_OPTIONS\x10\x02\x12\x17\n\x13RESTRICTION_TRACKER\x10\x03\"\xfc\x02\n\tStateSpec\x12G\n\nvalue_spec\x18\x01 \x01(\x0b\x32\x31.org.apache.beam.model.pipeline.v1.ValueStateSpecH\x00\x12\x43\n\x08\x62\x61g_spec\x18\x02 \x01(\x0b\x32/.org.apache.beam.model.pipeline.v1.BagStateSpecH\x00\x12O\n\x0e\x63ombining_spec\x18\x03 \x01(\x0b\x32\x35.org.apache.beam.model.pipeline.v1.CombiningStateSpecH\x00\x12\x43\n\x08map_spec\x18\x04 \x01(\x0b\x32/.org.apache.beam.model.pipeline.v1.MapStateSpecH\x00\x12\x43\n\x08set_spec\x18\x05 \x01(\x0b\x32/.org.apache.beam.model.pipeline.v1.SetStateSpecH\x00\x42\x06\n\x04spec\"\"\n\x0eValueStateSpec\x12\x10\n\x08\x63oder_id\x18\x01 \x01(\t\"(\n\x0c\x42\x61gStateSpec\x12\x18\n\x10\x65lement_coder_id\x18\x01 \x01(\t\"z\n\x12\x43ombiningStateSpec\x12\x1c\n\x14\x61\x63\x63umulator_coder_id\x18\x01 \x01(\t\x12\x46\n\ncombine_fn\x18\x02 \x01(\x0b\x32\x32.org.apache.beam.model.pipeline.v1.SdkFunctionSpec\"<\n\x0cMapStateSpec\x12\x14\n\x0ckey_coder_id\x18\x01 \x01(\t\x12\x16\n\x0evalue_coder_id\x18\x02 \x01(\t\"(\n\x0cSetStateSpec\x12\x18\n\x10\x65lement_coder_id\x18\x01 \x01(\t\"T\n\tTimerSpec\x12G\n\x0btime_domain\x18\x01 \x01(\x0e\x32\x32.org.apache.beam.model.pipeline.v1.TimeDomain.Enum\"@\n\tIsBounded\"3\n\x04\x45num\x12\x0f\n\x0bUNSPECIFIED\x10\x00\x12\r\n\tUNBOUNDED\x10\x01\x12\x0b\n\x07\x42OUNDED\x10\x02\"\x98\x01\n\x0bReadPayload\x12\x42\n\x06source\x18\x01 \x01(\x0b\x32\x32.org.apache.beam.model.pipeline.v1.SdkFunctionSpec\x12\x45\n\nis_bounded\x18\x02 \x01(\x0e\x32\x31.org.apache.beam.model.pipeline.v1.IsBounded.Enum\"Z\n\x11WindowIntoPayload\x12\x45\n\twindow_fn\x18\x01 \x01(\x0b\x32\x32.org.apache.beam.model.pipeline.v1.SdkFunctionSpec\"v\n\x0e\x43ombinePayload\x12\x46\n\ncombine_fn\x18\x01 \x01(\x0b\x32\x32.org.apache.beam.model.pipeline.v1.SdkFunctionSpec\x12\x1c\n\x14\x61\x63\x63umulator_coder_id\x18\x02 \x01(\t\"\xca\x05\n\x11TestStreamPayload\x12\x10\n\x08\x63oder_id\x18\x01 \x01(\t\x12J\n\x06\x65vents\x18\x02 \x03(\x0b\x32:.org.apache.beam.model.pipeline.v1.TestStreamPayload.Event\x1a\x94\x04\n\x05\x45vent\x12\x66\n\x0fwatermark_event\x18\x01 \x01(\x0b\x32K.org.apache.beam.model.pipeline.v1.TestStreamPayload.Event.AdvanceWatermarkH\x00\x12q\n\x15processing_time_event\x18\x02 \x01(\x0b\x32P.org.apache.beam.model.pipeline.v1.TestStreamPayload.Event.AdvanceProcessingTimeH\x00\x12_\n\relement_event\x18\x03 \x01(\x0b\x32\x46.org.apache.beam.model.pipeline.v1.TestStreamPayload.Event.AddElementsH\x00\x1a)\n\x10\x41\x64vanceWatermark\x12\x15\n\rnew_watermark\x18\x01 \x01(\x03\x1a\x31\n\x15\x41\x64vanceProcessingTime\x12\x18\n\x10\x61\x64vance_duration\x18\x01 \x01(\x03\x1ah\n\x0b\x41\x64\x64\x45lements\x12Y\n\x08\x65lements\x18\x01 \x03(\x0b\x32G.org.apache.beam.model.pipeline.v1.TestStreamPayload.TimestampedElementB\x07\n\x05\x65vent\x1a@\n\x12TimestampedElement\x12\x17\n\x0f\x65ncoded_element\x18\x01 \x01(\x0c\x12\x11\n\ttimestamp\x18\x02 \x01(\x03\"\x9b\x03\n\x11WriteFilesPayload\x12@\n\x04sink\x18\x01 \x01(\x0b\x32\x32.org.apache.beam.model.pipeline.v1.SdkFunctionSpec\x12K\n\x0f\x66ormat_function\x18\x02 \x01(\x0b\x32\x32.org.apache.beam.model.pipeline.v1.SdkFunctionSpec\x12\x17\n\x0fwindowed_writes\x18\x03 \x01(\x08\x12\"\n\x1arunner_determined_sharding\x18\x04 \x01(\x08\x12Y\n\x0bside_inputs\x18\x05 \x03(\x0b\x32\x44.org.apache.beam.model.pipeline.v1.WriteFilesPayload.SideInputsEntry\x1a_\n\x0fSideInputsEntry\x12\x0b\n\x03key\x18\x01 \x01(\t\x12;\n\x05value\x18\x02 \x01(\x0b\x32,.org.apache.beam.model.pipeline.v1.SideInput:\x02\x38\x01\"f\n\x05\x43oder\x12@\n\x04spec\x18\x01 \x01(\x0b\x32\x32.org.apache.beam.model.pipeline.v1.SdkFunctionSpec\x12\x1b\n\x13\x63omponent_coder_ids\x18\x02 \x03(\t\"\xb7\x03\n\x0eStandardCoders\"\xa4\x03\n\x04\x45num\x12$\n\x05\x42YTES\x10\x00\x1a\x19\xa2\xb4\xfa\xc2\x05\x13\x62\x65\x61m:coder:bytes:v1\x12\x1e\n\x02KV\x10\x01\x1a\x16\xa2\xb4\xfa\xc2\x05\x10\x62\x65\x61m:coder:kv:v1\x12&\n\x06VARINT\x10\x02\x1a\x1a\xa2\xb4\xfa\xc2\x05\x14\x62\x65\x61m:coder:varint:v1\x12*\n\x08ITERABLE\x10\x03\x1a\x1c\xa2\xb4\xfa\xc2\x05\x16\x62\x65\x61m:coder:iterable:v1\x12$\n\x05TIMER\x10\x04\x1a\x19\xa2\xb4\xfa\xc2\x05\x13\x62\x65\x61m:coder:timer:v1\x12\x38\n\x0fINTERVAL_WINDOW\x10\x05\x1a#\xa2\xb4\xfa\xc2\x05\x1d\x62\x65\x61m:coder:interval_window:v1\x12\x34\n\rLENGTH_PREFIX\x10\x06\x1a!\xa2\xb4\xfa\xc2\x05\x1b\x62\x65\x61m:coder:length_prefix:v1\x12\x34\n\rGLOBAL_WINDOW\x10\x07\x1a!\xa2\xb4\xfa\xc2\x05\x1b\x62\x65\x61m:coder:global_window:v1\x12\x36\n\x0eWINDOWED_VALUE\x10\x08\x1a\"\xa2\xb4\xfa\xc2\x05\x1c\x62\x65\x61m:coder:windowed_value:v1\"\xf5\x04\n\x11WindowingStrategy\x12\x45\n\twindow_fn\x18\x01 \x01(\x0b\x32\x32.org.apache.beam.model.pipeline.v1.SdkFunctionSpec\x12I\n\x0cmerge_status\x18\x02 \x01(\x0e\x32\x33.org.apache.beam.model.pipeline.v1.MergeStatus.Enum\x12\x17\n\x0fwindow_coder_id\x18\x03 \x01(\t\x12;\n\x07trigger\x18\x04 \x01(\x0b\x32*.org.apache.beam.model.pipeline.v1.Trigger\x12S\n\x11\x61\x63\x63umulation_mode\x18\x05 \x01(\x0e\x32\x38.org.apache.beam.model.pipeline.v1.AccumulationMode.Enum\x12G\n\x0boutput_time\x18\x06 \x01(\x0e\x32\x32.org.apache.beam.model.pipeline.v1.OutputTime.Enum\x12Q\n\x10\x63losing_behavior\x18\x07 \x01(\x0e\x32\x37.org.apache.beam.model.pipeline.v1.ClosingBehavior.Enum\x12\x18\n\x10\x61llowed_lateness\x18\x08 \x01(\x03\x12N\n\x0eOnTimeBehavior\x18\t \x01(\x0e\x32\x36.org.apache.beam.model.pipeline.v1.OnTimeBehavior.Enum\x12\x1d\n\x15\x61ssigns_to_one_window\x18\n \x01(\x08\"\\\n\x0bMergeStatus\"M\n\x04\x45num\x12\x0f\n\x0bUNSPECIFIED\x10\x00\x12\x0f\n\x0bNON_MERGING\x10\x01\x12\x0f\n\x0bNEEDS_MERGE\x10\x02\x12\x12\n\x0e\x41LREADY_MERGED\x10\x03\"M\n\x10\x41\x63\x63umulationMode\"9\n\x04\x45num\x12\x0f\n\x0bUNSPECIFIED\x10\x00\x12\x0e\n\nDISCARDING\x10\x01\x12\x10\n\x0c\x41\x43\x43UMULATING\x10\x02\"Q\n\x0f\x43losingBehavior\">\n\x04\x45num\x12\x0f\n\x0bUNSPECIFIED\x10\x00\x12\x0f\n\x0b\x45MIT_ALWAYS\x10\x01\x12\x14\n\x10\x45MIT_IF_NONEMPTY\x10\x02\"P\n\x0eOnTimeBehavior\">\n\x04\x45num\x12\x0f\n\x0bUNSPECIFIED\x10\x00\x12\x0f\n\x0b\x46IRE_ALWAYS\x10\x01\x12\x14\n\x10\x46IRE_IF_NONEMPTY\x10\x02\"b\n\nOutputTime\"T\n\x04\x45num\x12\x0f\n\x0bUNSPECIFIED\x10\x00\x12\x11\n\rEND_OF_WINDOW\x10\x01\x12\x12\n\x0eLATEST_IN_PANE\x10\x02\x12\x14\n\x10\x45\x41RLIEST_IN_PANE\x10\x03\"l\n\nTimeDomain\"^\n\x04\x45num\x12\x0f\n\x0bUNSPECIFIED\x10\x00\x12\x0e\n\nEVENT_TIME\x10\x01\x12\x13\n\x0fPROCESSING_TIME\x10\x02\x12 \n\x1cSYNCHRONIZED_PROCESSING_TIME\x10\x03\"\x82\x0e\n\x07Trigger\x12H\n\tafter_all\x18\x01 \x01(\x0b\x32\x33.org.apache.beam.model.pipeline.v1.Trigger.AfterAllH\x00\x12H\n\tafter_any\x18\x02 \x01(\x0b\x32\x33.org.apache.beam.model.pipeline.v1.Trigger.AfterAnyH\x00\x12J\n\nafter_each\x18\x03 \x01(\x0b\x32\x34.org.apache.beam.model.pipeline.v1.Trigger.AfterEachH\x00\x12Z\n\x13\x61\x66ter_end_of_window\x18\x04 \x01(\x0b\x32;.org.apache.beam.model.pipeline.v1.Trigger.AfterEndOfWindowH\x00\x12_\n\x15\x61\x66ter_processing_time\x18\x05 \x01(\x0b\x32>.org.apache.beam.model.pipeline.v1.Trigger.AfterProcessingTimeH\x00\x12x\n\"after_synchronized_processing_time\x18\x06 \x01(\x0b\x32J.org.apache.beam.model.pipeline.v1.Trigger.AfterSynchronizedProcessingTimeH\x00\x12\x43\n\x06\x61lways\x18\x0c \x01(\x0b\x32\x31.org.apache.beam.model.pipeline.v1.Trigger.AlwaysH\x00\x12\x45\n\x07\x64\x65\x66\x61ult\x18\x07 \x01(\x0b\x32\x32.org.apache.beam.model.pipeline.v1.Trigger.DefaultH\x00\x12P\n\relement_count\x18\x08 \x01(\x0b\x32\x37.org.apache.beam.model.pipeline.v1.Trigger.ElementCountH\x00\x12\x41\n\x05never\x18\t \x01(\x0b\x32\x30.org.apache.beam.model.pipeline.v1.Trigger.NeverH\x00\x12J\n\nor_finally\x18\n \x01(\x0b\x32\x34.org.apache.beam.model.pipeline.v1.Trigger.OrFinallyH\x00\x12\x43\n\x06repeat\x18\x0b \x01(\x0b\x32\x31.org.apache.beam.model.pipeline.v1.Trigger.RepeatH\x00\x1aK\n\x08\x41\x66terAll\x12?\n\x0bsubtriggers\x18\x01 \x03(\x0b\x32*.org.apache.beam.model.pipeline.v1.Trigger\x1aK\n\x08\x41\x66terAny\x12?\n\x0bsubtriggers\x18\x01 \x03(\x0b\x32*.org.apache.beam.model.pipeline.v1.Trigger\x1aL\n\tAfterEach\x12?\n\x0bsubtriggers\x18\x01 \x03(\x0b\x32*.org.apache.beam.model.pipeline.v1.Trigger\x1a\x97\x01\n\x10\x41\x66terEndOfWindow\x12\x41\n\rearly_firings\x18\x01 \x01(\x0b\x32*.org.apache.beam.model.pipeline.v1.Trigger\x12@\n\x0clate_firings\x18\x02 \x01(\x0b\x32*.org.apache.beam.model.pipeline.v1.Trigger\x1aj\n\x13\x41\x66terProcessingTime\x12S\n\x14timestamp_transforms\x18\x01 \x03(\x0b\x32\x35.org.apache.beam.model.pipeline.v1.TimestampTransform\x1a!\n\x1f\x41\x66terSynchronizedProcessingTime\x1a\t\n\x07\x44\x65\x66\x61ult\x1a%\n\x0c\x45lementCount\x12\x15\n\relement_count\x18\x01 \x01(\x05\x1a\x07\n\x05Never\x1a\x08\n\x06\x41lways\x1a\x82\x01\n\tOrFinally\x12\x38\n\x04main\x18\x01 \x01(\x0b\x32*.org.apache.beam.model.pipeline.v1.Trigger\x12;\n\x07\x66inally\x18\x02 \x01(\x0b\x32*.org.apache.beam.model.pipeline.v1.Trigger\x1aH\n\x06Repeat\x12>\n\nsubtrigger\x18\x01 \x01(\x0b\x32*.org.apache.beam.model.pipeline.v1.TriggerB\t\n\x07trigger\"\x96\x02\n\x12TimestampTransform\x12L\n\x05\x64\x65lay\x18\x01 \x01(\x0b\x32;.org.apache.beam.model.pipeline.v1.TimestampTransform.DelayH\x00\x12Q\n\x08\x61lign_to\x18\x02 \x01(\x0b\x32=.org.apache.beam.model.pipeline.v1.TimestampTransform.AlignToH\x00\x1a\x1d\n\x05\x44\x65lay\x12\x14\n\x0c\x64\x65lay_millis\x18\x01 \x01(\x03\x1a)\n\x07\x41lignTo\x12\x0e\n\x06period\x18\x03 \x01(\x03\x12\x0e\n\x06offset\x18\x04 \x01(\x03\x42\x15\n\x13timestamp_transform\"\xe8\x01\n\tSideInput\x12G\n\x0e\x61\x63\x63\x65ss_pattern\x18\x01 \x01(\x0b\x32/.org.apache.beam.model.pipeline.v1.FunctionSpec\x12\x43\n\x07view_fn\x18\x02 \x01(\x0b\x32\x32.org.apache.beam.model.pipeline.v1.SdkFunctionSpec\x12M\n\x11window_mapping_fn\x18\x03 \x01(\x0b\x32\x32.org.apache.beam.model.pipeline.v1.SdkFunctionSpec\"8\n\x0b\x45nvironment\x12\x0b\n\x03url\x18\x01 \x01(\t\x12\x0b\n\x03urn\x18\x02 \x01(\t\x12\x0f\n\x07payload\x18\x03 \x01(\x0c\"\x9f\x01\n\x14StandardEnvironments\"\x86\x01\n\x0c\x45nvironments\x12$\n\x06\x44OCKER\x10\x00\x1a\x18\xa2\xb4\xfa\xc2\x05\x12\x62\x65\x61m:env:docker:v1\x12&\n\x07PROCESS\x10\x01\x1a\x19\xa2\xb4\xfa\xc2\x05\x13\x62\x65\x61m:env:process:v1\x12(\n\x08\x45XTERNAL\x10\x02\x1a\x1a\xa2\xb4\xfa\xc2\x05\x14\x62\x65\x61m:env:external:v1\"(\n\rDockerPayload\x12\x17\n\x0f\x63ontainer_image\x18\x01 \x01(\t\"\xb0\x01\n\x0eProcessPayload\x12\n\n\x02os\x18\x01 \x01(\t\x12\x0c\n\x04\x61rch\x18\x02 \x01(\t\x12\x0f\n\x07\x63ommand\x18\x03 \x01(\t\x12G\n\x03\x65nv\x18\x04 \x03(\x0b\x32:.org.apache.beam.model.pipeline.v1.ProcessPayload.EnvEntry\x1a*\n\x08\x45nvEntry\x12\x0b\n\x03key\x18\x01 \x01(\t\x12\r\n\x05value\x18\x02 \x01(\t:\x02\x38\x01\"h\n\x0fSdkFunctionSpec\x12=\n\x04spec\x18\x01 \x01(\x0b\x32/.org.apache.beam.model.pipeline.v1.FunctionSpec\x12\x16\n\x0e\x65nvironment_id\x18\x02 \x01(\t\",\n\x0c\x46unctionSpec\x12\x0b\n\x03urn\x18\x01 \x01(\t\x12\x0f\n\x07payload\x18\x03 \x01(\x0c\"\xa1\x04\n\x0b\x44isplayData\x12\x42\n\x05items\x18\x01 \x03(\x0b\x32\x33.org.apache.beam.model.pipeline.v1.DisplayData.Item\x1a\x46\n\nIdentifier\x12\x14\n\x0ctransform_id\x18\x01 \x01(\t\x12\x15\n\rtransform_urn\x18\x02 \x01(\t\x12\x0b\n\x03key\x18\x03 \x01(\t\x1a\x86\x02\n\x04Item\x12\x45\n\x02id\x18\x01 \x01(\x0b\x32\x39.org.apache.beam.model.pipeline.v1.DisplayData.Identifier\x12\x46\n\x04type\x18\x02 \x01(\x0e\x32\x38.org.apache.beam.model.pipeline.v1.DisplayData.Type.Enum\x12#\n\x05value\x18\x03 \x01(\x0b\x32\x14.google.protobuf.Any\x12)\n\x0bshort_value\x18\x04 \x01(\x0b\x32\x14.google.protobuf.Any\x12\r\n\x05label\x18\x05 \x01(\t\x12\x10\n\x08link_url\x18\x06 \x01(\t\x1a}\n\x04Type\"u\n\x04\x45num\x12\x0f\n\x0bUNSPECIFIED\x10\x00\x12\n\n\x06STRING\x10\x01\x12\x0b\n\x07INTEGER\x10\x02\x12\t\n\x05\x46LOAT\x10\x03\x12\x0b\n\x07\x42OOLEAN\x10\x04\x12\r\n\tTIMESTAMP\x10\x05\x12\x0c\n\x08\x44URATION\x10\x06\x12\x0e\n\nJAVA_CLASS\x10\x07:6\n\x08\x62\x65\x61m_urn\x12!.google.protobuf.EnumValueOptions\x18\xc4\xa6\xafX \x01(\tB;\n!org.apache.beam.model.pipeline.v1B\tRunnerApiZ\x0bpipeline_v1b\x06proto3')
+  serialized_pb=_b('\n\x15\x62\x65\x61m_runner_api.proto\x12!org.apache.beam.model.pipeline.v1\x1a\x19google/protobuf/any.proto\x1a google/protobuf/descriptor.proto\"\xc2\x01\n\rBeamConstants\"\xb0\x01\n\tConstants\x12\x31\n\x14MIN_TIMESTAMP_MILLIS\x10\x00\x1a\x17\xaa\xb4\xfa\xc2\x05\x11-9223372036854775\x12\x30\n\x14MAX_TIMESTAMP_MILLIS\x10\x01\x1a\x16\xaa\xb4\xfa\xc2\x05\x10\x39\x32\x32\x33\x33\x37\x32\x30\x33\x36\x38\x35\x34\x37\x37\x35\x12>\n\"GLOBAL_WINDOW_MAX_TIMESTAMP_MILLIS\x10\x02\x1a\x16\xaa\xb4\xfa\xc2\x05\x10\x39\x32\x32\x33\x33\x37\x31\x39\x35\x30\x34\x35\x34\x37\x37\x35\"\xb5\x07\n\nComponents\x12Q\n\ntransforms\x18\x01 \x03(\x0b\x32=.org.apache.beam.model.pipeline.v1.Components.TransformsEntry\x12U\n\x0cpcollections\x18\x02 \x03(\x0b\x32?.org.apache.beam.model.pipeline.v1.Components.PcollectionsEntry\x12\x64\n\x14windowing_strategies\x18\x03 \x03(\x0b\x32\x46.org.apache.beam.model.pipeline.v1.Components.WindowingStrategiesEntry\x12I\n\x06\x63oders\x18\x04 \x03(\x0b\x32\x39.org.apache.beam.model.pipeline.v1.Components.CodersEntry\x12U\n\x0c\x65nvironments\x18\x05 \x03(\x0b\x32?.org.apache.beam.model.pipeline.v1.Components.EnvironmentsEntry\x1a`\n\x0fTransformsEntry\x12\x0b\n\x03key\x18\x01 \x01(\t\x12<\n\x05value\x18\x02 \x01(\x0b\x32-.org.apache.beam.model.pipeline.v1.PTransform:\x02\x38\x01\x1a\x63\n\x11PcollectionsEntry\x12\x0b\n\x03key\x18\x01 \x01(\t\x12=\n\x05value\x18\x02 \x01(\x0b\x32..org.apache.beam.model.pipeline.v1.PCollection:\x02\x38\x01\x1ap\n\x18WindowingStrategiesEntry\x12\x0b\n\x03key\x18\x01 \x01(\t\x12\x43\n\x05value\x18\x02 \x01(\x0b\x32\x34.org.apache.beam.model.pipeline.v1.WindowingStrategy:\x02\x38\x01\x1aW\n\x0b\x43odersEntry\x12\x0b\n\x03key\x18\x01 \x01(\t\x12\x37\n\x05value\x18\x02 \x01(\x0b\x32(.org.apache.beam.model.pipeline.v1.Coder:\x02\x38\x01\x1a\x63\n\x11\x45nvironmentsEntry\x12\x0b\n\x03key\x18\x01 \x01(\t\x12=\n\x05value\x18\x02 \x01(\x0b\x32..org.apache.beam.model.pipeline.v1.Environment:\x02\x38\x01\"\xaf\x01\n\x08Pipeline\x12\x41\n\ncomponents\x18\x01 \x01(\x0b\x32-.org.apache.beam.model.pipeline.v1.Components\x12\x1a\n\x12root_transform_ids\x18\x02 \x03(\t\x12\x44\n\x0c\x64isplay_data\x18\x03 \x01(\x0b\x32..org.apache.beam.model.pipeline.v1.DisplayData\"\xb4\x03\n\nPTransform\x12\x13\n\x0bunique_name\x18\x05 \x01(\t\x12=\n\x04spec\x18\x01 \x01(\x0b\x32/.org.apache.beam.model.pipeline.v1.FunctionSpec\x12\x15\n\rsubtransforms\x18\x02 \x03(\t\x12I\n\x06inputs\x18\x03 \x03(\x0b\x32\x39.org.apache.beam.model.pipeline.v1.PTransform.InputsEntry\x12K\n\x07outputs\x18\x04 \x03(\x0b\x32:.org.apache.beam.model.pipeline.v1.PTransform.OutputsEntry\x12\x44\n\x0c\x64isplay_data\x18\x06 \x01(\x0b\x32..org.apache.beam.model.pipeline.v1.DisplayData\x1a-\n\x0bInputsEntry\x12\x0b\n\x03key\x18\x01 \x01(\t\x12\r\n\x05value\x18\x02 \x01(\t:\x02\x38\x01\x1a.\n\x0cOutputsEntry\x12\x0b\n\x03key\x18\x01 \x01(\t\x12\r\n\x05value\x18\x02 \x01(\t:\x02\x38\x01\"\xd0\r\n\x13StandardPTransforms\"\xb1\x03\n\nPrimitives\x12-\n\x06PAR_DO\x10\x00\x1a!\xa2\xb4\xfa\xc2\x05\x1burn:beam:transform:pardo:v1\x12,\n\x07\x46LATTEN\x10\x01\x1a\x1f\xa2\xb4\xfa\xc2\x05\x19\x62\x65\x61m:transform:flatten:v1\x12\x36\n\x0cGROUP_BY_KEY\x10\x02\x1a$\xa2\xb4\xfa\xc2\x05\x1e\x62\x65\x61m:transform:group_by_key:v1\x12,\n\x07IMPULSE\x10\x03\x1a\x1f\xa2\xb4\xfa\xc2\x05\x19\x62\x65\x61m:transform:impulse:v1\x12\x37\n\x0e\x41SSIGN_WINDOWS\x10\x04\x1a#\xa2\xb4\xfa\xc2\x05\x1d\x62\x65\x61m:transform:window_into:v1\x12\x37\n\x0bTEST_STREAM\x10\x05\x1a&\xa2\xb4\xfa\xc2\x05 urn:beam:transform:teststream:v1\x12\x34\n\x0bMAP_WINDOWS\x10\x06\x1a#\xa2\xb4\xfa\xc2\x05\x1d\x62\x65\x61m:transform:map_windows:v1\x12\x38\n\rMERGE_WINDOWS\x10\x07\x1a%\xa2\xb4\xfa\xc2\x05\x1f\x62\x65\x61m:transform:merge_windows:v1\"t\n\x14\x44\x65precatedPrimitives\x12&\n\x04READ\x10\x00\x1a\x1c\xa2\xb4\xfa\xc2\x05\x16\x62\x65\x61m:transform:read:v1\x12\x34\n\x0b\x43REATE_VIEW\x10\x01\x1a#\xa2\xb4\xfa\xc2\x05\x1d\x62\x65\x61m:transform:create_view:v1\"\xf2\x01\n\nComposites\x12<\n\x0f\x43OMBINE_PER_KEY\x10\x00\x1a\'\xa2\xb4\xfa\xc2\x05!beam:transform:combine_per_key:v1\x12>\n\x10\x43OMBINE_GLOBALLY\x10\x01\x1a(\xa2\xb4\xfa\xc2\x05\"beam:transform:combine_globally:v1\x12\x30\n\tRESHUFFLE\x10\x02\x1a!\xa2\xb4\xfa\xc2\x05\x1b\x62\x65\x61m:transform:reshuffle:v1\x12\x34\n\x0bWRITE_FILES\x10\x03\x1a#\xa2\xb4\xfa\xc2\x05\x1d\x62\x65\x61m:transform:write_files:v1\"\xd3\x04\n\x11\x43ombineComponents\x12:\n\x0e\x43OMBINE_PGBKCV\x10\x00\x1a&\xa2\xb4\xfa\xc2\x05 beam:transform:combine_pgbkcv:v1\x12R\n\x1a\x43OMBINE_MERGE_ACCUMULATORS\x10\x01\x1a\x32\xa2\xb4\xfa\xc2\x05,beam:transform:combine_merge_accumulators:v1\x12L\n\x17\x43OMBINE_EXTRACT_OUTPUTS\x10\x02\x1a/\xa2\xb4\xfa\xc2\x05)beam:transform:combine_extract_outputs:v1\x12R\n\x1a\x43OMBINE_PER_KEY_PRECOMBINE\x10\x03\x1a\x32\xa2\xb4\xfa\xc2\x05,beam:transform:combine_per_key_precombine:v1\x12\x62\n\"COMBINE_PER_KEY_MERGE_ACCUMULATORS\x10\x04\x1a:\xa2\xb4\xfa\xc2\x05\x34\x62\x65\x61m:transform:combine_per_key_merge_accumulators:v1\x12\\\n\x1f\x43OMBINE_PER_KEY_EXTRACT_OUTPUTS\x10\x05\x1a\x37\xa2\xb4\xfa\xc2\x05\x31\x62\x65\x61m:transform:combine_per_key_extract_outputs:v1\x12J\n\x16\x43OMBINE_GROUPED_VALUES\x10\x06\x1a.\xa2\xb4\xfa\xc2\x05(beam:transform:combine_grouped_values:v1\"\xc3\x02\n\x19SplittableParDoComponents\x12L\n\x15PAIR_WITH_RESTRICTION\x10\x00\x1a\x31\xa2\xb4\xfa\xc2\x05+beam:transform:sdf_pair_with_restriction:v1\x12\x44\n\x11SPLIT_RESTRICTION\x10\x01\x1a-\xa2\xb4\xfa\xc2\x05\'beam:transform:sdf_split_restriction:v1\x12N\n\x16PROCESS_KEYED_ELEMENTS\x10\x02\x1a\x32\xa2\xb4\xfa\xc2\x05,beam:transform:sdf_process_keyed_elements:v1\x12\x42\n\x10PROCESS_ELEMENTS\x10\x03\x1a,\xa2\xb4\xfa\xc2\x05&beam:transform:sdf_process_elements:v1\"\x82\x01\n\x16StandardSideInputTypes\"h\n\x04\x45num\x12/\n\x08ITERABLE\x10\x00\x1a!\xa2\xb4\xfa\xc2\x05\x1b\x62\x65\x61m:side_input:iterable:v1\x12/\n\x08MULTIMAP\x10\x01\x1a!\xa2\xb4\xfa\xc2\x05\x1b\x62\x65\x61m:side_input:multimap:v1\"\xe0\x01\n\x0bPCollection\x12\x13\n\x0bunique_name\x18\x01 \x01(\t\x12\x10\n\x08\x63oder_id\x18\x02 \x01(\t\x12\x45\n\nis_bounded\x18\x03 \x01(\x0e\x32\x31.org.apache.beam.model.pipeline.v1.IsBounded.Enum\x12\x1d\n\x15windowing_strategy_id\x18\x04 \x01(\t\x12\x44\n\x0c\x64isplay_data\x18\x05 \x01(\x0b\x32..org.apache.beam.model.pipeline.v1.DisplayData\"\x89\x06\n\x0cParDoPayload\x12\x41\n\x05\x64o_fn\x18\x01 \x01(\x0b\x32\x32.org.apache.beam.model.pipeline.v1.SdkFunctionSpec\x12@\n\nparameters\x18\x02 \x03(\x0b\x32,.org.apache.beam.model.pipeline.v1.Parameter\x12T\n\x0bside_inputs\x18\x03 \x03(\x0b\x32?.org.apache.beam.model.pipeline.v1.ParDoPayload.SideInputsEntry\x12T\n\x0bstate_specs\x18\x04 \x03(\x0b\x32?.org.apache.beam.model.pipeline.v1.ParDoPayload.StateSpecsEntry\x12T\n\x0btimer_specs\x18\x05 \x03(\x0b\x32?.org.apache.beam.model.pipeline.v1.ParDoPayload.TimerSpecsEntry\x12\x12\n\nsplittable\x18\x06 \x01(\x08\x12\x1c\n\x14restriction_coder_id\x18\x07 \x01(\t\x12\x1d\n\x15requests_finalization\x18\x08 \x01(\x08\x1a_\n\x0fSideInputsEntry\x12\x0b\n\x03key\x18\x01 \x01(\t\x12;\n\x05value\x18\x02 \x01(\x0b\x32,.org.apache.beam.model.pipeline.v1.SideInput:\x02\x38\x01\x1a_\n\x0fStateSpecsEntry\x12\x0b\n\x03key\x18\x01 \x01(\t\x12;\n\x05value\x18\x02 \x01(\x0b\x32,.org.apache.beam.model.pipeline.v1.StateSpec:\x02\x38\x01\x1a_\n\x0fTimerSpecsEntry\x12\x0b\n\x03key\x18\x01 \x01(\t\x12;\n\x05value\x18\x02 \x01(\x0b\x32,.org.apache.beam.model.pipeline.v1.TimerSpec:\x02\x38\x01\"\xad\x01\n\tParameter\x12\x44\n\x04type\x18\x01 \x01(\x0e\x32\x36.org.apache.beam.model.pipeline.v1.Parameter.Type.Enum\x1aZ\n\x04Type\"R\n\x04\x45num\x12\x0f\n\x0bUNSPECIFIED\x10\x00\x12\n\n\x06WINDOW\x10\x01\x12\x14\n\x10PIPELINE_OPTIONS\x10\x02\x12\x17\n\x13RESTRICTION_TRACKER\x10\x03\"\xfc\x02\n\tStateSpec\x12G\n\nvalue_spec\x18\x01 \x01(\x0b\x32\x31.org.apache.beam.model.pipeline.v1.ValueStateSpecH\x00\x12\x43\n\x08\x62\x61g_spec\x18\x02 \x01(\x0b\x32/.org.apache.beam.model.pipeline.v1.BagStateSpecH\x00\x12O\n\x0e\x63ombining_spec\x18\x03 \x01(\x0b\x32\x35.org.apache.beam.model.pipeline.v1.CombiningStateSpecH\x00\x12\x43\n\x08map_spec\x18\x04 \x01(\x0b\x32/.org.apache.beam.model.pipeline.v1.MapStateSpecH\x00\x12\x43\n\x08set_spec\x18\x05 \x01(\x0b\x32/.org.apache.beam.model.pipeline.v1.SetStateSpecH\x00\x42\x06\n\x04spec\"\"\n\x0eValueStateSpec\x12\x10\n\x08\x63oder_id\x18\x01 \x01(\t\"(\n\x0c\x42\x61gStateSpec\x12\x18\n\x10\x65lement_coder_id\x18\x01 \x01(\t\"z\n\x12\x43ombiningStateSpec\x12\x1c\n\x14\x61\x63\x63umulator_coder_id\x18\x01 \x01(\t\x12\x46\n\ncombine_fn\x18\x02 \x01(\x0b\x32\x32.org.apache.beam.model.pipeline.v1.SdkFunctionSpec\"<\n\x0cMapStateSpec\x12\x14\n\x0ckey_coder_id\x18\x01 \x01(\t\x12\x16\n\x0evalue_coder_id\x18\x02 \x01(\t\"(\n\x0cSetStateSpec\x12\x18\n\x10\x65lement_coder_id\x18\x01 \x01(\t\"l\n\tTimerSpec\x12G\n\x0btime_domain\x18\x01 \x01(\x0e\x32\x32.org.apache.beam.model.pipeline.v1.TimeDomain.Enum\x12\x16\n\x0etimer_coder_id\x18\x02 \x01(\t\"@\n\tIsBounded\"3\n\x04\x45num\x12\x0f\n\x0bUNSPECIFIED\x10\x00\x12\r\n\tUNBOUNDED\x10\x01\x12\x0b\n\x07\x42OUNDED\x10\x02\"\x98\x01\n\x0bReadPayload\x12\x42\n\x06source\x18\x01 \x01(\x0b\x32\x32.org.apache.beam.model.pipeline.v1.SdkFunctionSpec\x12\x45\n\nis_bounded\x18\x02 \x01(\x0e\x32\x31.org.apache.beam.model.pipeline.v1.IsBounded.Enum\"Z\n\x11WindowIntoPayload\x12\x45\n\twindow_fn\x18\x01 \x01(\x0b\x32\x32.org.apache.beam.model.pipeline.v1.SdkFunctionSpec\"v\n\x0e\x43ombinePayload\x12\x46\n\ncombine_fn\x18\x01 \x01(\x0b\x32\x32.org.apache.beam.model.pipeline.v1.SdkFunctionSpec\x12\x1c\n\x14\x61\x63\x63umulator_coder_id\x18\x02 \x01(\t\"\xca\x05\n\x11TestStreamPayload\x12\x10\n\x08\x63oder_id\x18\x01 \x01(\t\x12J\n\x06\x65vents\x18\x02 \x03(\x0b\x32:.org.apache.beam.model.pipeline.v1.TestStreamPayload.Event\x1a\x94\x04\n\x05\x45vent\x12\x66\n\x0fwatermark_event\x18\x01 \x01(\x0b\x32K.org.apache.beam.model.pipeline.v1.TestStreamPayload.Event.AdvanceWatermarkH\x00\x12q\n\x15processing_time_event\x18\x02 \x01(\x0b\x32P.org.apache.beam.model.pipeline.v1.TestStreamPayload.Event.AdvanceProcessingTimeH\x00\x12_\n\relement_event\x18\x03 \x01(\x0b\x32\x46.org.apache.beam.model.pipeline.v1.TestStreamPayload.Event.AddElementsH\x00\x1a)\n\x10\x41\x64vanceWatermark\x12\x15\n\rnew_watermark\x18\x01 \x01(\x03\x1a\x31\n\x15\x41\x64vanceProcessingTime\x12\x18\n\x10\x61\x64vance_duration\x18\x01 \x01(\x03\x1ah\n\x0b\x41\x64\x64\x45lements\x12Y\n\x08\x65lements\x18\x01 \x03(\x0b\x32G.org.apache.beam.model.pipeline.v1.TestStreamPayload.TimestampedElementB\x07\n\x05\x65vent\x1a@\n\x12TimestampedElement\x12\x17\n\x0f\x65ncoded_element\x18\x01 \x01(\x0c\x12\x11\n\ttimestamp\x18\x02 \x01(\x03\"\x9b\x03\n\x11WriteFilesPayload\x12@\n\x04sink\x18\x01 \x01(\x0b\x32\x32.org.apache.beam.model.pipeline.v1.SdkFunctionSpec\x12K\n\x0f\x66ormat_function\x18\x02 \x01(\x0b\x32\x32.org.apache.beam.model.pipeline.v1.SdkFunctionSpec\x12\x17\n\x0fwindowed_writes\x18\x03 \x01(\x08\x12\"\n\x1arunner_determined_sharding\x18\x04 \x01(\x08\x12Y\n\x0bside_inputs\x18\x05 \x03(\x0b\x32\x44.org.apache.beam.model.pipeline.v1.WriteFilesPayload.SideInputsEntry\x1a_\n\x0fSideInputsEntry\x12\x0b\n\x03key\x18\x01 \x01(\t\x12;\n\x05value\x18\x02 \x01(\x0b\x32,.org.apache.beam.model.pipeline.v1.SideInput:\x02\x38\x01\"f\n\x05\x43oder\x12@\n\x04spec\x18\x01 \x01(\x0b\x32\x32.org.apache.beam.model.pipeline.v1.SdkFunctionSpec\x12\x1b\n\x13\x63omponent_coder_ids\x18\x02 \x03(\t\"\xb7\x03\n\x0eStandardCoders\"\xa4\x03\n\x04\x45num\x12$\n\x05\x42YTES\x10\x00\x1a\x19\xa2\xb4\xfa\xc2\x05\x13\x62\x65\x61m:coder:bytes:v1\x12\x1e\n\x02KV\x10\x01\x1a\x16\xa2\xb4\xfa\xc2\x05\x10\x62\x65\x61m:coder:kv:v1\x12&\n\x06VARINT\x10\x02\x1a\x1a\xa2\xb4\xfa\xc2\x05\x14\x62\x65\x61m:coder:varint:v1\x12*\n\x08ITERABLE\x10\x03\x1a\x1c\xa2\xb4\xfa\xc2\x05\x16\x62\x65\x61m:coder:iterable:v1\x12$\n\x05TIMER\x10\x04\x1a\x19\xa2\xb4\xfa\xc2\x05\x13\x62\x65\x61m:coder:timer:v1\x12\x38\n\x0fINTERVAL_WINDOW\x10\x05\x1a#\xa2\xb4\xfa\xc2\x05\x1d\x62\x65\x61m:coder:interval_window:v1\x12\x34\n\rLENGTH_PREFIX\x10\x06\x1a!\xa2\xb4\xfa\xc2\x05\x1b\x62\x65\x61m:coder:length_prefix:v1\x12\x34\n\rGLOBAL_WINDOW\x10\x07\x1a!\xa2\xb4\xfa\xc2\x05\x1b\x62\x65\x61m:coder:global_window:v1\x12\x36\n\x0eWINDOWED_VALUE\x10\x08\x1a\"\xa2\xb4\xfa\xc2\x05\x1c\x62\x65\x61m:coder:windowed_value:v1\"\xf5\x04\n\x11WindowingStrategy\x12\x45\n\twindow_fn\x18\x01 \x01(\x0b\x32\x32.org.apache.beam.model.pipeline.v1.SdkFunctionSpec\x12I\n\x0cmerge_status\x18\x02 \x01(\x0e\x32\x33.org.apache.beam.model.pipeline.v1.MergeStatus.Enum\x12\x17\n\x0fwindow_coder_id\x18\x03 \x01(\t\x12;\n\x07trigger\x18\x04 \x01(\x0b\x32*.org.apache.beam.model.pipeline.v1.Trigger\x12S\n\x11\x61\x63\x63umulation_mode\x18\x05 \x01(\x0e\x32\x38.org.apache.beam.model.pipeline.v1.AccumulationMode.Enum\x12G\n\x0boutput_time\x18\x06 \x01(\x0e\x32\x32.org.apache.beam.model.pipeline.v1.OutputTime.Enum\x12Q\n\x10\x63losing_behavior\x18\x07 \x01(\x0e\x32\x37.org.apache.beam.model.pipeline.v1.ClosingBehavior.Enum\x12\x18\n\x10\x61llowed_lateness\x18\x08 \x01(\x03\x12N\n\x0eOnTimeBehavior\x18\t \x01(\x0e\x32\x36.org.apache.beam.model.pipeline.v1.OnTimeBehavior.Enum\x12\x1d\n\x15\x61ssigns_to_one_window\x18\n \x01(\x08\"\\\n\x0bMergeStatus\"M\n\x04\x45num\x12\x0f\n\x0bUNSPECIFIED\x10\x00\x12\x0f\n\x0bNON_MERGING\x10\x01\x12\x0f\n\x0bNEEDS_MERGE\x10\x02\x12\x12\n\x0e\x41LREADY_MERGED\x10\x03\"M\n\x10\x41\x63\x63umulationMode\"9\n\x04\x45num\x12\x0f\n\x0bUNSPECIFIED\x10\x00\x12\x0e\n\nDISCARDING\x10\x01\x12\x10\n\x0c\x41\x43\x43UMULATING\x10\x02\"Q\n\x0f\x43losingBehavior\">\n\x04\x45num\x12\x0f\n\x0bUNSPECIFIED\x10\x00\x12\x0f\n\x0b\x45MIT_ALWAYS\x10\x01\x12\x14\n\x10\x45MIT_IF_NONEMPTY\x10\x02\"P\n\x0eOnTimeBehavior\">\n\x04\x45num\x12\x0f\n\x0bUNSPECIFIED\x10\x00\x12\x0f\n\x0b\x46IRE_ALWAYS\x10\x01\x12\x14\n\x10\x46IRE_IF_NONEMPTY\x10\x02\"b\n\nOutputTime\"T\n\x04\x45num\x12\x0f\n\x0bUNSPECIFIED\x10\x00\x12\x11\n\rEND_OF_WINDOW\x10\x01\x12\x12\n\x0eLATEST_IN_PANE\x10\x02\x12\x14\n\x10\x45\x41RLIEST_IN_PANE\x10\x03\"l\n\nTimeDomain\"^\n\x04\x45num\x12\x0f\n\x0bUNSPECIFIED\x10\x00\x12\x0e\n\nEVENT_TIME\x10\x01\x12\x13\n\x0fPROCESSING_TIME\x10\x02\x12 \n\x1cSYNCHRONIZED_PROCESSING_TIME\x10\x03\"\x82\x0e\n\x07Trigger\x12H\n\tafter_all\x18\x01 \x01(\x0b\x32\x33.org.apache.beam.model.pipeline.v1.Trigger.AfterAllH\x00\x12H\n\tafter_any\x18\x02 \x01(\x0b\x32\x33.org.apache.beam.model.pipeline.v1.Trigger.AfterAnyH\x00\x12J\n\nafter_each\x18\x03 \x01(\x0b\x32\x34.org.apache.beam.model.pipeline.v1.Trigger.AfterEachH\x00\x12Z\n\x13\x61\x66ter_end_of_window\x18\x04 \x01(\x0b\x32;.org.apache.beam.model.pipeline.v1.Trigger.AfterEndOfWindowH\x00\x12_\n\x15\x61\x66ter_processing_time\x18\x05 \x01(\x0b\x32>.org.apache.beam.model.pipeline.v1.Trigger.AfterProcessingTimeH\x00\x12x\n\"after_synchronized_processing_time\x18\x06 \x01(\x0b\x32J.org.apache.beam.model.pipeline.v1.Trigger.AfterSynchronizedProcessingTimeH\x00\x12\x43\n\x06\x61lways\x18\x0c \x01(\x0b\x32\x31.org.apache.beam.model.pipeline.v1.Trigger.AlwaysH\x00\x12\x45\n\x07\x64\x65\x66\x61ult\x18\x07 \x01(\x0b\x32\x32.org.apache.beam.model.pipeline.v1.Trigger.DefaultH\x00\x12P\n\relement_count\x18\x08 \x01(\x0b\x32\x37.org.apache.beam.model.pipeline.v1.Trigger.ElementCountH\x00\x12\x41\n\x05never\x18\t \x01(\x0b\x32\x30.org.apache.beam.model.pipeline.v1.Trigger.NeverH\x00\x12J\n\nor_finally\x18\n \x01(\x0b\x32\x34.org.apache.beam.model.pipeline.v1.Trigger.OrFinallyH\x00\x12\x43\n\x06repeat\x18\x0b \x01(\x0b\x32\x31.org.apache.beam.model.pipeline.v1.Trigger.RepeatH\x00\x1aK\n\x08\x41\x66terAll\x12?\n\x0bsubtriggers\x18\x01 \x03(\x0b\x32*.org.apache.beam.model.pipeline.v1.Trigger\x1aK\n\x08\x41\x66terAny\x12?\n\x0bsubtriggers\x18\x01 \x03(\x0b\x32*.org.apache.beam.model.pipeline.v1.Trigger\x1aL\n\tAfterEach\x12?\n\x0bsubtriggers\x18\x01 \x03(\x0b\x32*.org.apache.beam.model.pipeline.v1.Trigger\x1a\x97\x01\n\x10\x41\x66terEndOfWindow\x12\x41\n\rearly_firings\x18\x01 \x01(\x0b\x32*.org.apache.beam.model.pipeline.v1.Trigger\x12@\n\x0clate_firings\x18\x02 \x01(\x0b\x32*.org.apache.beam.model.pipeline.v1.Trigger\x1aj\n\x13\x41\x66terProcessingTime\x12S\n\x14timestamp_transforms\x18\x01 \x03(\x0b\x32\x35.org.apache.beam.model.pipeline.v1.TimestampTransform\x1a!\n\x1f\x41\x66terSynchronizedProcessingTime\x1a\t\n\x07\x44\x65\x66\x61ult\x1a%\n\x0c\x45lementCount\x12\x15\n\relement_count\x18\x01 \x01(\x05\x1a\x07\n\x05Never\x1a\x08\n\x06\x41lways\x1a\x82\x01\n\tOrFinally\x12\x38\n\x04main\x18\x01 \x01(\x0b\x32*.org.apache.beam.model.pipeline.v1.Trigger\x12;\n\x07\x66inally\x18\x02 \x01(\x0b\x32*.org.apache.beam.model.pipeline.v1.Trigger\x1aH\n\x06Repeat\x12>\n\nsubtrigger\x18\x01 \x01(\x0b\x32*.org.apache.beam.model.pipeline.v1.TriggerB\t\n\x07trigger\"\x96\x02\n\x12TimestampTransform\x12L\n\x05\x64\x65lay\x18\x01 \x01(\x0b\x32;.org.apache.beam.model.pipeline.v1.TimestampTransform.DelayH\x00\x12Q\n\x08\x61lign_to\x18\x02 \x01(\x0b\x32=.org.apache.beam.model.pipeline.v1.TimestampTransform.AlignToH\x00\x1a\x1d\n\x05\x44\x65lay\x12\x14\n\x0c\x64\x65lay_millis\x18\x01 \x01(\x03\x1a)\n\x07\x41lignTo\x12\x0e\n\x06period\x18\x03 \x01(\x03\x12\x0e\n\x06offset\x18\x04 \x01(\x03\x42\x15\n\x13timestamp_transform\"\xe8\x01\n\tSideInput\x12G\n\x0e\x61\x63\x63\x65ss_pattern\x18\x01 \x01(\x0b\x32/.org.apache.beam.model.pipeline.v1.FunctionSpec\x12\x43\n\x07view_fn\x18\x02 \x01(\x0b\x32\x32.org.apache.beam.model.pipeline.v1.SdkFunctionSpec\x12M\n\x11window_mapping_fn\x18\x03 \x01(\x0b\x32\x32.org.apache.beam.model.pipeline.v1.SdkFunctionSpec\"8\n\x0b\x45nvironment\x12\x0b\n\x03url\x18\x01 \x01(\t\x12\x0b\n\x03urn\x18\x02 \x01(\t\x12\x0f\n\x07payload\x18\x03 \x01(\x0c\"\x9f\x01\n\x14StandardEnvironments\"\x86\x01\n\x0c\x45nvironments\x12$\n\x06\x44OCKER\x10\x00\x1a\x18\xa2\xb4\xfa\xc2\x05\x12\x62\x65\x61m:env:docker:v1\x12&\n\x07PROCESS\x10\x01\x1a\x19\xa2\xb4\xfa\xc2\x05\x13\x62\x65\x61m:env:process:v1\x12(\n\x08\x45XTERNAL\x10\x02\x1a\x1a\xa2\xb4\xfa\xc2\x05\x14\x62\x65\x61m:env:external:v1\"(\n\rDockerPayload\x12\x17\n\x0f\x63ontainer_image\x18\x01 \x01(\t\"\xb0\x01\n\x0eProcessPayload\x12\n\n\x02os\x18\x01 \x01(\t\x12\x0c\n\x04\x61rch\x18\x02 \x01(\t\x12\x0f\n\x07\x63ommand\x18\x03 \x01(\t\x12G\n\x03\x65nv\x18\x04 \x03(\x0b\x32:.org.apache.beam.model.pipeline.v1.ProcessPayload.EnvEntry\x1a*\n\x08\x45nvEntry\x12\x0b\n\x03key\x18\x01 \x01(\t\x12\r\n\x05value\x18\x02 \x01(\t:\x02\x38\x01\"h\n\x0fSdkFunctionSpec\x12=\n\x04spec\x18\x01 \x01(\x0b\x32/.org.apache.beam.model.pipeline.v1.FunctionSpec\x12\x16\n\x0e\x65nvironment_id\x18\x02 \x01(\t\",\n\x0c\x46unctionSpec\x12\x0b\n\x03urn\x18\x01 \x01(\t\x12\x0f\n\x07payload\x18\x03 \x01(\x0c\"\xa1\x04\n\x0b\x44isplayData\x12\x42\n\x05items\x18\x01 \x03(\x0b\x32\x33.org.apache.beam.model.pipeline.v1.DisplayData.Item\x1a\x46\n\nIdentifier\x12\x14\n\x0ctransform_id\x18\x01 \x01(\t\x12\x15\n\rtransform_urn\x18\x02 \x01(\t\x12\x0b\n\x03key\x18\x03 \x01(\t\x1a\x86\x02\n\x04Item\x12\x45\n\x02id\x18\x01 \x01(\x0b\x32\x39.org.apache.beam.model.pipeline.v1.DisplayData.Identifier\x12\x46\n\x04type\x18\x02 \x01(\x0e\x32\x38.org.apache.beam.model.pipeline.v1.DisplayData.Type.Enum\x12#\n\x05value\x18\x03 \x01(\x0b\x32\x14.google.protobuf.Any\x12)\n\x0bshort_value\x18\x04 \x01(\x0b\x32\x14.google.protobuf.Any\x12\r\n\x05label\x18\x05 \x01(\t\x12\x10\n\x08link_url\x18\x06 \x01(\t\x1a}\n\x04Type\"u\n\x04\x45num\x12\x0f\n\x0bUNSPECIFIED\x10\x00\x12\n\n\x06STRING\x10\x01\x12\x0b\n\x07INTEGER\x10\x02\x12\t\n\x05\x46LOAT\x10\x03\x12\x0b\n\x07\x42OOLEAN\x10\x04\x12\r\n\tTIMESTAMP\x10\x05\x12\x0c\n\x08\x44URATION\x10\x06\x12\x0e\n\nJAVA_CLASS\x10\x07\"\x92\x07\n\x15MessageWithComponents\x12\x41\n\ncomponents\x18\x01 \x01(\x0b\x32-.org.apache.beam.model.pipeline.v1.Components\x12\x39\n\x05\x63oder\x18\x02 \x01(\x0b\x32(.org.apache.beam.model.pipeline.v1.CoderH\x00\x12L\n\x0f\x63ombine_payload\x18\x03 \x01(\x0b\x32\x31.org.apache.beam.model.pipeline.v1.CombinePayloadH\x00\x12O\n\x11sdk_function_spec\x18\x04 \x01(\x0b\x32\x32.org.apache.beam.model.pipeline.v1.SdkFunctionSpecH\x00\x12I\n\x0epar_do_payload\x18\x06 \x01(\x0b\x32/.org.apache.beam.model.pipeline.v1.ParDoPayloadH\x00\x12\x43\n\nptransform\x18\x07 \x01(\x0b\x32-.org.apache.beam.model.pipeline.v1.PTransformH\x00\x12\x45\n\x0bpcollection\x18\x08 \x01(\x0b\x32..org.apache.beam.model.pipeline.v1.PCollectionH\x00\x12\x46\n\x0cread_payload\x18\t \x01(\x0b\x32..org.apache.beam.model.pipeline.v1.ReadPayloadH\x00\x12\x42\n\nside_input\x18\x0b \x01(\x0b\x32,.org.apache.beam.model.pipeline.v1.SideInputH\x00\x12S\n\x13window_into_payload\x18\x0c \x01(\x0b\x32\x34.org.apache.beam.model.pipeline.v1.WindowIntoPayloadH\x00\x12R\n\x12windowing_strategy\x18\r \x01(\x0b\x32\x34.org.apache.beam.model.pipeline.v1.WindowingStrategyH\x00\x12H\n\rfunction_spec\x18\x0e \x01(\x0b\x32/.org.apache.beam.model.pipeline.v1.FunctionSpecH\x00\x42\x06\n\x04root\"\x86\x05\n\x16\x45xecutableStagePayload\x12\x43\n\x0b\x65nvironment\x18\x01 \x01(\x0b\x32..org.apache.beam.model.pipeline.v1.Environment\x12\r\n\x05input\x18\x02 \x01(\t\x12Z\n\x0bside_inputs\x18\x03 \x03(\x0b\x32\x45.org.apache.beam.model.pipeline.v1.ExecutableStagePayload.SideInputId\x12\x12\n\ntransforms\x18\x04 \x03(\t\x12\x0f\n\x07outputs\x18\x05 \x03(\t\x12\x41\n\ncomponents\x18\x06 \x01(\x0b\x32-.org.apache.beam.model.pipeline.v1.Components\x12Z\n\x0buser_states\x18\x07 \x03(\x0b\x32\x45.org.apache.beam.model.pipeline.v1.ExecutableStagePayload.UserStateId\x12Q\n\x06timers\x18\x08 \x03(\x0b\x32\x41.org.apache.beam.model.pipeline.v1.ExecutableStagePayload.TimerId\x1a\x37\n\x0bSideInputId\x12\x14\n\x0ctransform_id\x18\x01 \x01(\t\x12\x12\n\nlocal_name\x18\x02 \x01(\t\x1a\x37\n\x0bUserStateId\x12\x14\n\x0ctransform_id\x18\x01 \x01(\t\x12\x12\n\nlocal_name\x18\x02 \x01(\t\x1a\x33\n\x07TimerId\x12\x14\n\x0ctransform_id\x18\x01 \x01(\t\x12\x12\n\nlocal_name\x18\x02 \x01(\t:6\n\x08\x62\x65\x61m_urn\x12!.google.protobuf.EnumValueOptions\x18\xc4\xa6\xafX \x01(\t:;\n\rbeam_constant\x12!.google.protobuf.EnumValueOptions\x18\xc5\xa6\xafX \x01(\tB;\n!org.apache.beam.model.pipeline.v1B\tRunnerApiZ\x0bpipeline_v1b\x06proto3')
   ,
   dependencies=[google_dot_protobuf_dot_any__pb2.DESCRIPTOR,google_dot_protobuf_dot_descriptor__pb2.DESCRIPTOR,])
+_sym_db.RegisterFileDescriptor(DESCRIPTOR)
 
 
 BEAM_URN_FIELD_NUMBER = 185324356
 beam_urn = _descriptor.FieldDescriptor(
   name='beam_urn', full_name='org.apache.beam.model.pipeline.v1.beam_urn', index=0,
   number=185324356, type=9, cpp_type=9, label=1,
   has_default_value=False, default_value=_b("").decode('utf-8'),
   message_type=None, enum_type=None, containing_type=None,
   is_extension=True, extension_scope=None,
-  options=None, file=DESCRIPTOR)
+  options=None)
+BEAM_CONSTANT_FIELD_NUMBER = 185324357
+beam_constant = _descriptor.FieldDescriptor(
+  name='beam_constant', full_name='org.apache.beam.model.pipeline.v1.beam_constant', index=1,
+  number=185324357, type=9, cpp_type=9, label=1,
+  has_default_value=False, default_value=_b("").decode('utf-8'),
+  message_type=None, enum_type=None, containing_type=None,
+  is_extension=True, extension_scope=None,
+  options=None)
+
+_BEAMCONSTANTS_CONSTANTS = _descriptor.EnumDescriptor(
+  name='Constants',
+  full_name='org.apache.beam.model.pipeline.v1.BeamConstants.Constants',
+  filename=None,
+  file=DESCRIPTOR,
+  values=[
+    _descriptor.EnumValueDescriptor(
+      name='MIN_TIMESTAMP_MILLIS', index=0, number=0,
+      options=_descriptor._ParseOptions(descriptor_pb2.EnumValueOptions(), _b('\252\264\372\302\005\021-9223372036854775')),
+      type=None),
+    _descriptor.EnumValueDescriptor(
+      name='MAX_TIMESTAMP_MILLIS', index=1, number=1,
+      options=_descriptor._ParseOptions(descriptor_pb2.EnumValueOptions(), _b('\252\264\372\302\005\0209223372036854775')),
+      type=None),
+    _descriptor.EnumValueDescriptor(
+      name='GLOBAL_WINDOW_MAX_TIMESTAMP_MILLIS', index=2, number=2,
+      options=_descriptor._ParseOptions(descriptor_pb2.EnumValueOptions(), _b('\252\264\372\302\005\0209223371950454775')),
+      type=None),
+  ],
+  containing_type=None,
+  options=None,
+  serialized_start=140,
+  serialized_end=316,
+)
+_sym_db.RegisterEnumDescriptor(_BEAMCONSTANTS_CONSTANTS)
 
 _STANDARDPTRANSFORMS_PRIMITIVES = _descriptor.EnumDescriptor(
   name='Primitives',
   full_name='org.apache.beam.model.pipeline.v1.StandardPTransforms.Primitives',
   filename=None,
   file=DESCRIPTOR,
   values=[
@@ -72,16 +107,16 @@
     _descriptor.EnumValueDescriptor(
       name='MERGE_WINDOWS', index=7, number=7,
       options=_descriptor._ParseOptions(descriptor_pb2.EnumValueOptions(), _b('\242\264\372\302\005\037beam:transform:merge_windows:v1')),
       type=None),
   ],
   containing_type=None,
   options=None,
-  serialized_start=2632,
-  serialized_end=3065,
+  serialized_start=1912,
+  serialized_end=2345,
 )
 _sym_db.RegisterEnumDescriptor(_STANDARDPTRANSFORMS_PRIMITIVES)
 
 _STANDARDPTRANSFORMS_DEPRECATEDPRIMITIVES = _descriptor.EnumDescriptor(
   name='DeprecatedPrimitives',
   full_name='org.apache.beam.model.pipeline.v1.StandardPTransforms.DeprecatedPrimitives',
   filename=None,
@@ -94,16 +129,16 @@
     _descriptor.EnumValueDescriptor(
       name='CREATE_VIEW', index=1, number=1,
       options=_descriptor._ParseOptions(descriptor_pb2.EnumValueOptions(), _b('\242\264\372\302\005\035beam:transform:create_view:v1')),
       type=None),
   ],
   containing_type=None,
   options=None,
-  serialized_start=3067,
-  serialized_end=3183,
+  serialized_start=2347,
+  serialized_end=2463,
 )
 _sym_db.RegisterEnumDescriptor(_STANDARDPTRANSFORMS_DEPRECATEDPRIMITIVES)
 
 _STANDARDPTRANSFORMS_COMPOSITES = _descriptor.EnumDescriptor(
   name='Composites',
   full_name='org.apache.beam.model.pipeline.v1.StandardPTransforms.Composites',
   filename=None,
@@ -124,16 +159,16 @@
     _descriptor.EnumValueDescriptor(
       name='WRITE_FILES', index=3, number=3,
       options=_descriptor._ParseOptions(descriptor_pb2.EnumValueOptions(), _b('\242\264\372\302\005\035beam:transform:write_files:v1')),
       type=None),
   ],
   containing_type=None,
   options=None,
-  serialized_start=3186,
-  serialized_end=3428,
+  serialized_start=2466,
+  serialized_end=2708,
 )
 _sym_db.RegisterEnumDescriptor(_STANDARDPTRANSFORMS_COMPOSITES)
 
 _STANDARDPTRANSFORMS_COMBINECOMPONENTS = _descriptor.EnumDescriptor(
   name='CombineComponents',
   full_name='org.apache.beam.model.pipeline.v1.StandardPTransforms.CombineComponents',
   filename=None,
@@ -166,16 +201,16 @@
     _descriptor.EnumValueDescriptor(
       name='COMBINE_GROUPED_VALUES', index=6, number=6,
       options=_descriptor._ParseOptions(descriptor_pb2.EnumValueOptions(), _b('\242\264\372\302\005(beam:transform:combine_grouped_values:v1')),
       type=None),
   ],
   containing_type=None,
   options=None,
-  serialized_start=3431,
-  serialized_end=4026,
+  serialized_start=2711,
+  serialized_end=3306,
 )
 _sym_db.RegisterEnumDescriptor(_STANDARDPTRANSFORMS_COMBINECOMPONENTS)
 
 _STANDARDPTRANSFORMS_SPLITTABLEPARDOCOMPONENTS = _descriptor.EnumDescriptor(
   name='SplittableParDoComponents',
   full_name='org.apache.beam.model.pipeline.v1.StandardPTransforms.SplittableParDoComponents',
   filename=None,
@@ -196,16 +231,16 @@
     _descriptor.EnumValueDescriptor(
       name='PROCESS_ELEMENTS', index=3, number=3,
       options=_descriptor._ParseOptions(descriptor_pb2.EnumValueOptions(), _b('\242\264\372\302\005&beam:transform:sdf_process_elements:v1')),
       type=None),
   ],
   containing_type=None,
   options=None,
-  serialized_start=4029,
-  serialized_end=4352,
+  serialized_start=3309,
+  serialized_end=3632,
 )
 _sym_db.RegisterEnumDescriptor(_STANDARDPTRANSFORMS_SPLITTABLEPARDOCOMPONENTS)
 
 _STANDARDSIDEINPUTTYPES_ENUM = _descriptor.EnumDescriptor(
   name='Enum',
   full_name='org.apache.beam.model.pipeline.v1.StandardSideInputTypes.Enum',
   filename=None,
@@ -218,16 +253,16 @@
     _descriptor.EnumValueDescriptor(
       name='MULTIMAP', index=1, number=1,
       options=_descriptor._ParseOptions(descriptor_pb2.EnumValueOptions(), _b('\242\264\372\302\005\033beam:side_input:multimap:v1')),
       type=None),
   ],
   containing_type=None,
   options=None,
-  serialized_start=4381,
-  serialized_end=4485,
+  serialized_start=3661,
+  serialized_end=3765,
 )
 _sym_db.RegisterEnumDescriptor(_STANDARDSIDEINPUTTYPES_ENUM)
 
 _PARAMETER_TYPE_ENUM = _descriptor.EnumDescriptor(
   name='Enum',
   full_name='org.apache.beam.model.pipeline.v1.Parameter.Type.Enum',
   filename=None,
@@ -248,16 +283,16 @@
     _descriptor.EnumValueDescriptor(
       name='RESTRICTION_TRACKER', index=3, number=3,
       options=None,
       type=None),
   ],
   containing_type=None,
   options=None,
-  serialized_start=6204,
-  serialized_end=6286,
+  serialized_start=4866,
+  serialized_end=4948,
 )
 _sym_db.RegisterEnumDescriptor(_PARAMETER_TYPE_ENUM)
 
 _ISBOUNDED_ENUM = _descriptor.EnumDescriptor(
   name='Enum',
   full_name='org.apache.beam.model.pipeline.v1.IsBounded.Enum',
   filename=None,
@@ -274,16 +309,16 @@
     _descriptor.EnumValueDescriptor(
       name='BOUNDED', index=2, number=2,
       options=None,
       type=None),
   ],
   containing_type=None,
   options=None,
-  serialized_start=7076,
-  serialized_end=7127,
+  serialized_start=5762,
+  serialized_end=5813,
 )
 _sym_db.RegisterEnumDescriptor(_ISBOUNDED_ENUM)
 
 _STANDARDCODERS_ENUM = _descriptor.EnumDescriptor(
   name='Enum',
   full_name='org.apache.beam.model.pipeline.v1.StandardCoders.Enum',
   filename=None,
@@ -324,16 +359,16 @@
     _descriptor.EnumValueDescriptor(
       name='WINDOWED_VALUE', index=8, number=8,
       options=_descriptor._ParseOptions(descriptor_pb2.EnumValueOptions(), _b('\242\264\372\302\005\034beam:coder:windowed_value:v1')),
       type=None),
   ],
   containing_type=None,
   options=None,
-  serialized_start=8751,
-  serialized_end=9171,
+  serialized_start=7437,
+  serialized_end=7857,
 )
 _sym_db.RegisterEnumDescriptor(_STANDARDCODERS_ENUM)
 
 _MERGESTATUS_ENUM = _descriptor.EnumDescriptor(
   name='Enum',
   full_name='org.apache.beam.model.pipeline.v1.MergeStatus.Enum',
   filename=None,
@@ -354,16 +389,16 @@
     _descriptor.EnumValueDescriptor(
       name='ALREADY_MERGED', index=3, number=3,
       options=None,
       type=None),
   ],
   containing_type=None,
   options=None,
-  serialized_start=9820,
-  serialized_end=9897,
+  serialized_start=8506,
+  serialized_end=8583,
 )
 _sym_db.RegisterEnumDescriptor(_MERGESTATUS_ENUM)
 
 _ACCUMULATIONMODE_ENUM = _descriptor.EnumDescriptor(
   name='Enum',
   full_name='org.apache.beam.model.pipeline.v1.AccumulationMode.Enum',
   filename=None,
@@ -380,16 +415,16 @@
     _descriptor.EnumValueDescriptor(
       name='ACCUMULATING', index=2, number=2,
       options=None,
       type=None),
   ],
   containing_type=None,
   options=None,
-  serialized_start=9919,
-  serialized_end=9976,
+  serialized_start=8605,
+  serialized_end=8662,
 )
 _sym_db.RegisterEnumDescriptor(_ACCUMULATIONMODE_ENUM)
 
 _CLOSINGBEHAVIOR_ENUM = _descriptor.EnumDescriptor(
   name='Enum',
   full_name='org.apache.beam.model.pipeline.v1.ClosingBehavior.Enum',
   filename=None,
@@ -406,16 +441,16 @@
     _descriptor.EnumValueDescriptor(
       name='EMIT_IF_NONEMPTY', index=2, number=2,
       options=None,
       type=None),
   ],
   containing_type=None,
   options=None,
-  serialized_start=9997,
-  serialized_end=10059,
+  serialized_start=8683,
+  serialized_end=8745,
 )
 _sym_db.RegisterEnumDescriptor(_CLOSINGBEHAVIOR_ENUM)
 
 _ONTIMEBEHAVIOR_ENUM = _descriptor.EnumDescriptor(
   name='Enum',
   full_name='org.apache.beam.model.pipeline.v1.OnTimeBehavior.Enum',
   filename=None,
@@ -432,16 +467,16 @@
     _descriptor.EnumValueDescriptor(
       name='FIRE_IF_NONEMPTY', index=2, number=2,
       options=None,
       type=None),
   ],
   containing_type=None,
   options=None,
-  serialized_start=10079,
-  serialized_end=10141,
+  serialized_start=8765,
+  serialized_end=8827,
 )
 _sym_db.RegisterEnumDescriptor(_ONTIMEBEHAVIOR_ENUM)
 
 _OUTPUTTIME_ENUM = _descriptor.EnumDescriptor(
   name='Enum',
   full_name='org.apache.beam.model.pipeline.v1.OutputTime.Enum',
   filename=None,
@@ -462,16 +497,16 @@
     _descriptor.EnumValueDescriptor(
       name='EARLIEST_IN_PANE', index=3, number=3,
       options=None,
       type=None),
   ],
   containing_type=None,
   options=None,
-  serialized_start=10157,
-  serialized_end=10241,
+  serialized_start=8843,
+  serialized_end=8927,
 )
 _sym_db.RegisterEnumDescriptor(_OUTPUTTIME_ENUM)
 
 _TIMEDOMAIN_ENUM = _descriptor.EnumDescriptor(
   name='Enum',
   full_name='org.apache.beam.model.pipeline.v1.TimeDomain.Enum',
   filename=None,
@@ -492,16 +527,16 @@
     _descriptor.EnumValueDescriptor(
       name='SYNCHRONIZED_PROCESSING_TIME', index=3, number=3,
       options=None,
       type=None),
   ],
   containing_type=None,
   options=None,
-  serialized_start=10257,
-  serialized_end=10351,
+  serialized_start=8943,
+  serialized_end=9037,
 )
 _sym_db.RegisterEnumDescriptor(_TIMEDOMAIN_ENUM)
 
 _STANDARDENVIRONMENTS_ENVIRONMENTS = _descriptor.EnumDescriptor(
   name='Environments',
   full_name='org.apache.beam.model.pipeline.v1.StandardEnvironments.Environments',
   filename=None,
@@ -518,16 +553,16 @@
     _descriptor.EnumValueDescriptor(
       name='EXTERNAL', index=2, number=2,
       options=_descriptor._ParseOptions(descriptor_pb2.EnumValueOptions(), _b('\242\264\372\302\005\024beam:env:external:v1')),
       type=None),
   ],
   containing_type=None,
   options=None,
-  serialized_start=12750,
-  serialized_end=12884,
+  serialized_start=11436,
+  serialized_end=11570,
 )
 _sym_db.RegisterEnumDescriptor(_STANDARDENVIRONMENTS_ENVIRONMENTS)
 
 _DISPLAYDATA_TYPE_ENUM = _descriptor.EnumDescriptor(
   name='Enum',
   full_name='org.apache.beam.model.pipeline.v1.DisplayData.Type.Enum',
   filename=None,
@@ -564,55 +599,80 @@
     _descriptor.EnumValueDescriptor(
       name='JAVA_CLASS', index=7, number=7,
       options=None,
       type=None),
   ],
   containing_type=None,
   options=None,
-  serialized_start=13688,
-  serialized_end=13805,
+  serialized_start=12374,
+  serialized_end=12491,
 )
 _sym_db.RegisterEnumDescriptor(_DISPLAYDATA_TYPE_ENUM)
 
 
+_BEAMCONSTANTS = _descriptor.Descriptor(
+  name='BeamConstants',
+  full_name='org.apache.beam.model.pipeline.v1.BeamConstants',
+  filename=None,
+  file=DESCRIPTOR,
+  containing_type=None,
+  fields=[
+  ],
+  extensions=[
+  ],
+  nested_types=[],
+  enum_types=[
+    _BEAMCONSTANTS_CONSTANTS,
+  ],
+  options=None,
+  is_extendable=False,
+  syntax='proto3',
+  extension_ranges=[],
+  oneofs=[
+  ],
+  serialized_start=122,
+  serialized_end=316,
+)
+
+
 _COMPONENTS_TRANSFORMSENTRY = _descriptor.Descriptor(
   name='TransformsEntry',
   full_name='org.apache.beam.model.pipeline.v1.Components.TransformsEntry',
   filename=None,
   file=DESCRIPTOR,
   containing_type=None,
   fields=[
     _descriptor.FieldDescriptor(
       name='key', full_name='org.apache.beam.model.pipeline.v1.Components.TransformsEntry.key', index=0,
       number=1, type=9, cpp_type=9, label=1,
       has_default_value=False, default_value=_b("").decode('utf-8'),
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='value', full_name='org.apache.beam.model.pipeline.v1.Components.TransformsEntry.value', index=1,
       number=2, type=11, cpp_type=10, label=1,
       has_default_value=False, default_value=None,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
   ],
   extensions=[
   ],
   nested_types=[],
   enum_types=[
   ],
   options=_descriptor._ParseOptions(descriptor_pb2.MessageOptions(), _b('8\001')),
   is_extendable=False,
   syntax='proto3',
   extension_ranges=[],
   oneofs=[
   ],
-  serialized_start=570,
-  serialized_end=666,
+  serialized_start=767,
+  serialized_end=863,
 )
 
 _COMPONENTS_PCOLLECTIONSENTRY = _descriptor.Descriptor(
   name='PcollectionsEntry',
   full_name='org.apache.beam.model.pipeline.v1.Components.PcollectionsEntry',
   filename=None,
   file=DESCRIPTOR,
@@ -620,36 +680,36 @@
   fields=[
     _descriptor.FieldDescriptor(
       name='key', full_name='org.apache.beam.model.pipeline.v1.Components.PcollectionsEntry.key', index=0,
       number=1, type=9, cpp_type=9, label=1,
       has_default_value=False, default_value=_b("").decode('utf-8'),
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='value', full_name='org.apache.beam.model.pipeline.v1.Components.PcollectionsEntry.value', index=1,
       number=2, type=11, cpp_type=10, label=1,
       has_default_value=False, default_value=None,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
   ],
   extensions=[
   ],
   nested_types=[],
   enum_types=[
   ],
   options=_descriptor._ParseOptions(descriptor_pb2.MessageOptions(), _b('8\001')),
   is_extendable=False,
   syntax='proto3',
   extension_ranges=[],
   oneofs=[
   ],
-  serialized_start=668,
-  serialized_end=767,
+  serialized_start=865,
+  serialized_end=964,
 )
 
 _COMPONENTS_WINDOWINGSTRATEGIESENTRY = _descriptor.Descriptor(
   name='WindowingStrategiesEntry',
   full_name='org.apache.beam.model.pipeline.v1.Components.WindowingStrategiesEntry',
   filename=None,
   file=DESCRIPTOR,
@@ -657,36 +717,36 @@
   fields=[
     _descriptor.FieldDescriptor(
       name='key', full_name='org.apache.beam.model.pipeline.v1.Components.WindowingStrategiesEntry.key', index=0,
       number=1, type=9, cpp_type=9, label=1,
       has_default_value=False, default_value=_b("").decode('utf-8'),
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='value', full_name='org.apache.beam.model.pipeline.v1.Components.WindowingStrategiesEntry.value', index=1,
       number=2, type=11, cpp_type=10, label=1,
       has_default_value=False, default_value=None,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
   ],
   extensions=[
   ],
   nested_types=[],
   enum_types=[
   ],
   options=_descriptor._ParseOptions(descriptor_pb2.MessageOptions(), _b('8\001')),
   is_extendable=False,
   syntax='proto3',
   extension_ranges=[],
   oneofs=[
   ],
-  serialized_start=769,
-  serialized_end=881,
+  serialized_start=966,
+  serialized_end=1078,
 )
 
 _COMPONENTS_CODERSENTRY = _descriptor.Descriptor(
   name='CodersEntry',
   full_name='org.apache.beam.model.pipeline.v1.Components.CodersEntry',
   filename=None,
   file=DESCRIPTOR,
@@ -694,36 +754,36 @@
   fields=[
     _descriptor.FieldDescriptor(
       name='key', full_name='org.apache.beam.model.pipeline.v1.Components.CodersEntry.key', index=0,
       number=1, type=9, cpp_type=9, label=1,
       has_default_value=False, default_value=_b("").decode('utf-8'),
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='value', full_name='org.apache.beam.model.pipeline.v1.Components.CodersEntry.value', index=1,
       number=2, type=11, cpp_type=10, label=1,
       has_default_value=False, default_value=None,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
   ],
   extensions=[
   ],
   nested_types=[],
   enum_types=[
   ],
   options=_descriptor._ParseOptions(descriptor_pb2.MessageOptions(), _b('8\001')),
   is_extendable=False,
   syntax='proto3',
   extension_ranges=[],
   oneofs=[
   ],
-  serialized_start=883,
-  serialized_end=970,
+  serialized_start=1080,
+  serialized_end=1167,
 )
 
 _COMPONENTS_ENVIRONMENTSENTRY = _descriptor.Descriptor(
   name='EnvironmentsEntry',
   full_name='org.apache.beam.model.pipeline.v1.Components.EnvironmentsEntry',
   filename=None,
   file=DESCRIPTOR,
@@ -731,36 +791,36 @@
   fields=[
     _descriptor.FieldDescriptor(
       name='key', full_name='org.apache.beam.model.pipeline.v1.Components.EnvironmentsEntry.key', index=0,
       number=1, type=9, cpp_type=9, label=1,
       has_default_value=False, default_value=_b("").decode('utf-8'),
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='value', full_name='org.apache.beam.model.pipeline.v1.Components.EnvironmentsEntry.value', index=1,
       number=2, type=11, cpp_type=10, label=1,
       has_default_value=False, default_value=None,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
   ],
   extensions=[
   ],
   nested_types=[],
   enum_types=[
   ],
   options=_descriptor._ParseOptions(descriptor_pb2.MessageOptions(), _b('8\001')),
   is_extendable=False,
   syntax='proto3',
   extension_ranges=[],
   oneofs=[
   ],
-  serialized_start=972,
-  serialized_end=1071,
+  serialized_start=1169,
+  serialized_end=1268,
 )
 
 _COMPONENTS = _descriptor.Descriptor(
   name='Components',
   full_name='org.apache.beam.model.pipeline.v1.Components',
   filename=None,
   file=DESCRIPTOR,
@@ -768,168 +828,57 @@
   fields=[
     _descriptor.FieldDescriptor(
       name='transforms', full_name='org.apache.beam.model.pipeline.v1.Components.transforms', index=0,
       number=1, type=11, cpp_type=10, label=3,
       has_default_value=False, default_value=[],
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='pcollections', full_name='org.apache.beam.model.pipeline.v1.Components.pcollections', index=1,
       number=2, type=11, cpp_type=10, label=3,
       has_default_value=False, default_value=[],
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='windowing_strategies', full_name='org.apache.beam.model.pipeline.v1.Components.windowing_strategies', index=2,
       number=3, type=11, cpp_type=10, label=3,
       has_default_value=False, default_value=[],
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='coders', full_name='org.apache.beam.model.pipeline.v1.Components.coders', index=3,
       number=4, type=11, cpp_type=10, label=3,
       has_default_value=False, default_value=[],
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='environments', full_name='org.apache.beam.model.pipeline.v1.Components.environments', index=4,
       number=5, type=11, cpp_type=10, label=3,
       has_default_value=False, default_value=[],
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
   ],
   extensions=[
   ],
   nested_types=[_COMPONENTS_TRANSFORMSENTRY, _COMPONENTS_PCOLLECTIONSENTRY, _COMPONENTS_WINDOWINGSTRATEGIESENTRY, _COMPONENTS_CODERSENTRY, _COMPONENTS_ENVIRONMENTSENTRY, ],
   enum_types=[
   ],
   options=None,
   is_extendable=False,
   syntax='proto3',
   extension_ranges=[],
   oneofs=[
   ],
-  serialized_start=122,
-  serialized_end=1071,
-)
-
-
-_MESSAGEWITHCOMPONENTS = _descriptor.Descriptor(
-  name='MessageWithComponents',
-  full_name='org.apache.beam.model.pipeline.v1.MessageWithComponents',
-  filename=None,
-  file=DESCRIPTOR,
-  containing_type=None,
-  fields=[
-    _descriptor.FieldDescriptor(
-      name='components', full_name='org.apache.beam.model.pipeline.v1.MessageWithComponents.components', index=0,
-      number=1, type=11, cpp_type=10, label=1,
-      has_default_value=False, default_value=None,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='coder', full_name='org.apache.beam.model.pipeline.v1.MessageWithComponents.coder', index=1,
-      number=2, type=11, cpp_type=10, label=1,
-      has_default_value=False, default_value=None,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='combine_payload', full_name='org.apache.beam.model.pipeline.v1.MessageWithComponents.combine_payload', index=2,
-      number=3, type=11, cpp_type=10, label=1,
-      has_default_value=False, default_value=None,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='sdk_function_spec', full_name='org.apache.beam.model.pipeline.v1.MessageWithComponents.sdk_function_spec', index=3,
-      number=4, type=11, cpp_type=10, label=1,
-      has_default_value=False, default_value=None,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='par_do_payload', full_name='org.apache.beam.model.pipeline.v1.MessageWithComponents.par_do_payload', index=4,
-      number=6, type=11, cpp_type=10, label=1,
-      has_default_value=False, default_value=None,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='ptransform', full_name='org.apache.beam.model.pipeline.v1.MessageWithComponents.ptransform', index=5,
-      number=7, type=11, cpp_type=10, label=1,
-      has_default_value=False, default_value=None,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='pcollection', full_name='org.apache.beam.model.pipeline.v1.MessageWithComponents.pcollection', index=6,
-      number=8, type=11, cpp_type=10, label=1,
-      has_default_value=False, default_value=None,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='read_payload', full_name='org.apache.beam.model.pipeline.v1.MessageWithComponents.read_payload', index=7,
-      number=9, type=11, cpp_type=10, label=1,
-      has_default_value=False, default_value=None,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='side_input', full_name='org.apache.beam.model.pipeline.v1.MessageWithComponents.side_input', index=8,
-      number=11, type=11, cpp_type=10, label=1,
-      has_default_value=False, default_value=None,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='window_into_payload', full_name='org.apache.beam.model.pipeline.v1.MessageWithComponents.window_into_payload', index=9,
-      number=12, type=11, cpp_type=10, label=1,
-      has_default_value=False, default_value=None,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='windowing_strategy', full_name='org.apache.beam.model.pipeline.v1.MessageWithComponents.windowing_strategy', index=10,
-      number=13, type=11, cpp_type=10, label=1,
-      has_default_value=False, default_value=None,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='function_spec', full_name='org.apache.beam.model.pipeline.v1.MessageWithComponents.function_spec', index=11,
-      number=14, type=11, cpp_type=10, label=1,
-      has_default_value=False, default_value=None,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
-  ],
-  extensions=[
-  ],
-  nested_types=[],
-  enum_types=[
-  ],
-  options=None,
-  is_extendable=False,
-  syntax='proto3',
-  extension_ranges=[],
-  oneofs=[
-    _descriptor.OneofDescriptor(
-      name='root', full_name='org.apache.beam.model.pipeline.v1.MessageWithComponents.root',
-      index=0, containing_type=None, fields=[]),
-  ],
-  serialized_start=1074,
-  serialized_end=1988,
+  serialized_start=319,
+  serialized_end=1268,
 )
 
 
 _PIPELINE = _descriptor.Descriptor(
   name='Pipeline',
   full_name='org.apache.beam.model.pipeline.v1.Pipeline',
   filename=None,
@@ -938,43 +887,43 @@
   fields=[
     _descriptor.FieldDescriptor(
       name='components', full_name='org.apache.beam.model.pipeline.v1.Pipeline.components', index=0,
       number=1, type=11, cpp_type=10, label=1,
       has_default_value=False, default_value=None,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='root_transform_ids', full_name='org.apache.beam.model.pipeline.v1.Pipeline.root_transform_ids', index=1,
       number=2, type=9, cpp_type=9, label=3,
       has_default_value=False, default_value=[],
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='display_data', full_name='org.apache.beam.model.pipeline.v1.Pipeline.display_data', index=2,
       number=3, type=11, cpp_type=10, label=1,
       has_default_value=False, default_value=None,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
   ],
   extensions=[
   ],
   nested_types=[],
   enum_types=[
   ],
   options=None,
   is_extendable=False,
   syntax='proto3',
   extension_ranges=[],
   oneofs=[
   ],
-  serialized_start=1991,
-  serialized_end=2166,
+  serialized_start=1271,
+  serialized_end=1446,
 )
 
 
 _PTRANSFORM_INPUTSENTRY = _descriptor.Descriptor(
   name='InputsEntry',
   full_name='org.apache.beam.model.pipeline.v1.PTransform.InputsEntry',
   filename=None,
@@ -983,36 +932,36 @@
   fields=[
     _descriptor.FieldDescriptor(
       name='key', full_name='org.apache.beam.model.pipeline.v1.PTransform.InputsEntry.key', index=0,
       number=1, type=9, cpp_type=9, label=1,
       has_default_value=False, default_value=_b("").decode('utf-8'),
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='value', full_name='org.apache.beam.model.pipeline.v1.PTransform.InputsEntry.value', index=1,
       number=2, type=9, cpp_type=9, label=1,
       has_default_value=False, default_value=_b("").decode('utf-8'),
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
   ],
   extensions=[
   ],
   nested_types=[],
   enum_types=[
   ],
   options=_descriptor._ParseOptions(descriptor_pb2.MessageOptions(), _b('8\001')),
   is_extendable=False,
   syntax='proto3',
   extension_ranges=[],
   oneofs=[
   ],
-  serialized_start=2512,
-  serialized_end=2557,
+  serialized_start=1792,
+  serialized_end=1837,
 )
 
 _PTRANSFORM_OUTPUTSENTRY = _descriptor.Descriptor(
   name='OutputsEntry',
   full_name='org.apache.beam.model.pipeline.v1.PTransform.OutputsEntry',
   filename=None,
   file=DESCRIPTOR,
@@ -1020,36 +969,36 @@
   fields=[
     _descriptor.FieldDescriptor(
       name='key', full_name='org.apache.beam.model.pipeline.v1.PTransform.OutputsEntry.key', index=0,
       number=1, type=9, cpp_type=9, label=1,
       has_default_value=False, default_value=_b("").decode('utf-8'),
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='value', full_name='org.apache.beam.model.pipeline.v1.PTransform.OutputsEntry.value', index=1,
       number=2, type=9, cpp_type=9, label=1,
       has_default_value=False, default_value=_b("").decode('utf-8'),
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
   ],
   extensions=[
   ],
   nested_types=[],
   enum_types=[
   ],
   options=_descriptor._ParseOptions(descriptor_pb2.MessageOptions(), _b('8\001')),
   is_extendable=False,
   syntax='proto3',
   extension_ranges=[],
   oneofs=[
   ],
-  serialized_start=2559,
-  serialized_end=2605,
+  serialized_start=1839,
+  serialized_end=1885,
 )
 
 _PTRANSFORM = _descriptor.Descriptor(
   name='PTransform',
   full_name='org.apache.beam.model.pipeline.v1.PTransform',
   filename=None,
   file=DESCRIPTOR,
@@ -1057,64 +1006,64 @@
   fields=[
     _descriptor.FieldDescriptor(
       name='unique_name', full_name='org.apache.beam.model.pipeline.v1.PTransform.unique_name', index=0,
       number=5, type=9, cpp_type=9, label=1,
       has_default_value=False, default_value=_b("").decode('utf-8'),
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='spec', full_name='org.apache.beam.model.pipeline.v1.PTransform.spec', index=1,
       number=1, type=11, cpp_type=10, label=1,
       has_default_value=False, default_value=None,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='subtransforms', full_name='org.apache.beam.model.pipeline.v1.PTransform.subtransforms', index=2,
       number=2, type=9, cpp_type=9, label=3,
       has_default_value=False, default_value=[],
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='inputs', full_name='org.apache.beam.model.pipeline.v1.PTransform.inputs', index=3,
       number=3, type=11, cpp_type=10, label=3,
       has_default_value=False, default_value=[],
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='outputs', full_name='org.apache.beam.model.pipeline.v1.PTransform.outputs', index=4,
       number=4, type=11, cpp_type=10, label=3,
       has_default_value=False, default_value=[],
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='display_data', full_name='org.apache.beam.model.pipeline.v1.PTransform.display_data', index=5,
       number=6, type=11, cpp_type=10, label=1,
       has_default_value=False, default_value=None,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
   ],
   extensions=[
   ],
   nested_types=[_PTRANSFORM_INPUTSENTRY, _PTRANSFORM_OUTPUTSENTRY, ],
   enum_types=[
   ],
   options=None,
   is_extendable=False,
   syntax='proto3',
   extension_ranges=[],
   oneofs=[
   ],
-  serialized_start=2169,
-  serialized_end=2605,
+  serialized_start=1449,
+  serialized_end=1885,
 )
 
 
 _STANDARDPTRANSFORMS = _descriptor.Descriptor(
   name='StandardPTransforms',
   full_name='org.apache.beam.model.pipeline.v1.StandardPTransforms',
   filename=None,
@@ -1134,16 +1083,16 @@
   ],
   options=None,
   is_extendable=False,
   syntax='proto3',
   extension_ranges=[],
   oneofs=[
   ],
-  serialized_start=2608,
-  serialized_end=4352,
+  serialized_start=1888,
+  serialized_end=3632,
 )
 
 
 _STANDARDSIDEINPUTTYPES = _descriptor.Descriptor(
   name='StandardSideInputTypes',
   full_name='org.apache.beam.model.pipeline.v1.StandardSideInputTypes',
   filename=None,
@@ -1159,16 +1108,16 @@
   ],
   options=None,
   is_extendable=False,
   syntax='proto3',
   extension_ranges=[],
   oneofs=[
   ],
-  serialized_start=4355,
-  serialized_end=4485,
+  serialized_start=3635,
+  serialized_end=3765,
 )
 
 
 _PCOLLECTION = _descriptor.Descriptor(
   name='PCollection',
   full_name='org.apache.beam.model.pipeline.v1.PCollection',
   filename=None,
@@ -1177,248 +1126,57 @@
   fields=[
     _descriptor.FieldDescriptor(
       name='unique_name', full_name='org.apache.beam.model.pipeline.v1.PCollection.unique_name', index=0,
       number=1, type=9, cpp_type=9, label=1,
       has_default_value=False, default_value=_b("").decode('utf-8'),
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='coder_id', full_name='org.apache.beam.model.pipeline.v1.PCollection.coder_id', index=1,
       number=2, type=9, cpp_type=9, label=1,
       has_default_value=False, default_value=_b("").decode('utf-8'),
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='is_bounded', full_name='org.apache.beam.model.pipeline.v1.PCollection.is_bounded', index=2,
       number=3, type=14, cpp_type=8, label=1,
       has_default_value=False, default_value=0,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='windowing_strategy_id', full_name='org.apache.beam.model.pipeline.v1.PCollection.windowing_strategy_id', index=3,
       number=4, type=9, cpp_type=9, label=1,
       has_default_value=False, default_value=_b("").decode('utf-8'),
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='display_data', full_name='org.apache.beam.model.pipeline.v1.PCollection.display_data', index=4,
       number=5, type=11, cpp_type=10, label=1,
       has_default_value=False, default_value=None,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
-  ],
-  extensions=[
-  ],
-  nested_types=[],
-  enum_types=[
-  ],
-  options=None,
-  is_extendable=False,
-  syntax='proto3',
-  extension_ranges=[],
-  oneofs=[
-  ],
-  serialized_start=4488,
-  serialized_end=4712,
-)
-
-
-_EXECUTABLESTAGEPAYLOAD_SIDEINPUTID = _descriptor.Descriptor(
-  name='SideInputId',
-  full_name='org.apache.beam.model.pipeline.v1.ExecutableStagePayload.SideInputId',
-  filename=None,
-  file=DESCRIPTOR,
-  containing_type=None,
-  fields=[
-    _descriptor.FieldDescriptor(
-      name='transform_id', full_name='org.apache.beam.model.pipeline.v1.ExecutableStagePayload.SideInputId.transform_id', index=0,
-      number=1, type=9, cpp_type=9, label=1,
-      has_default_value=False, default_value=_b("").decode('utf-8'),
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='local_name', full_name='org.apache.beam.model.pipeline.v1.ExecutableStagePayload.SideInputId.local_name', index=1,
-      number=2, type=9, cpp_type=9, label=1,
-      has_default_value=False, default_value=_b("").decode('utf-8'),
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
-  ],
-  extensions=[
-  ],
-  nested_types=[],
-  enum_types=[
-  ],
-  options=None,
-  is_extendable=False,
-  syntax='proto3',
-  extension_ranges=[],
-  oneofs=[
-  ],
-  serialized_start=5196,
-  serialized_end=5251,
-)
-
-_EXECUTABLESTAGEPAYLOAD_USERSTATEID = _descriptor.Descriptor(
-  name='UserStateId',
-  full_name='org.apache.beam.model.pipeline.v1.ExecutableStagePayload.UserStateId',
-  filename=None,
-  file=DESCRIPTOR,
-  containing_type=None,
-  fields=[
-    _descriptor.FieldDescriptor(
-      name='transform_id', full_name='org.apache.beam.model.pipeline.v1.ExecutableStagePayload.UserStateId.transform_id', index=0,
-      number=1, type=9, cpp_type=9, label=1,
-      has_default_value=False, default_value=_b("").decode('utf-8'),
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='local_name', full_name='org.apache.beam.model.pipeline.v1.ExecutableStagePayload.UserStateId.local_name', index=1,
-      number=2, type=9, cpp_type=9, label=1,
-      has_default_value=False, default_value=_b("").decode('utf-8'),
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
   ],
   extensions=[
   ],
   nested_types=[],
   enum_types=[
   ],
   options=None,
   is_extendable=False,
   syntax='proto3',
   extension_ranges=[],
   oneofs=[
   ],
-  serialized_start=5253,
-  serialized_end=5308,
-)
-
-_EXECUTABLESTAGEPAYLOAD_TIMERID = _descriptor.Descriptor(
-  name='TimerId',
-  full_name='org.apache.beam.model.pipeline.v1.ExecutableStagePayload.TimerId',
-  filename=None,
-  file=DESCRIPTOR,
-  containing_type=None,
-  fields=[
-    _descriptor.FieldDescriptor(
-      name='transform_id', full_name='org.apache.beam.model.pipeline.v1.ExecutableStagePayload.TimerId.transform_id', index=0,
-      number=1, type=9, cpp_type=9, label=1,
-      has_default_value=False, default_value=_b("").decode('utf-8'),
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='local_name', full_name='org.apache.beam.model.pipeline.v1.ExecutableStagePayload.TimerId.local_name', index=1,
-      number=2, type=9, cpp_type=9, label=1,
-      has_default_value=False, default_value=_b("").decode('utf-8'),
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
-  ],
-  extensions=[
-  ],
-  nested_types=[],
-  enum_types=[
-  ],
-  options=None,
-  is_extendable=False,
-  syntax='proto3',
-  extension_ranges=[],
-  oneofs=[
-  ],
-  serialized_start=5310,
-  serialized_end=5361,
-)
-
-_EXECUTABLESTAGEPAYLOAD = _descriptor.Descriptor(
-  name='ExecutableStagePayload',
-  full_name='org.apache.beam.model.pipeline.v1.ExecutableStagePayload',
-  filename=None,
-  file=DESCRIPTOR,
-  containing_type=None,
-  fields=[
-    _descriptor.FieldDescriptor(
-      name='environment', full_name='org.apache.beam.model.pipeline.v1.ExecutableStagePayload.environment', index=0,
-      number=1, type=11, cpp_type=10, label=1,
-      has_default_value=False, default_value=None,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='input', full_name='org.apache.beam.model.pipeline.v1.ExecutableStagePayload.input', index=1,
-      number=2, type=9, cpp_type=9, label=1,
-      has_default_value=False, default_value=_b("").decode('utf-8'),
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='side_inputs', full_name='org.apache.beam.model.pipeline.v1.ExecutableStagePayload.side_inputs', index=2,
-      number=3, type=11, cpp_type=10, label=3,
-      has_default_value=False, default_value=[],
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='transforms', full_name='org.apache.beam.model.pipeline.v1.ExecutableStagePayload.transforms', index=3,
-      number=4, type=9, cpp_type=9, label=3,
-      has_default_value=False, default_value=[],
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='outputs', full_name='org.apache.beam.model.pipeline.v1.ExecutableStagePayload.outputs', index=4,
-      number=5, type=9, cpp_type=9, label=3,
-      has_default_value=False, default_value=[],
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='components', full_name='org.apache.beam.model.pipeline.v1.ExecutableStagePayload.components', index=5,
-      number=6, type=11, cpp_type=10, label=1,
-      has_default_value=False, default_value=None,
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='user_states', full_name='org.apache.beam.model.pipeline.v1.ExecutableStagePayload.user_states', index=6,
-      number=7, type=11, cpp_type=10, label=3,
-      has_default_value=False, default_value=[],
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='timers', full_name='org.apache.beam.model.pipeline.v1.ExecutableStagePayload.timers', index=7,
-      number=8, type=11, cpp_type=10, label=3,
-      has_default_value=False, default_value=[],
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
-  ],
-  extensions=[
-  ],
-  nested_types=[_EXECUTABLESTAGEPAYLOAD_SIDEINPUTID, _EXECUTABLESTAGEPAYLOAD_USERSTATEID, _EXECUTABLESTAGEPAYLOAD_TIMERID, ],
-  enum_types=[
-  ],
-  options=None,
-  is_extendable=False,
-  syntax='proto3',
-  extension_ranges=[],
-  oneofs=[
-  ],
-  serialized_start=4715,
-  serialized_end=5361,
+  serialized_start=3768,
+  serialized_end=3992,
 )
 
 
 _PARDOPAYLOAD_SIDEINPUTSENTRY = _descriptor.Descriptor(
   name='SideInputsEntry',
   full_name='org.apache.beam.model.pipeline.v1.ParDoPayload.SideInputsEntry',
   filename=None,
@@ -1427,36 +1185,36 @@
   fields=[
     _descriptor.FieldDescriptor(
       name='key', full_name='org.apache.beam.model.pipeline.v1.ParDoPayload.SideInputsEntry.key', index=0,
       number=1, type=9, cpp_type=9, label=1,
       has_default_value=False, default_value=_b("").decode('utf-8'),
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='value', full_name='org.apache.beam.model.pipeline.v1.ParDoPayload.SideInputsEntry.value', index=1,
       number=2, type=11, cpp_type=10, label=1,
       has_default_value=False, default_value=None,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
   ],
   extensions=[
   ],
   nested_types=[],
   enum_types=[
   ],
   options=_descriptor._ParseOptions(descriptor_pb2.MessageOptions(), _b('8\001')),
   is_extendable=False,
   syntax='proto3',
   extension_ranges=[],
   oneofs=[
   ],
-  serialized_start=5821,
-  serialized_end=5916,
+  serialized_start=4483,
+  serialized_end=4578,
 )
 
 _PARDOPAYLOAD_STATESPECSENTRY = _descriptor.Descriptor(
   name='StateSpecsEntry',
   full_name='org.apache.beam.model.pipeline.v1.ParDoPayload.StateSpecsEntry',
   filename=None,
   file=DESCRIPTOR,
@@ -1464,36 +1222,36 @@
   fields=[
     _descriptor.FieldDescriptor(
       name='key', full_name='org.apache.beam.model.pipeline.v1.ParDoPayload.StateSpecsEntry.key', index=0,
       number=1, type=9, cpp_type=9, label=1,
       has_default_value=False, default_value=_b("").decode('utf-8'),
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='value', full_name='org.apache.beam.model.pipeline.v1.ParDoPayload.StateSpecsEntry.value', index=1,
       number=2, type=11, cpp_type=10, label=1,
       has_default_value=False, default_value=None,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
   ],
   extensions=[
   ],
   nested_types=[],
   enum_types=[
   ],
   options=_descriptor._ParseOptions(descriptor_pb2.MessageOptions(), _b('8\001')),
   is_extendable=False,
   syntax='proto3',
   extension_ranges=[],
   oneofs=[
   ],
-  serialized_start=5918,
-  serialized_end=6013,
+  serialized_start=4580,
+  serialized_end=4675,
 )
 
 _PARDOPAYLOAD_TIMERSPECSENTRY = _descriptor.Descriptor(
   name='TimerSpecsEntry',
   full_name='org.apache.beam.model.pipeline.v1.ParDoPayload.TimerSpecsEntry',
   filename=None,
   file=DESCRIPTOR,
@@ -1501,36 +1259,36 @@
   fields=[
     _descriptor.FieldDescriptor(
       name='key', full_name='org.apache.beam.model.pipeline.v1.ParDoPayload.TimerSpecsEntry.key', index=0,
       number=1, type=9, cpp_type=9, label=1,
       has_default_value=False, default_value=_b("").decode('utf-8'),
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='value', full_name='org.apache.beam.model.pipeline.v1.ParDoPayload.TimerSpecsEntry.value', index=1,
       number=2, type=11, cpp_type=10, label=1,
       has_default_value=False, default_value=None,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
   ],
   extensions=[
   ],
   nested_types=[],
   enum_types=[
   ],
   options=_descriptor._ParseOptions(descriptor_pb2.MessageOptions(), _b('8\001')),
   is_extendable=False,
   syntax='proto3',
   extension_ranges=[],
   oneofs=[
   ],
-  serialized_start=6015,
-  serialized_end=6110,
+  serialized_start=4677,
+  serialized_end=4772,
 )
 
 _PARDOPAYLOAD = _descriptor.Descriptor(
   name='ParDoPayload',
   full_name='org.apache.beam.model.pipeline.v1.ParDoPayload',
   filename=None,
   file=DESCRIPTOR,
@@ -1538,71 +1296,78 @@
   fields=[
     _descriptor.FieldDescriptor(
       name='do_fn', full_name='org.apache.beam.model.pipeline.v1.ParDoPayload.do_fn', index=0,
       number=1, type=11, cpp_type=10, label=1,
       has_default_value=False, default_value=None,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='parameters', full_name='org.apache.beam.model.pipeline.v1.ParDoPayload.parameters', index=1,
       number=2, type=11, cpp_type=10, label=3,
       has_default_value=False, default_value=[],
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='side_inputs', full_name='org.apache.beam.model.pipeline.v1.ParDoPayload.side_inputs', index=2,
       number=3, type=11, cpp_type=10, label=3,
       has_default_value=False, default_value=[],
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='state_specs', full_name='org.apache.beam.model.pipeline.v1.ParDoPayload.state_specs', index=3,
       number=4, type=11, cpp_type=10, label=3,
       has_default_value=False, default_value=[],
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='timer_specs', full_name='org.apache.beam.model.pipeline.v1.ParDoPayload.timer_specs', index=4,
       number=5, type=11, cpp_type=10, label=3,
       has_default_value=False, default_value=[],
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='splittable', full_name='org.apache.beam.model.pipeline.v1.ParDoPayload.splittable', index=5,
       number=6, type=8, cpp_type=7, label=1,
       has_default_value=False, default_value=False,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='restriction_coder_id', full_name='org.apache.beam.model.pipeline.v1.ParDoPayload.restriction_coder_id', index=6,
       number=7, type=9, cpp_type=9, label=1,
       has_default_value=False, default_value=_b("").decode('utf-8'),
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
+    _descriptor.FieldDescriptor(
+      name='requests_finalization', full_name='org.apache.beam.model.pipeline.v1.ParDoPayload.requests_finalization', index=7,
+      number=8, type=8, cpp_type=7, label=1,
+      has_default_value=False, default_value=False,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      options=None),
   ],
   extensions=[
   ],
   nested_types=[_PARDOPAYLOAD_SIDEINPUTSENTRY, _PARDOPAYLOAD_STATESPECSENTRY, _PARDOPAYLOAD_TIMERSPECSENTRY, ],
   enum_types=[
   ],
   options=None,
   is_extendable=False,
   syntax='proto3',
   extension_ranges=[],
   oneofs=[
   ],
-  serialized_start=5364,
-  serialized_end=6110,
+  serialized_start=3995,
+  serialized_end=4772,
 )
 
 
 _PARAMETER_TYPE = _descriptor.Descriptor(
   name='Type',
   full_name='org.apache.beam.model.pipeline.v1.Parameter.Type',
   filename=None,
@@ -1618,16 +1383,16 @@
   ],
   options=None,
   is_extendable=False,
   syntax='proto3',
   extension_ranges=[],
   oneofs=[
   ],
-  serialized_start=6196,
-  serialized_end=6286,
+  serialized_start=4858,
+  serialized_end=4948,
 )
 
 _PARAMETER = _descriptor.Descriptor(
   name='Parameter',
   full_name='org.apache.beam.model.pipeline.v1.Parameter',
   filename=None,
   file=DESCRIPTOR,
@@ -1635,29 +1400,29 @@
   fields=[
     _descriptor.FieldDescriptor(
       name='type', full_name='org.apache.beam.model.pipeline.v1.Parameter.type', index=0,
       number=1, type=14, cpp_type=8, label=1,
       has_default_value=False, default_value=0,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
   ],
   extensions=[
   ],
   nested_types=[_PARAMETER_TYPE, ],
   enum_types=[
   ],
   options=None,
   is_extendable=False,
   syntax='proto3',
   extension_ranges=[],
   oneofs=[
   ],
-  serialized_start=6113,
-  serialized_end=6286,
+  serialized_start=4775,
+  serialized_end=4948,
 )
 
 
 _STATESPEC = _descriptor.Descriptor(
   name='StateSpec',
   full_name='org.apache.beam.model.pipeline.v1.StateSpec',
   filename=None,
@@ -1666,43 +1431,43 @@
   fields=[
     _descriptor.FieldDescriptor(
       name='value_spec', full_name='org.apache.beam.model.pipeline.v1.StateSpec.value_spec', index=0,
       number=1, type=11, cpp_type=10, label=1,
       has_default_value=False, default_value=None,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='bag_spec', full_name='org.apache.beam.model.pipeline.v1.StateSpec.bag_spec', index=1,
       number=2, type=11, cpp_type=10, label=1,
       has_default_value=False, default_value=None,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='combining_spec', full_name='org.apache.beam.model.pipeline.v1.StateSpec.combining_spec', index=2,
       number=3, type=11, cpp_type=10, label=1,
       has_default_value=False, default_value=None,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='map_spec', full_name='org.apache.beam.model.pipeline.v1.StateSpec.map_spec', index=3,
       number=4, type=11, cpp_type=10, label=1,
       has_default_value=False, default_value=None,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='set_spec', full_name='org.apache.beam.model.pipeline.v1.StateSpec.set_spec', index=4,
       number=5, type=11, cpp_type=10, label=1,
       has_default_value=False, default_value=None,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
   ],
   extensions=[
   ],
   nested_types=[],
   enum_types=[
   ],
   options=None,
@@ -1710,16 +1475,16 @@
   syntax='proto3',
   extension_ranges=[],
   oneofs=[
     _descriptor.OneofDescriptor(
       name='spec', full_name='org.apache.beam.model.pipeline.v1.StateSpec.spec',
       index=0, containing_type=None, fields=[]),
   ],
-  serialized_start=6289,
-  serialized_end=6669,
+  serialized_start=4951,
+  serialized_end=5331,
 )
 
 
 _VALUESTATESPEC = _descriptor.Descriptor(
   name='ValueStateSpec',
   full_name='org.apache.beam.model.pipeline.v1.ValueStateSpec',
   filename=None,
@@ -1728,29 +1493,29 @@
   fields=[
     _descriptor.FieldDescriptor(
       name='coder_id', full_name='org.apache.beam.model.pipeline.v1.ValueStateSpec.coder_id', index=0,
       number=1, type=9, cpp_type=9, label=1,
       has_default_value=False, default_value=_b("").decode('utf-8'),
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
   ],
   extensions=[
   ],
   nested_types=[],
   enum_types=[
   ],
   options=None,
   is_extendable=False,
   syntax='proto3',
   extension_ranges=[],
   oneofs=[
   ],
-  serialized_start=6671,
-  serialized_end=6705,
+  serialized_start=5333,
+  serialized_end=5367,
 )
 
 
 _BAGSTATESPEC = _descriptor.Descriptor(
   name='BagStateSpec',
   full_name='org.apache.beam.model.pipeline.v1.BagStateSpec',
   filename=None,
@@ -1759,29 +1524,29 @@
   fields=[
     _descriptor.FieldDescriptor(
       name='element_coder_id', full_name='org.apache.beam.model.pipeline.v1.BagStateSpec.element_coder_id', index=0,
       number=1, type=9, cpp_type=9, label=1,
       has_default_value=False, default_value=_b("").decode('utf-8'),
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
   ],
   extensions=[
   ],
   nested_types=[],
   enum_types=[
   ],
   options=None,
   is_extendable=False,
   syntax='proto3',
   extension_ranges=[],
   oneofs=[
   ],
-  serialized_start=6707,
-  serialized_end=6747,
+  serialized_start=5369,
+  serialized_end=5409,
 )
 
 
 _COMBININGSTATESPEC = _descriptor.Descriptor(
   name='CombiningStateSpec',
   full_name='org.apache.beam.model.pipeline.v1.CombiningStateSpec',
   filename=None,
@@ -1790,36 +1555,36 @@
   fields=[
     _descriptor.FieldDescriptor(
       name='accumulator_coder_id', full_name='org.apache.beam.model.pipeline.v1.CombiningStateSpec.accumulator_coder_id', index=0,
       number=1, type=9, cpp_type=9, label=1,
       has_default_value=False, default_value=_b("").decode('utf-8'),
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='combine_fn', full_name='org.apache.beam.model.pipeline.v1.CombiningStateSpec.combine_fn', index=1,
       number=2, type=11, cpp_type=10, label=1,
       has_default_value=False, default_value=None,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
   ],
   extensions=[
   ],
   nested_types=[],
   enum_types=[
   ],
   options=None,
   is_extendable=False,
   syntax='proto3',
   extension_ranges=[],
   oneofs=[
   ],
-  serialized_start=6749,
-  serialized_end=6871,
+  serialized_start=5411,
+  serialized_end=5533,
 )
 
 
 _MAPSTATESPEC = _descriptor.Descriptor(
   name='MapStateSpec',
   full_name='org.apache.beam.model.pipeline.v1.MapStateSpec',
   filename=None,
@@ -1828,36 +1593,36 @@
   fields=[
     _descriptor.FieldDescriptor(
       name='key_coder_id', full_name='org.apache.beam.model.pipeline.v1.MapStateSpec.key_coder_id', index=0,
       number=1, type=9, cpp_type=9, label=1,
       has_default_value=False, default_value=_b("").decode('utf-8'),
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='value_coder_id', full_name='org.apache.beam.model.pipeline.v1.MapStateSpec.value_coder_id', index=1,
       number=2, type=9, cpp_type=9, label=1,
       has_default_value=False, default_value=_b("").decode('utf-8'),
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
   ],
   extensions=[
   ],
   nested_types=[],
   enum_types=[
   ],
   options=None,
   is_extendable=False,
   syntax='proto3',
   extension_ranges=[],
   oneofs=[
   ],
-  serialized_start=6873,
-  serialized_end=6933,
+  serialized_start=5535,
+  serialized_end=5595,
 )
 
 
 _SETSTATESPEC = _descriptor.Descriptor(
   name='SetStateSpec',
   full_name='org.apache.beam.model.pipeline.v1.SetStateSpec',
   filename=None,
@@ -1866,29 +1631,29 @@
   fields=[
     _descriptor.FieldDescriptor(
       name='element_coder_id', full_name='org.apache.beam.model.pipeline.v1.SetStateSpec.element_coder_id', index=0,
       number=1, type=9, cpp_type=9, label=1,
       has_default_value=False, default_value=_b("").decode('utf-8'),
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
   ],
   extensions=[
   ],
   nested_types=[],
   enum_types=[
   ],
   options=None,
   is_extendable=False,
   syntax='proto3',
   extension_ranges=[],
   oneofs=[
   ],
-  serialized_start=6935,
-  serialized_end=6975,
+  serialized_start=5597,
+  serialized_end=5637,
 )
 
 
 _TIMERSPEC = _descriptor.Descriptor(
   name='TimerSpec',
   full_name='org.apache.beam.model.pipeline.v1.TimerSpec',
   filename=None,
@@ -1897,29 +1662,36 @@
   fields=[
     _descriptor.FieldDescriptor(
       name='time_domain', full_name='org.apache.beam.model.pipeline.v1.TimerSpec.time_domain', index=0,
       number=1, type=14, cpp_type=8, label=1,
       has_default_value=False, default_value=0,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
+    _descriptor.FieldDescriptor(
+      name='timer_coder_id', full_name='org.apache.beam.model.pipeline.v1.TimerSpec.timer_coder_id', index=1,
+      number=2, type=9, cpp_type=9, label=1,
+      has_default_value=False, default_value=_b("").decode('utf-8'),
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      options=None),
   ],
   extensions=[
   ],
   nested_types=[],
   enum_types=[
   ],
   options=None,
   is_extendable=False,
   syntax='proto3',
   extension_ranges=[],
   oneofs=[
   ],
-  serialized_start=6977,
-  serialized_end=7061,
+  serialized_start=5639,
+  serialized_end=5747,
 )
 
 
 _ISBOUNDED = _descriptor.Descriptor(
   name='IsBounded',
   full_name='org.apache.beam.model.pipeline.v1.IsBounded',
   filename=None,
@@ -1935,16 +1707,16 @@
   ],
   options=None,
   is_extendable=False,
   syntax='proto3',
   extension_ranges=[],
   oneofs=[
   ],
-  serialized_start=7063,
-  serialized_end=7127,
+  serialized_start=5749,
+  serialized_end=5813,
 )
 
 
 _READPAYLOAD = _descriptor.Descriptor(
   name='ReadPayload',
   full_name='org.apache.beam.model.pipeline.v1.ReadPayload',
   filename=None,
@@ -1953,36 +1725,36 @@
   fields=[
     _descriptor.FieldDescriptor(
       name='source', full_name='org.apache.beam.model.pipeline.v1.ReadPayload.source', index=0,
       number=1, type=11, cpp_type=10, label=1,
       has_default_value=False, default_value=None,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='is_bounded', full_name='org.apache.beam.model.pipeline.v1.ReadPayload.is_bounded', index=1,
       number=2, type=14, cpp_type=8, label=1,
       has_default_value=False, default_value=0,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
   ],
   extensions=[
   ],
   nested_types=[],
   enum_types=[
   ],
   options=None,
   is_extendable=False,
   syntax='proto3',
   extension_ranges=[],
   oneofs=[
   ],
-  serialized_start=7130,
-  serialized_end=7282,
+  serialized_start=5816,
+  serialized_end=5968,
 )
 
 
 _WINDOWINTOPAYLOAD = _descriptor.Descriptor(
   name='WindowIntoPayload',
   full_name='org.apache.beam.model.pipeline.v1.WindowIntoPayload',
   filename=None,
@@ -1991,29 +1763,29 @@
   fields=[
     _descriptor.FieldDescriptor(
       name='window_fn', full_name='org.apache.beam.model.pipeline.v1.WindowIntoPayload.window_fn', index=0,
       number=1, type=11, cpp_type=10, label=1,
       has_default_value=False, default_value=None,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
   ],
   extensions=[
   ],
   nested_types=[],
   enum_types=[
   ],
   options=None,
   is_extendable=False,
   syntax='proto3',
   extension_ranges=[],
   oneofs=[
   ],
-  serialized_start=7284,
-  serialized_end=7374,
+  serialized_start=5970,
+  serialized_end=6060,
 )
 
 
 _COMBINEPAYLOAD = _descriptor.Descriptor(
   name='CombinePayload',
   full_name='org.apache.beam.model.pipeline.v1.CombinePayload',
   filename=None,
@@ -2022,36 +1794,36 @@
   fields=[
     _descriptor.FieldDescriptor(
       name='combine_fn', full_name='org.apache.beam.model.pipeline.v1.CombinePayload.combine_fn', index=0,
       number=1, type=11, cpp_type=10, label=1,
       has_default_value=False, default_value=None,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='accumulator_coder_id', full_name='org.apache.beam.model.pipeline.v1.CombinePayload.accumulator_coder_id', index=1,
       number=2, type=9, cpp_type=9, label=1,
       has_default_value=False, default_value=_b("").decode('utf-8'),
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
   ],
   extensions=[
   ],
   nested_types=[],
   enum_types=[
   ],
   options=None,
   is_extendable=False,
   syntax='proto3',
   extension_ranges=[],
   oneofs=[
   ],
-  serialized_start=7376,
-  serialized_end=7494,
+  serialized_start=6062,
+  serialized_end=6180,
 )
 
 
 _TESTSTREAMPAYLOAD_EVENT_ADVANCEWATERMARK = _descriptor.Descriptor(
   name='AdvanceWatermark',
   full_name='org.apache.beam.model.pipeline.v1.TestStreamPayload.Event.AdvanceWatermark',
   filename=None,
@@ -2060,29 +1832,29 @@
   fields=[
     _descriptor.FieldDescriptor(
       name='new_watermark', full_name='org.apache.beam.model.pipeline.v1.TestStreamPayload.Event.AdvanceWatermark.new_watermark', index=0,
       number=1, type=3, cpp_type=2, label=1,
       has_default_value=False, default_value=0,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
   ],
   extensions=[
   ],
   nested_types=[],
   enum_types=[
   ],
   options=None,
   is_extendable=False,
   syntax='proto3',
   extension_ranges=[],
   oneofs=[
   ],
-  serialized_start=7938,
-  serialized_end=7979,
+  serialized_start=6624,
+  serialized_end=6665,
 )
 
 _TESTSTREAMPAYLOAD_EVENT_ADVANCEPROCESSINGTIME = _descriptor.Descriptor(
   name='AdvanceProcessingTime',
   full_name='org.apache.beam.model.pipeline.v1.TestStreamPayload.Event.AdvanceProcessingTime',
   filename=None,
   file=DESCRIPTOR,
@@ -2090,29 +1862,29 @@
   fields=[
     _descriptor.FieldDescriptor(
       name='advance_duration', full_name='org.apache.beam.model.pipeline.v1.TestStreamPayload.Event.AdvanceProcessingTime.advance_duration', index=0,
       number=1, type=3, cpp_type=2, label=1,
       has_default_value=False, default_value=0,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
   ],
   extensions=[
   ],
   nested_types=[],
   enum_types=[
   ],
   options=None,
   is_extendable=False,
   syntax='proto3',
   extension_ranges=[],
   oneofs=[
   ],
-  serialized_start=7981,
-  serialized_end=8030,
+  serialized_start=6667,
+  serialized_end=6716,
 )
 
 _TESTSTREAMPAYLOAD_EVENT_ADDELEMENTS = _descriptor.Descriptor(
   name='AddElements',
   full_name='org.apache.beam.model.pipeline.v1.TestStreamPayload.Event.AddElements',
   filename=None,
   file=DESCRIPTOR,
@@ -2120,29 +1892,29 @@
   fields=[
     _descriptor.FieldDescriptor(
       name='elements', full_name='org.apache.beam.model.pipeline.v1.TestStreamPayload.Event.AddElements.elements', index=0,
       number=1, type=11, cpp_type=10, label=3,
       has_default_value=False, default_value=[],
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
   ],
   extensions=[
   ],
   nested_types=[],
   enum_types=[
   ],
   options=None,
   is_extendable=False,
   syntax='proto3',
   extension_ranges=[],
   oneofs=[
   ],
-  serialized_start=8032,
-  serialized_end=8136,
+  serialized_start=6718,
+  serialized_end=6822,
 )
 
 _TESTSTREAMPAYLOAD_EVENT = _descriptor.Descriptor(
   name='Event',
   full_name='org.apache.beam.model.pipeline.v1.TestStreamPayload.Event',
   filename=None,
   file=DESCRIPTOR,
@@ -2150,29 +1922,29 @@
   fields=[
     _descriptor.FieldDescriptor(
       name='watermark_event', full_name='org.apache.beam.model.pipeline.v1.TestStreamPayload.Event.watermark_event', index=0,
       number=1, type=11, cpp_type=10, label=1,
       has_default_value=False, default_value=None,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='processing_time_event', full_name='org.apache.beam.model.pipeline.v1.TestStreamPayload.Event.processing_time_event', index=1,
       number=2, type=11, cpp_type=10, label=1,
       has_default_value=False, default_value=None,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='element_event', full_name='org.apache.beam.model.pipeline.v1.TestStreamPayload.Event.element_event', index=2,
       number=3, type=11, cpp_type=10, label=1,
       has_default_value=False, default_value=None,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
   ],
   extensions=[
   ],
   nested_types=[_TESTSTREAMPAYLOAD_EVENT_ADVANCEWATERMARK, _TESTSTREAMPAYLOAD_EVENT_ADVANCEPROCESSINGTIME, _TESTSTREAMPAYLOAD_EVENT_ADDELEMENTS, ],
   enum_types=[
   ],
   options=None,
@@ -2180,16 +1952,16 @@
   syntax='proto3',
   extension_ranges=[],
   oneofs=[
     _descriptor.OneofDescriptor(
       name='event', full_name='org.apache.beam.model.pipeline.v1.TestStreamPayload.Event.event',
       index=0, containing_type=None, fields=[]),
   ],
-  serialized_start=7613,
-  serialized_end=8145,
+  serialized_start=6299,
+  serialized_end=6831,
 )
 
 _TESTSTREAMPAYLOAD_TIMESTAMPEDELEMENT = _descriptor.Descriptor(
   name='TimestampedElement',
   full_name='org.apache.beam.model.pipeline.v1.TestStreamPayload.TimestampedElement',
   filename=None,
   file=DESCRIPTOR,
@@ -2197,36 +1969,36 @@
   fields=[
     _descriptor.FieldDescriptor(
       name='encoded_element', full_name='org.apache.beam.model.pipeline.v1.TestStreamPayload.TimestampedElement.encoded_element', index=0,
       number=1, type=12, cpp_type=9, label=1,
       has_default_value=False, default_value=_b(""),
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='timestamp', full_name='org.apache.beam.model.pipeline.v1.TestStreamPayload.TimestampedElement.timestamp', index=1,
       number=2, type=3, cpp_type=2, label=1,
       has_default_value=False, default_value=0,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
   ],
   extensions=[
   ],
   nested_types=[],
   enum_types=[
   ],
   options=None,
   is_extendable=False,
   syntax='proto3',
   extension_ranges=[],
   oneofs=[
   ],
-  serialized_start=8147,
-  serialized_end=8211,
+  serialized_start=6833,
+  serialized_end=6897,
 )
 
 _TESTSTREAMPAYLOAD = _descriptor.Descriptor(
   name='TestStreamPayload',
   full_name='org.apache.beam.model.pipeline.v1.TestStreamPayload',
   filename=None,
   file=DESCRIPTOR,
@@ -2234,36 +2006,36 @@
   fields=[
     _descriptor.FieldDescriptor(
       name='coder_id', full_name='org.apache.beam.model.pipeline.v1.TestStreamPayload.coder_id', index=0,
       number=1, type=9, cpp_type=9, label=1,
       has_default_value=False, default_value=_b("").decode('utf-8'),
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='events', full_name='org.apache.beam.model.pipeline.v1.TestStreamPayload.events', index=1,
       number=2, type=11, cpp_type=10, label=3,
       has_default_value=False, default_value=[],
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
   ],
   extensions=[
   ],
   nested_types=[_TESTSTREAMPAYLOAD_EVENT, _TESTSTREAMPAYLOAD_TIMESTAMPEDELEMENT, ],
   enum_types=[
   ],
   options=None,
   is_extendable=False,
   syntax='proto3',
   extension_ranges=[],
   oneofs=[
   ],
-  serialized_start=7497,
-  serialized_end=8211,
+  serialized_start=6183,
+  serialized_end=6897,
 )
 
 
 _WRITEFILESPAYLOAD_SIDEINPUTSENTRY = _descriptor.Descriptor(
   name='SideInputsEntry',
   full_name='org.apache.beam.model.pipeline.v1.WriteFilesPayload.SideInputsEntry',
   filename=None,
@@ -2272,36 +2044,36 @@
   fields=[
     _descriptor.FieldDescriptor(
       name='key', full_name='org.apache.beam.model.pipeline.v1.WriteFilesPayload.SideInputsEntry.key', index=0,
       number=1, type=9, cpp_type=9, label=1,
       has_default_value=False, default_value=_b("").decode('utf-8'),
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='value', full_name='org.apache.beam.model.pipeline.v1.WriteFilesPayload.SideInputsEntry.value', index=1,
       number=2, type=11, cpp_type=10, label=1,
       has_default_value=False, default_value=None,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
   ],
   extensions=[
   ],
   nested_types=[],
   enum_types=[
   ],
   options=_descriptor._ParseOptions(descriptor_pb2.MessageOptions(), _b('8\001')),
   is_extendable=False,
   syntax='proto3',
   extension_ranges=[],
   oneofs=[
   ],
-  serialized_start=5821,
-  serialized_end=5916,
+  serialized_start=4483,
+  serialized_end=4578,
 )
 
 _WRITEFILESPAYLOAD = _descriptor.Descriptor(
   name='WriteFilesPayload',
   full_name='org.apache.beam.model.pipeline.v1.WriteFilesPayload',
   filename=None,
   file=DESCRIPTOR,
@@ -2309,57 +2081,57 @@
   fields=[
     _descriptor.FieldDescriptor(
       name='sink', full_name='org.apache.beam.model.pipeline.v1.WriteFilesPayload.sink', index=0,
       number=1, type=11, cpp_type=10, label=1,
       has_default_value=False, default_value=None,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='format_function', full_name='org.apache.beam.model.pipeline.v1.WriteFilesPayload.format_function', index=1,
       number=2, type=11, cpp_type=10, label=1,
       has_default_value=False, default_value=None,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='windowed_writes', full_name='org.apache.beam.model.pipeline.v1.WriteFilesPayload.windowed_writes', index=2,
       number=3, type=8, cpp_type=7, label=1,
       has_default_value=False, default_value=False,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='runner_determined_sharding', full_name='org.apache.beam.model.pipeline.v1.WriteFilesPayload.runner_determined_sharding', index=3,
       number=4, type=8, cpp_type=7, label=1,
       has_default_value=False, default_value=False,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='side_inputs', full_name='org.apache.beam.model.pipeline.v1.WriteFilesPayload.side_inputs', index=4,
       number=5, type=11, cpp_type=10, label=3,
       has_default_value=False, default_value=[],
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
   ],
   extensions=[
   ],
   nested_types=[_WRITEFILESPAYLOAD_SIDEINPUTSENTRY, ],
   enum_types=[
   ],
   options=None,
   is_extendable=False,
   syntax='proto3',
   extension_ranges=[],
   oneofs=[
   ],
-  serialized_start=8214,
-  serialized_end=8625,
+  serialized_start=6900,
+  serialized_end=7311,
 )
 
 
 _CODER = _descriptor.Descriptor(
   name='Coder',
   full_name='org.apache.beam.model.pipeline.v1.Coder',
   filename=None,
@@ -2368,36 +2140,36 @@
   fields=[
     _descriptor.FieldDescriptor(
       name='spec', full_name='org.apache.beam.model.pipeline.v1.Coder.spec', index=0,
       number=1, type=11, cpp_type=10, label=1,
       has_default_value=False, default_value=None,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='component_coder_ids', full_name='org.apache.beam.model.pipeline.v1.Coder.component_coder_ids', index=1,
       number=2, type=9, cpp_type=9, label=3,
       has_default_value=False, default_value=[],
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
   ],
   extensions=[
   ],
   nested_types=[],
   enum_types=[
   ],
   options=None,
   is_extendable=False,
   syntax='proto3',
   extension_ranges=[],
   oneofs=[
   ],
-  serialized_start=8627,
-  serialized_end=8729,
+  serialized_start=7313,
+  serialized_end=7415,
 )
 
 
 _STANDARDCODERS = _descriptor.Descriptor(
   name='StandardCoders',
   full_name='org.apache.beam.model.pipeline.v1.StandardCoders',
   filename=None,
@@ -2413,16 +2185,16 @@
   ],
   options=None,
   is_extendable=False,
   syntax='proto3',
   extension_ranges=[],
   oneofs=[
   ],
-  serialized_start=8732,
-  serialized_end=9171,
+  serialized_start=7418,
+  serialized_end=7857,
 )
 
 
 _WINDOWINGSTRATEGY = _descriptor.Descriptor(
   name='WindowingStrategy',
   full_name='org.apache.beam.model.pipeline.v1.WindowingStrategy',
   filename=None,
@@ -2431,92 +2203,92 @@
   fields=[
     _descriptor.FieldDescriptor(
       name='window_fn', full_name='org.apache.beam.model.pipeline.v1.WindowingStrategy.window_fn', index=0,
       number=1, type=11, cpp_type=10, label=1,
       has_default_value=False, default_value=None,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='merge_status', full_name='org.apache.beam.model.pipeline.v1.WindowingStrategy.merge_status', index=1,
       number=2, type=14, cpp_type=8, label=1,
       has_default_value=False, default_value=0,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='window_coder_id', full_name='org.apache.beam.model.pipeline.v1.WindowingStrategy.window_coder_id', index=2,
       number=3, type=9, cpp_type=9, label=1,
       has_default_value=False, default_value=_b("").decode('utf-8'),
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='trigger', full_name='org.apache.beam.model.pipeline.v1.WindowingStrategy.trigger', index=3,
       number=4, type=11, cpp_type=10, label=1,
       has_default_value=False, default_value=None,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='accumulation_mode', full_name='org.apache.beam.model.pipeline.v1.WindowingStrategy.accumulation_mode', index=4,
       number=5, type=14, cpp_type=8, label=1,
       has_default_value=False, default_value=0,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='output_time', full_name='org.apache.beam.model.pipeline.v1.WindowingStrategy.output_time', index=5,
       number=6, type=14, cpp_type=8, label=1,
       has_default_value=False, default_value=0,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='closing_behavior', full_name='org.apache.beam.model.pipeline.v1.WindowingStrategy.closing_behavior', index=6,
       number=7, type=14, cpp_type=8, label=1,
       has_default_value=False, default_value=0,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='allowed_lateness', full_name='org.apache.beam.model.pipeline.v1.WindowingStrategy.allowed_lateness', index=7,
       number=8, type=3, cpp_type=2, label=1,
       has_default_value=False, default_value=0,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='OnTimeBehavior', full_name='org.apache.beam.model.pipeline.v1.WindowingStrategy.OnTimeBehavior', index=8,
       number=9, type=14, cpp_type=8, label=1,
       has_default_value=False, default_value=0,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='assigns_to_one_window', full_name='org.apache.beam.model.pipeline.v1.WindowingStrategy.assigns_to_one_window', index=9,
       number=10, type=8, cpp_type=7, label=1,
       has_default_value=False, default_value=False,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
   ],
   extensions=[
   ],
   nested_types=[],
   enum_types=[
   ],
   options=None,
   is_extendable=False,
   syntax='proto3',
   extension_ranges=[],
   oneofs=[
   ],
-  serialized_start=9174,
-  serialized_end=9803,
+  serialized_start=7860,
+  serialized_end=8489,
 )
 
 
 _MERGESTATUS = _descriptor.Descriptor(
   name='MergeStatus',
   full_name='org.apache.beam.model.pipeline.v1.MergeStatus',
   filename=None,
@@ -2532,16 +2304,16 @@
   ],
   options=None,
   is_extendable=False,
   syntax='proto3',
   extension_ranges=[],
   oneofs=[
   ],
-  serialized_start=9805,
-  serialized_end=9897,
+  serialized_start=8491,
+  serialized_end=8583,
 )
 
 
 _ACCUMULATIONMODE = _descriptor.Descriptor(
   name='AccumulationMode',
   full_name='org.apache.beam.model.pipeline.v1.AccumulationMode',
   filename=None,
@@ -2557,16 +2329,16 @@
   ],
   options=None,
   is_extendable=False,
   syntax='proto3',
   extension_ranges=[],
   oneofs=[
   ],
-  serialized_start=9899,
-  serialized_end=9976,
+  serialized_start=8585,
+  serialized_end=8662,
 )
 
 
 _CLOSINGBEHAVIOR = _descriptor.Descriptor(
   name='ClosingBehavior',
   full_name='org.apache.beam.model.pipeline.v1.ClosingBehavior',
   filename=None,
@@ -2582,16 +2354,16 @@
   ],
   options=None,
   is_extendable=False,
   syntax='proto3',
   extension_ranges=[],
   oneofs=[
   ],
-  serialized_start=9978,
-  serialized_end=10059,
+  serialized_start=8664,
+  serialized_end=8745,
 )
 
 
 _ONTIMEBEHAVIOR = _descriptor.Descriptor(
   name='OnTimeBehavior',
   full_name='org.apache.beam.model.pipeline.v1.OnTimeBehavior',
   filename=None,
@@ -2607,16 +2379,16 @@
   ],
   options=None,
   is_extendable=False,
   syntax='proto3',
   extension_ranges=[],
   oneofs=[
   ],
-  serialized_start=10061,
-  serialized_end=10141,
+  serialized_start=8747,
+  serialized_end=8827,
 )
 
 
 _OUTPUTTIME = _descriptor.Descriptor(
   name='OutputTime',
   full_name='org.apache.beam.model.pipeline.v1.OutputTime',
   filename=None,
@@ -2632,16 +2404,16 @@
   ],
   options=None,
   is_extendable=False,
   syntax='proto3',
   extension_ranges=[],
   oneofs=[
   ],
-  serialized_start=10143,
-  serialized_end=10241,
+  serialized_start=8829,
+  serialized_end=8927,
 )
 
 
 _TIMEDOMAIN = _descriptor.Descriptor(
   name='TimeDomain',
   full_name='org.apache.beam.model.pipeline.v1.TimeDomain',
   filename=None,
@@ -2657,16 +2429,16 @@
   ],
   options=None,
   is_extendable=False,
   syntax='proto3',
   extension_ranges=[],
   oneofs=[
   ],
-  serialized_start=10243,
-  serialized_end=10351,
+  serialized_start=8929,
+  serialized_end=9037,
 )
 
 
 _TRIGGER_AFTERALL = _descriptor.Descriptor(
   name='AfterAll',
   full_name='org.apache.beam.model.pipeline.v1.Trigger.AfterAll',
   filename=None,
@@ -2675,29 +2447,29 @@
   fields=[
     _descriptor.FieldDescriptor(
       name='subtriggers', full_name='org.apache.beam.model.pipeline.v1.Trigger.AfterAll.subtriggers', index=0,
       number=1, type=11, cpp_type=10, label=3,
       has_default_value=False, default_value=[],
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
   ],
   extensions=[
   ],
   nested_types=[],
   enum_types=[
   ],
   options=None,
   is_extendable=False,
   syntax='proto3',
   extension_ranges=[],
   oneofs=[
   ],
-  serialized_start=11334,
-  serialized_end=11409,
+  serialized_start=10020,
+  serialized_end=10095,
 )
 
 _TRIGGER_AFTERANY = _descriptor.Descriptor(
   name='AfterAny',
   full_name='org.apache.beam.model.pipeline.v1.Trigger.AfterAny',
   filename=None,
   file=DESCRIPTOR,
@@ -2705,29 +2477,29 @@
   fields=[
     _descriptor.FieldDescriptor(
       name='subtriggers', full_name='org.apache.beam.model.pipeline.v1.Trigger.AfterAny.subtriggers', index=0,
       number=1, type=11, cpp_type=10, label=3,
       has_default_value=False, default_value=[],
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
   ],
   extensions=[
   ],
   nested_types=[],
   enum_types=[
   ],
   options=None,
   is_extendable=False,
   syntax='proto3',
   extension_ranges=[],
   oneofs=[
   ],
-  serialized_start=11411,
-  serialized_end=11486,
+  serialized_start=10097,
+  serialized_end=10172,
 )
 
 _TRIGGER_AFTEREACH = _descriptor.Descriptor(
   name='AfterEach',
   full_name='org.apache.beam.model.pipeline.v1.Trigger.AfterEach',
   filename=None,
   file=DESCRIPTOR,
@@ -2735,29 +2507,29 @@
   fields=[
     _descriptor.FieldDescriptor(
       name='subtriggers', full_name='org.apache.beam.model.pipeline.v1.Trigger.AfterEach.subtriggers', index=0,
       number=1, type=11, cpp_type=10, label=3,
       has_default_value=False, default_value=[],
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
   ],
   extensions=[
   ],
   nested_types=[],
   enum_types=[
   ],
   options=None,
   is_extendable=False,
   syntax='proto3',
   extension_ranges=[],
   oneofs=[
   ],
-  serialized_start=11488,
-  serialized_end=11564,
+  serialized_start=10174,
+  serialized_end=10250,
 )
 
 _TRIGGER_AFTERENDOFWINDOW = _descriptor.Descriptor(
   name='AfterEndOfWindow',
   full_name='org.apache.beam.model.pipeline.v1.Trigger.AfterEndOfWindow',
   filename=None,
   file=DESCRIPTOR,
@@ -2765,36 +2537,36 @@
   fields=[
     _descriptor.FieldDescriptor(
       name='early_firings', full_name='org.apache.beam.model.pipeline.v1.Trigger.AfterEndOfWindow.early_firings', index=0,
       number=1, type=11, cpp_type=10, label=1,
       has_default_value=False, default_value=None,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='late_firings', full_name='org.apache.beam.model.pipeline.v1.Trigger.AfterEndOfWindow.late_firings', index=1,
       number=2, type=11, cpp_type=10, label=1,
       has_default_value=False, default_value=None,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
   ],
   extensions=[
   ],
   nested_types=[],
   enum_types=[
   ],
   options=None,
   is_extendable=False,
   syntax='proto3',
   extension_ranges=[],
   oneofs=[
   ],
-  serialized_start=11567,
-  serialized_end=11718,
+  serialized_start=10253,
+  serialized_end=10404,
 )
 
 _TRIGGER_AFTERPROCESSINGTIME = _descriptor.Descriptor(
   name='AfterProcessingTime',
   full_name='org.apache.beam.model.pipeline.v1.Trigger.AfterProcessingTime',
   filename=None,
   file=DESCRIPTOR,
@@ -2802,29 +2574,29 @@
   fields=[
     _descriptor.FieldDescriptor(
       name='timestamp_transforms', full_name='org.apache.beam.model.pipeline.v1.Trigger.AfterProcessingTime.timestamp_transforms', index=0,
       number=1, type=11, cpp_type=10, label=3,
       has_default_value=False, default_value=[],
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
   ],
   extensions=[
   ],
   nested_types=[],
   enum_types=[
   ],
   options=None,
   is_extendable=False,
   syntax='proto3',
   extension_ranges=[],
   oneofs=[
   ],
-  serialized_start=11720,
-  serialized_end=11826,
+  serialized_start=10406,
+  serialized_end=10512,
 )
 
 _TRIGGER_AFTERSYNCHRONIZEDPROCESSINGTIME = _descriptor.Descriptor(
   name='AfterSynchronizedProcessingTime',
   full_name='org.apache.beam.model.pipeline.v1.Trigger.AfterSynchronizedProcessingTime',
   filename=None,
   file=DESCRIPTOR,
@@ -2838,16 +2610,16 @@
   ],
   options=None,
   is_extendable=False,
   syntax='proto3',
   extension_ranges=[],
   oneofs=[
   ],
-  serialized_start=11828,
-  serialized_end=11861,
+  serialized_start=10514,
+  serialized_end=10547,
 )
 
 _TRIGGER_DEFAULT = _descriptor.Descriptor(
   name='Default',
   full_name='org.apache.beam.model.pipeline.v1.Trigger.Default',
   filename=None,
   file=DESCRIPTOR,
@@ -2861,16 +2633,16 @@
   ],
   options=None,
   is_extendable=False,
   syntax='proto3',
   extension_ranges=[],
   oneofs=[
   ],
-  serialized_start=11863,
-  serialized_end=11872,
+  serialized_start=10549,
+  serialized_end=10558,
 )
 
 _TRIGGER_ELEMENTCOUNT = _descriptor.Descriptor(
   name='ElementCount',
   full_name='org.apache.beam.model.pipeline.v1.Trigger.ElementCount',
   filename=None,
   file=DESCRIPTOR,
@@ -2878,29 +2650,29 @@
   fields=[
     _descriptor.FieldDescriptor(
       name='element_count', full_name='org.apache.beam.model.pipeline.v1.Trigger.ElementCount.element_count', index=0,
       number=1, type=5, cpp_type=1, label=1,
       has_default_value=False, default_value=0,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
   ],
   extensions=[
   ],
   nested_types=[],
   enum_types=[
   ],
   options=None,
   is_extendable=False,
   syntax='proto3',
   extension_ranges=[],
   oneofs=[
   ],
-  serialized_start=11874,
-  serialized_end=11911,
+  serialized_start=10560,
+  serialized_end=10597,
 )
 
 _TRIGGER_NEVER = _descriptor.Descriptor(
   name='Never',
   full_name='org.apache.beam.model.pipeline.v1.Trigger.Never',
   filename=None,
   file=DESCRIPTOR,
@@ -2914,16 +2686,16 @@
   ],
   options=None,
   is_extendable=False,
   syntax='proto3',
   extension_ranges=[],
   oneofs=[
   ],
-  serialized_start=11913,
-  serialized_end=11920,
+  serialized_start=10599,
+  serialized_end=10606,
 )
 
 _TRIGGER_ALWAYS = _descriptor.Descriptor(
   name='Always',
   full_name='org.apache.beam.model.pipeline.v1.Trigger.Always',
   filename=None,
   file=DESCRIPTOR,
@@ -2937,16 +2709,16 @@
   ],
   options=None,
   is_extendable=False,
   syntax='proto3',
   extension_ranges=[],
   oneofs=[
   ],
-  serialized_start=11922,
-  serialized_end=11930,
+  serialized_start=10608,
+  serialized_end=10616,
 )
 
 _TRIGGER_ORFINALLY = _descriptor.Descriptor(
   name='OrFinally',
   full_name='org.apache.beam.model.pipeline.v1.Trigger.OrFinally',
   filename=None,
   file=DESCRIPTOR,
@@ -2954,36 +2726,36 @@
   fields=[
     _descriptor.FieldDescriptor(
       name='main', full_name='org.apache.beam.model.pipeline.v1.Trigger.OrFinally.main', index=0,
       number=1, type=11, cpp_type=10, label=1,
       has_default_value=False, default_value=None,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='finally', full_name='org.apache.beam.model.pipeline.v1.Trigger.OrFinally.finally', index=1,
       number=2, type=11, cpp_type=10, label=1,
       has_default_value=False, default_value=None,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
   ],
   extensions=[
   ],
   nested_types=[],
   enum_types=[
   ],
   options=None,
   is_extendable=False,
   syntax='proto3',
   extension_ranges=[],
   oneofs=[
   ],
-  serialized_start=11933,
-  serialized_end=12063,
+  serialized_start=10619,
+  serialized_end=10749,
 )
 
 _TRIGGER_REPEAT = _descriptor.Descriptor(
   name='Repeat',
   full_name='org.apache.beam.model.pipeline.v1.Trigger.Repeat',
   filename=None,
   file=DESCRIPTOR,
@@ -2991,29 +2763,29 @@
   fields=[
     _descriptor.FieldDescriptor(
       name='subtrigger', full_name='org.apache.beam.model.pipeline.v1.Trigger.Repeat.subtrigger', index=0,
       number=1, type=11, cpp_type=10, label=1,
       has_default_value=False, default_value=None,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
   ],
   extensions=[
   ],
   nested_types=[],
   enum_types=[
   ],
   options=None,
   is_extendable=False,
   syntax='proto3',
   extension_ranges=[],
   oneofs=[
   ],
-  serialized_start=12065,
-  serialized_end=12137,
+  serialized_start=10751,
+  serialized_end=10823,
 )
 
 _TRIGGER = _descriptor.Descriptor(
   name='Trigger',
   full_name='org.apache.beam.model.pipeline.v1.Trigger',
   filename=None,
   file=DESCRIPTOR,
@@ -3021,92 +2793,92 @@
   fields=[
     _descriptor.FieldDescriptor(
       name='after_all', full_name='org.apache.beam.model.pipeline.v1.Trigger.after_all', index=0,
       number=1, type=11, cpp_type=10, label=1,
       has_default_value=False, default_value=None,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='after_any', full_name='org.apache.beam.model.pipeline.v1.Trigger.after_any', index=1,
       number=2, type=11, cpp_type=10, label=1,
       has_default_value=False, default_value=None,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='after_each', full_name='org.apache.beam.model.pipeline.v1.Trigger.after_each', index=2,
       number=3, type=11, cpp_type=10, label=1,
       has_default_value=False, default_value=None,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='after_end_of_window', full_name='org.apache.beam.model.pipeline.v1.Trigger.after_end_of_window', index=3,
       number=4, type=11, cpp_type=10, label=1,
       has_default_value=False, default_value=None,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='after_processing_time', full_name='org.apache.beam.model.pipeline.v1.Trigger.after_processing_time', index=4,
       number=5, type=11, cpp_type=10, label=1,
       has_default_value=False, default_value=None,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='after_synchronized_processing_time', full_name='org.apache.beam.model.pipeline.v1.Trigger.after_synchronized_processing_time', index=5,
       number=6, type=11, cpp_type=10, label=1,
       has_default_value=False, default_value=None,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='always', full_name='org.apache.beam.model.pipeline.v1.Trigger.always', index=6,
       number=12, type=11, cpp_type=10, label=1,
       has_default_value=False, default_value=None,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='default', full_name='org.apache.beam.model.pipeline.v1.Trigger.default', index=7,
       number=7, type=11, cpp_type=10, label=1,
       has_default_value=False, default_value=None,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='element_count', full_name='org.apache.beam.model.pipeline.v1.Trigger.element_count', index=8,
       number=8, type=11, cpp_type=10, label=1,
       has_default_value=False, default_value=None,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='never', full_name='org.apache.beam.model.pipeline.v1.Trigger.never', index=9,
       number=9, type=11, cpp_type=10, label=1,
       has_default_value=False, default_value=None,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='or_finally', full_name='org.apache.beam.model.pipeline.v1.Trigger.or_finally', index=10,
       number=10, type=11, cpp_type=10, label=1,
       has_default_value=False, default_value=None,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='repeat', full_name='org.apache.beam.model.pipeline.v1.Trigger.repeat', index=11,
       number=11, type=11, cpp_type=10, label=1,
       has_default_value=False, default_value=None,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
   ],
   extensions=[
   ],
   nested_types=[_TRIGGER_AFTERALL, _TRIGGER_AFTERANY, _TRIGGER_AFTEREACH, _TRIGGER_AFTERENDOFWINDOW, _TRIGGER_AFTERPROCESSINGTIME, _TRIGGER_AFTERSYNCHRONIZEDPROCESSINGTIME, _TRIGGER_DEFAULT, _TRIGGER_ELEMENTCOUNT, _TRIGGER_NEVER, _TRIGGER_ALWAYS, _TRIGGER_ORFINALLY, _TRIGGER_REPEAT, ],
   enum_types=[
   ],
   options=None,
@@ -3114,16 +2886,16 @@
   syntax='proto3',
   extension_ranges=[],
   oneofs=[
     _descriptor.OneofDescriptor(
       name='trigger', full_name='org.apache.beam.model.pipeline.v1.Trigger.trigger',
       index=0, containing_type=None, fields=[]),
   ],
-  serialized_start=10354,
-  serialized_end=12148,
+  serialized_start=9040,
+  serialized_end=10834,
 )
 
 
 _TIMESTAMPTRANSFORM_DELAY = _descriptor.Descriptor(
   name='Delay',
   full_name='org.apache.beam.model.pipeline.v1.TimestampTransform.Delay',
   filename=None,
@@ -3132,29 +2904,29 @@
   fields=[
     _descriptor.FieldDescriptor(
       name='delay_millis', full_name='org.apache.beam.model.pipeline.v1.TimestampTransform.Delay.delay_millis', index=0,
       number=1, type=3, cpp_type=2, label=1,
       has_default_value=False, default_value=0,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
   ],
   extensions=[
   ],
   nested_types=[],
   enum_types=[
   ],
   options=None,
   is_extendable=False,
   syntax='proto3',
   extension_ranges=[],
   oneofs=[
   ],
-  serialized_start=12334,
-  serialized_end=12363,
+  serialized_start=11020,
+  serialized_end=11049,
 )
 
 _TIMESTAMPTRANSFORM_ALIGNTO = _descriptor.Descriptor(
   name='AlignTo',
   full_name='org.apache.beam.model.pipeline.v1.TimestampTransform.AlignTo',
   filename=None,
   file=DESCRIPTOR,
@@ -3162,36 +2934,36 @@
   fields=[
     _descriptor.FieldDescriptor(
       name='period', full_name='org.apache.beam.model.pipeline.v1.TimestampTransform.AlignTo.period', index=0,
       number=3, type=3, cpp_type=2, label=1,
       has_default_value=False, default_value=0,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='offset', full_name='org.apache.beam.model.pipeline.v1.TimestampTransform.AlignTo.offset', index=1,
       number=4, type=3, cpp_type=2, label=1,
       has_default_value=False, default_value=0,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
   ],
   extensions=[
   ],
   nested_types=[],
   enum_types=[
   ],
   options=None,
   is_extendable=False,
   syntax='proto3',
   extension_ranges=[],
   oneofs=[
   ],
-  serialized_start=12365,
-  serialized_end=12406,
+  serialized_start=11051,
+  serialized_end=11092,
 )
 
 _TIMESTAMPTRANSFORM = _descriptor.Descriptor(
   name='TimestampTransform',
   full_name='org.apache.beam.model.pipeline.v1.TimestampTransform',
   filename=None,
   file=DESCRIPTOR,
@@ -3199,22 +2971,22 @@
   fields=[
     _descriptor.FieldDescriptor(
       name='delay', full_name='org.apache.beam.model.pipeline.v1.TimestampTransform.delay', index=0,
       number=1, type=11, cpp_type=10, label=1,
       has_default_value=False, default_value=None,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='align_to', full_name='org.apache.beam.model.pipeline.v1.TimestampTransform.align_to', index=1,
       number=2, type=11, cpp_type=10, label=1,
       has_default_value=False, default_value=None,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
   ],
   extensions=[
   ],
   nested_types=[_TIMESTAMPTRANSFORM_DELAY, _TIMESTAMPTRANSFORM_ALIGNTO, ],
   enum_types=[
   ],
   options=None,
@@ -3222,16 +2994,16 @@
   syntax='proto3',
   extension_ranges=[],
   oneofs=[
     _descriptor.OneofDescriptor(
       name='timestamp_transform', full_name='org.apache.beam.model.pipeline.v1.TimestampTransform.timestamp_transform',
       index=0, containing_type=None, fields=[]),
   ],
-  serialized_start=12151,
-  serialized_end=12429,
+  serialized_start=10837,
+  serialized_end=11115,
 )
 
 
 _SIDEINPUT = _descriptor.Descriptor(
   name='SideInput',
   full_name='org.apache.beam.model.pipeline.v1.SideInput',
   filename=None,
@@ -3240,43 +3012,43 @@
   fields=[
     _descriptor.FieldDescriptor(
       name='access_pattern', full_name='org.apache.beam.model.pipeline.v1.SideInput.access_pattern', index=0,
       number=1, type=11, cpp_type=10, label=1,
       has_default_value=False, default_value=None,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='view_fn', full_name='org.apache.beam.model.pipeline.v1.SideInput.view_fn', index=1,
       number=2, type=11, cpp_type=10, label=1,
       has_default_value=False, default_value=None,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='window_mapping_fn', full_name='org.apache.beam.model.pipeline.v1.SideInput.window_mapping_fn', index=2,
       number=3, type=11, cpp_type=10, label=1,
       has_default_value=False, default_value=None,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
   ],
   extensions=[
   ],
   nested_types=[],
   enum_types=[
   ],
   options=None,
   is_extendable=False,
   syntax='proto3',
   extension_ranges=[],
   oneofs=[
   ],
-  serialized_start=12432,
-  serialized_end=12664,
+  serialized_start=11118,
+  serialized_end=11350,
 )
 
 
 _ENVIRONMENT = _descriptor.Descriptor(
   name='Environment',
   full_name='org.apache.beam.model.pipeline.v1.Environment',
   filename=None,
@@ -3285,43 +3057,43 @@
   fields=[
     _descriptor.FieldDescriptor(
       name='url', full_name='org.apache.beam.model.pipeline.v1.Environment.url', index=0,
       number=1, type=9, cpp_type=9, label=1,
       has_default_value=False, default_value=_b("").decode('utf-8'),
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='urn', full_name='org.apache.beam.model.pipeline.v1.Environment.urn', index=1,
       number=2, type=9, cpp_type=9, label=1,
       has_default_value=False, default_value=_b("").decode('utf-8'),
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='payload', full_name='org.apache.beam.model.pipeline.v1.Environment.payload', index=2,
       number=3, type=12, cpp_type=9, label=1,
       has_default_value=False, default_value=_b(""),
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
   ],
   extensions=[
   ],
   nested_types=[],
   enum_types=[
   ],
   options=None,
   is_extendable=False,
   syntax='proto3',
   extension_ranges=[],
   oneofs=[
   ],
-  serialized_start=12666,
-  serialized_end=12722,
+  serialized_start=11352,
+  serialized_end=11408,
 )
 
 
 _STANDARDENVIRONMENTS = _descriptor.Descriptor(
   name='StandardEnvironments',
   full_name='org.apache.beam.model.pipeline.v1.StandardEnvironments',
   filename=None,
@@ -3337,16 +3109,16 @@
   ],
   options=None,
   is_extendable=False,
   syntax='proto3',
   extension_ranges=[],
   oneofs=[
   ],
-  serialized_start=12725,
-  serialized_end=12884,
+  serialized_start=11411,
+  serialized_end=11570,
 )
 
 
 _DOCKERPAYLOAD = _descriptor.Descriptor(
   name='DockerPayload',
   full_name='org.apache.beam.model.pipeline.v1.DockerPayload',
   filename=None,
@@ -3355,29 +3127,29 @@
   fields=[
     _descriptor.FieldDescriptor(
       name='container_image', full_name='org.apache.beam.model.pipeline.v1.DockerPayload.container_image', index=0,
       number=1, type=9, cpp_type=9, label=1,
       has_default_value=False, default_value=_b("").decode('utf-8'),
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
   ],
   extensions=[
   ],
   nested_types=[],
   enum_types=[
   ],
   options=None,
   is_extendable=False,
   syntax='proto3',
   extension_ranges=[],
   oneofs=[
   ],
-  serialized_start=12886,
-  serialized_end=12926,
+  serialized_start=11572,
+  serialized_end=11612,
 )
 
 
 _PROCESSPAYLOAD_ENVENTRY = _descriptor.Descriptor(
   name='EnvEntry',
   full_name='org.apache.beam.model.pipeline.v1.ProcessPayload.EnvEntry',
   filename=None,
@@ -3386,36 +3158,36 @@
   fields=[
     _descriptor.FieldDescriptor(
       name='key', full_name='org.apache.beam.model.pipeline.v1.ProcessPayload.EnvEntry.key', index=0,
       number=1, type=9, cpp_type=9, label=1,
       has_default_value=False, default_value=_b("").decode('utf-8'),
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='value', full_name='org.apache.beam.model.pipeline.v1.ProcessPayload.EnvEntry.value', index=1,
       number=2, type=9, cpp_type=9, label=1,
       has_default_value=False, default_value=_b("").decode('utf-8'),
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
   ],
   extensions=[
   ],
   nested_types=[],
   enum_types=[
   ],
   options=_descriptor._ParseOptions(descriptor_pb2.MessageOptions(), _b('8\001')),
   is_extendable=False,
   syntax='proto3',
   extension_ranges=[],
   oneofs=[
   ],
-  serialized_start=13063,
-  serialized_end=13105,
+  serialized_start=11749,
+  serialized_end=11791,
 )
 
 _PROCESSPAYLOAD = _descriptor.Descriptor(
   name='ProcessPayload',
   full_name='org.apache.beam.model.pipeline.v1.ProcessPayload',
   filename=None,
   file=DESCRIPTOR,
@@ -3423,50 +3195,50 @@
   fields=[
     _descriptor.FieldDescriptor(
       name='os', full_name='org.apache.beam.model.pipeline.v1.ProcessPayload.os', index=0,
       number=1, type=9, cpp_type=9, label=1,
       has_default_value=False, default_value=_b("").decode('utf-8'),
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='arch', full_name='org.apache.beam.model.pipeline.v1.ProcessPayload.arch', index=1,
       number=2, type=9, cpp_type=9, label=1,
       has_default_value=False, default_value=_b("").decode('utf-8'),
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='command', full_name='org.apache.beam.model.pipeline.v1.ProcessPayload.command', index=2,
       number=3, type=9, cpp_type=9, label=1,
       has_default_value=False, default_value=_b("").decode('utf-8'),
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='env', full_name='org.apache.beam.model.pipeline.v1.ProcessPayload.env', index=3,
       number=4, type=11, cpp_type=10, label=3,
       has_default_value=False, default_value=[],
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
   ],
   extensions=[
   ],
   nested_types=[_PROCESSPAYLOAD_ENVENTRY, ],
   enum_types=[
   ],
   options=None,
   is_extendable=False,
   syntax='proto3',
   extension_ranges=[],
   oneofs=[
   ],
-  serialized_start=12929,
-  serialized_end=13105,
+  serialized_start=11615,
+  serialized_end=11791,
 )
 
 
 _SDKFUNCTIONSPEC = _descriptor.Descriptor(
   name='SdkFunctionSpec',
   full_name='org.apache.beam.model.pipeline.v1.SdkFunctionSpec',
   filename=None,
@@ -3475,36 +3247,36 @@
   fields=[
     _descriptor.FieldDescriptor(
       name='spec', full_name='org.apache.beam.model.pipeline.v1.SdkFunctionSpec.spec', index=0,
       number=1, type=11, cpp_type=10, label=1,
       has_default_value=False, default_value=None,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='environment_id', full_name='org.apache.beam.model.pipeline.v1.SdkFunctionSpec.environment_id', index=1,
       number=2, type=9, cpp_type=9, label=1,
       has_default_value=False, default_value=_b("").decode('utf-8'),
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
   ],
   extensions=[
   ],
   nested_types=[],
   enum_types=[
   ],
   options=None,
   is_extendable=False,
   syntax='proto3',
   extension_ranges=[],
   oneofs=[
   ],
-  serialized_start=13107,
-  serialized_end=13211,
+  serialized_start=11793,
+  serialized_end=11897,
 )
 
 
 _FUNCTIONSPEC = _descriptor.Descriptor(
   name='FunctionSpec',
   full_name='org.apache.beam.model.pipeline.v1.FunctionSpec',
   filename=None,
@@ -3513,36 +3285,36 @@
   fields=[
     _descriptor.FieldDescriptor(
       name='urn', full_name='org.apache.beam.model.pipeline.v1.FunctionSpec.urn', index=0,
       number=1, type=9, cpp_type=9, label=1,
       has_default_value=False, default_value=_b("").decode('utf-8'),
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='payload', full_name='org.apache.beam.model.pipeline.v1.FunctionSpec.payload', index=1,
       number=3, type=12, cpp_type=9, label=1,
       has_default_value=False, default_value=_b(""),
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
   ],
   extensions=[
   ],
   nested_types=[],
   enum_types=[
   ],
   options=None,
   is_extendable=False,
   syntax='proto3',
   extension_ranges=[],
   oneofs=[
   ],
-  serialized_start=13213,
-  serialized_end=13257,
+  serialized_start=11899,
+  serialized_end=11943,
 )
 
 
 _DISPLAYDATA_IDENTIFIER = _descriptor.Descriptor(
   name='Identifier',
   full_name='org.apache.beam.model.pipeline.v1.DisplayData.Identifier',
   filename=None,
@@ -3551,43 +3323,43 @@
   fields=[
     _descriptor.FieldDescriptor(
       name='transform_id', full_name='org.apache.beam.model.pipeline.v1.DisplayData.Identifier.transform_id', index=0,
       number=1, type=9, cpp_type=9, label=1,
       has_default_value=False, default_value=_b("").decode('utf-8'),
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='transform_urn', full_name='org.apache.beam.model.pipeline.v1.DisplayData.Identifier.transform_urn', index=1,
       number=2, type=9, cpp_type=9, label=1,
       has_default_value=False, default_value=_b("").decode('utf-8'),
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='key', full_name='org.apache.beam.model.pipeline.v1.DisplayData.Identifier.key', index=2,
       number=3, type=9, cpp_type=9, label=1,
       has_default_value=False, default_value=_b("").decode('utf-8'),
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
   ],
   extensions=[
   ],
   nested_types=[],
   enum_types=[
   ],
   options=None,
   is_extendable=False,
   syntax='proto3',
   extension_ranges=[],
   oneofs=[
   ],
-  serialized_start=13343,
-  serialized_end=13413,
+  serialized_start=12029,
+  serialized_end=12099,
 )
 
 _DISPLAYDATA_ITEM = _descriptor.Descriptor(
   name='Item',
   full_name='org.apache.beam.model.pipeline.v1.DisplayData.Item',
   filename=None,
   file=DESCRIPTOR,
@@ -3595,64 +3367,64 @@
   fields=[
     _descriptor.FieldDescriptor(
       name='id', full_name='org.apache.beam.model.pipeline.v1.DisplayData.Item.id', index=0,
       number=1, type=11, cpp_type=10, label=1,
       has_default_value=False, default_value=None,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='type', full_name='org.apache.beam.model.pipeline.v1.DisplayData.Item.type', index=1,
       number=2, type=14, cpp_type=8, label=1,
       has_default_value=False, default_value=0,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='value', full_name='org.apache.beam.model.pipeline.v1.DisplayData.Item.value', index=2,
       number=3, type=11, cpp_type=10, label=1,
       has_default_value=False, default_value=None,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='short_value', full_name='org.apache.beam.model.pipeline.v1.DisplayData.Item.short_value', index=3,
       number=4, type=11, cpp_type=10, label=1,
       has_default_value=False, default_value=None,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='label', full_name='org.apache.beam.model.pipeline.v1.DisplayData.Item.label', index=4,
       number=5, type=9, cpp_type=9, label=1,
       has_default_value=False, default_value=_b("").decode('utf-8'),
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='link_url', full_name='org.apache.beam.model.pipeline.v1.DisplayData.Item.link_url', index=5,
       number=6, type=9, cpp_type=9, label=1,
       has_default_value=False, default_value=_b("").decode('utf-8'),
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
   ],
   extensions=[
   ],
   nested_types=[],
   enum_types=[
   ],
   options=None,
   is_extendable=False,
   syntax='proto3',
   extension_ranges=[],
   oneofs=[
   ],
-  serialized_start=13416,
-  serialized_end=13678,
+  serialized_start=12102,
+  serialized_end=12364,
 )
 
 _DISPLAYDATA_TYPE = _descriptor.Descriptor(
   name='Type',
   full_name='org.apache.beam.model.pipeline.v1.DisplayData.Type',
   filename=None,
   file=DESCRIPTOR,
@@ -3667,16 +3439,16 @@
   ],
   options=None,
   is_extendable=False,
   syntax='proto3',
   extension_ranges=[],
   oneofs=[
   ],
-  serialized_start=13680,
-  serialized_end=13805,
+  serialized_start=12366,
+  serialized_end=12491,
 )
 
 _DISPLAYDATA = _descriptor.Descriptor(
   name='DisplayData',
   full_name='org.apache.beam.model.pipeline.v1.DisplayData',
   filename=None,
   file=DESCRIPTOR,
@@ -3684,31 +3456,334 @@
   fields=[
     _descriptor.FieldDescriptor(
       name='items', full_name='org.apache.beam.model.pipeline.v1.DisplayData.items', index=0,
       number=1, type=11, cpp_type=10, label=3,
       has_default_value=False, default_value=[],
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
   ],
   extensions=[
   ],
   nested_types=[_DISPLAYDATA_IDENTIFIER, _DISPLAYDATA_ITEM, _DISPLAYDATA_TYPE, ],
   enum_types=[
   ],
   options=None,
   is_extendable=False,
   syntax='proto3',
   extension_ranges=[],
   oneofs=[
   ],
-  serialized_start=13260,
-  serialized_end=13805,
+  serialized_start=11946,
+  serialized_end=12491,
+)
+
+
+_MESSAGEWITHCOMPONENTS = _descriptor.Descriptor(
+  name='MessageWithComponents',
+  full_name='org.apache.beam.model.pipeline.v1.MessageWithComponents',
+  filename=None,
+  file=DESCRIPTOR,
+  containing_type=None,
+  fields=[
+    _descriptor.FieldDescriptor(
+      name='components', full_name='org.apache.beam.model.pipeline.v1.MessageWithComponents.components', index=0,
+      number=1, type=11, cpp_type=10, label=1,
+      has_default_value=False, default_value=None,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      options=None),
+    _descriptor.FieldDescriptor(
+      name='coder', full_name='org.apache.beam.model.pipeline.v1.MessageWithComponents.coder', index=1,
+      number=2, type=11, cpp_type=10, label=1,
+      has_default_value=False, default_value=None,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      options=None),
+    _descriptor.FieldDescriptor(
+      name='combine_payload', full_name='org.apache.beam.model.pipeline.v1.MessageWithComponents.combine_payload', index=2,
+      number=3, type=11, cpp_type=10, label=1,
+      has_default_value=False, default_value=None,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      options=None),
+    _descriptor.FieldDescriptor(
+      name='sdk_function_spec', full_name='org.apache.beam.model.pipeline.v1.MessageWithComponents.sdk_function_spec', index=3,
+      number=4, type=11, cpp_type=10, label=1,
+      has_default_value=False, default_value=None,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      options=None),
+    _descriptor.FieldDescriptor(
+      name='par_do_payload', full_name='org.apache.beam.model.pipeline.v1.MessageWithComponents.par_do_payload', index=4,
+      number=6, type=11, cpp_type=10, label=1,
+      has_default_value=False, default_value=None,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      options=None),
+    _descriptor.FieldDescriptor(
+      name='ptransform', full_name='org.apache.beam.model.pipeline.v1.MessageWithComponents.ptransform', index=5,
+      number=7, type=11, cpp_type=10, label=1,
+      has_default_value=False, default_value=None,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      options=None),
+    _descriptor.FieldDescriptor(
+      name='pcollection', full_name='org.apache.beam.model.pipeline.v1.MessageWithComponents.pcollection', index=6,
+      number=8, type=11, cpp_type=10, label=1,
+      has_default_value=False, default_value=None,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      options=None),
+    _descriptor.FieldDescriptor(
+      name='read_payload', full_name='org.apache.beam.model.pipeline.v1.MessageWithComponents.read_payload', index=7,
+      number=9, type=11, cpp_type=10, label=1,
+      has_default_value=False, default_value=None,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      options=None),
+    _descriptor.FieldDescriptor(
+      name='side_input', full_name='org.apache.beam.model.pipeline.v1.MessageWithComponents.side_input', index=8,
+      number=11, type=11, cpp_type=10, label=1,
+      has_default_value=False, default_value=None,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      options=None),
+    _descriptor.FieldDescriptor(
+      name='window_into_payload', full_name='org.apache.beam.model.pipeline.v1.MessageWithComponents.window_into_payload', index=9,
+      number=12, type=11, cpp_type=10, label=1,
+      has_default_value=False, default_value=None,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      options=None),
+    _descriptor.FieldDescriptor(
+      name='windowing_strategy', full_name='org.apache.beam.model.pipeline.v1.MessageWithComponents.windowing_strategy', index=10,
+      number=13, type=11, cpp_type=10, label=1,
+      has_default_value=False, default_value=None,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      options=None),
+    _descriptor.FieldDescriptor(
+      name='function_spec', full_name='org.apache.beam.model.pipeline.v1.MessageWithComponents.function_spec', index=11,
+      number=14, type=11, cpp_type=10, label=1,
+      has_default_value=False, default_value=None,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      options=None),
+  ],
+  extensions=[
+  ],
+  nested_types=[],
+  enum_types=[
+  ],
+  options=None,
+  is_extendable=False,
+  syntax='proto3',
+  extension_ranges=[],
+  oneofs=[
+    _descriptor.OneofDescriptor(
+      name='root', full_name='org.apache.beam.model.pipeline.v1.MessageWithComponents.root',
+      index=0, containing_type=None, fields=[]),
+  ],
+  serialized_start=12494,
+  serialized_end=13408,
+)
+
+
+_EXECUTABLESTAGEPAYLOAD_SIDEINPUTID = _descriptor.Descriptor(
+  name='SideInputId',
+  full_name='org.apache.beam.model.pipeline.v1.ExecutableStagePayload.SideInputId',
+  filename=None,
+  file=DESCRIPTOR,
+  containing_type=None,
+  fields=[
+    _descriptor.FieldDescriptor(
+      name='transform_id', full_name='org.apache.beam.model.pipeline.v1.ExecutableStagePayload.SideInputId.transform_id', index=0,
+      number=1, type=9, cpp_type=9, label=1,
+      has_default_value=False, default_value=_b("").decode('utf-8'),
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      options=None),
+    _descriptor.FieldDescriptor(
+      name='local_name', full_name='org.apache.beam.model.pipeline.v1.ExecutableStagePayload.SideInputId.local_name', index=1,
+      number=2, type=9, cpp_type=9, label=1,
+      has_default_value=False, default_value=_b("").decode('utf-8'),
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      options=None),
+  ],
+  extensions=[
+  ],
+  nested_types=[],
+  enum_types=[
+  ],
+  options=None,
+  is_extendable=False,
+  syntax='proto3',
+  extension_ranges=[],
+  oneofs=[
+  ],
+  serialized_start=13892,
+  serialized_end=13947,
+)
+
+_EXECUTABLESTAGEPAYLOAD_USERSTATEID = _descriptor.Descriptor(
+  name='UserStateId',
+  full_name='org.apache.beam.model.pipeline.v1.ExecutableStagePayload.UserStateId',
+  filename=None,
+  file=DESCRIPTOR,
+  containing_type=None,
+  fields=[
+    _descriptor.FieldDescriptor(
+      name='transform_id', full_name='org.apache.beam.model.pipeline.v1.ExecutableStagePayload.UserStateId.transform_id', index=0,
+      number=1, type=9, cpp_type=9, label=1,
+      has_default_value=False, default_value=_b("").decode('utf-8'),
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      options=None),
+    _descriptor.FieldDescriptor(
+      name='local_name', full_name='org.apache.beam.model.pipeline.v1.ExecutableStagePayload.UserStateId.local_name', index=1,
+      number=2, type=9, cpp_type=9, label=1,
+      has_default_value=False, default_value=_b("").decode('utf-8'),
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      options=None),
+  ],
+  extensions=[
+  ],
+  nested_types=[],
+  enum_types=[
+  ],
+  options=None,
+  is_extendable=False,
+  syntax='proto3',
+  extension_ranges=[],
+  oneofs=[
+  ],
+  serialized_start=13949,
+  serialized_end=14004,
+)
+
+_EXECUTABLESTAGEPAYLOAD_TIMERID = _descriptor.Descriptor(
+  name='TimerId',
+  full_name='org.apache.beam.model.pipeline.v1.ExecutableStagePayload.TimerId',
+  filename=None,
+  file=DESCRIPTOR,
+  containing_type=None,
+  fields=[
+    _descriptor.FieldDescriptor(
+      name='transform_id', full_name='org.apache.beam.model.pipeline.v1.ExecutableStagePayload.TimerId.transform_id', index=0,
+      number=1, type=9, cpp_type=9, label=1,
+      has_default_value=False, default_value=_b("").decode('utf-8'),
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      options=None),
+    _descriptor.FieldDescriptor(
+      name='local_name', full_name='org.apache.beam.model.pipeline.v1.ExecutableStagePayload.TimerId.local_name', index=1,
+      number=2, type=9, cpp_type=9, label=1,
+      has_default_value=False, default_value=_b("").decode('utf-8'),
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      options=None),
+  ],
+  extensions=[
+  ],
+  nested_types=[],
+  enum_types=[
+  ],
+  options=None,
+  is_extendable=False,
+  syntax='proto3',
+  extension_ranges=[],
+  oneofs=[
+  ],
+  serialized_start=14006,
+  serialized_end=14057,
+)
+
+_EXECUTABLESTAGEPAYLOAD = _descriptor.Descriptor(
+  name='ExecutableStagePayload',
+  full_name='org.apache.beam.model.pipeline.v1.ExecutableStagePayload',
+  filename=None,
+  file=DESCRIPTOR,
+  containing_type=None,
+  fields=[
+    _descriptor.FieldDescriptor(
+      name='environment', full_name='org.apache.beam.model.pipeline.v1.ExecutableStagePayload.environment', index=0,
+      number=1, type=11, cpp_type=10, label=1,
+      has_default_value=False, default_value=None,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      options=None),
+    _descriptor.FieldDescriptor(
+      name='input', full_name='org.apache.beam.model.pipeline.v1.ExecutableStagePayload.input', index=1,
+      number=2, type=9, cpp_type=9, label=1,
+      has_default_value=False, default_value=_b("").decode('utf-8'),
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      options=None),
+    _descriptor.FieldDescriptor(
+      name='side_inputs', full_name='org.apache.beam.model.pipeline.v1.ExecutableStagePayload.side_inputs', index=2,
+      number=3, type=11, cpp_type=10, label=3,
+      has_default_value=False, default_value=[],
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      options=None),
+    _descriptor.FieldDescriptor(
+      name='transforms', full_name='org.apache.beam.model.pipeline.v1.ExecutableStagePayload.transforms', index=3,
+      number=4, type=9, cpp_type=9, label=3,
+      has_default_value=False, default_value=[],
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      options=None),
+    _descriptor.FieldDescriptor(
+      name='outputs', full_name='org.apache.beam.model.pipeline.v1.ExecutableStagePayload.outputs', index=4,
+      number=5, type=9, cpp_type=9, label=3,
+      has_default_value=False, default_value=[],
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      options=None),
+    _descriptor.FieldDescriptor(
+      name='components', full_name='org.apache.beam.model.pipeline.v1.ExecutableStagePayload.components', index=5,
+      number=6, type=11, cpp_type=10, label=1,
+      has_default_value=False, default_value=None,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      options=None),
+    _descriptor.FieldDescriptor(
+      name='user_states', full_name='org.apache.beam.model.pipeline.v1.ExecutableStagePayload.user_states', index=6,
+      number=7, type=11, cpp_type=10, label=3,
+      has_default_value=False, default_value=[],
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      options=None),
+    _descriptor.FieldDescriptor(
+      name='timers', full_name='org.apache.beam.model.pipeline.v1.ExecutableStagePayload.timers', index=7,
+      number=8, type=11, cpp_type=10, label=3,
+      has_default_value=False, default_value=[],
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      options=None),
+  ],
+  extensions=[
+  ],
+  nested_types=[_EXECUTABLESTAGEPAYLOAD_SIDEINPUTID, _EXECUTABLESTAGEPAYLOAD_USERSTATEID, _EXECUTABLESTAGEPAYLOAD_TIMERID, ],
+  enum_types=[
+  ],
+  options=None,
+  is_extendable=False,
+  syntax='proto3',
+  extension_ranges=[],
+  oneofs=[
+  ],
+  serialized_start=13411,
+  serialized_end=14057,
 )
 
+_BEAMCONSTANTS_CONSTANTS.containing_type = _BEAMCONSTANTS
 _COMPONENTS_TRANSFORMSENTRY.fields_by_name['value'].message_type = _PTRANSFORM
 _COMPONENTS_TRANSFORMSENTRY.containing_type = _COMPONENTS
 _COMPONENTS_PCOLLECTIONSENTRY.fields_by_name['value'].message_type = _PCOLLECTION
 _COMPONENTS_PCOLLECTIONSENTRY.containing_type = _COMPONENTS
 _COMPONENTS_WINDOWINGSTRATEGIESENTRY.fields_by_name['value'].message_type = _WINDOWINGSTRATEGY
 _COMPONENTS_WINDOWINGSTRATEGIESENTRY.containing_type = _COMPONENTS
 _COMPONENTS_CODERSENTRY.fields_by_name['value'].message_type = _CODER
@@ -3716,59 +3791,14 @@
 _COMPONENTS_ENVIRONMENTSENTRY.fields_by_name['value'].message_type = _ENVIRONMENT
 _COMPONENTS_ENVIRONMENTSENTRY.containing_type = _COMPONENTS
 _COMPONENTS.fields_by_name['transforms'].message_type = _COMPONENTS_TRANSFORMSENTRY
 _COMPONENTS.fields_by_name['pcollections'].message_type = _COMPONENTS_PCOLLECTIONSENTRY
 _COMPONENTS.fields_by_name['windowing_strategies'].message_type = _COMPONENTS_WINDOWINGSTRATEGIESENTRY
 _COMPONENTS.fields_by_name['coders'].message_type = _COMPONENTS_CODERSENTRY
 _COMPONENTS.fields_by_name['environments'].message_type = _COMPONENTS_ENVIRONMENTSENTRY
-_MESSAGEWITHCOMPONENTS.fields_by_name['components'].message_type = _COMPONENTS
-_MESSAGEWITHCOMPONENTS.fields_by_name['coder'].message_type = _CODER
-_MESSAGEWITHCOMPONENTS.fields_by_name['combine_payload'].message_type = _COMBINEPAYLOAD
-_MESSAGEWITHCOMPONENTS.fields_by_name['sdk_function_spec'].message_type = _SDKFUNCTIONSPEC
-_MESSAGEWITHCOMPONENTS.fields_by_name['par_do_payload'].message_type = _PARDOPAYLOAD
-_MESSAGEWITHCOMPONENTS.fields_by_name['ptransform'].message_type = _PTRANSFORM
-_MESSAGEWITHCOMPONENTS.fields_by_name['pcollection'].message_type = _PCOLLECTION
-_MESSAGEWITHCOMPONENTS.fields_by_name['read_payload'].message_type = _READPAYLOAD
-_MESSAGEWITHCOMPONENTS.fields_by_name['side_input'].message_type = _SIDEINPUT
-_MESSAGEWITHCOMPONENTS.fields_by_name['window_into_payload'].message_type = _WINDOWINTOPAYLOAD
-_MESSAGEWITHCOMPONENTS.fields_by_name['windowing_strategy'].message_type = _WINDOWINGSTRATEGY
-_MESSAGEWITHCOMPONENTS.fields_by_name['function_spec'].message_type = _FUNCTIONSPEC
-_MESSAGEWITHCOMPONENTS.oneofs_by_name['root'].fields.append(
-  _MESSAGEWITHCOMPONENTS.fields_by_name['coder'])
-_MESSAGEWITHCOMPONENTS.fields_by_name['coder'].containing_oneof = _MESSAGEWITHCOMPONENTS.oneofs_by_name['root']
-_MESSAGEWITHCOMPONENTS.oneofs_by_name['root'].fields.append(
-  _MESSAGEWITHCOMPONENTS.fields_by_name['combine_payload'])
-_MESSAGEWITHCOMPONENTS.fields_by_name['combine_payload'].containing_oneof = _MESSAGEWITHCOMPONENTS.oneofs_by_name['root']
-_MESSAGEWITHCOMPONENTS.oneofs_by_name['root'].fields.append(
-  _MESSAGEWITHCOMPONENTS.fields_by_name['sdk_function_spec'])
-_MESSAGEWITHCOMPONENTS.fields_by_name['sdk_function_spec'].containing_oneof = _MESSAGEWITHCOMPONENTS.oneofs_by_name['root']
-_MESSAGEWITHCOMPONENTS.oneofs_by_name['root'].fields.append(
-  _MESSAGEWITHCOMPONENTS.fields_by_name['par_do_payload'])
-_MESSAGEWITHCOMPONENTS.fields_by_name['par_do_payload'].containing_oneof = _MESSAGEWITHCOMPONENTS.oneofs_by_name['root']
-_MESSAGEWITHCOMPONENTS.oneofs_by_name['root'].fields.append(
-  _MESSAGEWITHCOMPONENTS.fields_by_name['ptransform'])
-_MESSAGEWITHCOMPONENTS.fields_by_name['ptransform'].containing_oneof = _MESSAGEWITHCOMPONENTS.oneofs_by_name['root']
-_MESSAGEWITHCOMPONENTS.oneofs_by_name['root'].fields.append(
-  _MESSAGEWITHCOMPONENTS.fields_by_name['pcollection'])
-_MESSAGEWITHCOMPONENTS.fields_by_name['pcollection'].containing_oneof = _MESSAGEWITHCOMPONENTS.oneofs_by_name['root']
-_MESSAGEWITHCOMPONENTS.oneofs_by_name['root'].fields.append(
-  _MESSAGEWITHCOMPONENTS.fields_by_name['read_payload'])
-_MESSAGEWITHCOMPONENTS.fields_by_name['read_payload'].containing_oneof = _MESSAGEWITHCOMPONENTS.oneofs_by_name['root']
-_MESSAGEWITHCOMPONENTS.oneofs_by_name['root'].fields.append(
-  _MESSAGEWITHCOMPONENTS.fields_by_name['side_input'])
-_MESSAGEWITHCOMPONENTS.fields_by_name['side_input'].containing_oneof = _MESSAGEWITHCOMPONENTS.oneofs_by_name['root']
-_MESSAGEWITHCOMPONENTS.oneofs_by_name['root'].fields.append(
-  _MESSAGEWITHCOMPONENTS.fields_by_name['window_into_payload'])
-_MESSAGEWITHCOMPONENTS.fields_by_name['window_into_payload'].containing_oneof = _MESSAGEWITHCOMPONENTS.oneofs_by_name['root']
-_MESSAGEWITHCOMPONENTS.oneofs_by_name['root'].fields.append(
-  _MESSAGEWITHCOMPONENTS.fields_by_name['windowing_strategy'])
-_MESSAGEWITHCOMPONENTS.fields_by_name['windowing_strategy'].containing_oneof = _MESSAGEWITHCOMPONENTS.oneofs_by_name['root']
-_MESSAGEWITHCOMPONENTS.oneofs_by_name['root'].fields.append(
-  _MESSAGEWITHCOMPONENTS.fields_by_name['function_spec'])
-_MESSAGEWITHCOMPONENTS.fields_by_name['function_spec'].containing_oneof = _MESSAGEWITHCOMPONENTS.oneofs_by_name['root']
 _PIPELINE.fields_by_name['components'].message_type = _COMPONENTS
 _PIPELINE.fields_by_name['display_data'].message_type = _DISPLAYDATA
 _PTRANSFORM_INPUTSENTRY.containing_type = _PTRANSFORM
 _PTRANSFORM_OUTPUTSENTRY.containing_type = _PTRANSFORM
 _PTRANSFORM.fields_by_name['spec'].message_type = _FUNCTIONSPEC
 _PTRANSFORM.fields_by_name['inputs'].message_type = _PTRANSFORM_INPUTSENTRY
 _PTRANSFORM.fields_by_name['outputs'].message_type = _PTRANSFORM_OUTPUTSENTRY
@@ -3777,22 +3807,14 @@
 _STANDARDPTRANSFORMS_DEPRECATEDPRIMITIVES.containing_type = _STANDARDPTRANSFORMS
 _STANDARDPTRANSFORMS_COMPOSITES.containing_type = _STANDARDPTRANSFORMS
 _STANDARDPTRANSFORMS_COMBINECOMPONENTS.containing_type = _STANDARDPTRANSFORMS
 _STANDARDPTRANSFORMS_SPLITTABLEPARDOCOMPONENTS.containing_type = _STANDARDPTRANSFORMS
 _STANDARDSIDEINPUTTYPES_ENUM.containing_type = _STANDARDSIDEINPUTTYPES
 _PCOLLECTION.fields_by_name['is_bounded'].enum_type = _ISBOUNDED_ENUM
 _PCOLLECTION.fields_by_name['display_data'].message_type = _DISPLAYDATA
-_EXECUTABLESTAGEPAYLOAD_SIDEINPUTID.containing_type = _EXECUTABLESTAGEPAYLOAD
-_EXECUTABLESTAGEPAYLOAD_USERSTATEID.containing_type = _EXECUTABLESTAGEPAYLOAD
-_EXECUTABLESTAGEPAYLOAD_TIMERID.containing_type = _EXECUTABLESTAGEPAYLOAD
-_EXECUTABLESTAGEPAYLOAD.fields_by_name['environment'].message_type = _ENVIRONMENT
-_EXECUTABLESTAGEPAYLOAD.fields_by_name['side_inputs'].message_type = _EXECUTABLESTAGEPAYLOAD_SIDEINPUTID
-_EXECUTABLESTAGEPAYLOAD.fields_by_name['components'].message_type = _COMPONENTS
-_EXECUTABLESTAGEPAYLOAD.fields_by_name['user_states'].message_type = _EXECUTABLESTAGEPAYLOAD_USERSTATEID
-_EXECUTABLESTAGEPAYLOAD.fields_by_name['timers'].message_type = _EXECUTABLESTAGEPAYLOAD_TIMERID
 _PARDOPAYLOAD_SIDEINPUTSENTRY.fields_by_name['value'].message_type = _SIDEINPUT
 _PARDOPAYLOAD_SIDEINPUTSENTRY.containing_type = _PARDOPAYLOAD
 _PARDOPAYLOAD_STATESPECSENTRY.fields_by_name['value'].message_type = _STATESPEC
 _PARDOPAYLOAD_STATESPECSENTRY.containing_type = _PARDOPAYLOAD
 _PARDOPAYLOAD_TIMERSPECSENTRY.fields_by_name['value'].message_type = _TIMERSPEC
 _PARDOPAYLOAD_TIMERSPECSENTRY.containing_type = _PARDOPAYLOAD
 _PARDOPAYLOAD.fields_by_name['do_fn'].message_type = _SDKFUNCTIONSPEC
@@ -3960,22 +3982,74 @@
 _DISPLAYDATA_ITEM.fields_by_name['type'].enum_type = _DISPLAYDATA_TYPE_ENUM
 _DISPLAYDATA_ITEM.fields_by_name['value'].message_type = google_dot_protobuf_dot_any__pb2._ANY
 _DISPLAYDATA_ITEM.fields_by_name['short_value'].message_type = google_dot_protobuf_dot_any__pb2._ANY
 _DISPLAYDATA_ITEM.containing_type = _DISPLAYDATA
 _DISPLAYDATA_TYPE.containing_type = _DISPLAYDATA
 _DISPLAYDATA_TYPE_ENUM.containing_type = _DISPLAYDATA_TYPE
 _DISPLAYDATA.fields_by_name['items'].message_type = _DISPLAYDATA_ITEM
+_MESSAGEWITHCOMPONENTS.fields_by_name['components'].message_type = _COMPONENTS
+_MESSAGEWITHCOMPONENTS.fields_by_name['coder'].message_type = _CODER
+_MESSAGEWITHCOMPONENTS.fields_by_name['combine_payload'].message_type = _COMBINEPAYLOAD
+_MESSAGEWITHCOMPONENTS.fields_by_name['sdk_function_spec'].message_type = _SDKFUNCTIONSPEC
+_MESSAGEWITHCOMPONENTS.fields_by_name['par_do_payload'].message_type = _PARDOPAYLOAD
+_MESSAGEWITHCOMPONENTS.fields_by_name['ptransform'].message_type = _PTRANSFORM
+_MESSAGEWITHCOMPONENTS.fields_by_name['pcollection'].message_type = _PCOLLECTION
+_MESSAGEWITHCOMPONENTS.fields_by_name['read_payload'].message_type = _READPAYLOAD
+_MESSAGEWITHCOMPONENTS.fields_by_name['side_input'].message_type = _SIDEINPUT
+_MESSAGEWITHCOMPONENTS.fields_by_name['window_into_payload'].message_type = _WINDOWINTOPAYLOAD
+_MESSAGEWITHCOMPONENTS.fields_by_name['windowing_strategy'].message_type = _WINDOWINGSTRATEGY
+_MESSAGEWITHCOMPONENTS.fields_by_name['function_spec'].message_type = _FUNCTIONSPEC
+_MESSAGEWITHCOMPONENTS.oneofs_by_name['root'].fields.append(
+  _MESSAGEWITHCOMPONENTS.fields_by_name['coder'])
+_MESSAGEWITHCOMPONENTS.fields_by_name['coder'].containing_oneof = _MESSAGEWITHCOMPONENTS.oneofs_by_name['root']
+_MESSAGEWITHCOMPONENTS.oneofs_by_name['root'].fields.append(
+  _MESSAGEWITHCOMPONENTS.fields_by_name['combine_payload'])
+_MESSAGEWITHCOMPONENTS.fields_by_name['combine_payload'].containing_oneof = _MESSAGEWITHCOMPONENTS.oneofs_by_name['root']
+_MESSAGEWITHCOMPONENTS.oneofs_by_name['root'].fields.append(
+  _MESSAGEWITHCOMPONENTS.fields_by_name['sdk_function_spec'])
+_MESSAGEWITHCOMPONENTS.fields_by_name['sdk_function_spec'].containing_oneof = _MESSAGEWITHCOMPONENTS.oneofs_by_name['root']
+_MESSAGEWITHCOMPONENTS.oneofs_by_name['root'].fields.append(
+  _MESSAGEWITHCOMPONENTS.fields_by_name['par_do_payload'])
+_MESSAGEWITHCOMPONENTS.fields_by_name['par_do_payload'].containing_oneof = _MESSAGEWITHCOMPONENTS.oneofs_by_name['root']
+_MESSAGEWITHCOMPONENTS.oneofs_by_name['root'].fields.append(
+  _MESSAGEWITHCOMPONENTS.fields_by_name['ptransform'])
+_MESSAGEWITHCOMPONENTS.fields_by_name['ptransform'].containing_oneof = _MESSAGEWITHCOMPONENTS.oneofs_by_name['root']
+_MESSAGEWITHCOMPONENTS.oneofs_by_name['root'].fields.append(
+  _MESSAGEWITHCOMPONENTS.fields_by_name['pcollection'])
+_MESSAGEWITHCOMPONENTS.fields_by_name['pcollection'].containing_oneof = _MESSAGEWITHCOMPONENTS.oneofs_by_name['root']
+_MESSAGEWITHCOMPONENTS.oneofs_by_name['root'].fields.append(
+  _MESSAGEWITHCOMPONENTS.fields_by_name['read_payload'])
+_MESSAGEWITHCOMPONENTS.fields_by_name['read_payload'].containing_oneof = _MESSAGEWITHCOMPONENTS.oneofs_by_name['root']
+_MESSAGEWITHCOMPONENTS.oneofs_by_name['root'].fields.append(
+  _MESSAGEWITHCOMPONENTS.fields_by_name['side_input'])
+_MESSAGEWITHCOMPONENTS.fields_by_name['side_input'].containing_oneof = _MESSAGEWITHCOMPONENTS.oneofs_by_name['root']
+_MESSAGEWITHCOMPONENTS.oneofs_by_name['root'].fields.append(
+  _MESSAGEWITHCOMPONENTS.fields_by_name['window_into_payload'])
+_MESSAGEWITHCOMPONENTS.fields_by_name['window_into_payload'].containing_oneof = _MESSAGEWITHCOMPONENTS.oneofs_by_name['root']
+_MESSAGEWITHCOMPONENTS.oneofs_by_name['root'].fields.append(
+  _MESSAGEWITHCOMPONENTS.fields_by_name['windowing_strategy'])
+_MESSAGEWITHCOMPONENTS.fields_by_name['windowing_strategy'].containing_oneof = _MESSAGEWITHCOMPONENTS.oneofs_by_name['root']
+_MESSAGEWITHCOMPONENTS.oneofs_by_name['root'].fields.append(
+  _MESSAGEWITHCOMPONENTS.fields_by_name['function_spec'])
+_MESSAGEWITHCOMPONENTS.fields_by_name['function_spec'].containing_oneof = _MESSAGEWITHCOMPONENTS.oneofs_by_name['root']
+_EXECUTABLESTAGEPAYLOAD_SIDEINPUTID.containing_type = _EXECUTABLESTAGEPAYLOAD
+_EXECUTABLESTAGEPAYLOAD_USERSTATEID.containing_type = _EXECUTABLESTAGEPAYLOAD
+_EXECUTABLESTAGEPAYLOAD_TIMERID.containing_type = _EXECUTABLESTAGEPAYLOAD
+_EXECUTABLESTAGEPAYLOAD.fields_by_name['environment'].message_type = _ENVIRONMENT
+_EXECUTABLESTAGEPAYLOAD.fields_by_name['side_inputs'].message_type = _EXECUTABLESTAGEPAYLOAD_SIDEINPUTID
+_EXECUTABLESTAGEPAYLOAD.fields_by_name['components'].message_type = _COMPONENTS
+_EXECUTABLESTAGEPAYLOAD.fields_by_name['user_states'].message_type = _EXECUTABLESTAGEPAYLOAD_USERSTATEID
+_EXECUTABLESTAGEPAYLOAD.fields_by_name['timers'].message_type = _EXECUTABLESTAGEPAYLOAD_TIMERID
+DESCRIPTOR.message_types_by_name['BeamConstants'] = _BEAMCONSTANTS
 DESCRIPTOR.message_types_by_name['Components'] = _COMPONENTS
-DESCRIPTOR.message_types_by_name['MessageWithComponents'] = _MESSAGEWITHCOMPONENTS
 DESCRIPTOR.message_types_by_name['Pipeline'] = _PIPELINE
 DESCRIPTOR.message_types_by_name['PTransform'] = _PTRANSFORM
 DESCRIPTOR.message_types_by_name['StandardPTransforms'] = _STANDARDPTRANSFORMS
 DESCRIPTOR.message_types_by_name['StandardSideInputTypes'] = _STANDARDSIDEINPUTTYPES
 DESCRIPTOR.message_types_by_name['PCollection'] = _PCOLLECTION
-DESCRIPTOR.message_types_by_name['ExecutableStagePayload'] = _EXECUTABLESTAGEPAYLOAD
 DESCRIPTOR.message_types_by_name['ParDoPayload'] = _PARDOPAYLOAD
 DESCRIPTOR.message_types_by_name['Parameter'] = _PARAMETER
 DESCRIPTOR.message_types_by_name['StateSpec'] = _STATESPEC
 DESCRIPTOR.message_types_by_name['ValueStateSpec'] = _VALUESTATESPEC
 DESCRIPTOR.message_types_by_name['BagStateSpec'] = _BAGSTATESPEC
 DESCRIPTOR.message_types_by_name['CombiningStateSpec'] = _COMBININGSTATESPEC
 DESCRIPTOR.message_types_by_name['MapStateSpec'] = _MAPSTATESPEC
@@ -4002,16 +4076,25 @@
 DESCRIPTOR.message_types_by_name['Environment'] = _ENVIRONMENT
 DESCRIPTOR.message_types_by_name['StandardEnvironments'] = _STANDARDENVIRONMENTS
 DESCRIPTOR.message_types_by_name['DockerPayload'] = _DOCKERPAYLOAD
 DESCRIPTOR.message_types_by_name['ProcessPayload'] = _PROCESSPAYLOAD
 DESCRIPTOR.message_types_by_name['SdkFunctionSpec'] = _SDKFUNCTIONSPEC
 DESCRIPTOR.message_types_by_name['FunctionSpec'] = _FUNCTIONSPEC
 DESCRIPTOR.message_types_by_name['DisplayData'] = _DISPLAYDATA
+DESCRIPTOR.message_types_by_name['MessageWithComponents'] = _MESSAGEWITHCOMPONENTS
+DESCRIPTOR.message_types_by_name['ExecutableStagePayload'] = _EXECUTABLESTAGEPAYLOAD
 DESCRIPTOR.extensions_by_name['beam_urn'] = beam_urn
-_sym_db.RegisterFileDescriptor(DESCRIPTOR)
+DESCRIPTOR.extensions_by_name['beam_constant'] = beam_constant
+
+BeamConstants = _reflection.GeneratedProtocolMessageType('BeamConstants', (_message.Message,), dict(
+  DESCRIPTOR = _BEAMCONSTANTS,
+  __module__ = 'beam_runner_api_pb2'
+  # @@protoc_insertion_point(class_scope:org.apache.beam.model.pipeline.v1.BeamConstants)
+  ))
+_sym_db.RegisterMessage(BeamConstants)
 
 Components = _reflection.GeneratedProtocolMessageType('Components', (_message.Message,), dict(
 
   TransformsEntry = _reflection.GeneratedProtocolMessageType('TransformsEntry', (_message.Message,), dict(
     DESCRIPTOR = _COMPONENTS_TRANSFORMSENTRY,
     __module__ = 'beam_runner_api_pb2'
     # @@protoc_insertion_point(class_scope:org.apache.beam.model.pipeline.v1.Components.TransformsEntry)
@@ -4052,21 +4135,14 @@
 _sym_db.RegisterMessage(Components)
 _sym_db.RegisterMessage(Components.TransformsEntry)
 _sym_db.RegisterMessage(Components.PcollectionsEntry)
 _sym_db.RegisterMessage(Components.WindowingStrategiesEntry)
 _sym_db.RegisterMessage(Components.CodersEntry)
 _sym_db.RegisterMessage(Components.EnvironmentsEntry)
 
-MessageWithComponents = _reflection.GeneratedProtocolMessageType('MessageWithComponents', (_message.Message,), dict(
-  DESCRIPTOR = _MESSAGEWITHCOMPONENTS,
-  __module__ = 'beam_runner_api_pb2'
-  # @@protoc_insertion_point(class_scope:org.apache.beam.model.pipeline.v1.MessageWithComponents)
-  ))
-_sym_db.RegisterMessage(MessageWithComponents)
-
 Pipeline = _reflection.GeneratedProtocolMessageType('Pipeline', (_message.Message,), dict(
   DESCRIPTOR = _PIPELINE,
   __module__ = 'beam_runner_api_pb2'
   # @@protoc_insertion_point(class_scope:org.apache.beam.model.pipeline.v1.Pipeline)
   ))
 _sym_db.RegisterMessage(Pipeline)
 
@@ -4110,45 +4186,14 @@
 PCollection = _reflection.GeneratedProtocolMessageType('PCollection', (_message.Message,), dict(
   DESCRIPTOR = _PCOLLECTION,
   __module__ = 'beam_runner_api_pb2'
   # @@protoc_insertion_point(class_scope:org.apache.beam.model.pipeline.v1.PCollection)
   ))
 _sym_db.RegisterMessage(PCollection)
 
-ExecutableStagePayload = _reflection.GeneratedProtocolMessageType('ExecutableStagePayload', (_message.Message,), dict(
-
-  SideInputId = _reflection.GeneratedProtocolMessageType('SideInputId', (_message.Message,), dict(
-    DESCRIPTOR = _EXECUTABLESTAGEPAYLOAD_SIDEINPUTID,
-    __module__ = 'beam_runner_api_pb2'
-    # @@protoc_insertion_point(class_scope:org.apache.beam.model.pipeline.v1.ExecutableStagePayload.SideInputId)
-    ))
-  ,
-
-  UserStateId = _reflection.GeneratedProtocolMessageType('UserStateId', (_message.Message,), dict(
-    DESCRIPTOR = _EXECUTABLESTAGEPAYLOAD_USERSTATEID,
-    __module__ = 'beam_runner_api_pb2'
-    # @@protoc_insertion_point(class_scope:org.apache.beam.model.pipeline.v1.ExecutableStagePayload.UserStateId)
-    ))
-  ,
-
-  TimerId = _reflection.GeneratedProtocolMessageType('TimerId', (_message.Message,), dict(
-    DESCRIPTOR = _EXECUTABLESTAGEPAYLOAD_TIMERID,
-    __module__ = 'beam_runner_api_pb2'
-    # @@protoc_insertion_point(class_scope:org.apache.beam.model.pipeline.v1.ExecutableStagePayload.TimerId)
-    ))
-  ,
-  DESCRIPTOR = _EXECUTABLESTAGEPAYLOAD,
-  __module__ = 'beam_runner_api_pb2'
-  # @@protoc_insertion_point(class_scope:org.apache.beam.model.pipeline.v1.ExecutableStagePayload)
-  ))
-_sym_db.RegisterMessage(ExecutableStagePayload)
-_sym_db.RegisterMessage(ExecutableStagePayload.SideInputId)
-_sym_db.RegisterMessage(ExecutableStagePayload.UserStateId)
-_sym_db.RegisterMessage(ExecutableStagePayload.TimerId)
-
 ParDoPayload = _reflection.GeneratedProtocolMessageType('ParDoPayload', (_message.Message,), dict(
 
   SideInputsEntry = _reflection.GeneratedProtocolMessageType('SideInputsEntry', (_message.Message,), dict(
     DESCRIPTOR = _PARDOPAYLOAD_SIDEINPUTSENTRY,
     __module__ = 'beam_runner_api_pb2'
     # @@protoc_insertion_point(class_scope:org.apache.beam.model.pipeline.v1.ParDoPayload.SideInputsEntry)
     ))
@@ -4603,18 +4648,63 @@
   # @@protoc_insertion_point(class_scope:org.apache.beam.model.pipeline.v1.DisplayData)
   ))
 _sym_db.RegisterMessage(DisplayData)
 _sym_db.RegisterMessage(DisplayData.Identifier)
 _sym_db.RegisterMessage(DisplayData.Item)
 _sym_db.RegisterMessage(DisplayData.Type)
 
+MessageWithComponents = _reflection.GeneratedProtocolMessageType('MessageWithComponents', (_message.Message,), dict(
+  DESCRIPTOR = _MESSAGEWITHCOMPONENTS,
+  __module__ = 'beam_runner_api_pb2'
+  # @@protoc_insertion_point(class_scope:org.apache.beam.model.pipeline.v1.MessageWithComponents)
+  ))
+_sym_db.RegisterMessage(MessageWithComponents)
+
+ExecutableStagePayload = _reflection.GeneratedProtocolMessageType('ExecutableStagePayload', (_message.Message,), dict(
+
+  SideInputId = _reflection.GeneratedProtocolMessageType('SideInputId', (_message.Message,), dict(
+    DESCRIPTOR = _EXECUTABLESTAGEPAYLOAD_SIDEINPUTID,
+    __module__ = 'beam_runner_api_pb2'
+    # @@protoc_insertion_point(class_scope:org.apache.beam.model.pipeline.v1.ExecutableStagePayload.SideInputId)
+    ))
+  ,
+
+  UserStateId = _reflection.GeneratedProtocolMessageType('UserStateId', (_message.Message,), dict(
+    DESCRIPTOR = _EXECUTABLESTAGEPAYLOAD_USERSTATEID,
+    __module__ = 'beam_runner_api_pb2'
+    # @@protoc_insertion_point(class_scope:org.apache.beam.model.pipeline.v1.ExecutableStagePayload.UserStateId)
+    ))
+  ,
+
+  TimerId = _reflection.GeneratedProtocolMessageType('TimerId', (_message.Message,), dict(
+    DESCRIPTOR = _EXECUTABLESTAGEPAYLOAD_TIMERID,
+    __module__ = 'beam_runner_api_pb2'
+    # @@protoc_insertion_point(class_scope:org.apache.beam.model.pipeline.v1.ExecutableStagePayload.TimerId)
+    ))
+  ,
+  DESCRIPTOR = _EXECUTABLESTAGEPAYLOAD,
+  __module__ = 'beam_runner_api_pb2'
+  # @@protoc_insertion_point(class_scope:org.apache.beam.model.pipeline.v1.ExecutableStagePayload)
+  ))
+_sym_db.RegisterMessage(ExecutableStagePayload)
+_sym_db.RegisterMessage(ExecutableStagePayload.SideInputId)
+_sym_db.RegisterMessage(ExecutableStagePayload.UserStateId)
+_sym_db.RegisterMessage(ExecutableStagePayload.TimerId)
+
 google_dot_protobuf_dot_descriptor__pb2.EnumValueOptions.RegisterExtension(beam_urn)
+google_dot_protobuf_dot_descriptor__pb2.EnumValueOptions.RegisterExtension(beam_constant)
 
 DESCRIPTOR.has_options = True
 DESCRIPTOR._options = _descriptor._ParseOptions(descriptor_pb2.FileOptions(), _b('\n!org.apache.beam.model.pipeline.v1B\tRunnerApiZ\013pipeline_v1'))
+_BEAMCONSTANTS_CONSTANTS.values_by_name["MIN_TIMESTAMP_MILLIS"].has_options = True
+_BEAMCONSTANTS_CONSTANTS.values_by_name["MIN_TIMESTAMP_MILLIS"]._options = _descriptor._ParseOptions(descriptor_pb2.EnumValueOptions(), _b('\252\264\372\302\005\021-9223372036854775'))
+_BEAMCONSTANTS_CONSTANTS.values_by_name["MAX_TIMESTAMP_MILLIS"].has_options = True
+_BEAMCONSTANTS_CONSTANTS.values_by_name["MAX_TIMESTAMP_MILLIS"]._options = _descriptor._ParseOptions(descriptor_pb2.EnumValueOptions(), _b('\252\264\372\302\005\0209223372036854775'))
+_BEAMCONSTANTS_CONSTANTS.values_by_name["GLOBAL_WINDOW_MAX_TIMESTAMP_MILLIS"].has_options = True
+_BEAMCONSTANTS_CONSTANTS.values_by_name["GLOBAL_WINDOW_MAX_TIMESTAMP_MILLIS"]._options = _descriptor._ParseOptions(descriptor_pb2.EnumValueOptions(), _b('\252\264\372\302\005\0209223371950454775'))
 _COMPONENTS_TRANSFORMSENTRY.has_options = True
 _COMPONENTS_TRANSFORMSENTRY._options = _descriptor._ParseOptions(descriptor_pb2.MessageOptions(), _b('8\001'))
 _COMPONENTS_PCOLLECTIONSENTRY.has_options = True
 _COMPONENTS_PCOLLECTIONSENTRY._options = _descriptor._ParseOptions(descriptor_pb2.MessageOptions(), _b('8\001'))
 _COMPONENTS_WINDOWINGSTRATEGIESENTRY.has_options = True
 _COMPONENTS_WINDOWINGSTRATEGIESENTRY._options = _descriptor._ParseOptions(descriptor_pb2.MessageOptions(), _b('8\001'))
 _COMPONENTS_CODERSENTRY.has_options = True
```

## Comparing `apache-beam-2.8.0/apache_beam/portability/api/__init__.py` & `apache-beam-2.9.0/apache_beam/portability/api/__init__.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/portability/api/beam_fn_api_pb2.py` & `apache-beam-2.9.0/apache_beam/portability/api/beam_fn_api_pb2.py`

 * *Files 10% similar despite different names*

```diff
@@ -12,25 +12,27 @@
 # @@protoc_insertion_point(imports)
 
 _sym_db = _symbol_database.Default()
 
 
 from . import beam_runner_api_pb2 as beam__runner__api__pb2
 from . import endpoints_pb2 as endpoints__pb2
+from google.protobuf import descriptor_pb2 as google_dot_protobuf_dot_descriptor__pb2
 from google.protobuf import timestamp_pb2 as google_dot_protobuf_dot_timestamp__pb2
 from google.protobuf import wrappers_pb2 as google_dot_protobuf_dot_wrappers__pb2
 
 
 DESCRIPTOR = _descriptor.FileDescriptor(
   name='beam_fn_api.proto',
   package='org.apache.beam.model.fn_execution.v1',
   syntax='proto3',
-  serialized_pb=_b('\n\x11\x62\x65\x61m_fn_api.proto\x12%org.apache.beam.model.fn_execution.v1\x1a\x15\x62\x65\x61m_runner_api.proto\x1a\x0f\x65ndpoints.proto\x1a\x1fgoogle/protobuf/timestamp.proto\x1a\x1egoogle/protobuf/wrappers.proto\"\x84\x01\n\x06Target\x12%\n\x1dprimitive_transform_reference\x18\x01 \x01(\t\x12\x0c\n\x04name\x18\x02 \x01(\t\x1a\x45\n\x04List\x12=\n\x06target\x18\x01 \x03(\x0b\x32-.org.apache.beam.model.fn_execution.v1.Target\"{\n\x0eRemoteGrpcPort\x12W\n\x16\x61pi_service_descriptor\x18\x01 \x01(\x0b\x32\x37.org.apache.beam.model.pipeline.v1.ApiServiceDescriptor\x12\x10\n\x08\x63oder_id\x18\x02 \x01(\t\"\xa8\x03\n\x12InstructionRequest\x12\x16\n\x0einstruction_id\x18\x01 \x01(\t\x12K\n\x08register\x18\xe8\x07 \x01(\x0b\x32\x36.org.apache.beam.model.fn_execution.v1.RegisterRequestH\x00\x12V\n\x0eprocess_bundle\x18\xe9\x07 \x01(\x0b\x32;.org.apache.beam.model.fn_execution.v1.ProcessBundleRequestH\x00\x12g\n\x17process_bundle_progress\x18\xea\x07 \x01(\x0b\x32\x43.org.apache.beam.model.fn_execution.v1.ProcessBundleProgressRequestH\x00\x12\x61\n\x14process_bundle_split\x18\xeb\x07 \x01(\x0b\x32@.org.apache.beam.model.fn_execution.v1.ProcessBundleSplitRequestH\x00\x42\t\n\x07request\"\xbd\x03\n\x13InstructionResponse\x12\x16\n\x0einstruction_id\x18\x01 \x01(\t\x12\r\n\x05\x65rror\x18\x02 \x01(\t\x12L\n\x08register\x18\xe8\x07 \x01(\x0b\x32\x37.org.apache.beam.model.fn_execution.v1.RegisterResponseH\x00\x12W\n\x0eprocess_bundle\x18\xe9\x07 \x01(\x0b\x32<.org.apache.beam.model.fn_execution.v1.ProcessBundleResponseH\x00\x12h\n\x17process_bundle_progress\x18\xea\x07 \x01(\x0b\x32\x44.org.apache.beam.model.fn_execution.v1.ProcessBundleProgressResponseH\x00\x12\x62\n\x14process_bundle_split\x18\xeb\x07 \x01(\x0b\x32\x41.org.apache.beam.model.fn_execution.v1.ProcessBundleSplitResponseH\x00\x42\n\n\x08response\"t\n\x0fRegisterRequest\x12\x61\n\x19process_bundle_descriptor\x18\x01 \x03(\x0b\x32>.org.apache.beam.model.fn_execution.v1.ProcessBundleDescriptor\"\x12\n\x10RegisterResponse\"\x82\t\n\x17ProcessBundleDescriptor\x12\n\n\x02id\x18\x01 \x01(\t\x12\x62\n\ntransforms\x18\x02 \x03(\x0b\x32N.org.apache.beam.model.fn_execution.v1.ProcessBundleDescriptor.TransformsEntry\x12\x66\n\x0cpcollections\x18\x03 \x03(\x0b\x32P.org.apache.beam.model.fn_execution.v1.ProcessBundleDescriptor.PcollectionsEntry\x12u\n\x14windowing_strategies\x18\x04 \x03(\x0b\x32W.org.apache.beam.model.fn_execution.v1.ProcessBundleDescriptor.WindowingStrategiesEntry\x12Z\n\x06\x63oders\x18\x05 \x03(\x0b\x32J.org.apache.beam.model.fn_execution.v1.ProcessBundleDescriptor.CodersEntry\x12\x66\n\x0c\x65nvironments\x18\x06 \x03(\x0b\x32P.org.apache.beam.model.fn_execution.v1.ProcessBundleDescriptor.EnvironmentsEntry\x12]\n\x1cstate_api_service_descriptor\x18\x07 \x01(\x0b\x32\x37.org.apache.beam.model.pipeline.v1.ApiServiceDescriptor\x1a`\n\x0fTransformsEntry\x12\x0b\n\x03key\x18\x01 \x01(\t\x12<\n\x05value\x18\x02 \x01(\x0b\x32-.org.apache.beam.model.pipeline.v1.PTransform:\x02\x38\x01\x1a\x63\n\x11PcollectionsEntry\x12\x0b\n\x03key\x18\x01 \x01(\t\x12=\n\x05value\x18\x02 \x01(\x0b\x32..org.apache.beam.model.pipeline.v1.PCollection:\x02\x38\x01\x1ap\n\x18WindowingStrategiesEntry\x12\x0b\n\x03key\x18\x01 \x01(\t\x12\x43\n\x05value\x18\x02 \x01(\x0b\x32\x34.org.apache.beam.model.pipeline.v1.WindowingStrategy:\x02\x38\x01\x1aW\n\x0b\x43odersEntry\x12\x0b\n\x03key\x18\x01 \x01(\t\x12\x37\n\x05value\x18\x02 \x01(\x0b\x32(.org.apache.beam.model.pipeline.v1.Coder:\x02\x38\x01\x1a\x63\n\x11\x45nvironmentsEntry\x12\x0b\n\x03key\x18\x01 \x01(\t\x12=\n\x05value\x18\x02 \x01(\x0b\x32..org.apache.beam.model.pipeline.v1.Environment:\x02\x38\x01\"\xed\x04\n\x0b\x42undleSplit\x12U\n\rprimary_roots\x18\x01 \x03(\x0b\x32>.org.apache.beam.model.fn_execution.v1.BundleSplit.Application\x12]\n\x0eresidual_roots\x18\x02 \x03(\x0b\x32\x45.org.apache.beam.model.fn_execution.v1.BundleSplit.DelayedApplication\x1a\xa9\x02\n\x0b\x41pplication\x12\x15\n\rptransform_id\x18\x01 \x01(\t\x12\x10\n\x08input_id\x18\x02 \x01(\t\x12\x0f\n\x07\x65lement\x18\x03 \x01(\x0c\x12o\n\x11output_watermarks\x18\x04 \x03(\x0b\x32T.org.apache.beam.model.fn_execution.v1.BundleSplit.Application.OutputWatermarksEntry\x12\x36\n\x10\x66raction_of_work\x18\x05 \x01(\x0b\x32\x1c.google.protobuf.DoubleValue\x1a\x37\n\x15OutputWatermarksEntry\x12\x0b\n\x03key\x18\x01 \x01(\t\x12\r\n\x05value\x18\x02 \x01(\x03:\x02\x38\x01\x1a|\n\x12\x44\x65layedApplication\x12\x11\n\tdelay_sec\x18\x01 \x01(\x01\x12S\n\x0b\x61pplication\x18\x02 \x01(\x0b\x32>.org.apache.beam.model.fn_execution.v1.BundleSplit.Application\"Y\n\x14ProcessBundleRequest\x12+\n#process_bundle_descriptor_reference\x18\x01 \x01(\t\x12\x14\n\x0c\x63\x61\x63he_tokens\x18\x02 \x03(\x0c\"\x9b\x01\n\x15ProcessBundleResponse\x12?\n\x07metrics\x18\x01 \x01(\x0b\x32..org.apache.beam.model.fn_execution.v1.Metrics\x12\x41\n\x05split\x18\x02 \x01(\x0b\x32\x32.org.apache.beam.model.fn_execution.v1.BundleSplit\"=\n\x1cProcessBundleProgressRequest\x12\x1d\n\x15instruction_reference\x18\x01 \x01(\t\"\xbf\x03\n\x0eMonitoringInfo\x12\x0b\n\x03urn\x18\x01 \x01(\t\x12\x0c\n\x04type\x18\x02 \x01(\t\x12[\n\x15monitoring_table_data\x18\x03 \x01(\x0b\x32:.org.apache.beam.model.fn_execution.v1.MonitoringTableDataH\x00\x12?\n\x06metric\x18\x04 \x01(\x0b\x32-.org.apache.beam.model.fn_execution.v1.MetricH\x00\x12Q\n\x06labels\x18\x05 \x03(\x0b\x32\x41.org.apache.beam.model.fn_execution.v1.MonitoringInfo.LabelsEntry\x1a-\n\x0bLabelsEntry\x12\x0b\n\x03key\x18\x01 \x01(\t\x12\r\n\x05value\x18\x02 \x01(\t:\x02\x38\x01\"j\n\x14MonitoringInfoLabels\x12\r\n\tTRANSFORM\x10\x00\x12\x0f\n\x0bPCOLLECTION\x10\x01\x12\x16\n\x12WINDOWING_STRATEGY\x10\x02\x12\t\n\x05\x43ODER\x10\x03\x12\x0f\n\x0b\x45NVIRONMENT\x10\x04\x42\x06\n\x04\x64\x61ta\"\xfe\x01\n\x06Metric\x12J\n\x0c\x63ounter_data\x18\x01 \x01(\x0b\x32\x32.org.apache.beam.model.fn_execution.v1.CounterDataH\x00\x12T\n\x11\x64istribution_data\x18\x02 \x01(\x0b\x32\x37.org.apache.beam.model.fn_execution.v1.DistributionDataH\x00\x12J\n\x0c\x65xtrema_data\x18\x03 \x01(\x0b\x32\x32.org.apache.beam.model.fn_execution.v1.ExtremaDataH\x00\x42\x06\n\x04\x64\x61ta\"]\n\x0b\x43ounterData\x12\x15\n\x0bint64_value\x18\x01 \x01(\x03H\x00\x12\x16\n\x0c\x64ouble_value\x18\x02 \x01(\x01H\x00\x12\x16\n\x0cstring_value\x18\x03 \x01(\tH\x00\x42\x07\n\x05value\"\xc4\x01\n\x0b\x45xtremaData\x12Q\n\x10int_extrema_data\x18\x01 \x01(\x0b\x32\x35.org.apache.beam.model.fn_execution.v1.IntExtremaDataH\x00\x12W\n\x13\x64ouble_extrema_data\x18\x02 \x01(\x0b\x32\x38.org.apache.beam.model.fn_execution.v1.DoubleExtremaDataH\x00\x42\t\n\x07\x65xtrema\"$\n\x0eIntExtremaData\x12\x12\n\nint_values\x18\x01 \x03(\x03\"*\n\x11\x44oubleExtremaData\x12\x15\n\rdouble_values\x18\x02 \x03(\x01\"\xe2\x01\n\x10\x44istributionData\x12[\n\x15int_distribution_data\x18\x01 \x01(\x0b\x32:.org.apache.beam.model.fn_execution.v1.IntDistributionDataH\x00\x12\x61\n\x18\x64ouble_distribution_data\x18\x02 \x01(\x0b\x32=.org.apache.beam.model.fn_execution.v1.DoubleDistributionDataH\x00\x42\x0e\n\x0c\x64istribution\"K\n\x13IntDistributionData\x12\r\n\x05\x63ount\x18\x01 \x01(\x03\x12\x0b\n\x03sum\x18\x02 \x01(\x03\x12\x0b\n\x03min\x18\x03 \x01(\x03\x12\x0b\n\x03max\x18\x04 \x01(\x03\"N\n\x16\x44oubleDistributionData\x12\r\n\x05\x63ount\x18\x01 \x01(\x03\x12\x0b\n\x03sum\x18\x02 \x01(\x01\x12\x0b\n\x03min\x18\x03 \x01(\x01\x12\x0b\n\x03max\x18\x04 \x01(\x01\"\x95\x03\n\x13MonitoringTableData\x12\x14\n\x0c\x63olumn_names\x18\x01 \x03(\t\x12Z\n\x08row_data\x18\x02 \x03(\x0b\x32H.org.apache.beam.model.fn_execution.v1.MonitoringTableData.MonitoringRow\x1a\x98\x01\n\x15MonitoringColumnValue\x12\x15\n\x0bint64_value\x18\x01 \x01(\x03H\x00\x12\x16\n\x0c\x64ouble_value\x18\x02 \x01(\x01H\x00\x12\x16\n\x0cstring_value\x18\x03 \x01(\tH\x00\x12/\n\ttimestamp\x18\x04 \x01(\x0b\x32\x1a.google.protobuf.TimestampH\x00\x42\x07\n\x05value\x1aq\n\rMonitoringRow\x12`\n\x06values\x18\x01 \x03(\x0b\x32P.org.apache.beam.model.fn_execution.v1.MonitoringTableData.MonitoringColumnValue\"\xa9\x10\n\x07Metrics\x12T\n\x0bptransforms\x18\x01 \x03(\x0b\x32?.org.apache.beam.model.fn_execution.v1.Metrics.PtransformsEntry\x1a\xfc\t\n\nPTransform\x12g\n\x12processed_elements\x18\x01 \x01(\x0b\x32K.org.apache.beam.model.fn_execution.v1.Metrics.PTransform.ProcessedElements\x12\x61\n\x0f\x61\x63tive_elements\x18\x02 \x01(\x0b\x32H.org.apache.beam.model.fn_execution.v1.Metrics.PTransform.ActiveElements\x12]\n\nwatermarks\x18\x03 \x03(\x0b\x32I.org.apache.beam.model.fn_execution.v1.Metrics.PTransform.WatermarksEntry\x12\x41\n\x04user\x18\x04 \x03(\x0b\x32\x33.org.apache.beam.model.fn_execution.v1.Metrics.User\x1a\x91\x03\n\x08Measured\x12x\n\x14input_element_counts\x18\x01 \x03(\x0b\x32Z.org.apache.beam.model.fn_execution.v1.Metrics.PTransform.Measured.InputElementCountsEntry\x12z\n\x15output_element_counts\x18\x02 \x03(\x0b\x32[.org.apache.beam.model.fn_execution.v1.Metrics.PTransform.Measured.OutputElementCountsEntry\x12\x18\n\x10total_time_spent\x18\x03 \x01(\x01\x1a\x39\n\x17InputElementCountsEntry\x12\x0b\n\x03key\x18\x01 \x01(\t\x12\r\n\x05value\x18\x02 \x01(\x03:\x02\x38\x01\x1a:\n\x18OutputElementCountsEntry\x12\x0b\n\x03key\x18\x01 \x01(\t\x12\r\n\x05value\x18\x02 \x01(\x03:\x02\x38\x01\x1ai\n\x11ProcessedElements\x12T\n\x08measured\x18\x01 \x01(\x0b\x32\x42.org.apache.beam.model.fn_execution.v1.Metrics.PTransform.Measured\x1a\xcd\x02\n\x0e\x41\x63tiveElements\x12T\n\x08measured\x18\x01 \x01(\x0b\x32\x42.org.apache.beam.model.fn_execution.v1.Metrics.PTransform.Measured\x12\x1a\n\x12\x66raction_remaining\x18\x02 \x01(\x01\x12\x88\x01\n\x19output_elements_remaining\x18\x03 \x03(\x0b\x32\x65.org.apache.beam.model.fn_execution.v1.Metrics.PTransform.ActiveElements.OutputElementsRemainingEntry\x1a>\n\x1cOutputElementsRemainingEntry\x12\x0b\n\x03key\x18\x01 \x01(\t\x12\r\n\x05value\x18\x02 \x01(\x03:\x02\x38\x01\x1a\x31\n\x0fWatermarksEntry\x12\x0b\n\x03key\x18\x01 \x01(\t\x12\r\n\x05value\x18\x02 \x01(\x03:\x02\x38\x01\x1a\xd9\x04\n\x04User\x12S\n\x0bmetric_name\x18\x01 \x01(\x0b\x32>.org.apache.beam.model.fn_execution.v1.Metrics.User.MetricName\x12X\n\x0c\x63ounter_data\x18\xe9\x07 \x01(\x0b\x32?.org.apache.beam.model.fn_execution.v1.Metrics.User.CounterDataH\x00\x12\x62\n\x11\x64istribution_data\x18\xea\x07 \x01(\x0b\x32\x44.org.apache.beam.model.fn_execution.v1.Metrics.User.DistributionDataH\x00\x12T\n\ngauge_data\x18\xeb\x07 \x01(\x0b\x32=.org.apache.beam.model.fn_execution.v1.Metrics.User.GaugeDataH\x00\x1a-\n\nMetricName\x12\x11\n\tnamespace\x18\x02 \x01(\t\x12\x0c\n\x04name\x18\x03 \x01(\t\x1a\x1c\n\x0b\x43ounterData\x12\r\n\x05value\x18\x01 \x01(\x03\x1aH\n\x10\x44istributionData\x12\r\n\x05\x63ount\x18\x01 \x01(\x03\x12\x0b\n\x03sum\x18\x02 \x01(\x03\x12\x0b\n\x03min\x18\x03 \x01(\x03\x12\x0b\n\x03max\x18\x04 \x01(\x03\x1aI\n\tGaugeData\x12\r\n\x05value\x18\x01 \x01(\x03\x12-\n\ttimestamp\x18\x02 \x01(\x0b\x32\x1a.google.protobuf.TimestampB\x06\n\x04\x64\x61ta\x1am\n\x10PtransformsEntry\x12\x0b\n\x03key\x18\x01 \x01(\t\x12H\n\x05value\x18\x02 \x01(\x0b\x32\x39.org.apache.beam.model.fn_execution.v1.Metrics.PTransform:\x02\x38\x01\"\xa3\x01\n\x1dProcessBundleProgressResponse\x12?\n\x07metrics\x18\x01 \x01(\x0b\x32..org.apache.beam.model.fn_execution.v1.Metrics\x12\x41\n\x05split\x18\x02 \x01(\x0b\x32\x32.org.apache.beam.model.fn_execution.v1.BundleSplit\"w\n\x19ProcessBundleSplitRequest\x12\x1d\n\x15instruction_reference\x18\x01 \x01(\t\x12;\n\x15\x66raction_of_remainder\x18\x02 \x01(\x0b\x32\x1c.google.protobuf.DoubleValue\"\x1c\n\x1aProcessBundleSplitResponse\"\xc2\x01\n\x08\x45lements\x12\x42\n\x04\x64\x61ta\x18\x01 \x03(\x0b\x32\x34.org.apache.beam.model.fn_execution.v1.Elements.Data\x1ar\n\x04\x44\x61ta\x12\x1d\n\x15instruction_reference\x18\x01 \x01(\t\x12=\n\x06target\x18\x02 \x01(\x0b\x32-.org.apache.beam.model.fn_execution.v1.Target\x12\x0c\n\x04\x64\x61ta\x18\x03 \x01(\x0c\"\xea\x02\n\x0cStateRequest\x12\n\n\x02id\x18\x01 \x01(\t\x12\x1d\n\x15instruction_reference\x18\x02 \x01(\t\x12\x42\n\tstate_key\x18\x03 \x01(\x0b\x32/.org.apache.beam.model.fn_execution.v1.StateKey\x12\x46\n\x03get\x18\xe8\x07 \x01(\x0b\x32\x36.org.apache.beam.model.fn_execution.v1.StateGetRequestH\x00\x12L\n\x06\x61ppend\x18\xe9\x07 \x01(\x0b\x32\x39.org.apache.beam.model.fn_execution.v1.StateAppendRequestH\x00\x12J\n\x05\x63lear\x18\xea\x07 \x01(\x0b\x32\x38.org.apache.beam.model.fn_execution.v1.StateClearRequestH\x00\x42\t\n\x07request\"\xb0\x02\n\rStateResponse\x12\n\n\x02id\x18\x01 \x01(\t\x12\r\n\x05\x65rror\x18\x02 \x01(\t\x12\x13\n\x0b\x63\x61\x63he_token\x18\x03 \x01(\x0c\x12G\n\x03get\x18\xe8\x07 \x01(\x0b\x32\x37.org.apache.beam.model.fn_execution.v1.StateGetResponseH\x00\x12M\n\x06\x61ppend\x18\xe9\x07 \x01(\x0b\x32:.org.apache.beam.model.fn_execution.v1.StateAppendResponseH\x00\x12K\n\x05\x63lear\x18\xea\x07 \x01(\x0b\x32\x39.org.apache.beam.model.fn_execution.v1.StateClearResponseH\x00\x42\n\n\x08response\"\xe8\x03\n\x08StateKey\x12H\n\x06runner\x18\x01 \x01(\x0b\x32\x36.org.apache.beam.model.fn_execution.v1.StateKey.RunnerH\x00\x12`\n\x13multimap_side_input\x18\x02 \x01(\x0b\x32\x41.org.apache.beam.model.fn_execution.v1.StateKey.MultimapSideInputH\x00\x12V\n\x0e\x62\x61g_user_state\x18\x03 \x01(\x0b\x32<.org.apache.beam.model.fn_execution.v1.StateKey.BagUserStateH\x00\x1a\x15\n\x06Runner\x12\x0b\n\x03key\x18\x01 \x01(\x0c\x1a^\n\x11MultimapSideInput\x12\x15\n\rptransform_id\x18\x01 \x01(\t\x12\x15\n\rside_input_id\x18\x02 \x01(\t\x12\x0e\n\x06window\x18\x03 \x01(\x0c\x12\x0b\n\x03key\x18\x04 \x01(\x0c\x1aY\n\x0c\x42\x61gUserState\x12\x15\n\rptransform_id\x18\x01 \x01(\t\x12\x15\n\ruser_state_id\x18\x02 \x01(\t\x12\x0e\n\x06window\x18\x03 \x01(\x0c\x12\x0b\n\x03key\x18\x04 \x01(\x0c\x42\x06\n\x04type\"-\n\x0fStateGetRequest\x12\x1a\n\x12\x63ontinuation_token\x18\x01 \x01(\x0c\"<\n\x10StateGetResponse\x12\x1a\n\x12\x63ontinuation_token\x18\x01 \x01(\x0c\x12\x0c\n\x04\x64\x61ta\x18\x02 \x01(\x0c\"\"\n\x12StateAppendRequest\x12\x0c\n\x04\x64\x61ta\x18\x01 \x01(\x0c\"\x15\n\x13StateAppendResponse\"\x13\n\x11StateClearRequest\"\x14\n\x12StateClearResponse\"\xd8\x03\n\x08LogEntry\x12O\n\x08severity\x18\x01 \x01(\x0e\x32=.org.apache.beam.model.fn_execution.v1.LogEntry.Severity.Enum\x12-\n\ttimestamp\x18\x02 \x01(\x0b\x32\x1a.google.protobuf.Timestamp\x12\x0f\n\x07message\x18\x03 \x01(\t\x12\r\n\x05trace\x18\x04 \x01(\t\x12\x1d\n\x15instruction_reference\x18\x05 \x01(\t\x12%\n\x1dprimitive_transform_reference\x18\x06 \x01(\t\x12\x14\n\x0clog_location\x18\x07 \x01(\t\x12\x0e\n\x06thread\x18\x08 \x01(\t\x1aL\n\x04List\x12\x44\n\x0blog_entries\x18\x01 \x03(\x0b\x32/.org.apache.beam.model.fn_execution.v1.LogEntry\x1ar\n\x08Severity\"f\n\x04\x45num\x12\x0f\n\x0bUNSPECIFIED\x10\x00\x12\t\n\x05TRACE\x10\x01\x12\t\n\x05\x44\x45\x42UG\x10\x02\x12\x08\n\x04INFO\x10\x03\x12\n\n\x06NOTICE\x10\x04\x12\x08\n\x04WARN\x10\x05\x12\t\n\x05\x45RROR\x10\x06\x12\x0c\n\x08\x43RITICAL\x10\x07\"\x0c\n\nLogControl2\x98\x01\n\rBeamFnControl\x12\x86\x01\n\x07\x43ontrol\x12:.org.apache.beam.model.fn_execution.v1.InstructionResponse\x1a\x39.org.apache.beam.model.fn_execution.v1.InstructionRequest\"\x00(\x01\x30\x01\x32|\n\nBeamFnData\x12n\n\x04\x44\x61ta\x12/.org.apache.beam.model.fn_execution.v1.Elements\x1a/.org.apache.beam.model.fn_execution.v1.Elements\"\x00(\x01\x30\x01\x32\x87\x01\n\x0b\x42\x65\x61mFnState\x12x\n\x05State\x12\x33.org.apache.beam.model.fn_execution.v1.StateRequest\x1a\x34.org.apache.beam.model.fn_execution.v1.StateResponse\"\x00(\x01\x30\x01\x32\x89\x01\n\rBeamFnLogging\x12x\n\x07Logging\x12\x34.org.apache.beam.model.fn_execution.v1.LogEntry.List\x1a\x31.org.apache.beam.model.fn_execution.v1.LogControl\"\x00(\x01\x30\x01\x42\x41\n$org.apache.beam.model.fnexecution.v1B\tBeamFnApiZ\x0e\x66nexecution_v1b\x06proto3')
+  serialized_pb=_b('\n\x11\x62\x65\x61m_fn_api.proto\x12%org.apache.beam.model.fn_execution.v1\x1a\x15\x62\x65\x61m_runner_api.proto\x1a\x0f\x65ndpoints.proto\x1a google/protobuf/descriptor.proto\x1a\x1fgoogle/protobuf/timestamp.proto\x1a\x1egoogle/protobuf/wrappers.proto\"\x84\x01\n\x06Target\x12%\n\x1dprimitive_transform_reference\x18\x01 \x01(\t\x12\x0c\n\x04name\x18\x02 \x01(\t\x1a\x45\n\x04List\x12=\n\x06target\x18\x01 \x03(\x0b\x32-.org.apache.beam.model.fn_execution.v1.Target\"{\n\x0eRemoteGrpcPort\x12W\n\x16\x61pi_service_descriptor\x18\x01 \x01(\x0b\x32\x37.org.apache.beam.model.pipeline.v1.ApiServiceDescriptor\x12\x10\n\x08\x63oder_id\x18\x02 \x01(\t\"\x82\x04\n\x12InstructionRequest\x12\x16\n\x0einstruction_id\x18\x01 \x01(\t\x12K\n\x08register\x18\xe8\x07 \x01(\x0b\x32\x36.org.apache.beam.model.fn_execution.v1.RegisterRequestH\x00\x12V\n\x0eprocess_bundle\x18\xe9\x07 \x01(\x0b\x32;.org.apache.beam.model.fn_execution.v1.ProcessBundleRequestH\x00\x12g\n\x17process_bundle_progress\x18\xea\x07 \x01(\x0b\x32\x43.org.apache.beam.model.fn_execution.v1.ProcessBundleProgressRequestH\x00\x12\x61\n\x14process_bundle_split\x18\xeb\x07 \x01(\x0b\x32@.org.apache.beam.model.fn_execution.v1.ProcessBundleSplitRequestH\x00\x12X\n\x0f\x66inalize_bundle\x18\xec\x07 \x01(\x0b\x32<.org.apache.beam.model.fn_execution.v1.FinalizeBundleRequestH\x00\x42\t\n\x07request\"\x98\x04\n\x13InstructionResponse\x12\x16\n\x0einstruction_id\x18\x01 \x01(\t\x12\r\n\x05\x65rror\x18\x02 \x01(\t\x12L\n\x08register\x18\xe8\x07 \x01(\x0b\x32\x37.org.apache.beam.model.fn_execution.v1.RegisterResponseH\x00\x12W\n\x0eprocess_bundle\x18\xe9\x07 \x01(\x0b\x32<.org.apache.beam.model.fn_execution.v1.ProcessBundleResponseH\x00\x12h\n\x17process_bundle_progress\x18\xea\x07 \x01(\x0b\x32\x44.org.apache.beam.model.fn_execution.v1.ProcessBundleProgressResponseH\x00\x12\x62\n\x14process_bundle_split\x18\xeb\x07 \x01(\x0b\x32\x41.org.apache.beam.model.fn_execution.v1.ProcessBundleSplitResponseH\x00\x12Y\n\x0f\x66inalize_bundle\x18\xec\x07 \x01(\x0b\x32=.org.apache.beam.model.fn_execution.v1.FinalizeBundleResponseH\x00\x42\n\n\x08response\"t\n\x0fRegisterRequest\x12\x61\n\x19process_bundle_descriptor\x18\x01 \x03(\x0b\x32>.org.apache.beam.model.fn_execution.v1.ProcessBundleDescriptor\"\x12\n\x10RegisterResponse\"\x82\t\n\x17ProcessBundleDescriptor\x12\n\n\x02id\x18\x01 \x01(\t\x12\x62\n\ntransforms\x18\x02 \x03(\x0b\x32N.org.apache.beam.model.fn_execution.v1.ProcessBundleDescriptor.TransformsEntry\x12\x66\n\x0cpcollections\x18\x03 \x03(\x0b\x32P.org.apache.beam.model.fn_execution.v1.ProcessBundleDescriptor.PcollectionsEntry\x12u\n\x14windowing_strategies\x18\x04 \x03(\x0b\x32W.org.apache.beam.model.fn_execution.v1.ProcessBundleDescriptor.WindowingStrategiesEntry\x12Z\n\x06\x63oders\x18\x05 \x03(\x0b\x32J.org.apache.beam.model.fn_execution.v1.ProcessBundleDescriptor.CodersEntry\x12\x66\n\x0c\x65nvironments\x18\x06 \x03(\x0b\x32P.org.apache.beam.model.fn_execution.v1.ProcessBundleDescriptor.EnvironmentsEntry\x12]\n\x1cstate_api_service_descriptor\x18\x07 \x01(\x0b\x32\x37.org.apache.beam.model.pipeline.v1.ApiServiceDescriptor\x1a`\n\x0fTransformsEntry\x12\x0b\n\x03key\x18\x01 \x01(\t\x12<\n\x05value\x18\x02 \x01(\x0b\x32-.org.apache.beam.model.pipeline.v1.PTransform:\x02\x38\x01\x1a\x63\n\x11PcollectionsEntry\x12\x0b\n\x03key\x18\x01 \x01(\t\x12=\n\x05value\x18\x02 \x01(\x0b\x32..org.apache.beam.model.pipeline.v1.PCollection:\x02\x38\x01\x1ap\n\x18WindowingStrategiesEntry\x12\x0b\n\x03key\x18\x01 \x01(\t\x12\x43\n\x05value\x18\x02 \x01(\x0b\x32\x34.org.apache.beam.model.pipeline.v1.WindowingStrategy:\x02\x38\x01\x1aW\n\x0b\x43odersEntry\x12\x0b\n\x03key\x18\x01 \x01(\t\x12\x37\n\x05value\x18\x02 \x01(\x0b\x32(.org.apache.beam.model.pipeline.v1.Coder:\x02\x38\x01\x1a\x63\n\x11\x45nvironmentsEntry\x12\x0b\n\x03key\x18\x01 \x01(\t\x12=\n\x05value\x18\x02 \x01(\x0b\x32..org.apache.beam.model.pipeline.v1.Environment:\x02\x38\x01\"\xc8\x04\n\x11\x42undleApplication\x12\x15\n\rptransform_id\x18\x01 \x01(\t\x12\x10\n\x08input_id\x18\x02 \x01(\t\x12\x0f\n\x07\x65lement\x18\x03 \x01(\x0c\x12i\n\x11output_watermarks\x18\x04 \x03(\x0b\x32N.org.apache.beam.model.fn_execution.v1.BundleApplication.OutputWatermarksEntry\x12Q\n\x07\x62\x61\x63klog\x18\x05 \x01(\x0b\x32@.org.apache.beam.model.fn_execution.v1.BundleApplication.Backlog\x12\x45\n\nis_bounded\x18\x06 \x01(\x0e\x32\x31.org.apache.beam.model.pipeline.v1.IsBounded.Enum\x12O\n\x10monitoring_infos\x18\x07 \x03(\x0b\x32\x35.org.apache.beam.model.fn_execution.v1.MonitoringInfo\x1aS\n\x15OutputWatermarksEntry\x12\x0b\n\x03key\x18\x01 \x01(\t\x12)\n\x05value\x18\x02 \x01(\x0b\x32\x1a.google.protobuf.Timestamp:\x02\x38\x01\x1aN\n\x07\x42\x61\x63klog\x12\x11\n\tpartition\x18\x01 \x01(\x0c\x12\x10\n\x05\x62ytes\x18\xe8\x07 \x01(\x0cH\x00\x12\x15\n\nis_unknown\x18\xe9\x07 \x01(\x08H\x00\x42\x07\n\x05value\"\xa7\x01\n\x18\x44\x65layedBundleApplication\x12<\n\x18requested_execution_time\x18\x01 \x01(\x0b\x32\x1a.google.protobuf.Timestamp\x12M\n\x0b\x61pplication\x18\x02 \x01(\x0b\x32\x38.org.apache.beam.model.fn_execution.v1.BundleApplication\"Y\n\x14ProcessBundleRequest\x12+\n#process_bundle_descriptor_reference\x18\x01 \x01(\t\x12\x14\n\x0c\x63\x61\x63he_tokens\x18\x02 \x03(\x0c\"\xa1\x02\n\x15ProcessBundleResponse\x12?\n\x07metrics\x18\x01 \x01(\x0b\x32..org.apache.beam.model.fn_execution.v1.Metrics\x12W\n\x0eresidual_roots\x18\x02 \x03(\x0b\x32?.org.apache.beam.model.fn_execution.v1.DelayedBundleApplication\x12O\n\x10monitoring_infos\x18\x03 \x03(\x0b\x32\x35.org.apache.beam.model.fn_execution.v1.MonitoringInfo\x12\x1d\n\x15requires_finalization\x18\x04 \x01(\x08\"=\n\x1cProcessBundleProgressRequest\x12\x1d\n\x15instruction_reference\x18\x01 \x01(\t\"\xee\x03\n\x0eMonitoringInfo\x12\x0b\n\x03urn\x18\x01 \x01(\t\x12\x0c\n\x04type\x18\x02 \x01(\t\x12[\n\x15monitoring_table_data\x18\x03 \x01(\x0b\x32:.org.apache.beam.model.fn_execution.v1.MonitoringTableDataH\x00\x12?\n\x06metric\x18\x04 \x01(\x0b\x32-.org.apache.beam.model.fn_execution.v1.MetricH\x00\x12Q\n\x06labels\x18\x05 \x03(\x0b\x32\x41.org.apache.beam.model.fn_execution.v1.MonitoringInfo.LabelsEntry\x12-\n\ttimestamp\x18\x06 \x01(\x0b\x32\x1a.google.protobuf.Timestamp\x1a-\n\x0bLabelsEntry\x12\x0b\n\x03key\x18\x01 \x01(\t\x12\r\n\x05value\x18\x02 \x01(\t:\x02\x38\x01\"j\n\x14MonitoringInfoLabels\x12\r\n\tTRANSFORM\x10\x00\x12\x0f\n\x0bPCOLLECTION\x10\x01\x12\x16\n\x12WINDOWING_STRATEGY\x10\x02\x12\t\n\x05\x43ODER\x10\x03\x12\x0f\n\x0b\x45NVIRONMENT\x10\x04\x42\x06\n\x04\x64\x61ta\"\xde\x03\n\x12MonitoringInfoUrns\"\xc7\x03\n\x04\x45num\x12\x33\n\x17USER_COUNTER_URN_PREFIX\x10\x00\x1a\x16\xa2\xb4\xfa\xc2\x05\x10\x62\x65\x61m:metric:user\x12\x35\n\rELEMENT_COUNT\x10\x01\x1a\"\xa2\xb4\xfa\xc2\x05\x1c\x62\x65\x61m:metric:element_count:v1\x12T\n\x12START_BUNDLE_MSECS\x10\x02\x1a<\xa2\xb4\xfa\xc2\x05\x36\x62\x65\x61m:metric:pardo_execution_time:start_bundle_msecs:v1\x12X\n\x14PROCESS_BUNDLE_MSECS\x10\x03\x1a>\xa2\xb4\xfa\xc2\x05\x38\x62\x65\x61m:metric:pardo_execution_time:process_bundle_msecs:v1\x12V\n\x13\x46INISH_BUNDLE_MSECS\x10\x04\x1a=\xa2\xb4\xfa\xc2\x05\x37\x62\x65\x61m:metric:pardo_execution_time:finish_bundle_msecs:v1\x12K\n\x0bTOTAL_MSECS\x10\x05\x1a:\xa2\xb4\xfa\xc2\x05\x34\x62\x65\x61m:metric:ptransform_execution_time:total_msecs:v1\"\xd2\x01\n\x16MonitoringInfoTypeUrns\"\xb7\x01\n\x04\x45num\x12\x31\n\x0eSUM_INT64_TYPE\x10\x00\x1a\x1d\xa2\xb4\xfa\xc2\x05\x17\x62\x65\x61m:metrics:sum_int_64\x12\x43\n\x17\x44ISTRIBUTION_INT64_TYPE\x10\x01\x1a&\xa2\xb4\xfa\xc2\x05 beam:metrics:distribution_int_64\x12\x37\n\x11LATEST_INT64_TYPE\x10\x02\x1a \xa2\xb4\xfa\xc2\x05\x1a\x62\x65\x61m:metrics:latest_int_64\"\xfe\x01\n\x06Metric\x12J\n\x0c\x63ounter_data\x18\x01 \x01(\x0b\x32\x32.org.apache.beam.model.fn_execution.v1.CounterDataH\x00\x12T\n\x11\x64istribution_data\x18\x02 \x01(\x0b\x32\x37.org.apache.beam.model.fn_execution.v1.DistributionDataH\x00\x12J\n\x0c\x65xtrema_data\x18\x03 \x01(\x0b\x32\x32.org.apache.beam.model.fn_execution.v1.ExtremaDataH\x00\x42\x06\n\x04\x64\x61ta\"]\n\x0b\x43ounterData\x12\x15\n\x0bint64_value\x18\x01 \x01(\x03H\x00\x12\x16\n\x0c\x64ouble_value\x18\x02 \x01(\x01H\x00\x12\x16\n\x0cstring_value\x18\x03 \x01(\tH\x00\x42\x07\n\x05value\"\xc4\x01\n\x0b\x45xtremaData\x12Q\n\x10int_extrema_data\x18\x01 \x01(\x0b\x32\x35.org.apache.beam.model.fn_execution.v1.IntExtremaDataH\x00\x12W\n\x13\x64ouble_extrema_data\x18\x02 \x01(\x0b\x32\x38.org.apache.beam.model.fn_execution.v1.DoubleExtremaDataH\x00\x42\t\n\x07\x65xtrema\"$\n\x0eIntExtremaData\x12\x12\n\nint_values\x18\x01 \x03(\x03\"*\n\x11\x44oubleExtremaData\x12\x15\n\rdouble_values\x18\x02 \x03(\x01\"\xe2\x01\n\x10\x44istributionData\x12[\n\x15int_distribution_data\x18\x01 \x01(\x0b\x32:.org.apache.beam.model.fn_execution.v1.IntDistributionDataH\x00\x12\x61\n\x18\x64ouble_distribution_data\x18\x02 \x01(\x0b\x32=.org.apache.beam.model.fn_execution.v1.DoubleDistributionDataH\x00\x42\x0e\n\x0c\x64istribution\"K\n\x13IntDistributionData\x12\r\n\x05\x63ount\x18\x01 \x01(\x03\x12\x0b\n\x03sum\x18\x02 \x01(\x03\x12\x0b\n\x03min\x18\x03 \x01(\x03\x12\x0b\n\x03max\x18\x04 \x01(\x03\"N\n\x16\x44oubleDistributionData\x12\r\n\x05\x63ount\x18\x01 \x01(\x03\x12\x0b\n\x03sum\x18\x02 \x01(\x01\x12\x0b\n\x03min\x18\x03 \x01(\x01\x12\x0b\n\x03max\x18\x04 \x01(\x01\"\x95\x03\n\x13MonitoringTableData\x12\x14\n\x0c\x63olumn_names\x18\x01 \x03(\t\x12Z\n\x08row_data\x18\x02 \x03(\x0b\x32H.org.apache.beam.model.fn_execution.v1.MonitoringTableData.MonitoringRow\x1a\x98\x01\n\x15MonitoringColumnValue\x12\x15\n\x0bint64_value\x18\x01 \x01(\x03H\x00\x12\x16\n\x0c\x64ouble_value\x18\x02 \x01(\x01H\x00\x12\x16\n\x0cstring_value\x18\x03 \x01(\tH\x00\x12/\n\ttimestamp\x18\x04 \x01(\x0b\x32\x1a.google.protobuf.TimestampH\x00\x42\x07\n\x05value\x1aq\n\rMonitoringRow\x12`\n\x06values\x18\x01 \x03(\x0b\x32P.org.apache.beam.model.fn_execution.v1.MonitoringTableData.MonitoringColumnValue\"\xa9\x10\n\x07Metrics\x12T\n\x0bptransforms\x18\x01 \x03(\x0b\x32?.org.apache.beam.model.fn_execution.v1.Metrics.PtransformsEntry\x1a\xfc\t\n\nPTransform\x12g\n\x12processed_elements\x18\x01 \x01(\x0b\x32K.org.apache.beam.model.fn_execution.v1.Metrics.PTransform.ProcessedElements\x12\x61\n\x0f\x61\x63tive_elements\x18\x02 \x01(\x0b\x32H.org.apache.beam.model.fn_execution.v1.Metrics.PTransform.ActiveElements\x12]\n\nwatermarks\x18\x03 \x03(\x0b\x32I.org.apache.beam.model.fn_execution.v1.Metrics.PTransform.WatermarksEntry\x12\x41\n\x04user\x18\x04 \x03(\x0b\x32\x33.org.apache.beam.model.fn_execution.v1.Metrics.User\x1a\x91\x03\n\x08Measured\x12x\n\x14input_element_counts\x18\x01 \x03(\x0b\x32Z.org.apache.beam.model.fn_execution.v1.Metrics.PTransform.Measured.InputElementCountsEntry\x12z\n\x15output_element_counts\x18\x02 \x03(\x0b\x32[.org.apache.beam.model.fn_execution.v1.Metrics.PTransform.Measured.OutputElementCountsEntry\x12\x18\n\x10total_time_spent\x18\x03 \x01(\x01\x1a\x39\n\x17InputElementCountsEntry\x12\x0b\n\x03key\x18\x01 \x01(\t\x12\r\n\x05value\x18\x02 \x01(\x03:\x02\x38\x01\x1a:\n\x18OutputElementCountsEntry\x12\x0b\n\x03key\x18\x01 \x01(\t\x12\r\n\x05value\x18\x02 \x01(\x03:\x02\x38\x01\x1ai\n\x11ProcessedElements\x12T\n\x08measured\x18\x01 \x01(\x0b\x32\x42.org.apache.beam.model.fn_execution.v1.Metrics.PTransform.Measured\x1a\xcd\x02\n\x0e\x41\x63tiveElements\x12T\n\x08measured\x18\x01 \x01(\x0b\x32\x42.org.apache.beam.model.fn_execution.v1.Metrics.PTransform.Measured\x12\x1a\n\x12\x66raction_remaining\x18\x02 \x01(\x01\x12\x88\x01\n\x19output_elements_remaining\x18\x03 \x03(\x0b\x32\x65.org.apache.beam.model.fn_execution.v1.Metrics.PTransform.ActiveElements.OutputElementsRemainingEntry\x1a>\n\x1cOutputElementsRemainingEntry\x12\x0b\n\x03key\x18\x01 \x01(\t\x12\r\n\x05value\x18\x02 \x01(\x03:\x02\x38\x01\x1a\x31\n\x0fWatermarksEntry\x12\x0b\n\x03key\x18\x01 \x01(\t\x12\r\n\x05value\x18\x02 \x01(\x03:\x02\x38\x01\x1a\xd9\x04\n\x04User\x12S\n\x0bmetric_name\x18\x01 \x01(\x0b\x32>.org.apache.beam.model.fn_execution.v1.Metrics.User.MetricName\x12X\n\x0c\x63ounter_data\x18\xe9\x07 \x01(\x0b\x32?.org.apache.beam.model.fn_execution.v1.Metrics.User.CounterDataH\x00\x12\x62\n\x11\x64istribution_data\x18\xea\x07 \x01(\x0b\x32\x44.org.apache.beam.model.fn_execution.v1.Metrics.User.DistributionDataH\x00\x12T\n\ngauge_data\x18\xeb\x07 \x01(\x0b\x32=.org.apache.beam.model.fn_execution.v1.Metrics.User.GaugeDataH\x00\x1a-\n\nMetricName\x12\x11\n\tnamespace\x18\x02 \x01(\t\x12\x0c\n\x04name\x18\x03 \x01(\t\x1a\x1c\n\x0b\x43ounterData\x12\r\n\x05value\x18\x01 \x01(\x03\x1aH\n\x10\x44istributionData\x12\r\n\x05\x63ount\x18\x01 \x01(\x03\x12\x0b\n\x03sum\x18\x02 \x01(\x03\x12\x0b\n\x03min\x18\x03 \x01(\x03\x12\x0b\n\x03max\x18\x04 \x01(\x03\x1aI\n\tGaugeData\x12\r\n\x05value\x18\x01 \x01(\x03\x12-\n\ttimestamp\x18\x02 \x01(\x0b\x32\x1a.google.protobuf.TimestampB\x06\n\x04\x64\x61ta\x1am\n\x10PtransformsEntry\x12\x0b\n\x03key\x18\x01 \x01(\t\x12H\n\x05value\x18\x02 \x01(\x0b\x32\x39.org.apache.beam.model.fn_execution.v1.Metrics.PTransform:\x02\x38\x01\"\x82\x02\n\x1dProcessBundleProgressResponse\x12?\n\x07metrics\x18\x01 \x01(\x0b\x32..org.apache.beam.model.fn_execution.v1.Metrics\x12O\n\x10monitoring_infos\x18\x03 \x03(\x0b\x32\x35.org.apache.beam.model.fn_execution.v1.MonitoringInfo\x12O\n\rprimary_roots\x18\x04 \x03(\x0b\x32\x38.org.apache.beam.model.fn_execution.v1.BundleApplication\"\xe6\x01\n\x19ProcessBundleSplitRequest\x12\x1d\n\x15instruction_reference\x18\x01 \x01(\t\x12q\n\x11\x62\x61\x63klog_remaining\x18\x02 \x03(\x0b\x32V.org.apache.beam.model.fn_execution.v1.ProcessBundleSplitRequest.BacklogRemainingEntry\x1a\x37\n\x15\x42\x61\x63klogRemainingEntry\x12\x0b\n\x03key\x18\x01 \x01(\t\x12\r\n\x05value\x18\x02 \x01(\x0c:\x02\x38\x01\"\xc6\x01\n\x1aProcessBundleSplitResponse\x12O\n\rprimary_roots\x18\x01 \x03(\x0b\x32\x38.org.apache.beam.model.fn_execution.v1.BundleApplication\x12W\n\x0eresidual_roots\x18\x02 \x03(\x0b\x32?.org.apache.beam.model.fn_execution.v1.DelayedBundleApplication\"6\n\x15\x46inalizeBundleRequest\x12\x1d\n\x15instruction_reference\x18\x01 \x01(\t\"\x18\n\x16\x46inalizeBundleResponse\"\xc2\x01\n\x08\x45lements\x12\x42\n\x04\x64\x61ta\x18\x01 \x03(\x0b\x32\x34.org.apache.beam.model.fn_execution.v1.Elements.Data\x1ar\n\x04\x44\x61ta\x12\x1d\n\x15instruction_reference\x18\x01 \x01(\t\x12=\n\x06target\x18\x02 \x01(\x0b\x32-.org.apache.beam.model.fn_execution.v1.Target\x12\x0c\n\x04\x64\x61ta\x18\x03 \x01(\x0c\"\xea\x02\n\x0cStateRequest\x12\n\n\x02id\x18\x01 \x01(\t\x12\x1d\n\x15instruction_reference\x18\x02 \x01(\t\x12\x42\n\tstate_key\x18\x03 \x01(\x0b\x32/.org.apache.beam.model.fn_execution.v1.StateKey\x12\x46\n\x03get\x18\xe8\x07 \x01(\x0b\x32\x36.org.apache.beam.model.fn_execution.v1.StateGetRequestH\x00\x12L\n\x06\x61ppend\x18\xe9\x07 \x01(\x0b\x32\x39.org.apache.beam.model.fn_execution.v1.StateAppendRequestH\x00\x12J\n\x05\x63lear\x18\xea\x07 \x01(\x0b\x32\x38.org.apache.beam.model.fn_execution.v1.StateClearRequestH\x00\x42\t\n\x07request\"\xb0\x02\n\rStateResponse\x12\n\n\x02id\x18\x01 \x01(\t\x12\r\n\x05\x65rror\x18\x02 \x01(\t\x12\x13\n\x0b\x63\x61\x63he_token\x18\x03 \x01(\x0c\x12G\n\x03get\x18\xe8\x07 \x01(\x0b\x32\x37.org.apache.beam.model.fn_execution.v1.StateGetResponseH\x00\x12M\n\x06\x61ppend\x18\xe9\x07 \x01(\x0b\x32:.org.apache.beam.model.fn_execution.v1.StateAppendResponseH\x00\x12K\n\x05\x63lear\x18\xea\x07 \x01(\x0b\x32\x39.org.apache.beam.model.fn_execution.v1.StateClearResponseH\x00\x42\n\n\x08response\"\xe8\x03\n\x08StateKey\x12H\n\x06runner\x18\x01 \x01(\x0b\x32\x36.org.apache.beam.model.fn_execution.v1.StateKey.RunnerH\x00\x12`\n\x13multimap_side_input\x18\x02 \x01(\x0b\x32\x41.org.apache.beam.model.fn_execution.v1.StateKey.MultimapSideInputH\x00\x12V\n\x0e\x62\x61g_user_state\x18\x03 \x01(\x0b\x32<.org.apache.beam.model.fn_execution.v1.StateKey.BagUserStateH\x00\x1a\x15\n\x06Runner\x12\x0b\n\x03key\x18\x01 \x01(\x0c\x1a^\n\x11MultimapSideInput\x12\x15\n\rptransform_id\x18\x01 \x01(\t\x12\x15\n\rside_input_id\x18\x02 \x01(\t\x12\x0e\n\x06window\x18\x03 \x01(\x0c\x12\x0b\n\x03key\x18\x04 \x01(\x0c\x1aY\n\x0c\x42\x61gUserState\x12\x15\n\rptransform_id\x18\x01 \x01(\t\x12\x15\n\ruser_state_id\x18\x02 \x01(\t\x12\x0e\n\x06window\x18\x03 \x01(\x0c\x12\x0b\n\x03key\x18\x04 \x01(\x0c\x42\x06\n\x04type\"-\n\x0fStateGetRequest\x12\x1a\n\x12\x63ontinuation_token\x18\x01 \x01(\x0c\"<\n\x10StateGetResponse\x12\x1a\n\x12\x63ontinuation_token\x18\x01 \x01(\x0c\x12\x0c\n\x04\x64\x61ta\x18\x02 \x01(\x0c\"\"\n\x12StateAppendRequest\x12\x0c\n\x04\x64\x61ta\x18\x01 \x01(\x0c\"\x15\n\x13StateAppendResponse\"\x13\n\x11StateClearRequest\"\x14\n\x12StateClearResponse\"\xd8\x03\n\x08LogEntry\x12O\n\x08severity\x18\x01 \x01(\x0e\x32=.org.apache.beam.model.fn_execution.v1.LogEntry.Severity.Enum\x12-\n\ttimestamp\x18\x02 \x01(\x0b\x32\x1a.google.protobuf.Timestamp\x12\x0f\n\x07message\x18\x03 \x01(\t\x12\r\n\x05trace\x18\x04 \x01(\t\x12\x1d\n\x15instruction_reference\x18\x05 \x01(\t\x12%\n\x1dprimitive_transform_reference\x18\x06 \x01(\t\x12\x14\n\x0clog_location\x18\x07 \x01(\t\x12\x0e\n\x06thread\x18\x08 \x01(\t\x1aL\n\x04List\x12\x44\n\x0blog_entries\x18\x01 \x03(\x0b\x32/.org.apache.beam.model.fn_execution.v1.LogEntry\x1ar\n\x08Severity\"f\n\x04\x45num\x12\x0f\n\x0bUNSPECIFIED\x10\x00\x12\t\n\x05TRACE\x10\x01\x12\t\n\x05\x44\x45\x42UG\x10\x02\x12\x08\n\x04INFO\x10\x03\x12\n\n\x06NOTICE\x10\x04\x12\x08\n\x04WARN\x10\x05\x12\t\n\x05\x45RROR\x10\x06\x12\x0c\n\x08\x43RITICAL\x10\x07\"\x0c\n\nLogControl2\x98\x01\n\rBeamFnControl\x12\x86\x01\n\x07\x43ontrol\x12:.org.apache.beam.model.fn_execution.v1.InstructionResponse\x1a\x39.org.apache.beam.model.fn_execution.v1.InstructionRequest\"\x00(\x01\x30\x01\x32|\n\nBeamFnData\x12n\n\x04\x44\x61ta\x12/.org.apache.beam.model.fn_execution.v1.Elements\x1a/.org.apache.beam.model.fn_execution.v1.Elements\"\x00(\x01\x30\x01\x32\x87\x01\n\x0b\x42\x65\x61mFnState\x12x\n\x05State\x12\x33.org.apache.beam.model.fn_execution.v1.StateRequest\x1a\x34.org.apache.beam.model.fn_execution.v1.StateResponse\"\x00(\x01\x30\x01\x32\x89\x01\n\rBeamFnLogging\x12x\n\x07Logging\x12\x34.org.apache.beam.model.fn_execution.v1.LogEntry.List\x1a\x31.org.apache.beam.model.fn_execution.v1.LogControl\"\x00(\x01\x30\x01\x42\x41\n$org.apache.beam.model.fnexecution.v1B\tBeamFnApiZ\x0e\x66nexecution_v1b\x06proto3')
   ,
-  dependencies=[beam__runner__api__pb2.DESCRIPTOR,endpoints__pb2.DESCRIPTOR,google_dot_protobuf_dot_timestamp__pb2.DESCRIPTOR,google_dot_protobuf_dot_wrappers__pb2.DESCRIPTOR,])
+  dependencies=[beam__runner__api__pb2.DESCRIPTOR,endpoints__pb2.DESCRIPTOR,google_dot_protobuf_dot_descriptor__pb2.DESCRIPTOR,google_dot_protobuf_dot_timestamp__pb2.DESCRIPTOR,google_dot_protobuf_dot_wrappers__pb2.DESCRIPTOR,])
+_sym_db.RegisterFileDescriptor(DESCRIPTOR)
 
 
 
 _MONITORINGINFO_MONITORINGINFOLABELS = _descriptor.EnumDescriptor(
   name='MonitoringInfoLabels',
   full_name='org.apache.beam.model.fn_execution.v1.MonitoringInfo.MonitoringInfoLabels',
   filename=None,
@@ -55,19 +57,83 @@
     _descriptor.EnumValueDescriptor(
       name='ENVIRONMENT', index=4, number=4,
       options=None,
       type=None),
   ],
   containing_type=None,
   options=None,
-  serialized_start=3865,
-  serialized_end=3971,
+  serialized_start=4394,
+  serialized_end=4500,
 )
 _sym_db.RegisterEnumDescriptor(_MONITORINGINFO_MONITORINGINFOLABELS)
 
+_MONITORINGINFOURNS_ENUM = _descriptor.EnumDescriptor(
+  name='Enum',
+  full_name='org.apache.beam.model.fn_execution.v1.MonitoringInfoUrns.Enum',
+  filename=None,
+  file=DESCRIPTOR,
+  values=[
+    _descriptor.EnumValueDescriptor(
+      name='USER_COUNTER_URN_PREFIX', index=0, number=0,
+      options=_descriptor._ParseOptions(descriptor_pb2.EnumValueOptions(), _b('\242\264\372\302\005\020beam:metric:user')),
+      type=None),
+    _descriptor.EnumValueDescriptor(
+      name='ELEMENT_COUNT', index=1, number=1,
+      options=_descriptor._ParseOptions(descriptor_pb2.EnumValueOptions(), _b('\242\264\372\302\005\034beam:metric:element_count:v1')),
+      type=None),
+    _descriptor.EnumValueDescriptor(
+      name='START_BUNDLE_MSECS', index=2, number=2,
+      options=_descriptor._ParseOptions(descriptor_pb2.EnumValueOptions(), _b('\242\264\372\302\0056beam:metric:pardo_execution_time:start_bundle_msecs:v1')),
+      type=None),
+    _descriptor.EnumValueDescriptor(
+      name='PROCESS_BUNDLE_MSECS', index=3, number=3,
+      options=_descriptor._ParseOptions(descriptor_pb2.EnumValueOptions(), _b('\242\264\372\302\0058beam:metric:pardo_execution_time:process_bundle_msecs:v1')),
+      type=None),
+    _descriptor.EnumValueDescriptor(
+      name='FINISH_BUNDLE_MSECS', index=4, number=4,
+      options=_descriptor._ParseOptions(descriptor_pb2.EnumValueOptions(), _b('\242\264\372\302\0057beam:metric:pardo_execution_time:finish_bundle_msecs:v1')),
+      type=None),
+    _descriptor.EnumValueDescriptor(
+      name='TOTAL_MSECS', index=5, number=5,
+      options=_descriptor._ParseOptions(descriptor_pb2.EnumValueOptions(), _b('\242\264\372\302\0054beam:metric:ptransform_execution_time:total_msecs:v1')),
+      type=None),
+  ],
+  containing_type=None,
+  options=None,
+  serialized_start=4534,
+  serialized_end=4989,
+)
+_sym_db.RegisterEnumDescriptor(_MONITORINGINFOURNS_ENUM)
+
+_MONITORINGINFOTYPEURNS_ENUM = _descriptor.EnumDescriptor(
+  name='Enum',
+  full_name='org.apache.beam.model.fn_execution.v1.MonitoringInfoTypeUrns.Enum',
+  filename=None,
+  file=DESCRIPTOR,
+  values=[
+    _descriptor.EnumValueDescriptor(
+      name='SUM_INT64_TYPE', index=0, number=0,
+      options=_descriptor._ParseOptions(descriptor_pb2.EnumValueOptions(), _b('\242\264\372\302\005\027beam:metrics:sum_int_64')),
+      type=None),
+    _descriptor.EnumValueDescriptor(
+      name='DISTRIBUTION_INT64_TYPE', index=1, number=1,
+      options=_descriptor._ParseOptions(descriptor_pb2.EnumValueOptions(), _b('\242\264\372\302\005 beam:metrics:distribution_int_64')),
+      type=None),
+    _descriptor.EnumValueDescriptor(
+      name='LATEST_INT64_TYPE', index=2, number=2,
+      options=_descriptor._ParseOptions(descriptor_pb2.EnumValueOptions(), _b('\242\264\372\302\005\032beam:metrics:latest_int_64')),
+      type=None),
+  ],
+  containing_type=None,
+  options=None,
+  serialized_start=5019,
+  serialized_end=5202,
+)
+_sym_db.RegisterEnumDescriptor(_MONITORINGINFOTYPEURNS_ENUM)
+
 _LOGENTRY_SEVERITY_ENUM = _descriptor.EnumDescriptor(
   name='Enum',
   full_name='org.apache.beam.model.fn_execution.v1.LogEntry.Severity.Enum',
   filename=None,
   file=DESCRIPTOR,
   values=[
     _descriptor.EnumValueDescriptor(
@@ -101,16 +167,16 @@
     _descriptor.EnumValueDescriptor(
       name='CRITICAL', index=7, number=7,
       options=None,
       type=None),
   ],
   containing_type=None,
   options=None,
-  serialized_start=9759,
-  serialized_end=9861,
+  serialized_start=11442,
+  serialized_end=11544,
 )
 _sym_db.RegisterEnumDescriptor(_LOGENTRY_SEVERITY_ENUM)
 
 
 _TARGET_LIST = _descriptor.Descriptor(
   name='List',
   full_name='org.apache.beam.model.fn_execution.v1.Target.List',
@@ -120,29 +186,29 @@
   fields=[
     _descriptor.FieldDescriptor(
       name='target', full_name='org.apache.beam.model.fn_execution.v1.Target.List.target', index=0,
       number=1, type=11, cpp_type=10, label=3,
       has_default_value=False, default_value=[],
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
   ],
   extensions=[
   ],
   nested_types=[],
   enum_types=[
   ],
   options=None,
   is_extendable=False,
   syntax='proto3',
   extension_ranges=[],
   oneofs=[
   ],
-  serialized_start=229,
-  serialized_end=298,
+  serialized_start=263,
+  serialized_end=332,
 )
 
 _TARGET = _descriptor.Descriptor(
   name='Target',
   full_name='org.apache.beam.model.fn_execution.v1.Target',
   filename=None,
   file=DESCRIPTOR,
@@ -150,36 +216,36 @@
   fields=[
     _descriptor.FieldDescriptor(
       name='primitive_transform_reference', full_name='org.apache.beam.model.fn_execution.v1.Target.primitive_transform_reference', index=0,
       number=1, type=9, cpp_type=9, label=1,
       has_default_value=False, default_value=_b("").decode('utf-8'),
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='name', full_name='org.apache.beam.model.fn_execution.v1.Target.name', index=1,
       number=2, type=9, cpp_type=9, label=1,
       has_default_value=False, default_value=_b("").decode('utf-8'),
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
   ],
   extensions=[
   ],
   nested_types=[_TARGET_LIST, ],
   enum_types=[
   ],
   options=None,
   is_extendable=False,
   syntax='proto3',
   extension_ranges=[],
   oneofs=[
   ],
-  serialized_start=166,
-  serialized_end=298,
+  serialized_start=200,
+  serialized_end=332,
 )
 
 
 _REMOTEGRPCPORT = _descriptor.Descriptor(
   name='RemoteGrpcPort',
   full_name='org.apache.beam.model.fn_execution.v1.RemoteGrpcPort',
   filename=None,
@@ -188,36 +254,36 @@
   fields=[
     _descriptor.FieldDescriptor(
       name='api_service_descriptor', full_name='org.apache.beam.model.fn_execution.v1.RemoteGrpcPort.api_service_descriptor', index=0,
       number=1, type=11, cpp_type=10, label=1,
       has_default_value=False, default_value=None,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='coder_id', full_name='org.apache.beam.model.fn_execution.v1.RemoteGrpcPort.coder_id', index=1,
       number=2, type=9, cpp_type=9, label=1,
       has_default_value=False, default_value=_b("").decode('utf-8'),
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
   ],
   extensions=[
   ],
   nested_types=[],
   enum_types=[
   ],
   options=None,
   is_extendable=False,
   syntax='proto3',
   extension_ranges=[],
   oneofs=[
   ],
-  serialized_start=300,
-  serialized_end=423,
+  serialized_start=334,
+  serialized_end=457,
 )
 
 
 _INSTRUCTIONREQUEST = _descriptor.Descriptor(
   name='InstructionRequest',
   full_name='org.apache.beam.model.fn_execution.v1.InstructionRequest',
   filename=None,
@@ -226,43 +292,50 @@
   fields=[
     _descriptor.FieldDescriptor(
       name='instruction_id', full_name='org.apache.beam.model.fn_execution.v1.InstructionRequest.instruction_id', index=0,
       number=1, type=9, cpp_type=9, label=1,
       has_default_value=False, default_value=_b("").decode('utf-8'),
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='register', full_name='org.apache.beam.model.fn_execution.v1.InstructionRequest.register', index=1,
       number=1000, type=11, cpp_type=10, label=1,
       has_default_value=False, default_value=None,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='process_bundle', full_name='org.apache.beam.model.fn_execution.v1.InstructionRequest.process_bundle', index=2,
       number=1001, type=11, cpp_type=10, label=1,
       has_default_value=False, default_value=None,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='process_bundle_progress', full_name='org.apache.beam.model.fn_execution.v1.InstructionRequest.process_bundle_progress', index=3,
       number=1002, type=11, cpp_type=10, label=1,
       has_default_value=False, default_value=None,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='process_bundle_split', full_name='org.apache.beam.model.fn_execution.v1.InstructionRequest.process_bundle_split', index=4,
       number=1003, type=11, cpp_type=10, label=1,
       has_default_value=False, default_value=None,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
+    _descriptor.FieldDescriptor(
+      name='finalize_bundle', full_name='org.apache.beam.model.fn_execution.v1.InstructionRequest.finalize_bundle', index=5,
+      number=1004, type=11, cpp_type=10, label=1,
+      has_default_value=False, default_value=None,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      options=None),
   ],
   extensions=[
   ],
   nested_types=[],
   enum_types=[
   ],
   options=None,
@@ -270,16 +343,16 @@
   syntax='proto3',
   extension_ranges=[],
   oneofs=[
     _descriptor.OneofDescriptor(
       name='request', full_name='org.apache.beam.model.fn_execution.v1.InstructionRequest.request',
       index=0, containing_type=None, fields=[]),
   ],
-  serialized_start=426,
-  serialized_end=850,
+  serialized_start=460,
+  serialized_end=974,
 )
 
 
 _INSTRUCTIONRESPONSE = _descriptor.Descriptor(
   name='InstructionResponse',
   full_name='org.apache.beam.model.fn_execution.v1.InstructionResponse',
   filename=None,
@@ -288,50 +361,57 @@
   fields=[
     _descriptor.FieldDescriptor(
       name='instruction_id', full_name='org.apache.beam.model.fn_execution.v1.InstructionResponse.instruction_id', index=0,
       number=1, type=9, cpp_type=9, label=1,
       has_default_value=False, default_value=_b("").decode('utf-8'),
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='error', full_name='org.apache.beam.model.fn_execution.v1.InstructionResponse.error', index=1,
       number=2, type=9, cpp_type=9, label=1,
       has_default_value=False, default_value=_b("").decode('utf-8'),
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='register', full_name='org.apache.beam.model.fn_execution.v1.InstructionResponse.register', index=2,
       number=1000, type=11, cpp_type=10, label=1,
       has_default_value=False, default_value=None,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='process_bundle', full_name='org.apache.beam.model.fn_execution.v1.InstructionResponse.process_bundle', index=3,
       number=1001, type=11, cpp_type=10, label=1,
       has_default_value=False, default_value=None,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='process_bundle_progress', full_name='org.apache.beam.model.fn_execution.v1.InstructionResponse.process_bundle_progress', index=4,
       number=1002, type=11, cpp_type=10, label=1,
       has_default_value=False, default_value=None,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='process_bundle_split', full_name='org.apache.beam.model.fn_execution.v1.InstructionResponse.process_bundle_split', index=5,
       number=1003, type=11, cpp_type=10, label=1,
       has_default_value=False, default_value=None,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
+    _descriptor.FieldDescriptor(
+      name='finalize_bundle', full_name='org.apache.beam.model.fn_execution.v1.InstructionResponse.finalize_bundle', index=6,
+      number=1004, type=11, cpp_type=10, label=1,
+      has_default_value=False, default_value=None,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      options=None),
   ],
   extensions=[
   ],
   nested_types=[],
   enum_types=[
   ],
   options=None,
@@ -339,16 +419,16 @@
   syntax='proto3',
   extension_ranges=[],
   oneofs=[
     _descriptor.OneofDescriptor(
       name='response', full_name='org.apache.beam.model.fn_execution.v1.InstructionResponse.response',
       index=0, containing_type=None, fields=[]),
   ],
-  serialized_start=853,
-  serialized_end=1298,
+  serialized_start=977,
+  serialized_end=1513,
 )
 
 
 _REGISTERREQUEST = _descriptor.Descriptor(
   name='RegisterRequest',
   full_name='org.apache.beam.model.fn_execution.v1.RegisterRequest',
   filename=None,
@@ -357,29 +437,29 @@
   fields=[
     _descriptor.FieldDescriptor(
       name='process_bundle_descriptor', full_name='org.apache.beam.model.fn_execution.v1.RegisterRequest.process_bundle_descriptor', index=0,
       number=1, type=11, cpp_type=10, label=3,
       has_default_value=False, default_value=[],
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
   ],
   extensions=[
   ],
   nested_types=[],
   enum_types=[
   ],
   options=None,
   is_extendable=False,
   syntax='proto3',
   extension_ranges=[],
   oneofs=[
   ],
-  serialized_start=1300,
-  serialized_end=1416,
+  serialized_start=1515,
+  serialized_end=1631,
 )
 
 
 _REGISTERRESPONSE = _descriptor.Descriptor(
   name='RegisterResponse',
   full_name='org.apache.beam.model.fn_execution.v1.RegisterResponse',
   filename=None,
@@ -394,16 +474,16 @@
   ],
   options=None,
   is_extendable=False,
   syntax='proto3',
   extension_ranges=[],
   oneofs=[
   ],
-  serialized_start=1418,
-  serialized_end=1436,
+  serialized_start=1633,
+  serialized_end=1651,
 )
 
 
 _PROCESSBUNDLEDESCRIPTOR_TRANSFORMSENTRY = _descriptor.Descriptor(
   name='TransformsEntry',
   full_name='org.apache.beam.model.fn_execution.v1.ProcessBundleDescriptor.TransformsEntry',
   filename=None,
@@ -412,36 +492,36 @@
   fields=[
     _descriptor.FieldDescriptor(
       name='key', full_name='org.apache.beam.model.fn_execution.v1.ProcessBundleDescriptor.TransformsEntry.key', index=0,
       number=1, type=9, cpp_type=9, label=1,
       has_default_value=False, default_value=_b("").decode('utf-8'),
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='value', full_name='org.apache.beam.model.fn_execution.v1.ProcessBundleDescriptor.TransformsEntry.value', index=1,
       number=2, type=11, cpp_type=10, label=1,
       has_default_value=False, default_value=None,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
   ],
   extensions=[
   ],
   nested_types=[],
   enum_types=[
   ],
   options=_descriptor._ParseOptions(descriptor_pb2.MessageOptions(), _b('8\001')),
   is_extendable=False,
   syntax='proto3',
   extension_ranges=[],
   oneofs=[
   ],
-  serialized_start=2092,
-  serialized_end=2188,
+  serialized_start=2307,
+  serialized_end=2403,
 )
 
 _PROCESSBUNDLEDESCRIPTOR_PCOLLECTIONSENTRY = _descriptor.Descriptor(
   name='PcollectionsEntry',
   full_name='org.apache.beam.model.fn_execution.v1.ProcessBundleDescriptor.PcollectionsEntry',
   filename=None,
   file=DESCRIPTOR,
@@ -449,36 +529,36 @@
   fields=[
     _descriptor.FieldDescriptor(
       name='key', full_name='org.apache.beam.model.fn_execution.v1.ProcessBundleDescriptor.PcollectionsEntry.key', index=0,
       number=1, type=9, cpp_type=9, label=1,
       has_default_value=False, default_value=_b("").decode('utf-8'),
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='value', full_name='org.apache.beam.model.fn_execution.v1.ProcessBundleDescriptor.PcollectionsEntry.value', index=1,
       number=2, type=11, cpp_type=10, label=1,
       has_default_value=False, default_value=None,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
   ],
   extensions=[
   ],
   nested_types=[],
   enum_types=[
   ],
   options=_descriptor._ParseOptions(descriptor_pb2.MessageOptions(), _b('8\001')),
   is_extendable=False,
   syntax='proto3',
   extension_ranges=[],
   oneofs=[
   ],
-  serialized_start=2190,
-  serialized_end=2289,
+  serialized_start=2405,
+  serialized_end=2504,
 )
 
 _PROCESSBUNDLEDESCRIPTOR_WINDOWINGSTRATEGIESENTRY = _descriptor.Descriptor(
   name='WindowingStrategiesEntry',
   full_name='org.apache.beam.model.fn_execution.v1.ProcessBundleDescriptor.WindowingStrategiesEntry',
   filename=None,
   file=DESCRIPTOR,
@@ -486,36 +566,36 @@
   fields=[
     _descriptor.FieldDescriptor(
       name='key', full_name='org.apache.beam.model.fn_execution.v1.ProcessBundleDescriptor.WindowingStrategiesEntry.key', index=0,
       number=1, type=9, cpp_type=9, label=1,
       has_default_value=False, default_value=_b("").decode('utf-8'),
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='value', full_name='org.apache.beam.model.fn_execution.v1.ProcessBundleDescriptor.WindowingStrategiesEntry.value', index=1,
       number=2, type=11, cpp_type=10, label=1,
       has_default_value=False, default_value=None,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
   ],
   extensions=[
   ],
   nested_types=[],
   enum_types=[
   ],
   options=_descriptor._ParseOptions(descriptor_pb2.MessageOptions(), _b('8\001')),
   is_extendable=False,
   syntax='proto3',
   extension_ranges=[],
   oneofs=[
   ],
-  serialized_start=2291,
-  serialized_end=2403,
+  serialized_start=2506,
+  serialized_end=2618,
 )
 
 _PROCESSBUNDLEDESCRIPTOR_CODERSENTRY = _descriptor.Descriptor(
   name='CodersEntry',
   full_name='org.apache.beam.model.fn_execution.v1.ProcessBundleDescriptor.CodersEntry',
   filename=None,
   file=DESCRIPTOR,
@@ -523,36 +603,36 @@
   fields=[
     _descriptor.FieldDescriptor(
       name='key', full_name='org.apache.beam.model.fn_execution.v1.ProcessBundleDescriptor.CodersEntry.key', index=0,
       number=1, type=9, cpp_type=9, label=1,
       has_default_value=False, default_value=_b("").decode('utf-8'),
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='value', full_name='org.apache.beam.model.fn_execution.v1.ProcessBundleDescriptor.CodersEntry.value', index=1,
       number=2, type=11, cpp_type=10, label=1,
       has_default_value=False, default_value=None,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
   ],
   extensions=[
   ],
   nested_types=[],
   enum_types=[
   ],
   options=_descriptor._ParseOptions(descriptor_pb2.MessageOptions(), _b('8\001')),
   is_extendable=False,
   syntax='proto3',
   extension_ranges=[],
   oneofs=[
   ],
-  serialized_start=2405,
-  serialized_end=2492,
+  serialized_start=2620,
+  serialized_end=2707,
 )
 
 _PROCESSBUNDLEDESCRIPTOR_ENVIRONMENTSENTRY = _descriptor.Descriptor(
   name='EnvironmentsEntry',
   full_name='org.apache.beam.model.fn_execution.v1.ProcessBundleDescriptor.EnvironmentsEntry',
   filename=None,
   file=DESCRIPTOR,
@@ -560,36 +640,36 @@
   fields=[
     _descriptor.FieldDescriptor(
       name='key', full_name='org.apache.beam.model.fn_execution.v1.ProcessBundleDescriptor.EnvironmentsEntry.key', index=0,
       number=1, type=9, cpp_type=9, label=1,
       has_default_value=False, default_value=_b("").decode('utf-8'),
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='value', full_name='org.apache.beam.model.fn_execution.v1.ProcessBundleDescriptor.EnvironmentsEntry.value', index=1,
       number=2, type=11, cpp_type=10, label=1,
       has_default_value=False, default_value=None,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
   ],
   extensions=[
   ],
   nested_types=[],
   enum_types=[
   ],
   options=_descriptor._ParseOptions(descriptor_pb2.MessageOptions(), _b('8\001')),
   is_extendable=False,
   syntax='proto3',
   extension_ranges=[],
   oneofs=[
   ],
-  serialized_start=2494,
-  serialized_end=2593,
+  serialized_start=2709,
+  serialized_end=2808,
 )
 
 _PROCESSBUNDLEDESCRIPTOR = _descriptor.Descriptor(
   name='ProcessBundleDescriptor',
   full_name='org.apache.beam.model.fn_execution.v1.ProcessBundleDescriptor',
   filename=None,
   file=DESCRIPTOR,
@@ -597,241 +677,266 @@
   fields=[
     _descriptor.FieldDescriptor(
       name='id', full_name='org.apache.beam.model.fn_execution.v1.ProcessBundleDescriptor.id', index=0,
       number=1, type=9, cpp_type=9, label=1,
       has_default_value=False, default_value=_b("").decode('utf-8'),
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='transforms', full_name='org.apache.beam.model.fn_execution.v1.ProcessBundleDescriptor.transforms', index=1,
       number=2, type=11, cpp_type=10, label=3,
       has_default_value=False, default_value=[],
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='pcollections', full_name='org.apache.beam.model.fn_execution.v1.ProcessBundleDescriptor.pcollections', index=2,
       number=3, type=11, cpp_type=10, label=3,
       has_default_value=False, default_value=[],
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='windowing_strategies', full_name='org.apache.beam.model.fn_execution.v1.ProcessBundleDescriptor.windowing_strategies', index=3,
       number=4, type=11, cpp_type=10, label=3,
       has_default_value=False, default_value=[],
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='coders', full_name='org.apache.beam.model.fn_execution.v1.ProcessBundleDescriptor.coders', index=4,
       number=5, type=11, cpp_type=10, label=3,
       has_default_value=False, default_value=[],
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='environments', full_name='org.apache.beam.model.fn_execution.v1.ProcessBundleDescriptor.environments', index=5,
       number=6, type=11, cpp_type=10, label=3,
       has_default_value=False, default_value=[],
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='state_api_service_descriptor', full_name='org.apache.beam.model.fn_execution.v1.ProcessBundleDescriptor.state_api_service_descriptor', index=6,
       number=7, type=11, cpp_type=10, label=1,
       has_default_value=False, default_value=None,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
   ],
   extensions=[
   ],
   nested_types=[_PROCESSBUNDLEDESCRIPTOR_TRANSFORMSENTRY, _PROCESSBUNDLEDESCRIPTOR_PCOLLECTIONSENTRY, _PROCESSBUNDLEDESCRIPTOR_WINDOWINGSTRATEGIESENTRY, _PROCESSBUNDLEDESCRIPTOR_CODERSENTRY, _PROCESSBUNDLEDESCRIPTOR_ENVIRONMENTSENTRY, ],
   enum_types=[
   ],
   options=None,
   is_extendable=False,
   syntax='proto3',
   extension_ranges=[],
   oneofs=[
   ],
-  serialized_start=1439,
-  serialized_end=2593,
+  serialized_start=1654,
+  serialized_end=2808,
 )
 
 
-_BUNDLESPLIT_APPLICATION_OUTPUTWATERMARKSENTRY = _descriptor.Descriptor(
+_BUNDLEAPPLICATION_OUTPUTWATERMARKSENTRY = _descriptor.Descriptor(
   name='OutputWatermarksEntry',
-  full_name='org.apache.beam.model.fn_execution.v1.BundleSplit.Application.OutputWatermarksEntry',
+  full_name='org.apache.beam.model.fn_execution.v1.BundleApplication.OutputWatermarksEntry',
   filename=None,
   file=DESCRIPTOR,
   containing_type=None,
   fields=[
     _descriptor.FieldDescriptor(
-      name='key', full_name='org.apache.beam.model.fn_execution.v1.BundleSplit.Application.OutputWatermarksEntry.key', index=0,
+      name='key', full_name='org.apache.beam.model.fn_execution.v1.BundleApplication.OutputWatermarksEntry.key', index=0,
       number=1, type=9, cpp_type=9, label=1,
       has_default_value=False, default_value=_b("").decode('utf-8'),
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
-      name='value', full_name='org.apache.beam.model.fn_execution.v1.BundleSplit.Application.OutputWatermarksEntry.value', index=1,
-      number=2, type=3, cpp_type=2, label=1,
-      has_default_value=False, default_value=0,
+      name='value', full_name='org.apache.beam.model.fn_execution.v1.BundleApplication.OutputWatermarksEntry.value', index=1,
+      number=2, type=11, cpp_type=10, label=1,
+      has_default_value=False, default_value=None,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
   ],
   extensions=[
   ],
   nested_types=[],
   enum_types=[
   ],
   options=_descriptor._ParseOptions(descriptor_pb2.MessageOptions(), _b('8\001')),
   is_extendable=False,
   syntax='proto3',
   extension_ranges=[],
   oneofs=[
   ],
-  serialized_start=3036,
-  serialized_end=3091,
+  serialized_start=3232,
+  serialized_end=3315,
 )
 
-_BUNDLESPLIT_APPLICATION = _descriptor.Descriptor(
-  name='Application',
-  full_name='org.apache.beam.model.fn_execution.v1.BundleSplit.Application',
+_BUNDLEAPPLICATION_BACKLOG = _descriptor.Descriptor(
+  name='Backlog',
+  full_name='org.apache.beam.model.fn_execution.v1.BundleApplication.Backlog',
   filename=None,
   file=DESCRIPTOR,
   containing_type=None,
   fields=[
     _descriptor.FieldDescriptor(
-      name='ptransform_id', full_name='org.apache.beam.model.fn_execution.v1.BundleSplit.Application.ptransform_id', index=0,
-      number=1, type=9, cpp_type=9, label=1,
-      has_default_value=False, default_value=_b("").decode('utf-8'),
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='input_id', full_name='org.apache.beam.model.fn_execution.v1.BundleSplit.Application.input_id', index=1,
-      number=2, type=9, cpp_type=9, label=1,
-      has_default_value=False, default_value=_b("").decode('utf-8'),
-      message_type=None, enum_type=None, containing_type=None,
-      is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
-    _descriptor.FieldDescriptor(
-      name='element', full_name='org.apache.beam.model.fn_execution.v1.BundleSplit.Application.element', index=2,
-      number=3, type=12, cpp_type=9, label=1,
+      name='partition', full_name='org.apache.beam.model.fn_execution.v1.BundleApplication.Backlog.partition', index=0,
+      number=1, type=12, cpp_type=9, label=1,
       has_default_value=False, default_value=_b(""),
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
-      name='output_watermarks', full_name='org.apache.beam.model.fn_execution.v1.BundleSplit.Application.output_watermarks', index=3,
-      number=4, type=11, cpp_type=10, label=3,
-      has_default_value=False, default_value=[],
+      name='bytes', full_name='org.apache.beam.model.fn_execution.v1.BundleApplication.Backlog.bytes', index=1,
+      number=1000, type=12, cpp_type=9, label=1,
+      has_default_value=False, default_value=_b(""),
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
-      name='fraction_of_work', full_name='org.apache.beam.model.fn_execution.v1.BundleSplit.Application.fraction_of_work', index=4,
-      number=5, type=11, cpp_type=10, label=1,
-      has_default_value=False, default_value=None,
+      name='is_unknown', full_name='org.apache.beam.model.fn_execution.v1.BundleApplication.Backlog.is_unknown', index=2,
+      number=1001, type=8, cpp_type=7, label=1,
+      has_default_value=False, default_value=False,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
   ],
   extensions=[
   ],
-  nested_types=[_BUNDLESPLIT_APPLICATION_OUTPUTWATERMARKSENTRY, ],
+  nested_types=[],
   enum_types=[
   ],
   options=None,
   is_extendable=False,
   syntax='proto3',
   extension_ranges=[],
   oneofs=[
+    _descriptor.OneofDescriptor(
+      name='value', full_name='org.apache.beam.model.fn_execution.v1.BundleApplication.Backlog.value',
+      index=0, containing_type=None, fields=[]),
   ],
-  serialized_start=2794,
-  serialized_end=3091,
+  serialized_start=3317,
+  serialized_end=3395,
 )
 
-_BUNDLESPLIT_DELAYEDAPPLICATION = _descriptor.Descriptor(
-  name='DelayedApplication',
-  full_name='org.apache.beam.model.fn_execution.v1.BundleSplit.DelayedApplication',
+_BUNDLEAPPLICATION = _descriptor.Descriptor(
+  name='BundleApplication',
+  full_name='org.apache.beam.model.fn_execution.v1.BundleApplication',
   filename=None,
   file=DESCRIPTOR,
   containing_type=None,
   fields=[
     _descriptor.FieldDescriptor(
-      name='delay_sec', full_name='org.apache.beam.model.fn_execution.v1.BundleSplit.DelayedApplication.delay_sec', index=0,
-      number=1, type=1, cpp_type=5, label=1,
-      has_default_value=False, default_value=float(0),
+      name='ptransform_id', full_name='org.apache.beam.model.fn_execution.v1.BundleApplication.ptransform_id', index=0,
+      number=1, type=9, cpp_type=9, label=1,
+      has_default_value=False, default_value=_b("").decode('utf-8'),
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
-      name='application', full_name='org.apache.beam.model.fn_execution.v1.BundleSplit.DelayedApplication.application', index=1,
-      number=2, type=11, cpp_type=10, label=1,
+      name='input_id', full_name='org.apache.beam.model.fn_execution.v1.BundleApplication.input_id', index=1,
+      number=2, type=9, cpp_type=9, label=1,
+      has_default_value=False, default_value=_b("").decode('utf-8'),
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      options=None),
+    _descriptor.FieldDescriptor(
+      name='element', full_name='org.apache.beam.model.fn_execution.v1.BundleApplication.element', index=2,
+      number=3, type=12, cpp_type=9, label=1,
+      has_default_value=False, default_value=_b(""),
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      options=None),
+    _descriptor.FieldDescriptor(
+      name='output_watermarks', full_name='org.apache.beam.model.fn_execution.v1.BundleApplication.output_watermarks', index=3,
+      number=4, type=11, cpp_type=10, label=3,
+      has_default_value=False, default_value=[],
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      options=None),
+    _descriptor.FieldDescriptor(
+      name='backlog', full_name='org.apache.beam.model.fn_execution.v1.BundleApplication.backlog', index=4,
+      number=5, type=11, cpp_type=10, label=1,
       has_default_value=False, default_value=None,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
+    _descriptor.FieldDescriptor(
+      name='is_bounded', full_name='org.apache.beam.model.fn_execution.v1.BundleApplication.is_bounded', index=5,
+      number=6, type=14, cpp_type=8, label=1,
+      has_default_value=False, default_value=0,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      options=None),
+    _descriptor.FieldDescriptor(
+      name='monitoring_infos', full_name='org.apache.beam.model.fn_execution.v1.BundleApplication.monitoring_infos', index=6,
+      number=7, type=11, cpp_type=10, label=3,
+      has_default_value=False, default_value=[],
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      options=None),
   ],
   extensions=[
   ],
-  nested_types=[],
+  nested_types=[_BUNDLEAPPLICATION_OUTPUTWATERMARKSENTRY, _BUNDLEAPPLICATION_BACKLOG, ],
   enum_types=[
   ],
   options=None,
   is_extendable=False,
   syntax='proto3',
   extension_ranges=[],
   oneofs=[
   ],
-  serialized_start=3093,
-  serialized_end=3217,
+  serialized_start=2811,
+  serialized_end=3395,
 )
 
-_BUNDLESPLIT = _descriptor.Descriptor(
-  name='BundleSplit',
-  full_name='org.apache.beam.model.fn_execution.v1.BundleSplit',
+
+_DELAYEDBUNDLEAPPLICATION = _descriptor.Descriptor(
+  name='DelayedBundleApplication',
+  full_name='org.apache.beam.model.fn_execution.v1.DelayedBundleApplication',
   filename=None,
   file=DESCRIPTOR,
   containing_type=None,
   fields=[
     _descriptor.FieldDescriptor(
-      name='primary_roots', full_name='org.apache.beam.model.fn_execution.v1.BundleSplit.primary_roots', index=0,
-      number=1, type=11, cpp_type=10, label=3,
-      has_default_value=False, default_value=[],
+      name='requested_execution_time', full_name='org.apache.beam.model.fn_execution.v1.DelayedBundleApplication.requested_execution_time', index=0,
+      number=1, type=11, cpp_type=10, label=1,
+      has_default_value=False, default_value=None,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
-      name='residual_roots', full_name='org.apache.beam.model.fn_execution.v1.BundleSplit.residual_roots', index=1,
-      number=2, type=11, cpp_type=10, label=3,
-      has_default_value=False, default_value=[],
+      name='application', full_name='org.apache.beam.model.fn_execution.v1.DelayedBundleApplication.application', index=1,
+      number=2, type=11, cpp_type=10, label=1,
+      has_default_value=False, default_value=None,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
   ],
   extensions=[
   ],
-  nested_types=[_BUNDLESPLIT_APPLICATION, _BUNDLESPLIT_DELAYEDAPPLICATION, ],
+  nested_types=[],
   enum_types=[
   ],
   options=None,
   is_extendable=False,
   syntax='proto3',
   extension_ranges=[],
   oneofs=[
   ],
-  serialized_start=2596,
-  serialized_end=3217,
+  serialized_start=3398,
+  serialized_end=3565,
 )
 
 
 _PROCESSBUNDLEREQUEST = _descriptor.Descriptor(
   name='ProcessBundleRequest',
   full_name='org.apache.beam.model.fn_execution.v1.ProcessBundleRequest',
   filename=None,
@@ -840,36 +945,36 @@
   fields=[
     _descriptor.FieldDescriptor(
       name='process_bundle_descriptor_reference', full_name='org.apache.beam.model.fn_execution.v1.ProcessBundleRequest.process_bundle_descriptor_reference', index=0,
       number=1, type=9, cpp_type=9, label=1,
       has_default_value=False, default_value=_b("").decode('utf-8'),
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='cache_tokens', full_name='org.apache.beam.model.fn_execution.v1.ProcessBundleRequest.cache_tokens', index=1,
       number=2, type=12, cpp_type=9, label=3,
       has_default_value=False, default_value=[],
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
   ],
   extensions=[
   ],
   nested_types=[],
   enum_types=[
   ],
   options=None,
   is_extendable=False,
   syntax='proto3',
   extension_ranges=[],
   oneofs=[
   ],
-  serialized_start=3219,
-  serialized_end=3308,
+  serialized_start=3567,
+  serialized_end=3656,
 )
 
 
 _PROCESSBUNDLERESPONSE = _descriptor.Descriptor(
   name='ProcessBundleResponse',
   full_name='org.apache.beam.model.fn_execution.v1.ProcessBundleResponse',
   filename=None,
@@ -878,36 +983,50 @@
   fields=[
     _descriptor.FieldDescriptor(
       name='metrics', full_name='org.apache.beam.model.fn_execution.v1.ProcessBundleResponse.metrics', index=0,
       number=1, type=11, cpp_type=10, label=1,
       has_default_value=False, default_value=None,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
-      name='split', full_name='org.apache.beam.model.fn_execution.v1.ProcessBundleResponse.split', index=1,
-      number=2, type=11, cpp_type=10, label=1,
-      has_default_value=False, default_value=None,
+      name='residual_roots', full_name='org.apache.beam.model.fn_execution.v1.ProcessBundleResponse.residual_roots', index=1,
+      number=2, type=11, cpp_type=10, label=3,
+      has_default_value=False, default_value=[],
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      options=None),
+    _descriptor.FieldDescriptor(
+      name='monitoring_infos', full_name='org.apache.beam.model.fn_execution.v1.ProcessBundleResponse.monitoring_infos', index=2,
+      number=3, type=11, cpp_type=10, label=3,
+      has_default_value=False, default_value=[],
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
+    _descriptor.FieldDescriptor(
+      name='requires_finalization', full_name='org.apache.beam.model.fn_execution.v1.ProcessBundleResponse.requires_finalization', index=3,
+      number=4, type=8, cpp_type=7, label=1,
+      has_default_value=False, default_value=False,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      options=None),
   ],
   extensions=[
   ],
   nested_types=[],
   enum_types=[
   ],
   options=None,
   is_extendable=False,
   syntax='proto3',
   extension_ranges=[],
   oneofs=[
   ],
-  serialized_start=3311,
-  serialized_end=3466,
+  serialized_start=3659,
+  serialized_end=3948,
 )
 
 
 _PROCESSBUNDLEPROGRESSREQUEST = _descriptor.Descriptor(
   name='ProcessBundleProgressRequest',
   full_name='org.apache.beam.model.fn_execution.v1.ProcessBundleProgressRequest',
   filename=None,
@@ -916,29 +1035,29 @@
   fields=[
     _descriptor.FieldDescriptor(
       name='instruction_reference', full_name='org.apache.beam.model.fn_execution.v1.ProcessBundleProgressRequest.instruction_reference', index=0,
       number=1, type=9, cpp_type=9, label=1,
       has_default_value=False, default_value=_b("").decode('utf-8'),
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
   ],
   extensions=[
   ],
   nested_types=[],
   enum_types=[
   ],
   options=None,
   is_extendable=False,
   syntax='proto3',
   extension_ranges=[],
   oneofs=[
   ],
-  serialized_start=3468,
-  serialized_end=3529,
+  serialized_start=3950,
+  serialized_end=4011,
 )
 
 
 _MONITORINGINFO_LABELSENTRY = _descriptor.Descriptor(
   name='LabelsEntry',
   full_name='org.apache.beam.model.fn_execution.v1.MonitoringInfo.LabelsEntry',
   filename=None,
@@ -947,36 +1066,36 @@
   fields=[
     _descriptor.FieldDescriptor(
       name='key', full_name='org.apache.beam.model.fn_execution.v1.MonitoringInfo.LabelsEntry.key', index=0,
       number=1, type=9, cpp_type=9, label=1,
       has_default_value=False, default_value=_b("").decode('utf-8'),
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='value', full_name='org.apache.beam.model.fn_execution.v1.MonitoringInfo.LabelsEntry.value', index=1,
       number=2, type=9, cpp_type=9, label=1,
       has_default_value=False, default_value=_b("").decode('utf-8'),
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
   ],
   extensions=[
   ],
   nested_types=[],
   enum_types=[
   ],
   options=_descriptor._ParseOptions(descriptor_pb2.MessageOptions(), _b('8\001')),
   is_extendable=False,
   syntax='proto3',
   extension_ranges=[],
   oneofs=[
   ],
-  serialized_start=3818,
-  serialized_end=3863,
+  serialized_start=4347,
+  serialized_end=4392,
 )
 
 _MONITORINGINFO = _descriptor.Descriptor(
   name='MonitoringInfo',
   full_name='org.apache.beam.model.fn_execution.v1.MonitoringInfo',
   filename=None,
   file=DESCRIPTOR,
@@ -984,43 +1103,50 @@
   fields=[
     _descriptor.FieldDescriptor(
       name='urn', full_name='org.apache.beam.model.fn_execution.v1.MonitoringInfo.urn', index=0,
       number=1, type=9, cpp_type=9, label=1,
       has_default_value=False, default_value=_b("").decode('utf-8'),
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='type', full_name='org.apache.beam.model.fn_execution.v1.MonitoringInfo.type', index=1,
       number=2, type=9, cpp_type=9, label=1,
       has_default_value=False, default_value=_b("").decode('utf-8'),
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='monitoring_table_data', full_name='org.apache.beam.model.fn_execution.v1.MonitoringInfo.monitoring_table_data', index=2,
       number=3, type=11, cpp_type=10, label=1,
       has_default_value=False, default_value=None,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='metric', full_name='org.apache.beam.model.fn_execution.v1.MonitoringInfo.metric', index=3,
       number=4, type=11, cpp_type=10, label=1,
       has_default_value=False, default_value=None,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='labels', full_name='org.apache.beam.model.fn_execution.v1.MonitoringInfo.labels', index=4,
       number=5, type=11, cpp_type=10, label=3,
       has_default_value=False, default_value=[],
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
+    _descriptor.FieldDescriptor(
+      name='timestamp', full_name='org.apache.beam.model.fn_execution.v1.MonitoringInfo.timestamp', index=5,
+      number=6, type=11, cpp_type=10, label=1,
+      has_default_value=False, default_value=None,
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      options=None),
   ],
   extensions=[
   ],
   nested_types=[_MONITORINGINFO_LABELSENTRY, ],
   enum_types=[
     _MONITORINGINFO_MONITORINGINFOLABELS,
   ],
@@ -1029,16 +1155,66 @@
   syntax='proto3',
   extension_ranges=[],
   oneofs=[
     _descriptor.OneofDescriptor(
       name='data', full_name='org.apache.beam.model.fn_execution.v1.MonitoringInfo.data',
       index=0, containing_type=None, fields=[]),
   ],
-  serialized_start=3532,
-  serialized_end=3979,
+  serialized_start=4014,
+  serialized_end=4508,
+)
+
+
+_MONITORINGINFOURNS = _descriptor.Descriptor(
+  name='MonitoringInfoUrns',
+  full_name='org.apache.beam.model.fn_execution.v1.MonitoringInfoUrns',
+  filename=None,
+  file=DESCRIPTOR,
+  containing_type=None,
+  fields=[
+  ],
+  extensions=[
+  ],
+  nested_types=[],
+  enum_types=[
+    _MONITORINGINFOURNS_ENUM,
+  ],
+  options=None,
+  is_extendable=False,
+  syntax='proto3',
+  extension_ranges=[],
+  oneofs=[
+  ],
+  serialized_start=4511,
+  serialized_end=4989,
+)
+
+
+_MONITORINGINFOTYPEURNS = _descriptor.Descriptor(
+  name='MonitoringInfoTypeUrns',
+  full_name='org.apache.beam.model.fn_execution.v1.MonitoringInfoTypeUrns',
+  filename=None,
+  file=DESCRIPTOR,
+  containing_type=None,
+  fields=[
+  ],
+  extensions=[
+  ],
+  nested_types=[],
+  enum_types=[
+    _MONITORINGINFOTYPEURNS_ENUM,
+  ],
+  options=None,
+  is_extendable=False,
+  syntax='proto3',
+  extension_ranges=[],
+  oneofs=[
+  ],
+  serialized_start=4992,
+  serialized_end=5202,
 )
 
 
 _METRIC = _descriptor.Descriptor(
   name='Metric',
   full_name='org.apache.beam.model.fn_execution.v1.Metric',
   filename=None,
@@ -1047,29 +1223,29 @@
   fields=[
     _descriptor.FieldDescriptor(
       name='counter_data', full_name='org.apache.beam.model.fn_execution.v1.Metric.counter_data', index=0,
       number=1, type=11, cpp_type=10, label=1,
       has_default_value=False, default_value=None,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='distribution_data', full_name='org.apache.beam.model.fn_execution.v1.Metric.distribution_data', index=1,
       number=2, type=11, cpp_type=10, label=1,
       has_default_value=False, default_value=None,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='extrema_data', full_name='org.apache.beam.model.fn_execution.v1.Metric.extrema_data', index=2,
       number=3, type=11, cpp_type=10, label=1,
       has_default_value=False, default_value=None,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
   ],
   extensions=[
   ],
   nested_types=[],
   enum_types=[
   ],
   options=None,
@@ -1077,16 +1253,16 @@
   syntax='proto3',
   extension_ranges=[],
   oneofs=[
     _descriptor.OneofDescriptor(
       name='data', full_name='org.apache.beam.model.fn_execution.v1.Metric.data',
       index=0, containing_type=None, fields=[]),
   ],
-  serialized_start=3982,
-  serialized_end=4236,
+  serialized_start=5205,
+  serialized_end=5459,
 )
 
 
 _COUNTERDATA = _descriptor.Descriptor(
   name='CounterData',
   full_name='org.apache.beam.model.fn_execution.v1.CounterData',
   filename=None,
@@ -1095,29 +1271,29 @@
   fields=[
     _descriptor.FieldDescriptor(
       name='int64_value', full_name='org.apache.beam.model.fn_execution.v1.CounterData.int64_value', index=0,
       number=1, type=3, cpp_type=2, label=1,
       has_default_value=False, default_value=0,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='double_value', full_name='org.apache.beam.model.fn_execution.v1.CounterData.double_value', index=1,
       number=2, type=1, cpp_type=5, label=1,
       has_default_value=False, default_value=float(0),
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='string_value', full_name='org.apache.beam.model.fn_execution.v1.CounterData.string_value', index=2,
       number=3, type=9, cpp_type=9, label=1,
       has_default_value=False, default_value=_b("").decode('utf-8'),
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
   ],
   extensions=[
   ],
   nested_types=[],
   enum_types=[
   ],
   options=None,
@@ -1125,16 +1301,16 @@
   syntax='proto3',
   extension_ranges=[],
   oneofs=[
     _descriptor.OneofDescriptor(
       name='value', full_name='org.apache.beam.model.fn_execution.v1.CounterData.value',
       index=0, containing_type=None, fields=[]),
   ],
-  serialized_start=4238,
-  serialized_end=4331,
+  serialized_start=5461,
+  serialized_end=5554,
 )
 
 
 _EXTREMADATA = _descriptor.Descriptor(
   name='ExtremaData',
   full_name='org.apache.beam.model.fn_execution.v1.ExtremaData',
   filename=None,
@@ -1143,22 +1319,22 @@
   fields=[
     _descriptor.FieldDescriptor(
       name='int_extrema_data', full_name='org.apache.beam.model.fn_execution.v1.ExtremaData.int_extrema_data', index=0,
       number=1, type=11, cpp_type=10, label=1,
       has_default_value=False, default_value=None,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='double_extrema_data', full_name='org.apache.beam.model.fn_execution.v1.ExtremaData.double_extrema_data', index=1,
       number=2, type=11, cpp_type=10, label=1,
       has_default_value=False, default_value=None,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
   ],
   extensions=[
   ],
   nested_types=[],
   enum_types=[
   ],
   options=None,
@@ -1166,16 +1342,16 @@
   syntax='proto3',
   extension_ranges=[],
   oneofs=[
     _descriptor.OneofDescriptor(
       name='extrema', full_name='org.apache.beam.model.fn_execution.v1.ExtremaData.extrema',
       index=0, containing_type=None, fields=[]),
   ],
-  serialized_start=4334,
-  serialized_end=4530,
+  serialized_start=5557,
+  serialized_end=5753,
 )
 
 
 _INTEXTREMADATA = _descriptor.Descriptor(
   name='IntExtremaData',
   full_name='org.apache.beam.model.fn_execution.v1.IntExtremaData',
   filename=None,
@@ -1184,29 +1360,29 @@
   fields=[
     _descriptor.FieldDescriptor(
       name='int_values', full_name='org.apache.beam.model.fn_execution.v1.IntExtremaData.int_values', index=0,
       number=1, type=3, cpp_type=2, label=3,
       has_default_value=False, default_value=[],
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
   ],
   extensions=[
   ],
   nested_types=[],
   enum_types=[
   ],
   options=None,
   is_extendable=False,
   syntax='proto3',
   extension_ranges=[],
   oneofs=[
   ],
-  serialized_start=4532,
-  serialized_end=4568,
+  serialized_start=5755,
+  serialized_end=5791,
 )
 
 
 _DOUBLEEXTREMADATA = _descriptor.Descriptor(
   name='DoubleExtremaData',
   full_name='org.apache.beam.model.fn_execution.v1.DoubleExtremaData',
   filename=None,
@@ -1215,29 +1391,29 @@
   fields=[
     _descriptor.FieldDescriptor(
       name='double_values', full_name='org.apache.beam.model.fn_execution.v1.DoubleExtremaData.double_values', index=0,
       number=2, type=1, cpp_type=5, label=3,
       has_default_value=False, default_value=[],
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
   ],
   extensions=[
   ],
   nested_types=[],
   enum_types=[
   ],
   options=None,
   is_extendable=False,
   syntax='proto3',
   extension_ranges=[],
   oneofs=[
   ],
-  serialized_start=4570,
-  serialized_end=4612,
+  serialized_start=5793,
+  serialized_end=5835,
 )
 
 
 _DISTRIBUTIONDATA = _descriptor.Descriptor(
   name='DistributionData',
   full_name='org.apache.beam.model.fn_execution.v1.DistributionData',
   filename=None,
@@ -1246,22 +1422,22 @@
   fields=[
     _descriptor.FieldDescriptor(
       name='int_distribution_data', full_name='org.apache.beam.model.fn_execution.v1.DistributionData.int_distribution_data', index=0,
       number=1, type=11, cpp_type=10, label=1,
       has_default_value=False, default_value=None,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='double_distribution_data', full_name='org.apache.beam.model.fn_execution.v1.DistributionData.double_distribution_data', index=1,
       number=2, type=11, cpp_type=10, label=1,
       has_default_value=False, default_value=None,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
   ],
   extensions=[
   ],
   nested_types=[],
   enum_types=[
   ],
   options=None,
@@ -1269,16 +1445,16 @@
   syntax='proto3',
   extension_ranges=[],
   oneofs=[
     _descriptor.OneofDescriptor(
       name='distribution', full_name='org.apache.beam.model.fn_execution.v1.DistributionData.distribution',
       index=0, containing_type=None, fields=[]),
   ],
-  serialized_start=4615,
-  serialized_end=4841,
+  serialized_start=5838,
+  serialized_end=6064,
 )
 
 
 _INTDISTRIBUTIONDATA = _descriptor.Descriptor(
   name='IntDistributionData',
   full_name='org.apache.beam.model.fn_execution.v1.IntDistributionData',
   filename=None,
@@ -1287,50 +1463,50 @@
   fields=[
     _descriptor.FieldDescriptor(
       name='count', full_name='org.apache.beam.model.fn_execution.v1.IntDistributionData.count', index=0,
       number=1, type=3, cpp_type=2, label=1,
       has_default_value=False, default_value=0,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='sum', full_name='org.apache.beam.model.fn_execution.v1.IntDistributionData.sum', index=1,
       number=2, type=3, cpp_type=2, label=1,
       has_default_value=False, default_value=0,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='min', full_name='org.apache.beam.model.fn_execution.v1.IntDistributionData.min', index=2,
       number=3, type=3, cpp_type=2, label=1,
       has_default_value=False, default_value=0,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='max', full_name='org.apache.beam.model.fn_execution.v1.IntDistributionData.max', index=3,
       number=4, type=3, cpp_type=2, label=1,
       has_default_value=False, default_value=0,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
   ],
   extensions=[
   ],
   nested_types=[],
   enum_types=[
   ],
   options=None,
   is_extendable=False,
   syntax='proto3',
   extension_ranges=[],
   oneofs=[
   ],
-  serialized_start=4843,
-  serialized_end=4918,
+  serialized_start=6066,
+  serialized_end=6141,
 )
 
 
 _DOUBLEDISTRIBUTIONDATA = _descriptor.Descriptor(
   name='DoubleDistributionData',
   full_name='org.apache.beam.model.fn_execution.v1.DoubleDistributionData',
   filename=None,
@@ -1339,50 +1515,50 @@
   fields=[
     _descriptor.FieldDescriptor(
       name='count', full_name='org.apache.beam.model.fn_execution.v1.DoubleDistributionData.count', index=0,
       number=1, type=3, cpp_type=2, label=1,
       has_default_value=False, default_value=0,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='sum', full_name='org.apache.beam.model.fn_execution.v1.DoubleDistributionData.sum', index=1,
       number=2, type=1, cpp_type=5, label=1,
       has_default_value=False, default_value=float(0),
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='min', full_name='org.apache.beam.model.fn_execution.v1.DoubleDistributionData.min', index=2,
       number=3, type=1, cpp_type=5, label=1,
       has_default_value=False, default_value=float(0),
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='max', full_name='org.apache.beam.model.fn_execution.v1.DoubleDistributionData.max', index=3,
       number=4, type=1, cpp_type=5, label=1,
       has_default_value=False, default_value=float(0),
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
   ],
   extensions=[
   ],
   nested_types=[],
   enum_types=[
   ],
   options=None,
   is_extendable=False,
   syntax='proto3',
   extension_ranges=[],
   oneofs=[
   ],
-  serialized_start=4920,
-  serialized_end=4998,
+  serialized_start=6143,
+  serialized_end=6221,
 )
 
 
 _MONITORINGTABLEDATA_MONITORINGCOLUMNVALUE = _descriptor.Descriptor(
   name='MonitoringColumnValue',
   full_name='org.apache.beam.model.fn_execution.v1.MonitoringTableData.MonitoringColumnValue',
   filename=None,
@@ -1391,36 +1567,36 @@
   fields=[
     _descriptor.FieldDescriptor(
       name='int64_value', full_name='org.apache.beam.model.fn_execution.v1.MonitoringTableData.MonitoringColumnValue.int64_value', index=0,
       number=1, type=3, cpp_type=2, label=1,
       has_default_value=False, default_value=0,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='double_value', full_name='org.apache.beam.model.fn_execution.v1.MonitoringTableData.MonitoringColumnValue.double_value', index=1,
       number=2, type=1, cpp_type=5, label=1,
       has_default_value=False, default_value=float(0),
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='string_value', full_name='org.apache.beam.model.fn_execution.v1.MonitoringTableData.MonitoringColumnValue.string_value', index=2,
       number=3, type=9, cpp_type=9, label=1,
       has_default_value=False, default_value=_b("").decode('utf-8'),
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='timestamp', full_name='org.apache.beam.model.fn_execution.v1.MonitoringTableData.MonitoringColumnValue.timestamp', index=3,
       number=4, type=11, cpp_type=10, label=1,
       has_default_value=False, default_value=None,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
   ],
   extensions=[
   ],
   nested_types=[],
   enum_types=[
   ],
   options=None,
@@ -1428,16 +1604,16 @@
   syntax='proto3',
   extension_ranges=[],
   oneofs=[
     _descriptor.OneofDescriptor(
       name='value', full_name='org.apache.beam.model.fn_execution.v1.MonitoringTableData.MonitoringColumnValue.value',
       index=0, containing_type=None, fields=[]),
   ],
-  serialized_start=5139,
-  serialized_end=5291,
+  serialized_start=6362,
+  serialized_end=6514,
 )
 
 _MONITORINGTABLEDATA_MONITORINGROW = _descriptor.Descriptor(
   name='MonitoringRow',
   full_name='org.apache.beam.model.fn_execution.v1.MonitoringTableData.MonitoringRow',
   filename=None,
   file=DESCRIPTOR,
@@ -1445,29 +1621,29 @@
   fields=[
     _descriptor.FieldDescriptor(
       name='values', full_name='org.apache.beam.model.fn_execution.v1.MonitoringTableData.MonitoringRow.values', index=0,
       number=1, type=11, cpp_type=10, label=3,
       has_default_value=False, default_value=[],
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
   ],
   extensions=[
   ],
   nested_types=[],
   enum_types=[
   ],
   options=None,
   is_extendable=False,
   syntax='proto3',
   extension_ranges=[],
   oneofs=[
   ],
-  serialized_start=5293,
-  serialized_end=5406,
+  serialized_start=6516,
+  serialized_end=6629,
 )
 
 _MONITORINGTABLEDATA = _descriptor.Descriptor(
   name='MonitoringTableData',
   full_name='org.apache.beam.model.fn_execution.v1.MonitoringTableData',
   filename=None,
   file=DESCRIPTOR,
@@ -1475,36 +1651,36 @@
   fields=[
     _descriptor.FieldDescriptor(
       name='column_names', full_name='org.apache.beam.model.fn_execution.v1.MonitoringTableData.column_names', index=0,
       number=1, type=9, cpp_type=9, label=3,
       has_default_value=False, default_value=[],
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='row_data', full_name='org.apache.beam.model.fn_execution.v1.MonitoringTableData.row_data', index=1,
       number=2, type=11, cpp_type=10, label=3,
       has_default_value=False, default_value=[],
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
   ],
   extensions=[
   ],
   nested_types=[_MONITORINGTABLEDATA_MONITORINGCOLUMNVALUE, _MONITORINGTABLEDATA_MONITORINGROW, ],
   enum_types=[
   ],
   options=None,
   is_extendable=False,
   syntax='proto3',
   extension_ranges=[],
   oneofs=[
   ],
-  serialized_start=5001,
-  serialized_end=5406,
+  serialized_start=6224,
+  serialized_end=6629,
 )
 
 
 _METRICS_PTRANSFORM_MEASURED_INPUTELEMENTCOUNTSENTRY = _descriptor.Descriptor(
   name='InputElementCountsEntry',
   full_name='org.apache.beam.model.fn_execution.v1.Metrics.PTransform.Measured.InputElementCountsEntry',
   filename=None,
@@ -1513,36 +1689,36 @@
   fields=[
     _descriptor.FieldDescriptor(
       name='key', full_name='org.apache.beam.model.fn_execution.v1.Metrics.PTransform.Measured.InputElementCountsEntry.key', index=0,
       number=1, type=9, cpp_type=9, label=1,
       has_default_value=False, default_value=_b("").decode('utf-8'),
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='value', full_name='org.apache.beam.model.fn_execution.v1.Metrics.PTransform.Measured.InputElementCountsEntry.value', index=1,
       number=2, type=3, cpp_type=2, label=1,
       has_default_value=False, default_value=0,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
   ],
   extensions=[
   ],
   nested_types=[],
   enum_types=[
   ],
   options=_descriptor._ParseOptions(descriptor_pb2.MessageOptions(), _b('8\001')),
   is_extendable=False,
   syntax='proto3',
   extension_ranges=[],
   oneofs=[
   ],
-  serialized_start=6172,
-  serialized_end=6229,
+  serialized_start=7395,
+  serialized_end=7452,
 )
 
 _METRICS_PTRANSFORM_MEASURED_OUTPUTELEMENTCOUNTSENTRY = _descriptor.Descriptor(
   name='OutputElementCountsEntry',
   full_name='org.apache.beam.model.fn_execution.v1.Metrics.PTransform.Measured.OutputElementCountsEntry',
   filename=None,
   file=DESCRIPTOR,
@@ -1550,36 +1726,36 @@
   fields=[
     _descriptor.FieldDescriptor(
       name='key', full_name='org.apache.beam.model.fn_execution.v1.Metrics.PTransform.Measured.OutputElementCountsEntry.key', index=0,
       number=1, type=9, cpp_type=9, label=1,
       has_default_value=False, default_value=_b("").decode('utf-8'),
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='value', full_name='org.apache.beam.model.fn_execution.v1.Metrics.PTransform.Measured.OutputElementCountsEntry.value', index=1,
       number=2, type=3, cpp_type=2, label=1,
       has_default_value=False, default_value=0,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
   ],
   extensions=[
   ],
   nested_types=[],
   enum_types=[
   ],
   options=_descriptor._ParseOptions(descriptor_pb2.MessageOptions(), _b('8\001')),
   is_extendable=False,
   syntax='proto3',
   extension_ranges=[],
   oneofs=[
   ],
-  serialized_start=6231,
-  serialized_end=6289,
+  serialized_start=7454,
+  serialized_end=7512,
 )
 
 _METRICS_PTRANSFORM_MEASURED = _descriptor.Descriptor(
   name='Measured',
   full_name='org.apache.beam.model.fn_execution.v1.Metrics.PTransform.Measured',
   filename=None,
   file=DESCRIPTOR,
@@ -1587,43 +1763,43 @@
   fields=[
     _descriptor.FieldDescriptor(
       name='input_element_counts', full_name='org.apache.beam.model.fn_execution.v1.Metrics.PTransform.Measured.input_element_counts', index=0,
       number=1, type=11, cpp_type=10, label=3,
       has_default_value=False, default_value=[],
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='output_element_counts', full_name='org.apache.beam.model.fn_execution.v1.Metrics.PTransform.Measured.output_element_counts', index=1,
       number=2, type=11, cpp_type=10, label=3,
       has_default_value=False, default_value=[],
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='total_time_spent', full_name='org.apache.beam.model.fn_execution.v1.Metrics.PTransform.Measured.total_time_spent', index=2,
       number=3, type=1, cpp_type=5, label=1,
       has_default_value=False, default_value=float(0),
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
   ],
   extensions=[
   ],
   nested_types=[_METRICS_PTRANSFORM_MEASURED_INPUTELEMENTCOUNTSENTRY, _METRICS_PTRANSFORM_MEASURED_OUTPUTELEMENTCOUNTSENTRY, ],
   enum_types=[
   ],
   options=None,
   is_extendable=False,
   syntax='proto3',
   extension_ranges=[],
   oneofs=[
   ],
-  serialized_start=5888,
-  serialized_end=6289,
+  serialized_start=7111,
+  serialized_end=7512,
 )
 
 _METRICS_PTRANSFORM_PROCESSEDELEMENTS = _descriptor.Descriptor(
   name='ProcessedElements',
   full_name='org.apache.beam.model.fn_execution.v1.Metrics.PTransform.ProcessedElements',
   filename=None,
   file=DESCRIPTOR,
@@ -1631,29 +1807,29 @@
   fields=[
     _descriptor.FieldDescriptor(
       name='measured', full_name='org.apache.beam.model.fn_execution.v1.Metrics.PTransform.ProcessedElements.measured', index=0,
       number=1, type=11, cpp_type=10, label=1,
       has_default_value=False, default_value=None,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
   ],
   extensions=[
   ],
   nested_types=[],
   enum_types=[
   ],
   options=None,
   is_extendable=False,
   syntax='proto3',
   extension_ranges=[],
   oneofs=[
   ],
-  serialized_start=6291,
-  serialized_end=6396,
+  serialized_start=7514,
+  serialized_end=7619,
 )
 
 _METRICS_PTRANSFORM_ACTIVEELEMENTS_OUTPUTELEMENTSREMAININGENTRY = _descriptor.Descriptor(
   name='OutputElementsRemainingEntry',
   full_name='org.apache.beam.model.fn_execution.v1.Metrics.PTransform.ActiveElements.OutputElementsRemainingEntry',
   filename=None,
   file=DESCRIPTOR,
@@ -1661,36 +1837,36 @@
   fields=[
     _descriptor.FieldDescriptor(
       name='key', full_name='org.apache.beam.model.fn_execution.v1.Metrics.PTransform.ActiveElements.OutputElementsRemainingEntry.key', index=0,
       number=1, type=9, cpp_type=9, label=1,
       has_default_value=False, default_value=_b("").decode('utf-8'),
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='value', full_name='org.apache.beam.model.fn_execution.v1.Metrics.PTransform.ActiveElements.OutputElementsRemainingEntry.value', index=1,
       number=2, type=3, cpp_type=2, label=1,
       has_default_value=False, default_value=0,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
   ],
   extensions=[
   ],
   nested_types=[],
   enum_types=[
   ],
   options=_descriptor._ParseOptions(descriptor_pb2.MessageOptions(), _b('8\001')),
   is_extendable=False,
   syntax='proto3',
   extension_ranges=[],
   oneofs=[
   ],
-  serialized_start=6670,
-  serialized_end=6732,
+  serialized_start=7893,
+  serialized_end=7955,
 )
 
 _METRICS_PTRANSFORM_ACTIVEELEMENTS = _descriptor.Descriptor(
   name='ActiveElements',
   full_name='org.apache.beam.model.fn_execution.v1.Metrics.PTransform.ActiveElements',
   filename=None,
   file=DESCRIPTOR,
@@ -1698,43 +1874,43 @@
   fields=[
     _descriptor.FieldDescriptor(
       name='measured', full_name='org.apache.beam.model.fn_execution.v1.Metrics.PTransform.ActiveElements.measured', index=0,
       number=1, type=11, cpp_type=10, label=1,
       has_default_value=False, default_value=None,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='fraction_remaining', full_name='org.apache.beam.model.fn_execution.v1.Metrics.PTransform.ActiveElements.fraction_remaining', index=1,
       number=2, type=1, cpp_type=5, label=1,
       has_default_value=False, default_value=float(0),
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='output_elements_remaining', full_name='org.apache.beam.model.fn_execution.v1.Metrics.PTransform.ActiveElements.output_elements_remaining', index=2,
       number=3, type=11, cpp_type=10, label=3,
       has_default_value=False, default_value=[],
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
   ],
   extensions=[
   ],
   nested_types=[_METRICS_PTRANSFORM_ACTIVEELEMENTS_OUTPUTELEMENTSREMAININGENTRY, ],
   enum_types=[
   ],
   options=None,
   is_extendable=False,
   syntax='proto3',
   extension_ranges=[],
   oneofs=[
   ],
-  serialized_start=6399,
-  serialized_end=6732,
+  serialized_start=7622,
+  serialized_end=7955,
 )
 
 _METRICS_PTRANSFORM_WATERMARKSENTRY = _descriptor.Descriptor(
   name='WatermarksEntry',
   full_name='org.apache.beam.model.fn_execution.v1.Metrics.PTransform.WatermarksEntry',
   filename=None,
   file=DESCRIPTOR,
@@ -1742,36 +1918,36 @@
   fields=[
     _descriptor.FieldDescriptor(
       name='key', full_name='org.apache.beam.model.fn_execution.v1.Metrics.PTransform.WatermarksEntry.key', index=0,
       number=1, type=9, cpp_type=9, label=1,
       has_default_value=False, default_value=_b("").decode('utf-8'),
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='value', full_name='org.apache.beam.model.fn_execution.v1.Metrics.PTransform.WatermarksEntry.value', index=1,
       number=2, type=3, cpp_type=2, label=1,
       has_default_value=False, default_value=0,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
   ],
   extensions=[
   ],
   nested_types=[],
   enum_types=[
   ],
   options=_descriptor._ParseOptions(descriptor_pb2.MessageOptions(), _b('8\001')),
   is_extendable=False,
   syntax='proto3',
   extension_ranges=[],
   oneofs=[
   ],
-  serialized_start=6734,
-  serialized_end=6783,
+  serialized_start=7957,
+  serialized_end=8006,
 )
 
 _METRICS_PTRANSFORM = _descriptor.Descriptor(
   name='PTransform',
   full_name='org.apache.beam.model.fn_execution.v1.Metrics.PTransform',
   filename=None,
   file=DESCRIPTOR,
@@ -1779,50 +1955,50 @@
   fields=[
     _descriptor.FieldDescriptor(
       name='processed_elements', full_name='org.apache.beam.model.fn_execution.v1.Metrics.PTransform.processed_elements', index=0,
       number=1, type=11, cpp_type=10, label=1,
       has_default_value=False, default_value=None,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='active_elements', full_name='org.apache.beam.model.fn_execution.v1.Metrics.PTransform.active_elements', index=1,
       number=2, type=11, cpp_type=10, label=1,
       has_default_value=False, default_value=None,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='watermarks', full_name='org.apache.beam.model.fn_execution.v1.Metrics.PTransform.watermarks', index=2,
       number=3, type=11, cpp_type=10, label=3,
       has_default_value=False, default_value=[],
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='user', full_name='org.apache.beam.model.fn_execution.v1.Metrics.PTransform.user', index=3,
       number=4, type=11, cpp_type=10, label=3,
       has_default_value=False, default_value=[],
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
   ],
   extensions=[
   ],
   nested_types=[_METRICS_PTRANSFORM_MEASURED, _METRICS_PTRANSFORM_PROCESSEDELEMENTS, _METRICS_PTRANSFORM_ACTIVEELEMENTS, _METRICS_PTRANSFORM_WATERMARKSENTRY, ],
   enum_types=[
   ],
   options=None,
   is_extendable=False,
   syntax='proto3',
   extension_ranges=[],
   oneofs=[
   ],
-  serialized_start=5507,
-  serialized_end=6783,
+  serialized_start=6730,
+  serialized_end=8006,
 )
 
 _METRICS_USER_METRICNAME = _descriptor.Descriptor(
   name='MetricName',
   full_name='org.apache.beam.model.fn_execution.v1.Metrics.User.MetricName',
   filename=None,
   file=DESCRIPTOR,
@@ -1830,36 +2006,36 @@
   fields=[
     _descriptor.FieldDescriptor(
       name='namespace', full_name='org.apache.beam.model.fn_execution.v1.Metrics.User.MetricName.namespace', index=0,
       number=2, type=9, cpp_type=9, label=1,
       has_default_value=False, default_value=_b("").decode('utf-8'),
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='name', full_name='org.apache.beam.model.fn_execution.v1.Metrics.User.MetricName.name', index=1,
       number=3, type=9, cpp_type=9, label=1,
       has_default_value=False, default_value=_b("").decode('utf-8'),
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
   ],
   extensions=[
   ],
   nested_types=[],
   enum_types=[
   ],
   options=None,
   is_extendable=False,
   syntax='proto3',
   extension_ranges=[],
   oneofs=[
   ],
-  serialized_start=7155,
-  serialized_end=7200,
+  serialized_start=8378,
+  serialized_end=8423,
 )
 
 _METRICS_USER_COUNTERDATA = _descriptor.Descriptor(
   name='CounterData',
   full_name='org.apache.beam.model.fn_execution.v1.Metrics.User.CounterData',
   filename=None,
   file=DESCRIPTOR,
@@ -1867,29 +2043,29 @@
   fields=[
     _descriptor.FieldDescriptor(
       name='value', full_name='org.apache.beam.model.fn_execution.v1.Metrics.User.CounterData.value', index=0,
       number=1, type=3, cpp_type=2, label=1,
       has_default_value=False, default_value=0,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
   ],
   extensions=[
   ],
   nested_types=[],
   enum_types=[
   ],
   options=None,
   is_extendable=False,
   syntax='proto3',
   extension_ranges=[],
   oneofs=[
   ],
-  serialized_start=7202,
-  serialized_end=7230,
+  serialized_start=8425,
+  serialized_end=8453,
 )
 
 _METRICS_USER_DISTRIBUTIONDATA = _descriptor.Descriptor(
   name='DistributionData',
   full_name='org.apache.beam.model.fn_execution.v1.Metrics.User.DistributionData',
   filename=None,
   file=DESCRIPTOR,
@@ -1897,50 +2073,50 @@
   fields=[
     _descriptor.FieldDescriptor(
       name='count', full_name='org.apache.beam.model.fn_execution.v1.Metrics.User.DistributionData.count', index=0,
       number=1, type=3, cpp_type=2, label=1,
       has_default_value=False, default_value=0,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='sum', full_name='org.apache.beam.model.fn_execution.v1.Metrics.User.DistributionData.sum', index=1,
       number=2, type=3, cpp_type=2, label=1,
       has_default_value=False, default_value=0,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='min', full_name='org.apache.beam.model.fn_execution.v1.Metrics.User.DistributionData.min', index=2,
       number=3, type=3, cpp_type=2, label=1,
       has_default_value=False, default_value=0,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='max', full_name='org.apache.beam.model.fn_execution.v1.Metrics.User.DistributionData.max', index=3,
       number=4, type=3, cpp_type=2, label=1,
       has_default_value=False, default_value=0,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
   ],
   extensions=[
   ],
   nested_types=[],
   enum_types=[
   ],
   options=None,
   is_extendable=False,
   syntax='proto3',
   extension_ranges=[],
   oneofs=[
   ],
-  serialized_start=7232,
-  serialized_end=7304,
+  serialized_start=8455,
+  serialized_end=8527,
 )
 
 _METRICS_USER_GAUGEDATA = _descriptor.Descriptor(
   name='GaugeData',
   full_name='org.apache.beam.model.fn_execution.v1.Metrics.User.GaugeData',
   filename=None,
   file=DESCRIPTOR,
@@ -1948,36 +2124,36 @@
   fields=[
     _descriptor.FieldDescriptor(
       name='value', full_name='org.apache.beam.model.fn_execution.v1.Metrics.User.GaugeData.value', index=0,
       number=1, type=3, cpp_type=2, label=1,
       has_default_value=False, default_value=0,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='timestamp', full_name='org.apache.beam.model.fn_execution.v1.Metrics.User.GaugeData.timestamp', index=1,
       number=2, type=11, cpp_type=10, label=1,
       has_default_value=False, default_value=None,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
   ],
   extensions=[
   ],
   nested_types=[],
   enum_types=[
   ],
   options=None,
   is_extendable=False,
   syntax='proto3',
   extension_ranges=[],
   oneofs=[
   ],
-  serialized_start=7306,
-  serialized_end=7379,
+  serialized_start=8529,
+  serialized_end=8602,
 )
 
 _METRICS_USER = _descriptor.Descriptor(
   name='User',
   full_name='org.apache.beam.model.fn_execution.v1.Metrics.User',
   filename=None,
   file=DESCRIPTOR,
@@ -1985,36 +2161,36 @@
   fields=[
     _descriptor.FieldDescriptor(
       name='metric_name', full_name='org.apache.beam.model.fn_execution.v1.Metrics.User.metric_name', index=0,
       number=1, type=11, cpp_type=10, label=1,
       has_default_value=False, default_value=None,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='counter_data', full_name='org.apache.beam.model.fn_execution.v1.Metrics.User.counter_data', index=1,
       number=1001, type=11, cpp_type=10, label=1,
       has_default_value=False, default_value=None,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='distribution_data', full_name='org.apache.beam.model.fn_execution.v1.Metrics.User.distribution_data', index=2,
       number=1002, type=11, cpp_type=10, label=1,
       has_default_value=False, default_value=None,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='gauge_data', full_name='org.apache.beam.model.fn_execution.v1.Metrics.User.gauge_data', index=3,
       number=1003, type=11, cpp_type=10, label=1,
       has_default_value=False, default_value=None,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
   ],
   extensions=[
   ],
   nested_types=[_METRICS_USER_METRICNAME, _METRICS_USER_COUNTERDATA, _METRICS_USER_DISTRIBUTIONDATA, _METRICS_USER_GAUGEDATA, ],
   enum_types=[
   ],
   options=None,
@@ -2022,16 +2198,16 @@
   syntax='proto3',
   extension_ranges=[],
   oneofs=[
     _descriptor.OneofDescriptor(
       name='data', full_name='org.apache.beam.model.fn_execution.v1.Metrics.User.data',
       index=0, containing_type=None, fields=[]),
   ],
-  serialized_start=6786,
-  serialized_end=7387,
+  serialized_start=8009,
+  serialized_end=8610,
 )
 
 _METRICS_PTRANSFORMSENTRY = _descriptor.Descriptor(
   name='PtransformsEntry',
   full_name='org.apache.beam.model.fn_execution.v1.Metrics.PtransformsEntry',
   filename=None,
   file=DESCRIPTOR,
@@ -2039,36 +2215,36 @@
   fields=[
     _descriptor.FieldDescriptor(
       name='key', full_name='org.apache.beam.model.fn_execution.v1.Metrics.PtransformsEntry.key', index=0,
       number=1, type=9, cpp_type=9, label=1,
       has_default_value=False, default_value=_b("").decode('utf-8'),
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='value', full_name='org.apache.beam.model.fn_execution.v1.Metrics.PtransformsEntry.value', index=1,
       number=2, type=11, cpp_type=10, label=1,
       has_default_value=False, default_value=None,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
   ],
   extensions=[
   ],
   nested_types=[],
   enum_types=[
   ],
   options=_descriptor._ParseOptions(descriptor_pb2.MessageOptions(), _b('8\001')),
   is_extendable=False,
   syntax='proto3',
   extension_ranges=[],
   oneofs=[
   ],
-  serialized_start=7389,
-  serialized_end=7498,
+  serialized_start=8612,
+  serialized_end=8721,
 )
 
 _METRICS = _descriptor.Descriptor(
   name='Metrics',
   full_name='org.apache.beam.model.fn_execution.v1.Metrics',
   filename=None,
   file=DESCRIPTOR,
@@ -2076,29 +2252,29 @@
   fields=[
     _descriptor.FieldDescriptor(
       name='ptransforms', full_name='org.apache.beam.model.fn_execution.v1.Metrics.ptransforms', index=0,
       number=1, type=11, cpp_type=10, label=3,
       has_default_value=False, default_value=[],
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
   ],
   extensions=[
   ],
   nested_types=[_METRICS_PTRANSFORM, _METRICS_USER, _METRICS_PTRANSFORMSENTRY, ],
   enum_types=[
   ],
   options=None,
   is_extendable=False,
   syntax='proto3',
   extension_ranges=[],
   oneofs=[
   ],
-  serialized_start=5409,
-  serialized_end=7498,
+  serialized_start=6632,
+  serialized_end=8721,
 )
 
 
 _PROCESSBUNDLEPROGRESSRESPONSE = _descriptor.Descriptor(
   name='ProcessBundleProgressResponse',
   full_name='org.apache.beam.model.fn_execution.v1.ProcessBundleProgressResponse',
   filename=None,
@@ -2107,98 +2283,211 @@
   fields=[
     _descriptor.FieldDescriptor(
       name='metrics', full_name='org.apache.beam.model.fn_execution.v1.ProcessBundleProgressResponse.metrics', index=0,
       number=1, type=11, cpp_type=10, label=1,
       has_default_value=False, default_value=None,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
-      name='split', full_name='org.apache.beam.model.fn_execution.v1.ProcessBundleProgressResponse.split', index=1,
-      number=2, type=11, cpp_type=10, label=1,
-      has_default_value=False, default_value=None,
+      name='monitoring_infos', full_name='org.apache.beam.model.fn_execution.v1.ProcessBundleProgressResponse.monitoring_infos', index=1,
+      number=3, type=11, cpp_type=10, label=3,
+      has_default_value=False, default_value=[],
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      options=None),
+    _descriptor.FieldDescriptor(
+      name='primary_roots', full_name='org.apache.beam.model.fn_execution.v1.ProcessBundleProgressResponse.primary_roots', index=2,
+      number=4, type=11, cpp_type=10, label=3,
+      has_default_value=False, default_value=[],
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
   ],
   extensions=[
   ],
   nested_types=[],
   enum_types=[
   ],
   options=None,
   is_extendable=False,
   syntax='proto3',
   extension_ranges=[],
   oneofs=[
   ],
-  serialized_start=7501,
-  serialized_end=7664,
+  serialized_start=8724,
+  serialized_end=8982,
 )
 
 
+_PROCESSBUNDLESPLITREQUEST_BACKLOGREMAININGENTRY = _descriptor.Descriptor(
+  name='BacklogRemainingEntry',
+  full_name='org.apache.beam.model.fn_execution.v1.ProcessBundleSplitRequest.BacklogRemainingEntry',
+  filename=None,
+  file=DESCRIPTOR,
+  containing_type=None,
+  fields=[
+    _descriptor.FieldDescriptor(
+      name='key', full_name='org.apache.beam.model.fn_execution.v1.ProcessBundleSplitRequest.BacklogRemainingEntry.key', index=0,
+      number=1, type=9, cpp_type=9, label=1,
+      has_default_value=False, default_value=_b("").decode('utf-8'),
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      options=None),
+    _descriptor.FieldDescriptor(
+      name='value', full_name='org.apache.beam.model.fn_execution.v1.ProcessBundleSplitRequest.BacklogRemainingEntry.value', index=1,
+      number=2, type=12, cpp_type=9, label=1,
+      has_default_value=False, default_value=_b(""),
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      options=None),
+  ],
+  extensions=[
+  ],
+  nested_types=[],
+  enum_types=[
+  ],
+  options=_descriptor._ParseOptions(descriptor_pb2.MessageOptions(), _b('8\001')),
+  is_extendable=False,
+  syntax='proto3',
+  extension_ranges=[],
+  oneofs=[
+  ],
+  serialized_start=9160,
+  serialized_end=9215,
+)
+
 _PROCESSBUNDLESPLITREQUEST = _descriptor.Descriptor(
   name='ProcessBundleSplitRequest',
   full_name='org.apache.beam.model.fn_execution.v1.ProcessBundleSplitRequest',
   filename=None,
   file=DESCRIPTOR,
   containing_type=None,
   fields=[
     _descriptor.FieldDescriptor(
       name='instruction_reference', full_name='org.apache.beam.model.fn_execution.v1.ProcessBundleSplitRequest.instruction_reference', index=0,
       number=1, type=9, cpp_type=9, label=1,
       has_default_value=False, default_value=_b("").decode('utf-8'),
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
-      name='fraction_of_remainder', full_name='org.apache.beam.model.fn_execution.v1.ProcessBundleSplitRequest.fraction_of_remainder', index=1,
-      number=2, type=11, cpp_type=10, label=1,
-      has_default_value=False, default_value=None,
+      name='backlog_remaining', full_name='org.apache.beam.model.fn_execution.v1.ProcessBundleSplitRequest.backlog_remaining', index=1,
+      number=2, type=11, cpp_type=10, label=3,
+      has_default_value=False, default_value=[],
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
   ],
   extensions=[
   ],
-  nested_types=[],
+  nested_types=[_PROCESSBUNDLESPLITREQUEST_BACKLOGREMAININGENTRY, ],
   enum_types=[
   ],
   options=None,
   is_extendable=False,
   syntax='proto3',
   extension_ranges=[],
   oneofs=[
   ],
-  serialized_start=7666,
-  serialized_end=7785,
+  serialized_start=8985,
+  serialized_end=9215,
 )
 
 
 _PROCESSBUNDLESPLITRESPONSE = _descriptor.Descriptor(
   name='ProcessBundleSplitResponse',
   full_name='org.apache.beam.model.fn_execution.v1.ProcessBundleSplitResponse',
   filename=None,
   file=DESCRIPTOR,
   containing_type=None,
   fields=[
+    _descriptor.FieldDescriptor(
+      name='primary_roots', full_name='org.apache.beam.model.fn_execution.v1.ProcessBundleSplitResponse.primary_roots', index=0,
+      number=1, type=11, cpp_type=10, label=3,
+      has_default_value=False, default_value=[],
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      options=None),
+    _descriptor.FieldDescriptor(
+      name='residual_roots', full_name='org.apache.beam.model.fn_execution.v1.ProcessBundleSplitResponse.residual_roots', index=1,
+      number=2, type=11, cpp_type=10, label=3,
+      has_default_value=False, default_value=[],
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      options=None),
   ],
   extensions=[
   ],
   nested_types=[],
   enum_types=[
   ],
   options=None,
   is_extendable=False,
   syntax='proto3',
   extension_ranges=[],
   oneofs=[
   ],
-  serialized_start=7787,
-  serialized_end=7815,
+  serialized_start=9218,
+  serialized_end=9416,
+)
+
+
+_FINALIZEBUNDLEREQUEST = _descriptor.Descriptor(
+  name='FinalizeBundleRequest',
+  full_name='org.apache.beam.model.fn_execution.v1.FinalizeBundleRequest',
+  filename=None,
+  file=DESCRIPTOR,
+  containing_type=None,
+  fields=[
+    _descriptor.FieldDescriptor(
+      name='instruction_reference', full_name='org.apache.beam.model.fn_execution.v1.FinalizeBundleRequest.instruction_reference', index=0,
+      number=1, type=9, cpp_type=9, label=1,
+      has_default_value=False, default_value=_b("").decode('utf-8'),
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      options=None),
+  ],
+  extensions=[
+  ],
+  nested_types=[],
+  enum_types=[
+  ],
+  options=None,
+  is_extendable=False,
+  syntax='proto3',
+  extension_ranges=[],
+  oneofs=[
+  ],
+  serialized_start=9418,
+  serialized_end=9472,
+)
+
+
+_FINALIZEBUNDLERESPONSE = _descriptor.Descriptor(
+  name='FinalizeBundleResponse',
+  full_name='org.apache.beam.model.fn_execution.v1.FinalizeBundleResponse',
+  filename=None,
+  file=DESCRIPTOR,
+  containing_type=None,
+  fields=[
+  ],
+  extensions=[
+  ],
+  nested_types=[],
+  enum_types=[
+  ],
+  options=None,
+  is_extendable=False,
+  syntax='proto3',
+  extension_ranges=[],
+  oneofs=[
+  ],
+  serialized_start=9474,
+  serialized_end=9498,
 )
 
 
 _ELEMENTS_DATA = _descriptor.Descriptor(
   name='Data',
   full_name='org.apache.beam.model.fn_execution.v1.Elements.Data',
   filename=None,
@@ -2207,43 +2496,43 @@
   fields=[
     _descriptor.FieldDescriptor(
       name='instruction_reference', full_name='org.apache.beam.model.fn_execution.v1.Elements.Data.instruction_reference', index=0,
       number=1, type=9, cpp_type=9, label=1,
       has_default_value=False, default_value=_b("").decode('utf-8'),
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='target', full_name='org.apache.beam.model.fn_execution.v1.Elements.Data.target', index=1,
       number=2, type=11, cpp_type=10, label=1,
       has_default_value=False, default_value=None,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='data', full_name='org.apache.beam.model.fn_execution.v1.Elements.Data.data', index=2,
       number=3, type=12, cpp_type=9, label=1,
       has_default_value=False, default_value=_b(""),
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
   ],
   extensions=[
   ],
   nested_types=[],
   enum_types=[
   ],
   options=None,
   is_extendable=False,
   syntax='proto3',
   extension_ranges=[],
   oneofs=[
   ],
-  serialized_start=7898,
-  serialized_end=8012,
+  serialized_start=9581,
+  serialized_end=9695,
 )
 
 _ELEMENTS = _descriptor.Descriptor(
   name='Elements',
   full_name='org.apache.beam.model.fn_execution.v1.Elements',
   filename=None,
   file=DESCRIPTOR,
@@ -2251,29 +2540,29 @@
   fields=[
     _descriptor.FieldDescriptor(
       name='data', full_name='org.apache.beam.model.fn_execution.v1.Elements.data', index=0,
       number=1, type=11, cpp_type=10, label=3,
       has_default_value=False, default_value=[],
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
   ],
   extensions=[
   ],
   nested_types=[_ELEMENTS_DATA, ],
   enum_types=[
   ],
   options=None,
   is_extendable=False,
   syntax='proto3',
   extension_ranges=[],
   oneofs=[
   ],
-  serialized_start=7818,
-  serialized_end=8012,
+  serialized_start=9501,
+  serialized_end=9695,
 )
 
 
 _STATEREQUEST = _descriptor.Descriptor(
   name='StateRequest',
   full_name='org.apache.beam.model.fn_execution.v1.StateRequest',
   filename=None,
@@ -2282,50 +2571,50 @@
   fields=[
     _descriptor.FieldDescriptor(
       name='id', full_name='org.apache.beam.model.fn_execution.v1.StateRequest.id', index=0,
       number=1, type=9, cpp_type=9, label=1,
       has_default_value=False, default_value=_b("").decode('utf-8'),
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='instruction_reference', full_name='org.apache.beam.model.fn_execution.v1.StateRequest.instruction_reference', index=1,
       number=2, type=9, cpp_type=9, label=1,
       has_default_value=False, default_value=_b("").decode('utf-8'),
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='state_key', full_name='org.apache.beam.model.fn_execution.v1.StateRequest.state_key', index=2,
       number=3, type=11, cpp_type=10, label=1,
       has_default_value=False, default_value=None,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='get', full_name='org.apache.beam.model.fn_execution.v1.StateRequest.get', index=3,
       number=1000, type=11, cpp_type=10, label=1,
       has_default_value=False, default_value=None,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='append', full_name='org.apache.beam.model.fn_execution.v1.StateRequest.append', index=4,
       number=1001, type=11, cpp_type=10, label=1,
       has_default_value=False, default_value=None,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='clear', full_name='org.apache.beam.model.fn_execution.v1.StateRequest.clear', index=5,
       number=1002, type=11, cpp_type=10, label=1,
       has_default_value=False, default_value=None,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
   ],
   extensions=[
   ],
   nested_types=[],
   enum_types=[
   ],
   options=None,
@@ -2333,16 +2622,16 @@
   syntax='proto3',
   extension_ranges=[],
   oneofs=[
     _descriptor.OneofDescriptor(
       name='request', full_name='org.apache.beam.model.fn_execution.v1.StateRequest.request',
       index=0, containing_type=None, fields=[]),
   ],
-  serialized_start=8015,
-  serialized_end=8377,
+  serialized_start=9698,
+  serialized_end=10060,
 )
 
 
 _STATERESPONSE = _descriptor.Descriptor(
   name='StateResponse',
   full_name='org.apache.beam.model.fn_execution.v1.StateResponse',
   filename=None,
@@ -2351,50 +2640,50 @@
   fields=[
     _descriptor.FieldDescriptor(
       name='id', full_name='org.apache.beam.model.fn_execution.v1.StateResponse.id', index=0,
       number=1, type=9, cpp_type=9, label=1,
       has_default_value=False, default_value=_b("").decode('utf-8'),
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='error', full_name='org.apache.beam.model.fn_execution.v1.StateResponse.error', index=1,
       number=2, type=9, cpp_type=9, label=1,
       has_default_value=False, default_value=_b("").decode('utf-8'),
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='cache_token', full_name='org.apache.beam.model.fn_execution.v1.StateResponse.cache_token', index=2,
       number=3, type=12, cpp_type=9, label=1,
       has_default_value=False, default_value=_b(""),
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='get', full_name='org.apache.beam.model.fn_execution.v1.StateResponse.get', index=3,
       number=1000, type=11, cpp_type=10, label=1,
       has_default_value=False, default_value=None,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='append', full_name='org.apache.beam.model.fn_execution.v1.StateResponse.append', index=4,
       number=1001, type=11, cpp_type=10, label=1,
       has_default_value=False, default_value=None,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='clear', full_name='org.apache.beam.model.fn_execution.v1.StateResponse.clear', index=5,
       number=1002, type=11, cpp_type=10, label=1,
       has_default_value=False, default_value=None,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
   ],
   extensions=[
   ],
   nested_types=[],
   enum_types=[
   ],
   options=None,
@@ -2402,16 +2691,16 @@
   syntax='proto3',
   extension_ranges=[],
   oneofs=[
     _descriptor.OneofDescriptor(
       name='response', full_name='org.apache.beam.model.fn_execution.v1.StateResponse.response',
       index=0, containing_type=None, fields=[]),
   ],
-  serialized_start=8380,
-  serialized_end=8684,
+  serialized_start=10063,
+  serialized_end=10367,
 )
 
 
 _STATEKEY_RUNNER = _descriptor.Descriptor(
   name='Runner',
   full_name='org.apache.beam.model.fn_execution.v1.StateKey.Runner',
   filename=None,
@@ -2420,29 +2709,29 @@
   fields=[
     _descriptor.FieldDescriptor(
       name='key', full_name='org.apache.beam.model.fn_execution.v1.StateKey.Runner.key', index=0,
       number=1, type=12, cpp_type=9, label=1,
       has_default_value=False, default_value=_b(""),
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
   ],
   extensions=[
   ],
   nested_types=[],
   enum_types=[
   ],
   options=None,
   is_extendable=False,
   syntax='proto3',
   extension_ranges=[],
   oneofs=[
   ],
-  serialized_start=8959,
-  serialized_end=8980,
+  serialized_start=10642,
+  serialized_end=10663,
 )
 
 _STATEKEY_MULTIMAPSIDEINPUT = _descriptor.Descriptor(
   name='MultimapSideInput',
   full_name='org.apache.beam.model.fn_execution.v1.StateKey.MultimapSideInput',
   filename=None,
   file=DESCRIPTOR,
@@ -2450,50 +2739,50 @@
   fields=[
     _descriptor.FieldDescriptor(
       name='ptransform_id', full_name='org.apache.beam.model.fn_execution.v1.StateKey.MultimapSideInput.ptransform_id', index=0,
       number=1, type=9, cpp_type=9, label=1,
       has_default_value=False, default_value=_b("").decode('utf-8'),
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='side_input_id', full_name='org.apache.beam.model.fn_execution.v1.StateKey.MultimapSideInput.side_input_id', index=1,
       number=2, type=9, cpp_type=9, label=1,
       has_default_value=False, default_value=_b("").decode('utf-8'),
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='window', full_name='org.apache.beam.model.fn_execution.v1.StateKey.MultimapSideInput.window', index=2,
       number=3, type=12, cpp_type=9, label=1,
       has_default_value=False, default_value=_b(""),
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='key', full_name='org.apache.beam.model.fn_execution.v1.StateKey.MultimapSideInput.key', index=3,
       number=4, type=12, cpp_type=9, label=1,
       has_default_value=False, default_value=_b(""),
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
   ],
   extensions=[
   ],
   nested_types=[],
   enum_types=[
   ],
   options=None,
   is_extendable=False,
   syntax='proto3',
   extension_ranges=[],
   oneofs=[
   ],
-  serialized_start=8982,
-  serialized_end=9076,
+  serialized_start=10665,
+  serialized_end=10759,
 )
 
 _STATEKEY_BAGUSERSTATE = _descriptor.Descriptor(
   name='BagUserState',
   full_name='org.apache.beam.model.fn_execution.v1.StateKey.BagUserState',
   filename=None,
   file=DESCRIPTOR,
@@ -2501,50 +2790,50 @@
   fields=[
     _descriptor.FieldDescriptor(
       name='ptransform_id', full_name='org.apache.beam.model.fn_execution.v1.StateKey.BagUserState.ptransform_id', index=0,
       number=1, type=9, cpp_type=9, label=1,
       has_default_value=False, default_value=_b("").decode('utf-8'),
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='user_state_id', full_name='org.apache.beam.model.fn_execution.v1.StateKey.BagUserState.user_state_id', index=1,
       number=2, type=9, cpp_type=9, label=1,
       has_default_value=False, default_value=_b("").decode('utf-8'),
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='window', full_name='org.apache.beam.model.fn_execution.v1.StateKey.BagUserState.window', index=2,
       number=3, type=12, cpp_type=9, label=1,
       has_default_value=False, default_value=_b(""),
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='key', full_name='org.apache.beam.model.fn_execution.v1.StateKey.BagUserState.key', index=3,
       number=4, type=12, cpp_type=9, label=1,
       has_default_value=False, default_value=_b(""),
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
   ],
   extensions=[
   ],
   nested_types=[],
   enum_types=[
   ],
   options=None,
   is_extendable=False,
   syntax='proto3',
   extension_ranges=[],
   oneofs=[
   ],
-  serialized_start=9078,
-  serialized_end=9167,
+  serialized_start=10761,
+  serialized_end=10850,
 )
 
 _STATEKEY = _descriptor.Descriptor(
   name='StateKey',
   full_name='org.apache.beam.model.fn_execution.v1.StateKey',
   filename=None,
   file=DESCRIPTOR,
@@ -2552,29 +2841,29 @@
   fields=[
     _descriptor.FieldDescriptor(
       name='runner', full_name='org.apache.beam.model.fn_execution.v1.StateKey.runner', index=0,
       number=1, type=11, cpp_type=10, label=1,
       has_default_value=False, default_value=None,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='multimap_side_input', full_name='org.apache.beam.model.fn_execution.v1.StateKey.multimap_side_input', index=1,
       number=2, type=11, cpp_type=10, label=1,
       has_default_value=False, default_value=None,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='bag_user_state', full_name='org.apache.beam.model.fn_execution.v1.StateKey.bag_user_state', index=2,
       number=3, type=11, cpp_type=10, label=1,
       has_default_value=False, default_value=None,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
   ],
   extensions=[
   ],
   nested_types=[_STATEKEY_RUNNER, _STATEKEY_MULTIMAPSIDEINPUT, _STATEKEY_BAGUSERSTATE, ],
   enum_types=[
   ],
   options=None,
@@ -2582,16 +2871,16 @@
   syntax='proto3',
   extension_ranges=[],
   oneofs=[
     _descriptor.OneofDescriptor(
       name='type', full_name='org.apache.beam.model.fn_execution.v1.StateKey.type',
       index=0, containing_type=None, fields=[]),
   ],
-  serialized_start=8687,
-  serialized_end=9175,
+  serialized_start=10370,
+  serialized_end=10858,
 )
 
 
 _STATEGETREQUEST = _descriptor.Descriptor(
   name='StateGetRequest',
   full_name='org.apache.beam.model.fn_execution.v1.StateGetRequest',
   filename=None,
@@ -2600,29 +2889,29 @@
   fields=[
     _descriptor.FieldDescriptor(
       name='continuation_token', full_name='org.apache.beam.model.fn_execution.v1.StateGetRequest.continuation_token', index=0,
       number=1, type=12, cpp_type=9, label=1,
       has_default_value=False, default_value=_b(""),
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
   ],
   extensions=[
   ],
   nested_types=[],
   enum_types=[
   ],
   options=None,
   is_extendable=False,
   syntax='proto3',
   extension_ranges=[],
   oneofs=[
   ],
-  serialized_start=9177,
-  serialized_end=9222,
+  serialized_start=10860,
+  serialized_end=10905,
 )
 
 
 _STATEGETRESPONSE = _descriptor.Descriptor(
   name='StateGetResponse',
   full_name='org.apache.beam.model.fn_execution.v1.StateGetResponse',
   filename=None,
@@ -2631,36 +2920,36 @@
   fields=[
     _descriptor.FieldDescriptor(
       name='continuation_token', full_name='org.apache.beam.model.fn_execution.v1.StateGetResponse.continuation_token', index=0,
       number=1, type=12, cpp_type=9, label=1,
       has_default_value=False, default_value=_b(""),
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='data', full_name='org.apache.beam.model.fn_execution.v1.StateGetResponse.data', index=1,
       number=2, type=12, cpp_type=9, label=1,
       has_default_value=False, default_value=_b(""),
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
   ],
   extensions=[
   ],
   nested_types=[],
   enum_types=[
   ],
   options=None,
   is_extendable=False,
   syntax='proto3',
   extension_ranges=[],
   oneofs=[
   ],
-  serialized_start=9224,
-  serialized_end=9284,
+  serialized_start=10907,
+  serialized_end=10967,
 )
 
 
 _STATEAPPENDREQUEST = _descriptor.Descriptor(
   name='StateAppendRequest',
   full_name='org.apache.beam.model.fn_execution.v1.StateAppendRequest',
   filename=None,
@@ -2669,29 +2958,29 @@
   fields=[
     _descriptor.FieldDescriptor(
       name='data', full_name='org.apache.beam.model.fn_execution.v1.StateAppendRequest.data', index=0,
       number=1, type=12, cpp_type=9, label=1,
       has_default_value=False, default_value=_b(""),
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
   ],
   extensions=[
   ],
   nested_types=[],
   enum_types=[
   ],
   options=None,
   is_extendable=False,
   syntax='proto3',
   extension_ranges=[],
   oneofs=[
   ],
-  serialized_start=9286,
-  serialized_end=9320,
+  serialized_start=10969,
+  serialized_end=11003,
 )
 
 
 _STATEAPPENDRESPONSE = _descriptor.Descriptor(
   name='StateAppendResponse',
   full_name='org.apache.beam.model.fn_execution.v1.StateAppendResponse',
   filename=None,
@@ -2706,16 +2995,16 @@
   ],
   options=None,
   is_extendable=False,
   syntax='proto3',
   extension_ranges=[],
   oneofs=[
   ],
-  serialized_start=9322,
-  serialized_end=9343,
+  serialized_start=11005,
+  serialized_end=11026,
 )
 
 
 _STATECLEARREQUEST = _descriptor.Descriptor(
   name='StateClearRequest',
   full_name='org.apache.beam.model.fn_execution.v1.StateClearRequest',
   filename=None,
@@ -2730,16 +3019,16 @@
   ],
   options=None,
   is_extendable=False,
   syntax='proto3',
   extension_ranges=[],
   oneofs=[
   ],
-  serialized_start=9345,
-  serialized_end=9364,
+  serialized_start=11028,
+  serialized_end=11047,
 )
 
 
 _STATECLEARRESPONSE = _descriptor.Descriptor(
   name='StateClearResponse',
   full_name='org.apache.beam.model.fn_execution.v1.StateClearResponse',
   filename=None,
@@ -2754,16 +3043,16 @@
   ],
   options=None,
   is_extendable=False,
   syntax='proto3',
   extension_ranges=[],
   oneofs=[
   ],
-  serialized_start=9366,
-  serialized_end=9386,
+  serialized_start=11049,
+  serialized_end=11069,
 )
 
 
 _LOGENTRY_LIST = _descriptor.Descriptor(
   name='List',
   full_name='org.apache.beam.model.fn_execution.v1.LogEntry.List',
   filename=None,
@@ -2772,29 +3061,29 @@
   fields=[
     _descriptor.FieldDescriptor(
       name='log_entries', full_name='org.apache.beam.model.fn_execution.v1.LogEntry.List.log_entries', index=0,
       number=1, type=11, cpp_type=10, label=3,
       has_default_value=False, default_value=[],
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
   ],
   extensions=[
   ],
   nested_types=[],
   enum_types=[
   ],
   options=None,
   is_extendable=False,
   syntax='proto3',
   extension_ranges=[],
   oneofs=[
   ],
-  serialized_start=9669,
-  serialized_end=9745,
+  serialized_start=11352,
+  serialized_end=11428,
 )
 
 _LOGENTRY_SEVERITY = _descriptor.Descriptor(
   name='Severity',
   full_name='org.apache.beam.model.fn_execution.v1.LogEntry.Severity',
   filename=None,
   file=DESCRIPTOR,
@@ -2809,16 +3098,16 @@
   ],
   options=None,
   is_extendable=False,
   syntax='proto3',
   extension_ranges=[],
   oneofs=[
   ],
-  serialized_start=9747,
-  serialized_end=9861,
+  serialized_start=11430,
+  serialized_end=11544,
 )
 
 _LOGENTRY = _descriptor.Descriptor(
   name='LogEntry',
   full_name='org.apache.beam.model.fn_execution.v1.LogEntry',
   filename=None,
   file=DESCRIPTOR,
@@ -2826,78 +3115,78 @@
   fields=[
     _descriptor.FieldDescriptor(
       name='severity', full_name='org.apache.beam.model.fn_execution.v1.LogEntry.severity', index=0,
       number=1, type=14, cpp_type=8, label=1,
       has_default_value=False, default_value=0,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='timestamp', full_name='org.apache.beam.model.fn_execution.v1.LogEntry.timestamp', index=1,
       number=2, type=11, cpp_type=10, label=1,
       has_default_value=False, default_value=None,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='message', full_name='org.apache.beam.model.fn_execution.v1.LogEntry.message', index=2,
       number=3, type=9, cpp_type=9, label=1,
       has_default_value=False, default_value=_b("").decode('utf-8'),
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='trace', full_name='org.apache.beam.model.fn_execution.v1.LogEntry.trace', index=3,
       number=4, type=9, cpp_type=9, label=1,
       has_default_value=False, default_value=_b("").decode('utf-8'),
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='instruction_reference', full_name='org.apache.beam.model.fn_execution.v1.LogEntry.instruction_reference', index=4,
       number=5, type=9, cpp_type=9, label=1,
       has_default_value=False, default_value=_b("").decode('utf-8'),
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='primitive_transform_reference', full_name='org.apache.beam.model.fn_execution.v1.LogEntry.primitive_transform_reference', index=5,
       number=6, type=9, cpp_type=9, label=1,
       has_default_value=False, default_value=_b("").decode('utf-8'),
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='log_location', full_name='org.apache.beam.model.fn_execution.v1.LogEntry.log_location', index=6,
       number=7, type=9, cpp_type=9, label=1,
       has_default_value=False, default_value=_b("").decode('utf-8'),
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='thread', full_name='org.apache.beam.model.fn_execution.v1.LogEntry.thread', index=7,
       number=8, type=9, cpp_type=9, label=1,
       has_default_value=False, default_value=_b("").decode('utf-8'),
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
   ],
   extensions=[
   ],
   nested_types=[_LOGENTRY_LIST, _LOGENTRY_SEVERITY, ],
   enum_types=[
   ],
   options=None,
   is_extendable=False,
   syntax='proto3',
   extension_ranges=[],
   oneofs=[
   ],
-  serialized_start=9389,
-  serialized_end=9861,
+  serialized_start=11072,
+  serialized_end=11544,
 )
 
 
 _LOGCONTROL = _descriptor.Descriptor(
   name='LogControl',
   full_name='org.apache.beam.model.fn_execution.v1.LogControl',
   filename=None,
@@ -2912,53 +3201,61 @@
   ],
   options=None,
   is_extendable=False,
   syntax='proto3',
   extension_ranges=[],
   oneofs=[
   ],
-  serialized_start=9863,
-  serialized_end=9875,
+  serialized_start=11546,
+  serialized_end=11558,
 )
 
 _TARGET_LIST.fields_by_name['target'].message_type = _TARGET
 _TARGET_LIST.containing_type = _TARGET
 _REMOTEGRPCPORT.fields_by_name['api_service_descriptor'].message_type = endpoints__pb2._APISERVICEDESCRIPTOR
 _INSTRUCTIONREQUEST.fields_by_name['register'].message_type = _REGISTERREQUEST
 _INSTRUCTIONREQUEST.fields_by_name['process_bundle'].message_type = _PROCESSBUNDLEREQUEST
 _INSTRUCTIONREQUEST.fields_by_name['process_bundle_progress'].message_type = _PROCESSBUNDLEPROGRESSREQUEST
 _INSTRUCTIONREQUEST.fields_by_name['process_bundle_split'].message_type = _PROCESSBUNDLESPLITREQUEST
+_INSTRUCTIONREQUEST.fields_by_name['finalize_bundle'].message_type = _FINALIZEBUNDLEREQUEST
 _INSTRUCTIONREQUEST.oneofs_by_name['request'].fields.append(
   _INSTRUCTIONREQUEST.fields_by_name['register'])
 _INSTRUCTIONREQUEST.fields_by_name['register'].containing_oneof = _INSTRUCTIONREQUEST.oneofs_by_name['request']
 _INSTRUCTIONREQUEST.oneofs_by_name['request'].fields.append(
   _INSTRUCTIONREQUEST.fields_by_name['process_bundle'])
 _INSTRUCTIONREQUEST.fields_by_name['process_bundle'].containing_oneof = _INSTRUCTIONREQUEST.oneofs_by_name['request']
 _INSTRUCTIONREQUEST.oneofs_by_name['request'].fields.append(
   _INSTRUCTIONREQUEST.fields_by_name['process_bundle_progress'])
 _INSTRUCTIONREQUEST.fields_by_name['process_bundle_progress'].containing_oneof = _INSTRUCTIONREQUEST.oneofs_by_name['request']
 _INSTRUCTIONREQUEST.oneofs_by_name['request'].fields.append(
   _INSTRUCTIONREQUEST.fields_by_name['process_bundle_split'])
 _INSTRUCTIONREQUEST.fields_by_name['process_bundle_split'].containing_oneof = _INSTRUCTIONREQUEST.oneofs_by_name['request']
+_INSTRUCTIONREQUEST.oneofs_by_name['request'].fields.append(
+  _INSTRUCTIONREQUEST.fields_by_name['finalize_bundle'])
+_INSTRUCTIONREQUEST.fields_by_name['finalize_bundle'].containing_oneof = _INSTRUCTIONREQUEST.oneofs_by_name['request']
 _INSTRUCTIONRESPONSE.fields_by_name['register'].message_type = _REGISTERRESPONSE
 _INSTRUCTIONRESPONSE.fields_by_name['process_bundle'].message_type = _PROCESSBUNDLERESPONSE
 _INSTRUCTIONRESPONSE.fields_by_name['process_bundle_progress'].message_type = _PROCESSBUNDLEPROGRESSRESPONSE
 _INSTRUCTIONRESPONSE.fields_by_name['process_bundle_split'].message_type = _PROCESSBUNDLESPLITRESPONSE
+_INSTRUCTIONRESPONSE.fields_by_name['finalize_bundle'].message_type = _FINALIZEBUNDLERESPONSE
 _INSTRUCTIONRESPONSE.oneofs_by_name['response'].fields.append(
   _INSTRUCTIONRESPONSE.fields_by_name['register'])
 _INSTRUCTIONRESPONSE.fields_by_name['register'].containing_oneof = _INSTRUCTIONRESPONSE.oneofs_by_name['response']
 _INSTRUCTIONRESPONSE.oneofs_by_name['response'].fields.append(
   _INSTRUCTIONRESPONSE.fields_by_name['process_bundle'])
 _INSTRUCTIONRESPONSE.fields_by_name['process_bundle'].containing_oneof = _INSTRUCTIONRESPONSE.oneofs_by_name['response']
 _INSTRUCTIONRESPONSE.oneofs_by_name['response'].fields.append(
   _INSTRUCTIONRESPONSE.fields_by_name['process_bundle_progress'])
 _INSTRUCTIONRESPONSE.fields_by_name['process_bundle_progress'].containing_oneof = _INSTRUCTIONRESPONSE.oneofs_by_name['response']
 _INSTRUCTIONRESPONSE.oneofs_by_name['response'].fields.append(
   _INSTRUCTIONRESPONSE.fields_by_name['process_bundle_split'])
 _INSTRUCTIONRESPONSE.fields_by_name['process_bundle_split'].containing_oneof = _INSTRUCTIONRESPONSE.oneofs_by_name['response']
+_INSTRUCTIONRESPONSE.oneofs_by_name['response'].fields.append(
+  _INSTRUCTIONRESPONSE.fields_by_name['finalize_bundle'])
+_INSTRUCTIONRESPONSE.fields_by_name['finalize_bundle'].containing_oneof = _INSTRUCTIONRESPONSE.oneofs_by_name['response']
 _REGISTERREQUEST.fields_by_name['process_bundle_descriptor'].message_type = _PROCESSBUNDLEDESCRIPTOR
 _PROCESSBUNDLEDESCRIPTOR_TRANSFORMSENTRY.fields_by_name['value'].message_type = beam__runner__api__pb2._PTRANSFORM
 _PROCESSBUNDLEDESCRIPTOR_TRANSFORMSENTRY.containing_type = _PROCESSBUNDLEDESCRIPTOR
 _PROCESSBUNDLEDESCRIPTOR_PCOLLECTIONSENTRY.fields_by_name['value'].message_type = beam__runner__api__pb2._PCOLLECTION
 _PROCESSBUNDLEDESCRIPTOR_PCOLLECTIONSENTRY.containing_type = _PROCESSBUNDLEDESCRIPTOR
 _PROCESSBUNDLEDESCRIPTOR_WINDOWINGSTRATEGIESENTRY.fields_by_name['value'].message_type = beam__runner__api__pb2._WINDOWINGSTRATEGY
 _PROCESSBUNDLEDESCRIPTOR_WINDOWINGSTRATEGIESENTRY.containing_type = _PROCESSBUNDLEDESCRIPTOR
@@ -2968,35 +3265,46 @@
 _PROCESSBUNDLEDESCRIPTOR_ENVIRONMENTSENTRY.containing_type = _PROCESSBUNDLEDESCRIPTOR
 _PROCESSBUNDLEDESCRIPTOR.fields_by_name['transforms'].message_type = _PROCESSBUNDLEDESCRIPTOR_TRANSFORMSENTRY
 _PROCESSBUNDLEDESCRIPTOR.fields_by_name['pcollections'].message_type = _PROCESSBUNDLEDESCRIPTOR_PCOLLECTIONSENTRY
 _PROCESSBUNDLEDESCRIPTOR.fields_by_name['windowing_strategies'].message_type = _PROCESSBUNDLEDESCRIPTOR_WINDOWINGSTRATEGIESENTRY
 _PROCESSBUNDLEDESCRIPTOR.fields_by_name['coders'].message_type = _PROCESSBUNDLEDESCRIPTOR_CODERSENTRY
 _PROCESSBUNDLEDESCRIPTOR.fields_by_name['environments'].message_type = _PROCESSBUNDLEDESCRIPTOR_ENVIRONMENTSENTRY
 _PROCESSBUNDLEDESCRIPTOR.fields_by_name['state_api_service_descriptor'].message_type = endpoints__pb2._APISERVICEDESCRIPTOR
-_BUNDLESPLIT_APPLICATION_OUTPUTWATERMARKSENTRY.containing_type = _BUNDLESPLIT_APPLICATION
-_BUNDLESPLIT_APPLICATION.fields_by_name['output_watermarks'].message_type = _BUNDLESPLIT_APPLICATION_OUTPUTWATERMARKSENTRY
-_BUNDLESPLIT_APPLICATION.fields_by_name['fraction_of_work'].message_type = google_dot_protobuf_dot_wrappers__pb2._DOUBLEVALUE
-_BUNDLESPLIT_APPLICATION.containing_type = _BUNDLESPLIT
-_BUNDLESPLIT_DELAYEDAPPLICATION.fields_by_name['application'].message_type = _BUNDLESPLIT_APPLICATION
-_BUNDLESPLIT_DELAYEDAPPLICATION.containing_type = _BUNDLESPLIT
-_BUNDLESPLIT.fields_by_name['primary_roots'].message_type = _BUNDLESPLIT_APPLICATION
-_BUNDLESPLIT.fields_by_name['residual_roots'].message_type = _BUNDLESPLIT_DELAYEDAPPLICATION
+_BUNDLEAPPLICATION_OUTPUTWATERMARKSENTRY.fields_by_name['value'].message_type = google_dot_protobuf_dot_timestamp__pb2._TIMESTAMP
+_BUNDLEAPPLICATION_OUTPUTWATERMARKSENTRY.containing_type = _BUNDLEAPPLICATION
+_BUNDLEAPPLICATION_BACKLOG.containing_type = _BUNDLEAPPLICATION
+_BUNDLEAPPLICATION_BACKLOG.oneofs_by_name['value'].fields.append(
+  _BUNDLEAPPLICATION_BACKLOG.fields_by_name['bytes'])
+_BUNDLEAPPLICATION_BACKLOG.fields_by_name['bytes'].containing_oneof = _BUNDLEAPPLICATION_BACKLOG.oneofs_by_name['value']
+_BUNDLEAPPLICATION_BACKLOG.oneofs_by_name['value'].fields.append(
+  _BUNDLEAPPLICATION_BACKLOG.fields_by_name['is_unknown'])
+_BUNDLEAPPLICATION_BACKLOG.fields_by_name['is_unknown'].containing_oneof = _BUNDLEAPPLICATION_BACKLOG.oneofs_by_name['value']
+_BUNDLEAPPLICATION.fields_by_name['output_watermarks'].message_type = _BUNDLEAPPLICATION_OUTPUTWATERMARKSENTRY
+_BUNDLEAPPLICATION.fields_by_name['backlog'].message_type = _BUNDLEAPPLICATION_BACKLOG
+_BUNDLEAPPLICATION.fields_by_name['is_bounded'].enum_type = beam__runner__api__pb2._ISBOUNDED_ENUM
+_BUNDLEAPPLICATION.fields_by_name['monitoring_infos'].message_type = _MONITORINGINFO
+_DELAYEDBUNDLEAPPLICATION.fields_by_name['requested_execution_time'].message_type = google_dot_protobuf_dot_timestamp__pb2._TIMESTAMP
+_DELAYEDBUNDLEAPPLICATION.fields_by_name['application'].message_type = _BUNDLEAPPLICATION
 _PROCESSBUNDLERESPONSE.fields_by_name['metrics'].message_type = _METRICS
-_PROCESSBUNDLERESPONSE.fields_by_name['split'].message_type = _BUNDLESPLIT
+_PROCESSBUNDLERESPONSE.fields_by_name['residual_roots'].message_type = _DELAYEDBUNDLEAPPLICATION
+_PROCESSBUNDLERESPONSE.fields_by_name['monitoring_infos'].message_type = _MONITORINGINFO
 _MONITORINGINFO_LABELSENTRY.containing_type = _MONITORINGINFO
 _MONITORINGINFO.fields_by_name['monitoring_table_data'].message_type = _MONITORINGTABLEDATA
 _MONITORINGINFO.fields_by_name['metric'].message_type = _METRIC
 _MONITORINGINFO.fields_by_name['labels'].message_type = _MONITORINGINFO_LABELSENTRY
+_MONITORINGINFO.fields_by_name['timestamp'].message_type = google_dot_protobuf_dot_timestamp__pb2._TIMESTAMP
 _MONITORINGINFO_MONITORINGINFOLABELS.containing_type = _MONITORINGINFO
 _MONITORINGINFO.oneofs_by_name['data'].fields.append(
   _MONITORINGINFO.fields_by_name['monitoring_table_data'])
 _MONITORINGINFO.fields_by_name['monitoring_table_data'].containing_oneof = _MONITORINGINFO.oneofs_by_name['data']
 _MONITORINGINFO.oneofs_by_name['data'].fields.append(
   _MONITORINGINFO.fields_by_name['metric'])
 _MONITORINGINFO.fields_by_name['metric'].containing_oneof = _MONITORINGINFO.oneofs_by_name['data']
+_MONITORINGINFOURNS_ENUM.containing_type = _MONITORINGINFOURNS
+_MONITORINGINFOTYPEURNS_ENUM.containing_type = _MONITORINGINFOTYPEURNS
 _METRIC.fields_by_name['counter_data'].message_type = _COUNTERDATA
 _METRIC.fields_by_name['distribution_data'].message_type = _DISTRIBUTIONDATA
 _METRIC.fields_by_name['extrema_data'].message_type = _EXTREMADATA
 _METRIC.oneofs_by_name['data'].fields.append(
   _METRIC.fields_by_name['counter_data'])
 _METRIC.fields_by_name['counter_data'].containing_oneof = _METRIC.oneofs_by_name['data']
 _METRIC.oneofs_by_name['data'].fields.append(
@@ -3083,16 +3391,20 @@
 _METRICS_USER.oneofs_by_name['data'].fields.append(
   _METRICS_USER.fields_by_name['gauge_data'])
 _METRICS_USER.fields_by_name['gauge_data'].containing_oneof = _METRICS_USER.oneofs_by_name['data']
 _METRICS_PTRANSFORMSENTRY.fields_by_name['value'].message_type = _METRICS_PTRANSFORM
 _METRICS_PTRANSFORMSENTRY.containing_type = _METRICS
 _METRICS.fields_by_name['ptransforms'].message_type = _METRICS_PTRANSFORMSENTRY
 _PROCESSBUNDLEPROGRESSRESPONSE.fields_by_name['metrics'].message_type = _METRICS
-_PROCESSBUNDLEPROGRESSRESPONSE.fields_by_name['split'].message_type = _BUNDLESPLIT
-_PROCESSBUNDLESPLITREQUEST.fields_by_name['fraction_of_remainder'].message_type = google_dot_protobuf_dot_wrappers__pb2._DOUBLEVALUE
+_PROCESSBUNDLEPROGRESSRESPONSE.fields_by_name['monitoring_infos'].message_type = _MONITORINGINFO
+_PROCESSBUNDLEPROGRESSRESPONSE.fields_by_name['primary_roots'].message_type = _BUNDLEAPPLICATION
+_PROCESSBUNDLESPLITREQUEST_BACKLOGREMAININGENTRY.containing_type = _PROCESSBUNDLESPLITREQUEST
+_PROCESSBUNDLESPLITREQUEST.fields_by_name['backlog_remaining'].message_type = _PROCESSBUNDLESPLITREQUEST_BACKLOGREMAININGENTRY
+_PROCESSBUNDLESPLITRESPONSE.fields_by_name['primary_roots'].message_type = _BUNDLEAPPLICATION
+_PROCESSBUNDLESPLITRESPONSE.fields_by_name['residual_roots'].message_type = _DELAYEDBUNDLEAPPLICATION
 _ELEMENTS_DATA.fields_by_name['target'].message_type = _TARGET
 _ELEMENTS_DATA.containing_type = _ELEMENTS
 _ELEMENTS.fields_by_name['data'].message_type = _ELEMENTS_DATA
 _STATEREQUEST.fields_by_name['state_key'].message_type = _STATEKEY
 _STATEREQUEST.fields_by_name['get'].message_type = _STATEGETREQUEST
 _STATEREQUEST.fields_by_name['append'].message_type = _STATEAPPENDREQUEST
 _STATEREQUEST.fields_by_name['clear'].message_type = _STATECLEARREQUEST
@@ -3141,45 +3453,49 @@
 DESCRIPTOR.message_types_by_name['Target'] = _TARGET
 DESCRIPTOR.message_types_by_name['RemoteGrpcPort'] = _REMOTEGRPCPORT
 DESCRIPTOR.message_types_by_name['InstructionRequest'] = _INSTRUCTIONREQUEST
 DESCRIPTOR.message_types_by_name['InstructionResponse'] = _INSTRUCTIONRESPONSE
 DESCRIPTOR.message_types_by_name['RegisterRequest'] = _REGISTERREQUEST
 DESCRIPTOR.message_types_by_name['RegisterResponse'] = _REGISTERRESPONSE
 DESCRIPTOR.message_types_by_name['ProcessBundleDescriptor'] = _PROCESSBUNDLEDESCRIPTOR
-DESCRIPTOR.message_types_by_name['BundleSplit'] = _BUNDLESPLIT
+DESCRIPTOR.message_types_by_name['BundleApplication'] = _BUNDLEAPPLICATION
+DESCRIPTOR.message_types_by_name['DelayedBundleApplication'] = _DELAYEDBUNDLEAPPLICATION
 DESCRIPTOR.message_types_by_name['ProcessBundleRequest'] = _PROCESSBUNDLEREQUEST
 DESCRIPTOR.message_types_by_name['ProcessBundleResponse'] = _PROCESSBUNDLERESPONSE
 DESCRIPTOR.message_types_by_name['ProcessBundleProgressRequest'] = _PROCESSBUNDLEPROGRESSREQUEST
 DESCRIPTOR.message_types_by_name['MonitoringInfo'] = _MONITORINGINFO
+DESCRIPTOR.message_types_by_name['MonitoringInfoUrns'] = _MONITORINGINFOURNS
+DESCRIPTOR.message_types_by_name['MonitoringInfoTypeUrns'] = _MONITORINGINFOTYPEURNS
 DESCRIPTOR.message_types_by_name['Metric'] = _METRIC
 DESCRIPTOR.message_types_by_name['CounterData'] = _COUNTERDATA
 DESCRIPTOR.message_types_by_name['ExtremaData'] = _EXTREMADATA
 DESCRIPTOR.message_types_by_name['IntExtremaData'] = _INTEXTREMADATA
 DESCRIPTOR.message_types_by_name['DoubleExtremaData'] = _DOUBLEEXTREMADATA
 DESCRIPTOR.message_types_by_name['DistributionData'] = _DISTRIBUTIONDATA
 DESCRIPTOR.message_types_by_name['IntDistributionData'] = _INTDISTRIBUTIONDATA
 DESCRIPTOR.message_types_by_name['DoubleDistributionData'] = _DOUBLEDISTRIBUTIONDATA
 DESCRIPTOR.message_types_by_name['MonitoringTableData'] = _MONITORINGTABLEDATA
 DESCRIPTOR.message_types_by_name['Metrics'] = _METRICS
 DESCRIPTOR.message_types_by_name['ProcessBundleProgressResponse'] = _PROCESSBUNDLEPROGRESSRESPONSE
 DESCRIPTOR.message_types_by_name['ProcessBundleSplitRequest'] = _PROCESSBUNDLESPLITREQUEST
 DESCRIPTOR.message_types_by_name['ProcessBundleSplitResponse'] = _PROCESSBUNDLESPLITRESPONSE
+DESCRIPTOR.message_types_by_name['FinalizeBundleRequest'] = _FINALIZEBUNDLEREQUEST
+DESCRIPTOR.message_types_by_name['FinalizeBundleResponse'] = _FINALIZEBUNDLERESPONSE
 DESCRIPTOR.message_types_by_name['Elements'] = _ELEMENTS
 DESCRIPTOR.message_types_by_name['StateRequest'] = _STATEREQUEST
 DESCRIPTOR.message_types_by_name['StateResponse'] = _STATERESPONSE
 DESCRIPTOR.message_types_by_name['StateKey'] = _STATEKEY
 DESCRIPTOR.message_types_by_name['StateGetRequest'] = _STATEGETREQUEST
 DESCRIPTOR.message_types_by_name['StateGetResponse'] = _STATEGETRESPONSE
 DESCRIPTOR.message_types_by_name['StateAppendRequest'] = _STATEAPPENDREQUEST
 DESCRIPTOR.message_types_by_name['StateAppendResponse'] = _STATEAPPENDRESPONSE
 DESCRIPTOR.message_types_by_name['StateClearRequest'] = _STATECLEARREQUEST
 DESCRIPTOR.message_types_by_name['StateClearResponse'] = _STATECLEARRESPONSE
 DESCRIPTOR.message_types_by_name['LogEntry'] = _LOGENTRY
 DESCRIPTOR.message_types_by_name['LogControl'] = _LOGCONTROL
-_sym_db.RegisterFileDescriptor(DESCRIPTOR)
 
 Target = _reflection.GeneratedProtocolMessageType('Target', (_message.Message,), dict(
 
   List = _reflection.GeneratedProtocolMessageType('List', (_message.Message,), dict(
     DESCRIPTOR = _TARGET_LIST,
     __module__ = 'beam_fn_api_pb2'
     # @@protoc_insertion_point(class_scope:org.apache.beam.model.fn_execution.v1.Target.List)
@@ -3270,44 +3586,43 @@
 _sym_db.RegisterMessage(ProcessBundleDescriptor)
 _sym_db.RegisterMessage(ProcessBundleDescriptor.TransformsEntry)
 _sym_db.RegisterMessage(ProcessBundleDescriptor.PcollectionsEntry)
 _sym_db.RegisterMessage(ProcessBundleDescriptor.WindowingStrategiesEntry)
 _sym_db.RegisterMessage(ProcessBundleDescriptor.CodersEntry)
 _sym_db.RegisterMessage(ProcessBundleDescriptor.EnvironmentsEntry)
 
-BundleSplit = _reflection.GeneratedProtocolMessageType('BundleSplit', (_message.Message,), dict(
-
-  Application = _reflection.GeneratedProtocolMessageType('Application', (_message.Message,), dict(
+BundleApplication = _reflection.GeneratedProtocolMessageType('BundleApplication', (_message.Message,), dict(
 
-    OutputWatermarksEntry = _reflection.GeneratedProtocolMessageType('OutputWatermarksEntry', (_message.Message,), dict(
-      DESCRIPTOR = _BUNDLESPLIT_APPLICATION_OUTPUTWATERMARKSENTRY,
-      __module__ = 'beam_fn_api_pb2'
-      # @@protoc_insertion_point(class_scope:org.apache.beam.model.fn_execution.v1.BundleSplit.Application.OutputWatermarksEntry)
-      ))
-    ,
-    DESCRIPTOR = _BUNDLESPLIT_APPLICATION,
+  OutputWatermarksEntry = _reflection.GeneratedProtocolMessageType('OutputWatermarksEntry', (_message.Message,), dict(
+    DESCRIPTOR = _BUNDLEAPPLICATION_OUTPUTWATERMARKSENTRY,
     __module__ = 'beam_fn_api_pb2'
-    # @@protoc_insertion_point(class_scope:org.apache.beam.model.fn_execution.v1.BundleSplit.Application)
+    # @@protoc_insertion_point(class_scope:org.apache.beam.model.fn_execution.v1.BundleApplication.OutputWatermarksEntry)
     ))
   ,
 
-  DelayedApplication = _reflection.GeneratedProtocolMessageType('DelayedApplication', (_message.Message,), dict(
-    DESCRIPTOR = _BUNDLESPLIT_DELAYEDAPPLICATION,
+  Backlog = _reflection.GeneratedProtocolMessageType('Backlog', (_message.Message,), dict(
+    DESCRIPTOR = _BUNDLEAPPLICATION_BACKLOG,
     __module__ = 'beam_fn_api_pb2'
-    # @@protoc_insertion_point(class_scope:org.apache.beam.model.fn_execution.v1.BundleSplit.DelayedApplication)
+    # @@protoc_insertion_point(class_scope:org.apache.beam.model.fn_execution.v1.BundleApplication.Backlog)
     ))
   ,
-  DESCRIPTOR = _BUNDLESPLIT,
+  DESCRIPTOR = _BUNDLEAPPLICATION,
   __module__ = 'beam_fn_api_pb2'
-  # @@protoc_insertion_point(class_scope:org.apache.beam.model.fn_execution.v1.BundleSplit)
+  # @@protoc_insertion_point(class_scope:org.apache.beam.model.fn_execution.v1.BundleApplication)
   ))
-_sym_db.RegisterMessage(BundleSplit)
-_sym_db.RegisterMessage(BundleSplit.Application)
-_sym_db.RegisterMessage(BundleSplit.Application.OutputWatermarksEntry)
-_sym_db.RegisterMessage(BundleSplit.DelayedApplication)
+_sym_db.RegisterMessage(BundleApplication)
+_sym_db.RegisterMessage(BundleApplication.OutputWatermarksEntry)
+_sym_db.RegisterMessage(BundleApplication.Backlog)
+
+DelayedBundleApplication = _reflection.GeneratedProtocolMessageType('DelayedBundleApplication', (_message.Message,), dict(
+  DESCRIPTOR = _DELAYEDBUNDLEAPPLICATION,
+  __module__ = 'beam_fn_api_pb2'
+  # @@protoc_insertion_point(class_scope:org.apache.beam.model.fn_execution.v1.DelayedBundleApplication)
+  ))
+_sym_db.RegisterMessage(DelayedBundleApplication)
 
 ProcessBundleRequest = _reflection.GeneratedProtocolMessageType('ProcessBundleRequest', (_message.Message,), dict(
   DESCRIPTOR = _PROCESSBUNDLEREQUEST,
   __module__ = 'beam_fn_api_pb2'
   # @@protoc_insertion_point(class_scope:org.apache.beam.model.fn_execution.v1.ProcessBundleRequest)
   ))
 _sym_db.RegisterMessage(ProcessBundleRequest)
@@ -3337,14 +3652,28 @@
   DESCRIPTOR = _MONITORINGINFO,
   __module__ = 'beam_fn_api_pb2'
   # @@protoc_insertion_point(class_scope:org.apache.beam.model.fn_execution.v1.MonitoringInfo)
   ))
 _sym_db.RegisterMessage(MonitoringInfo)
 _sym_db.RegisterMessage(MonitoringInfo.LabelsEntry)
 
+MonitoringInfoUrns = _reflection.GeneratedProtocolMessageType('MonitoringInfoUrns', (_message.Message,), dict(
+  DESCRIPTOR = _MONITORINGINFOURNS,
+  __module__ = 'beam_fn_api_pb2'
+  # @@protoc_insertion_point(class_scope:org.apache.beam.model.fn_execution.v1.MonitoringInfoUrns)
+  ))
+_sym_db.RegisterMessage(MonitoringInfoUrns)
+
+MonitoringInfoTypeUrns = _reflection.GeneratedProtocolMessageType('MonitoringInfoTypeUrns', (_message.Message,), dict(
+  DESCRIPTOR = _MONITORINGINFOTYPEURNS,
+  __module__ = 'beam_fn_api_pb2'
+  # @@protoc_insertion_point(class_scope:org.apache.beam.model.fn_execution.v1.MonitoringInfoTypeUrns)
+  ))
+_sym_db.RegisterMessage(MonitoringInfoTypeUrns)
+
 Metric = _reflection.GeneratedProtocolMessageType('Metric', (_message.Message,), dict(
   DESCRIPTOR = _METRIC,
   __module__ = 'beam_fn_api_pb2'
   # @@protoc_insertion_point(class_scope:org.apache.beam.model.fn_execution.v1.Metric)
   ))
 _sym_db.RegisterMessage(Metric)
 
@@ -3543,27 +3872,49 @@
   DESCRIPTOR = _PROCESSBUNDLEPROGRESSRESPONSE,
   __module__ = 'beam_fn_api_pb2'
   # @@protoc_insertion_point(class_scope:org.apache.beam.model.fn_execution.v1.ProcessBundleProgressResponse)
   ))
 _sym_db.RegisterMessage(ProcessBundleProgressResponse)
 
 ProcessBundleSplitRequest = _reflection.GeneratedProtocolMessageType('ProcessBundleSplitRequest', (_message.Message,), dict(
+
+  BacklogRemainingEntry = _reflection.GeneratedProtocolMessageType('BacklogRemainingEntry', (_message.Message,), dict(
+    DESCRIPTOR = _PROCESSBUNDLESPLITREQUEST_BACKLOGREMAININGENTRY,
+    __module__ = 'beam_fn_api_pb2'
+    # @@protoc_insertion_point(class_scope:org.apache.beam.model.fn_execution.v1.ProcessBundleSplitRequest.BacklogRemainingEntry)
+    ))
+  ,
   DESCRIPTOR = _PROCESSBUNDLESPLITREQUEST,
   __module__ = 'beam_fn_api_pb2'
   # @@protoc_insertion_point(class_scope:org.apache.beam.model.fn_execution.v1.ProcessBundleSplitRequest)
   ))
 _sym_db.RegisterMessage(ProcessBundleSplitRequest)
+_sym_db.RegisterMessage(ProcessBundleSplitRequest.BacklogRemainingEntry)
 
 ProcessBundleSplitResponse = _reflection.GeneratedProtocolMessageType('ProcessBundleSplitResponse', (_message.Message,), dict(
   DESCRIPTOR = _PROCESSBUNDLESPLITRESPONSE,
   __module__ = 'beam_fn_api_pb2'
   # @@protoc_insertion_point(class_scope:org.apache.beam.model.fn_execution.v1.ProcessBundleSplitResponse)
   ))
 _sym_db.RegisterMessage(ProcessBundleSplitResponse)
 
+FinalizeBundleRequest = _reflection.GeneratedProtocolMessageType('FinalizeBundleRequest', (_message.Message,), dict(
+  DESCRIPTOR = _FINALIZEBUNDLEREQUEST,
+  __module__ = 'beam_fn_api_pb2'
+  # @@protoc_insertion_point(class_scope:org.apache.beam.model.fn_execution.v1.FinalizeBundleRequest)
+  ))
+_sym_db.RegisterMessage(FinalizeBundleRequest)
+
+FinalizeBundleResponse = _reflection.GeneratedProtocolMessageType('FinalizeBundleResponse', (_message.Message,), dict(
+  DESCRIPTOR = _FINALIZEBUNDLERESPONSE,
+  __module__ = 'beam_fn_api_pb2'
+  # @@protoc_insertion_point(class_scope:org.apache.beam.model.fn_execution.v1.FinalizeBundleResponse)
+  ))
+_sym_db.RegisterMessage(FinalizeBundleResponse)
+
 Elements = _reflection.GeneratedProtocolMessageType('Elements', (_message.Message,), dict(
 
   Data = _reflection.GeneratedProtocolMessageType('Data', (_message.Message,), dict(
     DESCRIPTOR = _ELEMENTS_DATA,
     __module__ = 'beam_fn_api_pb2'
     # @@protoc_insertion_point(class_scope:org.apache.beam.model.fn_execution.v1.Elements.Data)
     ))
@@ -3701,118 +4052,42 @@
 _PROCESSBUNDLEDESCRIPTOR_PCOLLECTIONSENTRY._options = _descriptor._ParseOptions(descriptor_pb2.MessageOptions(), _b('8\001'))
 _PROCESSBUNDLEDESCRIPTOR_WINDOWINGSTRATEGIESENTRY.has_options = True
 _PROCESSBUNDLEDESCRIPTOR_WINDOWINGSTRATEGIESENTRY._options = _descriptor._ParseOptions(descriptor_pb2.MessageOptions(), _b('8\001'))
 _PROCESSBUNDLEDESCRIPTOR_CODERSENTRY.has_options = True
 _PROCESSBUNDLEDESCRIPTOR_CODERSENTRY._options = _descriptor._ParseOptions(descriptor_pb2.MessageOptions(), _b('8\001'))
 _PROCESSBUNDLEDESCRIPTOR_ENVIRONMENTSENTRY.has_options = True
 _PROCESSBUNDLEDESCRIPTOR_ENVIRONMENTSENTRY._options = _descriptor._ParseOptions(descriptor_pb2.MessageOptions(), _b('8\001'))
-_BUNDLESPLIT_APPLICATION_OUTPUTWATERMARKSENTRY.has_options = True
-_BUNDLESPLIT_APPLICATION_OUTPUTWATERMARKSENTRY._options = _descriptor._ParseOptions(descriptor_pb2.MessageOptions(), _b('8\001'))
+_BUNDLEAPPLICATION_OUTPUTWATERMARKSENTRY.has_options = True
+_BUNDLEAPPLICATION_OUTPUTWATERMARKSENTRY._options = _descriptor._ParseOptions(descriptor_pb2.MessageOptions(), _b('8\001'))
 _MONITORINGINFO_LABELSENTRY.has_options = True
 _MONITORINGINFO_LABELSENTRY._options = _descriptor._ParseOptions(descriptor_pb2.MessageOptions(), _b('8\001'))
+_MONITORINGINFOURNS_ENUM.values_by_name["USER_COUNTER_URN_PREFIX"].has_options = True
+_MONITORINGINFOURNS_ENUM.values_by_name["USER_COUNTER_URN_PREFIX"]._options = _descriptor._ParseOptions(descriptor_pb2.EnumValueOptions(), _b('\242\264\372\302\005\020beam:metric:user'))
+_MONITORINGINFOURNS_ENUM.values_by_name["ELEMENT_COUNT"].has_options = True
+_MONITORINGINFOURNS_ENUM.values_by_name["ELEMENT_COUNT"]._options = _descriptor._ParseOptions(descriptor_pb2.EnumValueOptions(), _b('\242\264\372\302\005\034beam:metric:element_count:v1'))
+_MONITORINGINFOURNS_ENUM.values_by_name["START_BUNDLE_MSECS"].has_options = True
+_MONITORINGINFOURNS_ENUM.values_by_name["START_BUNDLE_MSECS"]._options = _descriptor._ParseOptions(descriptor_pb2.EnumValueOptions(), _b('\242\264\372\302\0056beam:metric:pardo_execution_time:start_bundle_msecs:v1'))
+_MONITORINGINFOURNS_ENUM.values_by_name["PROCESS_BUNDLE_MSECS"].has_options = True
+_MONITORINGINFOURNS_ENUM.values_by_name["PROCESS_BUNDLE_MSECS"]._options = _descriptor._ParseOptions(descriptor_pb2.EnumValueOptions(), _b('\242\264\372\302\0058beam:metric:pardo_execution_time:process_bundle_msecs:v1'))
+_MONITORINGINFOURNS_ENUM.values_by_name["FINISH_BUNDLE_MSECS"].has_options = True
+_MONITORINGINFOURNS_ENUM.values_by_name["FINISH_BUNDLE_MSECS"]._options = _descriptor._ParseOptions(descriptor_pb2.EnumValueOptions(), _b('\242\264\372\302\0057beam:metric:pardo_execution_time:finish_bundle_msecs:v1'))
+_MONITORINGINFOURNS_ENUM.values_by_name["TOTAL_MSECS"].has_options = True
+_MONITORINGINFOURNS_ENUM.values_by_name["TOTAL_MSECS"]._options = _descriptor._ParseOptions(descriptor_pb2.EnumValueOptions(), _b('\242\264\372\302\0054beam:metric:ptransform_execution_time:total_msecs:v1'))
+_MONITORINGINFOTYPEURNS_ENUM.values_by_name["SUM_INT64_TYPE"].has_options = True
+_MONITORINGINFOTYPEURNS_ENUM.values_by_name["SUM_INT64_TYPE"]._options = _descriptor._ParseOptions(descriptor_pb2.EnumValueOptions(), _b('\242\264\372\302\005\027beam:metrics:sum_int_64'))
+_MONITORINGINFOTYPEURNS_ENUM.values_by_name["DISTRIBUTION_INT64_TYPE"].has_options = True
+_MONITORINGINFOTYPEURNS_ENUM.values_by_name["DISTRIBUTION_INT64_TYPE"]._options = _descriptor._ParseOptions(descriptor_pb2.EnumValueOptions(), _b('\242\264\372\302\005 beam:metrics:distribution_int_64'))
+_MONITORINGINFOTYPEURNS_ENUM.values_by_name["LATEST_INT64_TYPE"].has_options = True
+_MONITORINGINFOTYPEURNS_ENUM.values_by_name["LATEST_INT64_TYPE"]._options = _descriptor._ParseOptions(descriptor_pb2.EnumValueOptions(), _b('\242\264\372\302\005\032beam:metrics:latest_int_64'))
 _METRICS_PTRANSFORM_MEASURED_INPUTELEMENTCOUNTSENTRY.has_options = True
 _METRICS_PTRANSFORM_MEASURED_INPUTELEMENTCOUNTSENTRY._options = _descriptor._ParseOptions(descriptor_pb2.MessageOptions(), _b('8\001'))
 _METRICS_PTRANSFORM_MEASURED_OUTPUTELEMENTCOUNTSENTRY.has_options = True
 _METRICS_PTRANSFORM_MEASURED_OUTPUTELEMENTCOUNTSENTRY._options = _descriptor._ParseOptions(descriptor_pb2.MessageOptions(), _b('8\001'))
 _METRICS_PTRANSFORM_ACTIVEELEMENTS_OUTPUTELEMENTSREMAININGENTRY.has_options = True
 _METRICS_PTRANSFORM_ACTIVEELEMENTS_OUTPUTELEMENTSREMAININGENTRY._options = _descriptor._ParseOptions(descriptor_pb2.MessageOptions(), _b('8\001'))
 _METRICS_PTRANSFORM_WATERMARKSENTRY.has_options = True
 _METRICS_PTRANSFORM_WATERMARKSENTRY._options = _descriptor._ParseOptions(descriptor_pb2.MessageOptions(), _b('8\001'))
 _METRICS_PTRANSFORMSENTRY.has_options = True
 _METRICS_PTRANSFORMSENTRY._options = _descriptor._ParseOptions(descriptor_pb2.MessageOptions(), _b('8\001'))
-
-_BEAMFNCONTROL = _descriptor.ServiceDescriptor(
-  name='BeamFnControl',
-  full_name='org.apache.beam.model.fn_execution.v1.BeamFnControl',
-  file=DESCRIPTOR,
-  index=0,
-  options=None,
-  serialized_start=9878,
-  serialized_end=10030,
-  methods=[
-  _descriptor.MethodDescriptor(
-    name='Control',
-    full_name='org.apache.beam.model.fn_execution.v1.BeamFnControl.Control',
-    index=0,
-    containing_service=None,
-    input_type=_INSTRUCTIONRESPONSE,
-    output_type=_INSTRUCTIONREQUEST,
-    options=None,
-  ),
-])
-_sym_db.RegisterServiceDescriptor(_BEAMFNCONTROL)
-
-DESCRIPTOR.services_by_name['BeamFnControl'] = _BEAMFNCONTROL
-
-
-_BEAMFNDATA = _descriptor.ServiceDescriptor(
-  name='BeamFnData',
-  full_name='org.apache.beam.model.fn_execution.v1.BeamFnData',
-  file=DESCRIPTOR,
-  index=1,
-  options=None,
-  serialized_start=10032,
-  serialized_end=10156,
-  methods=[
-  _descriptor.MethodDescriptor(
-    name='Data',
-    full_name='org.apache.beam.model.fn_execution.v1.BeamFnData.Data',
-    index=0,
-    containing_service=None,
-    input_type=_ELEMENTS,
-    output_type=_ELEMENTS,
-    options=None,
-  ),
-])
-_sym_db.RegisterServiceDescriptor(_BEAMFNDATA)
-
-DESCRIPTOR.services_by_name['BeamFnData'] = _BEAMFNDATA
-
-
-_BEAMFNSTATE = _descriptor.ServiceDescriptor(
-  name='BeamFnState',
-  full_name='org.apache.beam.model.fn_execution.v1.BeamFnState',
-  file=DESCRIPTOR,
-  index=2,
-  options=None,
-  serialized_start=10159,
-  serialized_end=10294,
-  methods=[
-  _descriptor.MethodDescriptor(
-    name='State',
-    full_name='org.apache.beam.model.fn_execution.v1.BeamFnState.State',
-    index=0,
-    containing_service=None,
-    input_type=_STATEREQUEST,
-    output_type=_STATERESPONSE,
-    options=None,
-  ),
-])
-_sym_db.RegisterServiceDescriptor(_BEAMFNSTATE)
-
-DESCRIPTOR.services_by_name['BeamFnState'] = _BEAMFNSTATE
-
-
-_BEAMFNLOGGING = _descriptor.ServiceDescriptor(
-  name='BeamFnLogging',
-  full_name='org.apache.beam.model.fn_execution.v1.BeamFnLogging',
-  file=DESCRIPTOR,
-  index=3,
-  options=None,
-  serialized_start=10297,
-  serialized_end=10434,
-  methods=[
-  _descriptor.MethodDescriptor(
-    name='Logging',
-    full_name='org.apache.beam.model.fn_execution.v1.BeamFnLogging.Logging',
-    index=0,
-    containing_service=None,
-    input_type=_LOGENTRY_LIST,
-    output_type=_LOGCONTROL,
-    options=None,
-  ),
-])
-_sym_db.RegisterServiceDescriptor(_BEAMFNLOGGING)
-
-DESCRIPTOR.services_by_name['BeamFnLogging'] = _BEAMFNLOGGING
-
+_PROCESSBUNDLESPLITREQUEST_BACKLOGREMAININGENTRY.has_options = True
+_PROCESSBUNDLESPLITREQUEST_BACKLOGREMAININGENTRY._options = _descriptor._ParseOptions(descriptor_pb2.MessageOptions(), _b('8\001'))
 # @@protoc_insertion_point(module_scope)
```

## Comparing `apache-beam-2.8.0/apache_beam/portability/api/beam_job_api_pb2_grpc.py` & `apache-beam-2.9.0/apache_beam/portability/api/beam_job_api_pb2_grpc.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/portability/api/beam_provision_api_pb2.py` & `apache-beam-2.9.0/apache_beam/portability/api/beam_provision_api_pb2.py`

 * *Files 4% similar despite different names*

```diff
@@ -19,14 +19,15 @@
 DESCRIPTOR = _descriptor.FileDescriptor(
   name='beam_provision_api.proto',
   package='org.apache.beam.model.fn_execution.v1',
   syntax='proto3',
   serialized_pb=_b('\n\x18\x62\x65\x61m_provision_api.proto\x12%org.apache.beam.model.fn_execution.v1\x1a\x1cgoogle/protobuf/struct.proto\"\x19\n\x17GetProvisionInfoRequest\"^\n\x18GetProvisionInfoResponse\x12\x42\n\x04info\x18\x01 \x01(\x0b\x32\x34.org.apache.beam.model.fn_execution.v1.ProvisionInfo\"\xdb\x01\n\rProvisionInfo\x12\x0e\n\x06job_id\x18\x01 \x01(\t\x12\x10\n\x08job_name\x18\x02 \x01(\t\x12\x11\n\tworker_id\x18\x05 \x01(\t\x12\x31\n\x10pipeline_options\x18\x03 \x01(\x0b\x32\x17.google.protobuf.Struct\x12I\n\x0fresource_limits\x18\x04 \x01(\x0b\x32\x30.org.apache.beam.model.fn_execution.v1.Resources\x12\x17\n\x0fretrieval_token\x18\x06 \x01(\t\"\xb1\x02\n\tResources\x12G\n\x06memory\x18\x01 \x01(\x0b\x32\x37.org.apache.beam.model.fn_execution.v1.Resources.Memory\x12\x41\n\x03\x63pu\x18\x02 \x01(\x0b\x32\x34.org.apache.beam.model.fn_execution.v1.Resources.Cpu\x12S\n\x14semi_persistent_disk\x18\x03 \x01(\x0b\x32\x35.org.apache.beam.model.fn_execution.v1.Resources.Disk\x1a\x16\n\x06Memory\x12\x0c\n\x04size\x18\x01 \x01(\x04\x1a\x15\n\x03\x43pu\x12\x0e\n\x06shares\x18\x01 \x01(\x02\x1a\x14\n\x04\x44isk\x12\x0c\n\x04size\x18\x01 \x01(\x04\x32\xa8\x01\n\x10ProvisionService\x12\x93\x01\n\x10GetProvisionInfo\x12>.org.apache.beam.model.fn_execution.v1.GetProvisionInfoRequest\x1a?.org.apache.beam.model.fn_execution.v1.GetProvisionInfoResponseBD\n$org.apache.beam.model.fnexecution.v1B\x0cProvisionApiZ\x0e\x66nexecution_v1b\x06proto3')
   ,
   dependencies=[google_dot_protobuf_dot_struct__pb2.DESCRIPTOR,])
+_sym_db.RegisterFileDescriptor(DESCRIPTOR)
 
 
 
 
 _GETPROVISIONINFOREQUEST = _descriptor.Descriptor(
   name='GetProvisionInfoRequest',
   full_name='org.apache.beam.model.fn_execution.v1.GetProvisionInfoRequest',
@@ -60,15 +61,15 @@
   fields=[
     _descriptor.FieldDescriptor(
       name='info', full_name='org.apache.beam.model.fn_execution.v1.GetProvisionInfoResponse.info', index=0,
       number=1, type=11, cpp_type=10, label=1,
       has_default_value=False, default_value=None,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
   ],
   extensions=[
   ],
   nested_types=[],
   enum_types=[
   ],
   options=None,
@@ -91,50 +92,50 @@
   fields=[
     _descriptor.FieldDescriptor(
       name='job_id', full_name='org.apache.beam.model.fn_execution.v1.ProvisionInfo.job_id', index=0,
       number=1, type=9, cpp_type=9, label=1,
       has_default_value=False, default_value=_b("").decode('utf-8'),
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='job_name', full_name='org.apache.beam.model.fn_execution.v1.ProvisionInfo.job_name', index=1,
       number=2, type=9, cpp_type=9, label=1,
       has_default_value=False, default_value=_b("").decode('utf-8'),
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='worker_id', full_name='org.apache.beam.model.fn_execution.v1.ProvisionInfo.worker_id', index=2,
       number=5, type=9, cpp_type=9, label=1,
       has_default_value=False, default_value=_b("").decode('utf-8'),
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='pipeline_options', full_name='org.apache.beam.model.fn_execution.v1.ProvisionInfo.pipeline_options', index=3,
       number=3, type=11, cpp_type=10, label=1,
       has_default_value=False, default_value=None,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='resource_limits', full_name='org.apache.beam.model.fn_execution.v1.ProvisionInfo.resource_limits', index=4,
       number=4, type=11, cpp_type=10, label=1,
       has_default_value=False, default_value=None,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='retrieval_token', full_name='org.apache.beam.model.fn_execution.v1.ProvisionInfo.retrieval_token', index=5,
       number=6, type=9, cpp_type=9, label=1,
       has_default_value=False, default_value=_b("").decode('utf-8'),
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
   ],
   extensions=[
   ],
   nested_types=[],
   enum_types=[
   ],
   options=None,
@@ -157,15 +158,15 @@
   fields=[
     _descriptor.FieldDescriptor(
       name='size', full_name='org.apache.beam.model.fn_execution.v1.Resources.Memory.size', index=0,
       number=1, type=4, cpp_type=4, label=1,
       has_default_value=False, default_value=0,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
   ],
   extensions=[
   ],
   nested_types=[],
   enum_types=[
   ],
   options=None,
@@ -187,15 +188,15 @@
   fields=[
     _descriptor.FieldDescriptor(
       name='shares', full_name='org.apache.beam.model.fn_execution.v1.Resources.Cpu.shares', index=0,
       number=1, type=2, cpp_type=6, label=1,
       has_default_value=False, default_value=float(0),
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
   ],
   extensions=[
   ],
   nested_types=[],
   enum_types=[
   ],
   options=None,
@@ -217,15 +218,15 @@
   fields=[
     _descriptor.FieldDescriptor(
       name='size', full_name='org.apache.beam.model.fn_execution.v1.Resources.Disk.size', index=0,
       number=1, type=4, cpp_type=4, label=1,
       has_default_value=False, default_value=0,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
   ],
   extensions=[
   ],
   nested_types=[],
   enum_types=[
   ],
   options=None,
@@ -247,29 +248,29 @@
   fields=[
     _descriptor.FieldDescriptor(
       name='memory', full_name='org.apache.beam.model.fn_execution.v1.Resources.memory', index=0,
       number=1, type=11, cpp_type=10, label=1,
       has_default_value=False, default_value=None,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='cpu', full_name='org.apache.beam.model.fn_execution.v1.Resources.cpu', index=1,
       number=2, type=11, cpp_type=10, label=1,
       has_default_value=False, default_value=None,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='semi_persistent_disk', full_name='org.apache.beam.model.fn_execution.v1.Resources.semi_persistent_disk', index=2,
       number=3, type=11, cpp_type=10, label=1,
       has_default_value=False, default_value=None,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
   ],
   extensions=[
   ],
   nested_types=[_RESOURCES_MEMORY, _RESOURCES_CPU, _RESOURCES_DISK, ],
   enum_types=[
   ],
   options=None,
@@ -291,15 +292,14 @@
 _RESOURCES.fields_by_name['memory'].message_type = _RESOURCES_MEMORY
 _RESOURCES.fields_by_name['cpu'].message_type = _RESOURCES_CPU
 _RESOURCES.fields_by_name['semi_persistent_disk'].message_type = _RESOURCES_DISK
 DESCRIPTOR.message_types_by_name['GetProvisionInfoRequest'] = _GETPROVISIONINFOREQUEST
 DESCRIPTOR.message_types_by_name['GetProvisionInfoResponse'] = _GETPROVISIONINFORESPONSE
 DESCRIPTOR.message_types_by_name['ProvisionInfo'] = _PROVISIONINFO
 DESCRIPTOR.message_types_by_name['Resources'] = _RESOURCES
-_sym_db.RegisterFileDescriptor(DESCRIPTOR)
 
 GetProvisionInfoRequest = _reflection.GeneratedProtocolMessageType('GetProvisionInfoRequest', (_message.Message,), dict(
   DESCRIPTOR = _GETPROVISIONINFOREQUEST,
   __module__ = 'beam_provision_api_pb2'
   # @@protoc_insertion_point(class_scope:org.apache.beam.model.fn_execution.v1.GetProvisionInfoRequest)
   ))
 _sym_db.RegisterMessage(GetProvisionInfoRequest)
@@ -348,32 +348,8 @@
 _sym_db.RegisterMessage(Resources.Memory)
 _sym_db.RegisterMessage(Resources.Cpu)
 _sym_db.RegisterMessage(Resources.Disk)
 
 
 DESCRIPTOR.has_options = True
 DESCRIPTOR._options = _descriptor._ParseOptions(descriptor_pb2.FileOptions(), _b('\n$org.apache.beam.model.fnexecution.v1B\014ProvisionApiZ\016fnexecution_v1'))
-
-_PROVISIONSERVICE = _descriptor.ServiceDescriptor(
-  name='ProvisionService',
-  full_name='org.apache.beam.model.fn_execution.v1.ProvisionService',
-  file=DESCRIPTOR,
-  index=0,
-  options=None,
-  serialized_start=751,
-  serialized_end=919,
-  methods=[
-  _descriptor.MethodDescriptor(
-    name='GetProvisionInfo',
-    full_name='org.apache.beam.model.fn_execution.v1.ProvisionService.GetProvisionInfo',
-    index=0,
-    containing_service=None,
-    input_type=_GETPROVISIONINFOREQUEST,
-    output_type=_GETPROVISIONINFORESPONSE,
-    options=None,
-  ),
-])
-_sym_db.RegisterServiceDescriptor(_PROVISIONSERVICE)
-
-DESCRIPTOR.services_by_name['ProvisionService'] = _PROVISIONSERVICE
-
 # @@protoc_insertion_point(module_scope)
```

## Comparing `apache-beam-2.8.0/apache_beam/portability/api/beam_artifact_api_pb2.py` & `apache-beam-2.9.0/apache_beam/portability/api/beam_artifact_api_pb2.py`

 * *Files 10% similar despite different names*

```diff
@@ -15,16 +15,17 @@
 
 
 
 DESCRIPTOR = _descriptor.FileDescriptor(
   name='beam_artifact_api.proto',
   package='org.apache.beam.model.job_management.v1',
   syntax='proto3',
-  serialized_pb=_b('\n\x17\x62\x65\x61m_artifact_api.proto\x12\'org.apache.beam.model.job_management.v1\"B\n\x10\x41rtifactMetadata\x12\x0c\n\x04name\x18\x01 \x01(\t\x12\x13\n\x0bpermissions\x18\x02 \x01(\r\x12\x0b\n\x03md5\x18\x03 \x01(\t\"W\n\x08Manifest\x12K\n\x08\x61rtifact\x18\x01 \x03(\x0b\x32\x39.org.apache.beam.model.job_management.v1.ArtifactMetadata\"\xce\x01\n\rProxyManifest\x12\x43\n\x08manifest\x18\x01 \x01(\x0b\x32\x31.org.apache.beam.model.job_management.v1.Manifest\x12Q\n\x08location\x18\x02 \x03(\x0b\x32?.org.apache.beam.model.job_management.v1.ProxyManifest.Location\x1a%\n\x08Location\x12\x0c\n\x04name\x18\x01 \x01(\t\x12\x0b\n\x03uri\x18\x02 \x01(\t\"-\n\x12GetManifestRequest\x12\x17\n\x0fretrieval_token\x18\x01 \x01(\t\"Z\n\x13GetManifestResponse\x12\x43\n\x08manifest\x18\x01 \x01(\x0b\x32\x31.org.apache.beam.model.job_management.v1.Manifest\";\n\x12GetArtifactRequest\x12\x0c\n\x04name\x18\x01 \x01(\t\x12\x17\n\x0fretrieval_token\x18\x02 \x01(\t\"\x1d\n\rArtifactChunk\x12\x0c\n\x04\x64\x61ta\x18\x01 \x01(\x0c\"\x81\x01\n\x13PutArtifactMetadata\x12\x1d\n\x15staging_session_token\x18\x01 \x01(\t\x12K\n\x08metadata\x18\x02 \x01(\x0b\x32\x39.org.apache.beam.model.job_management.v1.ArtifactMetadata\"\xb9\x01\n\x12PutArtifactRequest\x12P\n\x08metadata\x18\x01 \x01(\x0b\x32<.org.apache.beam.model.job_management.v1.PutArtifactMetadataH\x00\x12\x46\n\x04\x64\x61ta\x18\x02 \x01(\x0b\x32\x36.org.apache.beam.model.job_management.v1.ArtifactChunkH\x00\x42\t\n\x07\x63ontent\"\x15\n\x13PutArtifactResponse\"{\n\x15\x43ommitManifestRequest\x12\x43\n\x08manifest\x18\x01 \x01(\x0b\x32\x31.org.apache.beam.model.job_management.v1.Manifest\x12\x1d\n\x15staging_session_token\x18\x02 \x01(\t\"1\n\x16\x43ommitManifestResponse\x12\x17\n\x0fretrieval_token\x18\x01 \x01(\t2\xb9\x02\n\x16\x41rtifactStagingService\x12\x8a\x01\n\x0bPutArtifact\x12;.org.apache.beam.model.job_management.v1.PutArtifactRequest\x1a<.org.apache.beam.model.job_management.v1.PutArtifactResponse(\x01\x12\x91\x01\n\x0e\x43ommitManifest\x12>.org.apache.beam.model.job_management.v1.CommitManifestRequest\x1a?.org.apache.beam.model.job_management.v1.CommitManifestResponse2\xac\x02\n\x18\x41rtifactRetrievalService\x12\x88\x01\n\x0bGetManifest\x12;.org.apache.beam.model.job_management.v1.GetManifestRequest\x1a<.org.apache.beam.model.job_management.v1.GetManifestResponse\x12\x84\x01\n\x0bGetArtifact\x12;.org.apache.beam.model.job_management.v1.GetArtifactRequest\x1a\x36.org.apache.beam.model.job_management.v1.ArtifactChunk0\x01\x42G\n&org.apache.beam.model.jobmanagement.v1B\x0b\x41rtifactApiZ\x10jobmanagement_v1b\x06proto3')
+  serialized_pb=_b('\n\x17\x62\x65\x61m_artifact_api.proto\x12\'org.apache.beam.model.job_management.v1\"S\n\x10\x41rtifactMetadata\x12\x0c\n\x04name\x18\x01 \x01(\t\x12\x13\n\x0bpermissions\x18\x02 \x01(\r\x12\x0c\n\x04md5X\x18\x03 \x01(\t\x12\x0e\n\x06sha256\x18\x04 \x01(\t\"W\n\x08Manifest\x12K\n\x08\x61rtifact\x18\x01 \x03(\x0b\x32\x39.org.apache.beam.model.job_management.v1.ArtifactMetadata\"\xce\x01\n\rProxyManifest\x12\x43\n\x08manifest\x18\x01 \x01(\x0b\x32\x31.org.apache.beam.model.job_management.v1.Manifest\x12Q\n\x08location\x18\x02 \x03(\x0b\x32?.org.apache.beam.model.job_management.v1.ProxyManifest.Location\x1a%\n\x08Location\x12\x0c\n\x04name\x18\x01 \x01(\t\x12\x0b\n\x03uri\x18\x02 \x01(\t\"-\n\x12GetManifestRequest\x12\x17\n\x0fretrieval_token\x18\x01 \x01(\t\"Z\n\x13GetManifestResponse\x12\x43\n\x08manifest\x18\x01 \x01(\x0b\x32\x31.org.apache.beam.model.job_management.v1.Manifest\";\n\x12GetArtifactRequest\x12\x0c\n\x04name\x18\x01 \x01(\t\x12\x17\n\x0fretrieval_token\x18\x02 \x01(\t\"\x1d\n\rArtifactChunk\x12\x0c\n\x04\x64\x61ta\x18\x01 \x01(\x0c\"\x81\x01\n\x13PutArtifactMetadata\x12\x1d\n\x15staging_session_token\x18\x01 \x01(\t\x12K\n\x08metadata\x18\x02 \x01(\x0b\x32\x39.org.apache.beam.model.job_management.v1.ArtifactMetadata\"\xb9\x01\n\x12PutArtifactRequest\x12P\n\x08metadata\x18\x01 \x01(\x0b\x32<.org.apache.beam.model.job_management.v1.PutArtifactMetadataH\x00\x12\x46\n\x04\x64\x61ta\x18\x02 \x01(\x0b\x32\x36.org.apache.beam.model.job_management.v1.ArtifactChunkH\x00\x42\t\n\x07\x63ontent\"\x15\n\x13PutArtifactResponse\"{\n\x15\x43ommitManifestRequest\x12\x43\n\x08manifest\x18\x01 \x01(\x0b\x32\x31.org.apache.beam.model.job_management.v1.Manifest\x12\x1d\n\x15staging_session_token\x18\x02 \x01(\t\"1\n\x16\x43ommitManifestResponse\x12\x17\n\x0fretrieval_token\x18\x01 \x01(\t2\xb9\x02\n\x16\x41rtifactStagingService\x12\x8a\x01\n\x0bPutArtifact\x12;.org.apache.beam.model.job_management.v1.PutArtifactRequest\x1a<.org.apache.beam.model.job_management.v1.PutArtifactResponse(\x01\x12\x91\x01\n\x0e\x43ommitManifest\x12>.org.apache.beam.model.job_management.v1.CommitManifestRequest\x1a?.org.apache.beam.model.job_management.v1.CommitManifestResponse2\xac\x02\n\x18\x41rtifactRetrievalService\x12\x88\x01\n\x0bGetManifest\x12;.org.apache.beam.model.job_management.v1.GetManifestRequest\x1a<.org.apache.beam.model.job_management.v1.GetManifestResponse\x12\x84\x01\n\x0bGetArtifact\x12;.org.apache.beam.model.job_management.v1.GetArtifactRequest\x1a\x36.org.apache.beam.model.job_management.v1.ArtifactChunk0\x01\x42G\n&org.apache.beam.model.jobmanagement.v1B\x0b\x41rtifactApiZ\x10jobmanagement_v1b\x06proto3')
 )
+_sym_db.RegisterFileDescriptor(DESCRIPTOR)
 
 
 
 
 _ARTIFACTMETADATA = _descriptor.Descriptor(
   name='ArtifactMetadata',
   full_name='org.apache.beam.model.job_management.v1.ArtifactMetadata',
@@ -34,43 +35,50 @@
   fields=[
     _descriptor.FieldDescriptor(
       name='name', full_name='org.apache.beam.model.job_management.v1.ArtifactMetadata.name', index=0,
       number=1, type=9, cpp_type=9, label=1,
       has_default_value=False, default_value=_b("").decode('utf-8'),
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='permissions', full_name='org.apache.beam.model.job_management.v1.ArtifactMetadata.permissions', index=1,
       number=2, type=13, cpp_type=3, label=1,
       has_default_value=False, default_value=0,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
-      name='md5', full_name='org.apache.beam.model.job_management.v1.ArtifactMetadata.md5', index=2,
+      name='md5X', full_name='org.apache.beam.model.job_management.v1.ArtifactMetadata.md5X', index=2,
       number=3, type=9, cpp_type=9, label=1,
       has_default_value=False, default_value=_b("").decode('utf-8'),
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
+    _descriptor.FieldDescriptor(
+      name='sha256', full_name='org.apache.beam.model.job_management.v1.ArtifactMetadata.sha256', index=3,
+      number=4, type=9, cpp_type=9, label=1,
+      has_default_value=False, default_value=_b("").decode('utf-8'),
+      message_type=None, enum_type=None, containing_type=None,
+      is_extension=False, extension_scope=None,
+      options=None),
   ],
   extensions=[
   ],
   nested_types=[],
   enum_types=[
   ],
   options=None,
   is_extendable=False,
   syntax='proto3',
   extension_ranges=[],
   oneofs=[
   ],
   serialized_start=68,
-  serialized_end=134,
+  serialized_end=151,
 )
 
 
 _MANIFEST = _descriptor.Descriptor(
   name='Manifest',
   full_name='org.apache.beam.model.job_management.v1.Manifest',
   filename=None,
@@ -79,29 +87,29 @@
   fields=[
     _descriptor.FieldDescriptor(
       name='artifact', full_name='org.apache.beam.model.job_management.v1.Manifest.artifact', index=0,
       number=1, type=11, cpp_type=10, label=3,
       has_default_value=False, default_value=[],
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
   ],
   extensions=[
   ],
   nested_types=[],
   enum_types=[
   ],
   options=None,
   is_extendable=False,
   syntax='proto3',
   extension_ranges=[],
   oneofs=[
   ],
-  serialized_start=136,
-  serialized_end=223,
+  serialized_start=153,
+  serialized_end=240,
 )
 
 
 _PROXYMANIFEST_LOCATION = _descriptor.Descriptor(
   name='Location',
   full_name='org.apache.beam.model.job_management.v1.ProxyManifest.Location',
   filename=None,
@@ -110,36 +118,36 @@
   fields=[
     _descriptor.FieldDescriptor(
       name='name', full_name='org.apache.beam.model.job_management.v1.ProxyManifest.Location.name', index=0,
       number=1, type=9, cpp_type=9, label=1,
       has_default_value=False, default_value=_b("").decode('utf-8'),
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='uri', full_name='org.apache.beam.model.job_management.v1.ProxyManifest.Location.uri', index=1,
       number=2, type=9, cpp_type=9, label=1,
       has_default_value=False, default_value=_b("").decode('utf-8'),
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
   ],
   extensions=[
   ],
   nested_types=[],
   enum_types=[
   ],
   options=None,
   is_extendable=False,
   syntax='proto3',
   extension_ranges=[],
   oneofs=[
   ],
-  serialized_start=395,
-  serialized_end=432,
+  serialized_start=412,
+  serialized_end=449,
 )
 
 _PROXYMANIFEST = _descriptor.Descriptor(
   name='ProxyManifest',
   full_name='org.apache.beam.model.job_management.v1.ProxyManifest',
   filename=None,
   file=DESCRIPTOR,
@@ -147,36 +155,36 @@
   fields=[
     _descriptor.FieldDescriptor(
       name='manifest', full_name='org.apache.beam.model.job_management.v1.ProxyManifest.manifest', index=0,
       number=1, type=11, cpp_type=10, label=1,
       has_default_value=False, default_value=None,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='location', full_name='org.apache.beam.model.job_management.v1.ProxyManifest.location', index=1,
       number=2, type=11, cpp_type=10, label=3,
       has_default_value=False, default_value=[],
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
   ],
   extensions=[
   ],
   nested_types=[_PROXYMANIFEST_LOCATION, ],
   enum_types=[
   ],
   options=None,
   is_extendable=False,
   syntax='proto3',
   extension_ranges=[],
   oneofs=[
   ],
-  serialized_start=226,
-  serialized_end=432,
+  serialized_start=243,
+  serialized_end=449,
 )
 
 
 _GETMANIFESTREQUEST = _descriptor.Descriptor(
   name='GetManifestRequest',
   full_name='org.apache.beam.model.job_management.v1.GetManifestRequest',
   filename=None,
@@ -185,29 +193,29 @@
   fields=[
     _descriptor.FieldDescriptor(
       name='retrieval_token', full_name='org.apache.beam.model.job_management.v1.GetManifestRequest.retrieval_token', index=0,
       number=1, type=9, cpp_type=9, label=1,
       has_default_value=False, default_value=_b("").decode('utf-8'),
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
   ],
   extensions=[
   ],
   nested_types=[],
   enum_types=[
   ],
   options=None,
   is_extendable=False,
   syntax='proto3',
   extension_ranges=[],
   oneofs=[
   ],
-  serialized_start=434,
-  serialized_end=479,
+  serialized_start=451,
+  serialized_end=496,
 )
 
 
 _GETMANIFESTRESPONSE = _descriptor.Descriptor(
   name='GetManifestResponse',
   full_name='org.apache.beam.model.job_management.v1.GetManifestResponse',
   filename=None,
@@ -216,29 +224,29 @@
   fields=[
     _descriptor.FieldDescriptor(
       name='manifest', full_name='org.apache.beam.model.job_management.v1.GetManifestResponse.manifest', index=0,
       number=1, type=11, cpp_type=10, label=1,
       has_default_value=False, default_value=None,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
   ],
   extensions=[
   ],
   nested_types=[],
   enum_types=[
   ],
   options=None,
   is_extendable=False,
   syntax='proto3',
   extension_ranges=[],
   oneofs=[
   ],
-  serialized_start=481,
-  serialized_end=571,
+  serialized_start=498,
+  serialized_end=588,
 )
 
 
 _GETARTIFACTREQUEST = _descriptor.Descriptor(
   name='GetArtifactRequest',
   full_name='org.apache.beam.model.job_management.v1.GetArtifactRequest',
   filename=None,
@@ -247,36 +255,36 @@
   fields=[
     _descriptor.FieldDescriptor(
       name='name', full_name='org.apache.beam.model.job_management.v1.GetArtifactRequest.name', index=0,
       number=1, type=9, cpp_type=9, label=1,
       has_default_value=False, default_value=_b("").decode('utf-8'),
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='retrieval_token', full_name='org.apache.beam.model.job_management.v1.GetArtifactRequest.retrieval_token', index=1,
       number=2, type=9, cpp_type=9, label=1,
       has_default_value=False, default_value=_b("").decode('utf-8'),
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
   ],
   extensions=[
   ],
   nested_types=[],
   enum_types=[
   ],
   options=None,
   is_extendable=False,
   syntax='proto3',
   extension_ranges=[],
   oneofs=[
   ],
-  serialized_start=573,
-  serialized_end=632,
+  serialized_start=590,
+  serialized_end=649,
 )
 
 
 _ARTIFACTCHUNK = _descriptor.Descriptor(
   name='ArtifactChunk',
   full_name='org.apache.beam.model.job_management.v1.ArtifactChunk',
   filename=None,
@@ -285,29 +293,29 @@
   fields=[
     _descriptor.FieldDescriptor(
       name='data', full_name='org.apache.beam.model.job_management.v1.ArtifactChunk.data', index=0,
       number=1, type=12, cpp_type=9, label=1,
       has_default_value=False, default_value=_b(""),
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
   ],
   extensions=[
   ],
   nested_types=[],
   enum_types=[
   ],
   options=None,
   is_extendable=False,
   syntax='proto3',
   extension_ranges=[],
   oneofs=[
   ],
-  serialized_start=634,
-  serialized_end=663,
+  serialized_start=651,
+  serialized_end=680,
 )
 
 
 _PUTARTIFACTMETADATA = _descriptor.Descriptor(
   name='PutArtifactMetadata',
   full_name='org.apache.beam.model.job_management.v1.PutArtifactMetadata',
   filename=None,
@@ -316,36 +324,36 @@
   fields=[
     _descriptor.FieldDescriptor(
       name='staging_session_token', full_name='org.apache.beam.model.job_management.v1.PutArtifactMetadata.staging_session_token', index=0,
       number=1, type=9, cpp_type=9, label=1,
       has_default_value=False, default_value=_b("").decode('utf-8'),
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='metadata', full_name='org.apache.beam.model.job_management.v1.PutArtifactMetadata.metadata', index=1,
       number=2, type=11, cpp_type=10, label=1,
       has_default_value=False, default_value=None,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
   ],
   extensions=[
   ],
   nested_types=[],
   enum_types=[
   ],
   options=None,
   is_extendable=False,
   syntax='proto3',
   extension_ranges=[],
   oneofs=[
   ],
-  serialized_start=666,
-  serialized_end=795,
+  serialized_start=683,
+  serialized_end=812,
 )
 
 
 _PUTARTIFACTREQUEST = _descriptor.Descriptor(
   name='PutArtifactRequest',
   full_name='org.apache.beam.model.job_management.v1.PutArtifactRequest',
   filename=None,
@@ -354,22 +362,22 @@
   fields=[
     _descriptor.FieldDescriptor(
       name='metadata', full_name='org.apache.beam.model.job_management.v1.PutArtifactRequest.metadata', index=0,
       number=1, type=11, cpp_type=10, label=1,
       has_default_value=False, default_value=None,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='data', full_name='org.apache.beam.model.job_management.v1.PutArtifactRequest.data', index=1,
       number=2, type=11, cpp_type=10, label=1,
       has_default_value=False, default_value=None,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
   ],
   extensions=[
   ],
   nested_types=[],
   enum_types=[
   ],
   options=None,
@@ -377,16 +385,16 @@
   syntax='proto3',
   extension_ranges=[],
   oneofs=[
     _descriptor.OneofDescriptor(
       name='content', full_name='org.apache.beam.model.job_management.v1.PutArtifactRequest.content',
       index=0, containing_type=None, fields=[]),
   ],
-  serialized_start=798,
-  serialized_end=983,
+  serialized_start=815,
+  serialized_end=1000,
 )
 
 
 _PUTARTIFACTRESPONSE = _descriptor.Descriptor(
   name='PutArtifactResponse',
   full_name='org.apache.beam.model.job_management.v1.PutArtifactResponse',
   filename=None,
@@ -401,16 +409,16 @@
   ],
   options=None,
   is_extendable=False,
   syntax='proto3',
   extension_ranges=[],
   oneofs=[
   ],
-  serialized_start=985,
-  serialized_end=1006,
+  serialized_start=1002,
+  serialized_end=1023,
 )
 
 
 _COMMITMANIFESTREQUEST = _descriptor.Descriptor(
   name='CommitManifestRequest',
   full_name='org.apache.beam.model.job_management.v1.CommitManifestRequest',
   filename=None,
@@ -419,36 +427,36 @@
   fields=[
     _descriptor.FieldDescriptor(
       name='manifest', full_name='org.apache.beam.model.job_management.v1.CommitManifestRequest.manifest', index=0,
       number=1, type=11, cpp_type=10, label=1,
       has_default_value=False, default_value=None,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='staging_session_token', full_name='org.apache.beam.model.job_management.v1.CommitManifestRequest.staging_session_token', index=1,
       number=2, type=9, cpp_type=9, label=1,
       has_default_value=False, default_value=_b("").decode('utf-8'),
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
   ],
   extensions=[
   ],
   nested_types=[],
   enum_types=[
   ],
   options=None,
   is_extendable=False,
   syntax='proto3',
   extension_ranges=[],
   oneofs=[
   ],
-  serialized_start=1008,
-  serialized_end=1131,
+  serialized_start=1025,
+  serialized_end=1148,
 )
 
 
 _COMMITMANIFESTRESPONSE = _descriptor.Descriptor(
   name='CommitManifestResponse',
   full_name='org.apache.beam.model.job_management.v1.CommitManifestResponse',
   filename=None,
@@ -457,29 +465,29 @@
   fields=[
     _descriptor.FieldDescriptor(
       name='retrieval_token', full_name='org.apache.beam.model.job_management.v1.CommitManifestResponse.retrieval_token', index=0,
       number=1, type=9, cpp_type=9, label=1,
       has_default_value=False, default_value=_b("").decode('utf-8'),
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
   ],
   extensions=[
   ],
   nested_types=[],
   enum_types=[
   ],
   options=None,
   is_extendable=False,
   syntax='proto3',
   extension_ranges=[],
   oneofs=[
   ],
-  serialized_start=1133,
-  serialized_end=1182,
+  serialized_start=1150,
+  serialized_end=1199,
 )
 
 _MANIFEST.fields_by_name['artifact'].message_type = _ARTIFACTMETADATA
 _PROXYMANIFEST_LOCATION.containing_type = _PROXYMANIFEST
 _PROXYMANIFEST.fields_by_name['manifest'].message_type = _MANIFEST
 _PROXYMANIFEST.fields_by_name['location'].message_type = _PROXYMANIFEST_LOCATION
 _GETMANIFESTRESPONSE.fields_by_name['manifest'].message_type = _MANIFEST
@@ -501,15 +509,14 @@
 DESCRIPTOR.message_types_by_name['GetArtifactRequest'] = _GETARTIFACTREQUEST
 DESCRIPTOR.message_types_by_name['ArtifactChunk'] = _ARTIFACTCHUNK
 DESCRIPTOR.message_types_by_name['PutArtifactMetadata'] = _PUTARTIFACTMETADATA
 DESCRIPTOR.message_types_by_name['PutArtifactRequest'] = _PUTARTIFACTREQUEST
 DESCRIPTOR.message_types_by_name['PutArtifactResponse'] = _PUTARTIFACTRESPONSE
 DESCRIPTOR.message_types_by_name['CommitManifestRequest'] = _COMMITMANIFESTREQUEST
 DESCRIPTOR.message_types_by_name['CommitManifestResponse'] = _COMMITMANIFESTRESPONSE
-_sym_db.RegisterFileDescriptor(DESCRIPTOR)
 
 ArtifactMetadata = _reflection.GeneratedProtocolMessageType('ArtifactMetadata', (_message.Message,), dict(
   DESCRIPTOR = _ARTIFACTMETADATA,
   __module__ = 'beam_artifact_api_pb2'
   # @@protoc_insertion_point(class_scope:org.apache.beam.model.job_management.v1.ArtifactMetadata)
   ))
 _sym_db.RegisterMessage(ArtifactMetadata)
@@ -598,74 +605,8 @@
   # @@protoc_insertion_point(class_scope:org.apache.beam.model.job_management.v1.CommitManifestResponse)
   ))
 _sym_db.RegisterMessage(CommitManifestResponse)
 
 
 DESCRIPTOR.has_options = True
 DESCRIPTOR._options = _descriptor._ParseOptions(descriptor_pb2.FileOptions(), _b('\n&org.apache.beam.model.jobmanagement.v1B\013ArtifactApiZ\020jobmanagement_v1'))
-
-_ARTIFACTSTAGINGSERVICE = _descriptor.ServiceDescriptor(
-  name='ArtifactStagingService',
-  full_name='org.apache.beam.model.job_management.v1.ArtifactStagingService',
-  file=DESCRIPTOR,
-  index=0,
-  options=None,
-  serialized_start=1185,
-  serialized_end=1498,
-  methods=[
-  _descriptor.MethodDescriptor(
-    name='PutArtifact',
-    full_name='org.apache.beam.model.job_management.v1.ArtifactStagingService.PutArtifact',
-    index=0,
-    containing_service=None,
-    input_type=_PUTARTIFACTREQUEST,
-    output_type=_PUTARTIFACTRESPONSE,
-    options=None,
-  ),
-  _descriptor.MethodDescriptor(
-    name='CommitManifest',
-    full_name='org.apache.beam.model.job_management.v1.ArtifactStagingService.CommitManifest',
-    index=1,
-    containing_service=None,
-    input_type=_COMMITMANIFESTREQUEST,
-    output_type=_COMMITMANIFESTRESPONSE,
-    options=None,
-  ),
-])
-_sym_db.RegisterServiceDescriptor(_ARTIFACTSTAGINGSERVICE)
-
-DESCRIPTOR.services_by_name['ArtifactStagingService'] = _ARTIFACTSTAGINGSERVICE
-
-
-_ARTIFACTRETRIEVALSERVICE = _descriptor.ServiceDescriptor(
-  name='ArtifactRetrievalService',
-  full_name='org.apache.beam.model.job_management.v1.ArtifactRetrievalService',
-  file=DESCRIPTOR,
-  index=1,
-  options=None,
-  serialized_start=1501,
-  serialized_end=1801,
-  methods=[
-  _descriptor.MethodDescriptor(
-    name='GetManifest',
-    full_name='org.apache.beam.model.job_management.v1.ArtifactRetrievalService.GetManifest',
-    index=0,
-    containing_service=None,
-    input_type=_GETMANIFESTREQUEST,
-    output_type=_GETMANIFESTRESPONSE,
-    options=None,
-  ),
-  _descriptor.MethodDescriptor(
-    name='GetArtifact',
-    full_name='org.apache.beam.model.job_management.v1.ArtifactRetrievalService.GetArtifact',
-    index=1,
-    containing_service=None,
-    input_type=_GETARTIFACTREQUEST,
-    output_type=_ARTIFACTCHUNK,
-    options=None,
-  ),
-])
-_sym_db.RegisterServiceDescriptor(_ARTIFACTRETRIEVALSERVICE)
-
-DESCRIPTOR.services_by_name['ArtifactRetrievalService'] = _ARTIFACTRETRIEVALSERVICE
-
 # @@protoc_insertion_point(module_scope)
```

## Comparing `apache-beam-2.8.0/apache_beam/portability/api/beam_provision_api_pb2_grpc.py` & `apache-beam-2.9.0/apache_beam/portability/api/beam_provision_api_pb2_grpc.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/portability/api/endpoints_pb2.py` & `apache-beam-2.9.0/apache_beam/portability/api/endpoints_pb2.py`

 * *Files 2% similar despite different names*

```diff
@@ -17,14 +17,15 @@
 
 DESCRIPTOR = _descriptor.FileDescriptor(
   name='endpoints.proto',
   package='org.apache.beam.model.pipeline.v1',
   syntax='proto3',
   serialized_pb=_b('\n\x0f\x65ndpoints.proto\x12!org.apache.beam.model.pipeline.v1\"\xa1\x01\n\x14\x41piServiceDescriptor\x12\x0b\n\x03url\x18\x02 \x01(\t\x12j\n\x1foauth2_client_credentials_grant\x18\x03 \x01(\x0b\x32?.org.apache.beam.model.pipeline.v1.OAuth2ClientCredentialsGrantH\x00\x42\x10\n\x0e\x61uthentication\"+\n\x1cOAuth2ClientCredentialsGrant\x12\x0b\n\x03url\x18\x01 \x01(\tB;\n!org.apache.beam.model.pipeline.v1B\tEndpointsZ\x0bpipeline_v1b\x06proto3')
 )
+_sym_db.RegisterFileDescriptor(DESCRIPTOR)
 
 
 
 
 _APISERVICEDESCRIPTOR = _descriptor.Descriptor(
   name='ApiServiceDescriptor',
   full_name='org.apache.beam.model.pipeline.v1.ApiServiceDescriptor',
@@ -34,22 +35,22 @@
   fields=[
     _descriptor.FieldDescriptor(
       name='url', full_name='org.apache.beam.model.pipeline.v1.ApiServiceDescriptor.url', index=0,
       number=2, type=9, cpp_type=9, label=1,
       has_default_value=False, default_value=_b("").decode('utf-8'),
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='oauth2_client_credentials_grant', full_name='org.apache.beam.model.pipeline.v1.ApiServiceDescriptor.oauth2_client_credentials_grant', index=1,
       number=3, type=11, cpp_type=10, label=1,
       has_default_value=False, default_value=None,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
   ],
   extensions=[
   ],
   nested_types=[],
   enum_types=[
   ],
   options=None,
@@ -75,15 +76,15 @@
   fields=[
     _descriptor.FieldDescriptor(
       name='url', full_name='org.apache.beam.model.pipeline.v1.OAuth2ClientCredentialsGrant.url', index=0,
       number=1, type=9, cpp_type=9, label=1,
       has_default_value=False, default_value=_b("").decode('utf-8'),
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
   ],
   extensions=[
   ],
   nested_types=[],
   enum_types=[
   ],
   options=None,
@@ -98,15 +99,14 @@
 
 _APISERVICEDESCRIPTOR.fields_by_name['oauth2_client_credentials_grant'].message_type = _OAUTH2CLIENTCREDENTIALSGRANT
 _APISERVICEDESCRIPTOR.oneofs_by_name['authentication'].fields.append(
   _APISERVICEDESCRIPTOR.fields_by_name['oauth2_client_credentials_grant'])
 _APISERVICEDESCRIPTOR.fields_by_name['oauth2_client_credentials_grant'].containing_oneof = _APISERVICEDESCRIPTOR.oneofs_by_name['authentication']
 DESCRIPTOR.message_types_by_name['ApiServiceDescriptor'] = _APISERVICEDESCRIPTOR
 DESCRIPTOR.message_types_by_name['OAuth2ClientCredentialsGrant'] = _OAUTH2CLIENTCREDENTIALSGRANT
-_sym_db.RegisterFileDescriptor(DESCRIPTOR)
 
 ApiServiceDescriptor = _reflection.GeneratedProtocolMessageType('ApiServiceDescriptor', (_message.Message,), dict(
   DESCRIPTOR = _APISERVICEDESCRIPTOR,
   __module__ = 'endpoints_pb2'
   # @@protoc_insertion_point(class_scope:org.apache.beam.model.pipeline.v1.ApiServiceDescriptor)
   ))
 _sym_db.RegisterMessage(ApiServiceDescriptor)
```

## Comparing `apache-beam-2.8.0/apache_beam/portability/api/beam_artifact_api_pb2_grpc.py` & `apache-beam-2.9.0/apache_beam/portability/api/beam_artifact_api_pb2_grpc.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/portability/api/beam_job_api_pb2.py` & `apache-beam-2.9.0/apache_beam/portability/api/beam_job_api_pb2.py`

 * *Files 4% similar despite different names*

```diff
@@ -22,14 +22,15 @@
 DESCRIPTOR = _descriptor.FileDescriptor(
   name='beam_job_api.proto',
   package='org.apache.beam.model.job_management.v1',
   syntax='proto3',
   serialized_pb=_b('\n\x12\x62\x65\x61m_job_api.proto\x12\'org.apache.beam.model.job_management.v1\x1a\x15\x62\x65\x61m_runner_api.proto\x1a\x0f\x65ndpoints.proto\x1a\x1cgoogle/protobuf/struct.proto\"\x97\x01\n\x11PrepareJobRequest\x12=\n\x08pipeline\x18\x01 \x01(\x0b\x32+.org.apache.beam.model.pipeline.v1.Pipeline\x12\x31\n\x10pipeline_options\x18\x02 \x01(\x0b\x32\x17.google.protobuf.Struct\x12\x10\n\x08job_name\x18\x03 \x01(\t\"\xa7\x01\n\x12PrepareJobResponse\x12\x16\n\x0epreparation_id\x18\x01 \x01(\t\x12Z\n\x19\x61rtifact_staging_endpoint\x18\x02 \x01(\x0b\x32\x37.org.apache.beam.model.pipeline.v1.ApiServiceDescriptor\x12\x1d\n\x15staging_session_token\x18\x03 \x01(\t\"@\n\rRunJobRequest\x12\x16\n\x0epreparation_id\x18\x01 \x01(\t\x12\x17\n\x0fretrieval_token\x18\x02 \x01(\t\" \n\x0eRunJobResponse\x12\x0e\n\x06job_id\x18\x01 \x01(\t\"\"\n\x10\x43\x61ncelJobRequest\x12\x0e\n\x06job_id\x18\x01 \x01(\t\"Z\n\x11\x43\x61ncelJobResponse\x12\x45\n\x05state\x18\x01 \x01(\x0e\x32\x36.org.apache.beam.model.job_management.v1.JobState.Enum\"$\n\x12GetJobStateRequest\x12\x0e\n\x06job_id\x18\x01 \x01(\t\"\\\n\x13GetJobStateResponse\x12\x45\n\x05state\x18\x01 \x01(\x0e\x32\x36.org.apache.beam.model.job_management.v1.JobState.Enum\"$\n\x12JobMessagesRequest\x12\x0e\n\x06job_id\x18\x01 \x01(\t\"\xd1\x02\n\nJobMessage\x12\x12\n\nmessage_id\x18\x01 \x01(\t\x12\x0c\n\x04time\x18\x02 \x01(\t\x12Y\n\nimportance\x18\x03 \x01(\x0e\x32\x45.org.apache.beam.model.job_management.v1.JobMessage.MessageImportance\x12\x14\n\x0cmessage_text\x18\x04 \x01(\t\"\xaf\x01\n\x11MessageImportance\x12\"\n\x1eMESSAGE_IMPORTANCE_UNSPECIFIED\x10\x00\x12\x15\n\x11JOB_MESSAGE_DEBUG\x10\x01\x12\x18\n\x14JOB_MESSAGE_DETAILED\x10\x02\x12\x15\n\x11JOB_MESSAGE_BASIC\x10\x03\x12\x17\n\x13JOB_MESSAGE_WARNING\x10\x04\x12\x15\n\x11JOB_MESSAGE_ERROR\x10\x05\"\xca\x01\n\x13JobMessagesResponse\x12O\n\x10message_response\x18\x01 \x01(\x0b\x32\x33.org.apache.beam.model.job_management.v1.JobMessageH\x00\x12V\n\x0estate_response\x18\x02 \x01(\x0b\x32<.org.apache.beam.model.job_management.v1.GetJobStateResponseH\x00\x42\n\n\x08response\"\xa9\x01\n\x08JobState\"\x9c\x01\n\x04\x45num\x12\x0f\n\x0bUNSPECIFIED\x10\x00\x12\x0b\n\x07STOPPED\x10\x01\x12\x0b\n\x07RUNNING\x10\x02\x12\x08\n\x04\x44ONE\x10\x03\x12\n\n\x06\x46\x41ILED\x10\x04\x12\r\n\tCANCELLED\x10\x05\x12\x0b\n\x07UPDATED\x10\x06\x12\x0c\n\x08\x44RAINING\x10\x07\x12\x0b\n\x07\x44RAINED\x10\x08\x12\x0c\n\x08STARTING\x10\t\x12\x0e\n\nCANCELLING\x10\n2\xb4\x06\n\nJobService\x12\x82\x01\n\x07Prepare\x12:.org.apache.beam.model.job_management.v1.PrepareJobRequest\x1a;.org.apache.beam.model.job_management.v1.PrepareJobResponse\x12v\n\x03Run\x12\x36.org.apache.beam.model.job_management.v1.RunJobRequest\x1a\x37.org.apache.beam.model.job_management.v1.RunJobResponse\x12\x85\x01\n\x08GetState\x12;.org.apache.beam.model.job_management.v1.GetJobStateRequest\x1a<.org.apache.beam.model.job_management.v1.GetJobStateResponse\x12\x7f\n\x06\x43\x61ncel\x12\x39.org.apache.beam.model.job_management.v1.CancelJobRequest\x1a:.org.apache.beam.model.job_management.v1.CancelJobResponse\x12\x8d\x01\n\x0eGetStateStream\x12;.org.apache.beam.model.job_management.v1.GetJobStateRequest\x1a<.org.apache.beam.model.job_management.v1.GetJobStateResponse0\x01\x12\x8f\x01\n\x10GetMessageStream\x12;.org.apache.beam.model.job_management.v1.JobMessagesRequest\x1a<.org.apache.beam.model.job_management.v1.JobMessagesResponse0\x01\x42\x42\n&org.apache.beam.model.jobmanagement.v1B\x06JobApiZ\x10jobmanagement_v1b\x06proto3')
   ,
   dependencies=[beam__runner__api__pb2.DESCRIPTOR,endpoints__pb2.DESCRIPTOR,google_dot_protobuf_dot_struct__pb2.DESCRIPTOR,])
+_sym_db.RegisterFileDescriptor(DESCRIPTOR)
 
 
 
 _JOBMESSAGE_MESSAGEIMPORTANCE = _descriptor.EnumDescriptor(
   name='MessageImportance',
   full_name='org.apache.beam.model.job_management.v1.JobMessage.MessageImportance',
   filename=None,
@@ -135,29 +136,29 @@
   fields=[
     _descriptor.FieldDescriptor(
       name='pipeline', full_name='org.apache.beam.model.job_management.v1.PrepareJobRequest.pipeline', index=0,
       number=1, type=11, cpp_type=10, label=1,
       has_default_value=False, default_value=None,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='pipeline_options', full_name='org.apache.beam.model.job_management.v1.PrepareJobRequest.pipeline_options', index=1,
       number=2, type=11, cpp_type=10, label=1,
       has_default_value=False, default_value=None,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='job_name', full_name='org.apache.beam.model.job_management.v1.PrepareJobRequest.job_name', index=2,
       number=3, type=9, cpp_type=9, label=1,
       has_default_value=False, default_value=_b("").decode('utf-8'),
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
   ],
   extensions=[
   ],
   nested_types=[],
   enum_types=[
   ],
   options=None,
@@ -180,29 +181,29 @@
   fields=[
     _descriptor.FieldDescriptor(
       name='preparation_id', full_name='org.apache.beam.model.job_management.v1.PrepareJobResponse.preparation_id', index=0,
       number=1, type=9, cpp_type=9, label=1,
       has_default_value=False, default_value=_b("").decode('utf-8'),
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='artifact_staging_endpoint', full_name='org.apache.beam.model.job_management.v1.PrepareJobResponse.artifact_staging_endpoint', index=1,
       number=2, type=11, cpp_type=10, label=1,
       has_default_value=False, default_value=None,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='staging_session_token', full_name='org.apache.beam.model.job_management.v1.PrepareJobResponse.staging_session_token', index=2,
       number=3, type=9, cpp_type=9, label=1,
       has_default_value=False, default_value=_b("").decode('utf-8'),
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
   ],
   extensions=[
   ],
   nested_types=[],
   enum_types=[
   ],
   options=None,
@@ -225,22 +226,22 @@
   fields=[
     _descriptor.FieldDescriptor(
       name='preparation_id', full_name='org.apache.beam.model.job_management.v1.RunJobRequest.preparation_id', index=0,
       number=1, type=9, cpp_type=9, label=1,
       has_default_value=False, default_value=_b("").decode('utf-8'),
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='retrieval_token', full_name='org.apache.beam.model.job_management.v1.RunJobRequest.retrieval_token', index=1,
       number=2, type=9, cpp_type=9, label=1,
       has_default_value=False, default_value=_b("").decode('utf-8'),
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
   ],
   extensions=[
   ],
   nested_types=[],
   enum_types=[
   ],
   options=None,
@@ -263,15 +264,15 @@
   fields=[
     _descriptor.FieldDescriptor(
       name='job_id', full_name='org.apache.beam.model.job_management.v1.RunJobResponse.job_id', index=0,
       number=1, type=9, cpp_type=9, label=1,
       has_default_value=False, default_value=_b("").decode('utf-8'),
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
   ],
   extensions=[
   ],
   nested_types=[],
   enum_types=[
   ],
   options=None,
@@ -294,15 +295,15 @@
   fields=[
     _descriptor.FieldDescriptor(
       name='job_id', full_name='org.apache.beam.model.job_management.v1.CancelJobRequest.job_id', index=0,
       number=1, type=9, cpp_type=9, label=1,
       has_default_value=False, default_value=_b("").decode('utf-8'),
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
   ],
   extensions=[
   ],
   nested_types=[],
   enum_types=[
   ],
   options=None,
@@ -325,15 +326,15 @@
   fields=[
     _descriptor.FieldDescriptor(
       name='state', full_name='org.apache.beam.model.job_management.v1.CancelJobResponse.state', index=0,
       number=1, type=14, cpp_type=8, label=1,
       has_default_value=False, default_value=0,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
   ],
   extensions=[
   ],
   nested_types=[],
   enum_types=[
   ],
   options=None,
@@ -356,15 +357,15 @@
   fields=[
     _descriptor.FieldDescriptor(
       name='job_id', full_name='org.apache.beam.model.job_management.v1.GetJobStateRequest.job_id', index=0,
       number=1, type=9, cpp_type=9, label=1,
       has_default_value=False, default_value=_b("").decode('utf-8'),
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
   ],
   extensions=[
   ],
   nested_types=[],
   enum_types=[
   ],
   options=None,
@@ -387,15 +388,15 @@
   fields=[
     _descriptor.FieldDescriptor(
       name='state', full_name='org.apache.beam.model.job_management.v1.GetJobStateResponse.state', index=0,
       number=1, type=14, cpp_type=8, label=1,
       has_default_value=False, default_value=0,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
   ],
   extensions=[
   ],
   nested_types=[],
   enum_types=[
   ],
   options=None,
@@ -418,15 +419,15 @@
   fields=[
     _descriptor.FieldDescriptor(
       name='job_id', full_name='org.apache.beam.model.job_management.v1.JobMessagesRequest.job_id', index=0,
       number=1, type=9, cpp_type=9, label=1,
       has_default_value=False, default_value=_b("").decode('utf-8'),
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
   ],
   extensions=[
   ],
   nested_types=[],
   enum_types=[
   ],
   options=None,
@@ -449,36 +450,36 @@
   fields=[
     _descriptor.FieldDescriptor(
       name='message_id', full_name='org.apache.beam.model.job_management.v1.JobMessage.message_id', index=0,
       number=1, type=9, cpp_type=9, label=1,
       has_default_value=False, default_value=_b("").decode('utf-8'),
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='time', full_name='org.apache.beam.model.job_management.v1.JobMessage.time', index=1,
       number=2, type=9, cpp_type=9, label=1,
       has_default_value=False, default_value=_b("").decode('utf-8'),
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='importance', full_name='org.apache.beam.model.job_management.v1.JobMessage.importance', index=2,
       number=3, type=14, cpp_type=8, label=1,
       has_default_value=False, default_value=0,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='message_text', full_name='org.apache.beam.model.job_management.v1.JobMessage.message_text', index=3,
       number=4, type=9, cpp_type=9, label=1,
       has_default_value=False, default_value=_b("").decode('utf-8'),
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
   ],
   extensions=[
   ],
   nested_types=[],
   enum_types=[
     _JOBMESSAGE_MESSAGEIMPORTANCE,
   ],
@@ -502,22 +503,22 @@
   fields=[
     _descriptor.FieldDescriptor(
       name='message_response', full_name='org.apache.beam.model.job_management.v1.JobMessagesResponse.message_response', index=0,
       number=1, type=11, cpp_type=10, label=1,
       has_default_value=False, default_value=None,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
     _descriptor.FieldDescriptor(
       name='state_response', full_name='org.apache.beam.model.job_management.v1.JobMessagesResponse.state_response', index=1,
       number=2, type=11, cpp_type=10, label=1,
       has_default_value=False, default_value=None,
       message_type=None, enum_type=None, containing_type=None,
       is_extension=False, extension_scope=None,
-      options=None, file=DESCRIPTOR),
+      options=None),
   ],
   extensions=[
   ],
   nested_types=[],
   enum_types=[
   ],
   options=None,
@@ -582,15 +583,14 @@
 DESCRIPTOR.message_types_by_name['CancelJobResponse'] = _CANCELJOBRESPONSE
 DESCRIPTOR.message_types_by_name['GetJobStateRequest'] = _GETJOBSTATEREQUEST
 DESCRIPTOR.message_types_by_name['GetJobStateResponse'] = _GETJOBSTATERESPONSE
 DESCRIPTOR.message_types_by_name['JobMessagesRequest'] = _JOBMESSAGESREQUEST
 DESCRIPTOR.message_types_by_name['JobMessage'] = _JOBMESSAGE
 DESCRIPTOR.message_types_by_name['JobMessagesResponse'] = _JOBMESSAGESRESPONSE
 DESCRIPTOR.message_types_by_name['JobState'] = _JOBSTATE
-_sym_db.RegisterFileDescriptor(DESCRIPTOR)
 
 PrepareJobRequest = _reflection.GeneratedProtocolMessageType('PrepareJobRequest', (_message.Message,), dict(
   DESCRIPTOR = _PREPAREJOBREQUEST,
   __module__ = 'beam_job_api_pb2'
   # @@protoc_insertion_point(class_scope:org.apache.beam.model.job_management.v1.PrepareJobRequest)
   ))
 _sym_db.RegisterMessage(PrepareJobRequest)
@@ -671,77 +671,8 @@
   # @@protoc_insertion_point(class_scope:org.apache.beam.model.job_management.v1.JobState)
   ))
 _sym_db.RegisterMessage(JobState)
 
 
 DESCRIPTOR.has_options = True
 DESCRIPTOR._options = _descriptor._ParseOptions(descriptor_pb2.FileOptions(), _b('\n&org.apache.beam.model.jobmanagement.v1B\006JobApiZ\020jobmanagement_v1'))
-
-_JOBSERVICE = _descriptor.ServiceDescriptor(
-  name='JobService',
-  full_name='org.apache.beam.model.job_management.v1.JobService',
-  file=DESCRIPTOR,
-  index=0,
-  options=None,
-  serialized_start=1573,
-  serialized_end=2393,
-  methods=[
-  _descriptor.MethodDescriptor(
-    name='Prepare',
-    full_name='org.apache.beam.model.job_management.v1.JobService.Prepare',
-    index=0,
-    containing_service=None,
-    input_type=_PREPAREJOBREQUEST,
-    output_type=_PREPAREJOBRESPONSE,
-    options=None,
-  ),
-  _descriptor.MethodDescriptor(
-    name='Run',
-    full_name='org.apache.beam.model.job_management.v1.JobService.Run',
-    index=1,
-    containing_service=None,
-    input_type=_RUNJOBREQUEST,
-    output_type=_RUNJOBRESPONSE,
-    options=None,
-  ),
-  _descriptor.MethodDescriptor(
-    name='GetState',
-    full_name='org.apache.beam.model.job_management.v1.JobService.GetState',
-    index=2,
-    containing_service=None,
-    input_type=_GETJOBSTATEREQUEST,
-    output_type=_GETJOBSTATERESPONSE,
-    options=None,
-  ),
-  _descriptor.MethodDescriptor(
-    name='Cancel',
-    full_name='org.apache.beam.model.job_management.v1.JobService.Cancel',
-    index=3,
-    containing_service=None,
-    input_type=_CANCELJOBREQUEST,
-    output_type=_CANCELJOBRESPONSE,
-    options=None,
-  ),
-  _descriptor.MethodDescriptor(
-    name='GetStateStream',
-    full_name='org.apache.beam.model.job_management.v1.JobService.GetStateStream',
-    index=4,
-    containing_service=None,
-    input_type=_GETJOBSTATEREQUEST,
-    output_type=_GETJOBSTATERESPONSE,
-    options=None,
-  ),
-  _descriptor.MethodDescriptor(
-    name='GetMessageStream',
-    full_name='org.apache.beam.model.job_management.v1.JobService.GetMessageStream',
-    index=5,
-    containing_service=None,
-    input_type=_JOBMESSAGESREQUEST,
-    output_type=_JOBMESSAGESRESPONSE,
-    options=None,
-  ),
-])
-_sym_db.RegisterServiceDescriptor(_JOBSERVICE)
-
-DESCRIPTOR.services_by_name['JobService'] = _JOBSERVICE
-
 # @@protoc_insertion_point(module_scope)
```

## Comparing `apache-beam-2.8.0/apache_beam/options/pipeline_options_validator.py` & `apache-beam-2.9.0/apache_beam/options/pipeline_options_validator.py`

 * *Files 2% similar despite different names*

```diff
@@ -166,14 +166,18 @@
     else:
       if self.is_full_string_match(self.PROJECT_NUMBER_PATTERN, project):
         errors.extend(
             self._validate_error(self.ERR_INVALID_PROJECT_NUMBER, project))
       elif not self.is_full_string_match(self.PROJECT_ID_PATTERN, project):
         errors.extend(
             self._validate_error(self.ERR_INVALID_PROJECT_ID, project))
+    if view.update:
+      if not view.job_name:
+        errors.extend(self._validate_error(
+            'Existing job name must be provided when updating a pipeline.'))
     return errors
 
   def validate_optional_argument_positive(self, view, arg_name):
     """Validates that an optional argument (if set) has a positive value."""
     arg = getattr(view, arg_name, None)
     if arg is not None and int(arg) <= 0:
       return self._validate_error(self.ERR_INVALID_NOT_POSITIVE, arg, arg_name)
```

## Comparing `apache-beam-2.8.0/apache_beam/options/__init__.py` & `apache-beam-2.9.0/apache_beam/options/__init__.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/options/value_provider.py` & `apache-beam-2.9.0/apache_beam/options/value_provider.py`

 * *Files 2% similar despite different names*

```diff
@@ -65,14 +65,18 @@
       return True
     if isinstance(other, StaticValueProvider):
       if (self.value_type == other.value_type and
           self.value == other.value):
         return True
     return False
 
+  def __ne__(self, other):
+    # TODO(BEAM-5949): Needed for Python 2 compatibility.
+    return not self == other
+
   def __hash__(self):
     return hash((type(self), self.value_type, self.value))
 
 
 class RuntimeValueProvider(ValueProvider):
   runtime_options = None
   experiments = set()
```

## Comparing `apache-beam-2.8.0/apache_beam/options/pipeline_options.py` & `apache-beam-2.9.0/apache_beam/options/pipeline_options.py`

 * *Files 2% similar despite different names*

```diff
@@ -102,14 +102,22 @@
         value_type=value_type,
         default_value=default_value
     )
 
     # have add_argument do most of the work
     self.add_argument(*args, **kwargs)
 
+  # The argparse package by default tries to autocomplete option names. This
+  # results in an "ambiguous option" error from argparse when an unknown option
+  # matching multiple known ones are used. This suppresses that behavior.
+  def error(self, message):
+    if message.startswith('ambiguous option: '):
+      return
+    super(_BeamArgumentParser, self).error(message)
+
 
 class PipelineOptions(HasDisplayData):
   """Pipeline options class used as container for command line options.
 
   The class is essentially a wrapper over the standard argparse Python module
   (see https://docs.python.org/3/library/argparse.html).  To define one option
   or a group of options you subclass from PipelineOptions::
@@ -317,14 +325,20 @@
     parser.add_argument(
         '--no_direct_runner_use_stacked_bundle',
         action='store_false',
         dest='direct_runner_use_stacked_bundle',
         help='DirectRunner uses stacked WindowedValues within a Bundle for '
         'memory optimization. Set --no_direct_runner_use_stacked_bundle to '
         'avoid it.')
+    parser.add_argument(
+        '--direct_runner_bundle_repeat',
+        type=int,
+        default=0,
+        help='replay every bundle this many extra times, for profiling'
+        'and debugging')
 
 
 class GoogleCloudOptions(PipelineOptions):
   """Google Cloud Dataflow service execution options."""
 
   BIGQUERY_API_SERVICE = 'bigquery.googleapis.com'
   COMPUTE_API_SERVICE = 'compute.googleapis.com'
@@ -371,21 +385,28 @@
                         default=None,
                         help='Identity to run virtual machines as.')
     parser.add_argument('--no_auth', dest='no_auth', type=bool, default=False)
     # Option to run templated pipelines
     parser.add_argument('--template_location',
                         default=None,
                         help='Save job to specified local or GCS location.')
-    parser.add_argument(
-        '--label', '--labels',
-        dest='labels',
-        action='append',
-        default=None,
-        help='Labels that will be applied to this Dataflow job. Labels are key '
-        'value pairs separated by = (e.g. --label key=value).')
+    parser.add_argument('--label', '--labels',
+                        dest='labels',
+                        action='append',
+                        default=None,
+                        help='Labels to be applied to this Dataflow job. '
+                        'Labels are key value pairs separated by = '
+                        '(e.g. --label key=value).')
+    parser.add_argument('--update',
+                        default=False,
+                        action='store_true',
+                        help='Update an existing streaming Cloud Dataflow job. '
+                        'Experimental. '
+                        'See https://cloud.google.com/dataflow/pipelines/'
+                        'updating-a-pipeline')
 
   def validate(self, validator):
     errors = []
     if validator.is_service_runner():
       errors.extend(validator.validate_cloud_options(self))
       errors.extend(validator.validate_gcs_path(self, 'temp_location'))
       if getattr(self, 'staging_location',
@@ -516,14 +537,22 @@
         help='Whether to assign only private IP addresses to the worker VMs.')
     parser.add_argument(
         '--min_cpu_platform',
         dest='min_cpu_platform',
         type=str,
         help='GCE minimum CPU platform. Default is determined by GCP.'
     )
+    parser.add_argument(
+        '--dataflow_worker_jar',
+        dest='dataflow_worker_jar',
+        type=str,
+        help='Dataflow worker jar file. If specified, the jar file is staged '
+             'in GCS, then gets loaded by workers. End users usually '
+             'should not use this feature.'
+    )
 
   def validate(self, validator):
     errors = []
     if validator.is_service_runner():
       errors.extend(
           validator.validate_optional_argument_positive(self, 'num_workers'))
     return errors
@@ -555,15 +584,20 @@
                         action='store_true',
                         help='Enable work item CPU profiling.')
     parser.add_argument('--profile_memory',
                         action='store_true',
                         help='Enable work item heap profiling.')
     parser.add_argument('--profile_location',
                         default=None,
-                        help='GCS path for saving profiler data.')
+                        help='path for saving profiler data.')
+    parser.add_argument('--profile_sample_rate',
+                        type=float,
+                        default=1.0,
+                        help='A number between 0 and 1 indicating the ratio '
+                        'of bundles that should be profiled.')
 
 
 class SetupOptions(PipelineOptions):
 
   @classmethod
   def _add_argparse_args(cls, parser):
     # Options for installing dependencies in the worker.
```

## Comparing `apache-beam-2.8.0/apache_beam/options/pipeline_options_test.py` & `apache-beam-2.9.0/apache_beam/options/pipeline_options_test.py`

 * *Files 3% similar despite different names*

```diff
@@ -21,14 +21,16 @@
 
 import logging
 import unittest
 
 import hamcrest as hc
 
 from apache_beam.options.pipeline_options import PipelineOptions
+from apache_beam.options.pipeline_options import ProfilingOptions
+from apache_beam.options.pipeline_options import TypeOptions
 from apache_beam.options.value_provider import RuntimeValueProvider
 from apache_beam.options.value_provider import StaticValueProvider
 from apache_beam.transforms.display import DisplayData
 from apache_beam.transforms.display_test import DisplayDataItemMatcher
 
 
 class PipelineOptionsTest(unittest.TestCase):
@@ -277,11 +279,26 @@
                     '%s is not accessible' % options.pot_vp_arg2)
     self.assertEqual(options.pot_vp_arg2.get(), 'bye')
     self.assertFalse(options.pot_non_vp_arg1.is_accessible())
 
     with self.assertRaises(RuntimeError):
       options.pot_non_vp_arg1.get()
 
+  # The argparse package by default tries to autocomplete option names. This
+  # results in an "ambiguous option" error from argparse when an unknown option
+  # matching multiple known ones are used. This tests that we suppress this
+  # error.
+  def test_unknown_option_prefix(self):
+    # Test that the "ambiguous option" error is suppressed.
+    options = PipelineOptions(['--profi', 'val1'])
+    options.view_as(ProfilingOptions)
+
+    # Test that valid errors are not suppressed.
+    with self.assertRaises(SystemExit):
+      # Invalid option choice.
+      options = PipelineOptions(['--type_check_strictness', 'blahblah'])
+      options.view_as(TypeOptions)
+
 
 if __name__ == '__main__':
   logging.getLogger().setLevel(logging.INFO)
   unittest.main()
```

## Comparing `apache-beam-2.8.0/apache_beam/options/pipeline_options_validator_test.py` & `apache-beam-2.9.0/apache_beam/options/pipeline_options_validator_test.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam/options/value_provider_test.py` & `apache-beam-2.9.0/apache_beam/options/value_provider_test.py`

 * *Files identical despite different names*

## Comparing `apache-beam-2.8.0/apache_beam.egg-info/SOURCES.txt` & `apache-beam-2.9.0/apache_beam.egg-info/SOURCES.txt`

 * *Files 2% similar despite different names*

```diff
@@ -13,26 +13,28 @@
 apache_beam.egg-info/SOURCES.txt
 apache_beam.egg-info/dependency_links.txt
 apache_beam.egg-info/entry_points.txt
 apache_beam.egg-info/not-zip-safe
 apache_beam.egg-info/requires.txt
 apache_beam.egg-info/top_level.txt
 apache_beam/coders/__init__.py
+apache_beam/coders/coder_impl.c
 apache_beam/coders/coder_impl.pxd
 apache_beam/coders/coder_impl.py
 apache_beam/coders/coders.py
 apache_beam/coders/coders_test.py
 apache_beam/coders/coders_test_common.py
 apache_beam/coders/fast_coders_test.py
 apache_beam/coders/observable.py
 apache_beam/coders/observable_test.py
 apache_beam/coders/proto2_coder_test_messages_pb2.py
 apache_beam/coders/slow_coders_test.py
 apache_beam/coders/slow_stream.py
 apache_beam/coders/standard_coders_test.py
+apache_beam/coders/stream.c
 apache_beam/coders/stream.pxd
 apache_beam/coders/stream.pyx
 apache_beam/coders/stream_test.py
 apache_beam/coders/typecoders.py
 apache_beam/coders/typecoders_test.py
 apache_beam/examples/__init__.py
 apache_beam/examples/avro_bitcoin.py
@@ -96,14 +98,16 @@
 apache_beam/examples/cookbook/filters_test.py
 apache_beam/examples/cookbook/group_with_coder.py
 apache_beam/examples/cookbook/group_with_coder_test.py
 apache_beam/examples/cookbook/mergecontacts.py
 apache_beam/examples/cookbook/mergecontacts_test.py
 apache_beam/examples/cookbook/multiple_output_pardo.py
 apache_beam/examples/cookbook/multiple_output_pardo_test.py
+apache_beam/examples/flink/__init__.py
+apache_beam/examples/flink/flink_streaming_impulse.py
 apache_beam/examples/snippets/__init__.py
 apache_beam/examples/snippets/snippets.py
 apache_beam/examples/snippets/snippets_test.py
 apache_beam/internal/__init__.py
 apache_beam/internal/module_test.py
 apache_beam/internal/pickler.py
 apache_beam/internal/pickler_test.py
@@ -143,14 +147,16 @@
 apache_beam/io/textio.py
 apache_beam/io/textio_test.py
 apache_beam/io/tfrecordio.py
 apache_beam/io/tfrecordio_test.py
 apache_beam/io/utils.py
 apache_beam/io/vcfio.py
 apache_beam/io/vcfio_test.py
+apache_beam/io/flink/__init__.py
+apache_beam/io/flink/flink_streaming_impulse_source.py
 apache_beam/io/gcp/__init__.py
 apache_beam/io/gcp/big_query_query_to_table_it_test.py
 apache_beam/io/gcp/big_query_query_to_table_pipeline.py
 apache_beam/io/gcp/bigquery.py
 apache_beam/io/gcp/bigquery_io_read_it_test.py
 apache_beam/io/gcp/bigquery_io_read_pipeline.py
 apache_beam/io/gcp/bigquery_test.py
@@ -191,20 +197,22 @@
 apache_beam/io/gcp/tests/pubsub_matcher.py
 apache_beam/io/gcp/tests/pubsub_matcher_test.py
 apache_beam/io/gcp/tests/utils.py
 apache_beam/io/gcp/tests/utils_test.py
 apache_beam/metrics/__init__.py
 apache_beam/metrics/cells.py
 apache_beam/metrics/cells_test.py
+apache_beam/metrics/execution.c
 apache_beam/metrics/execution.pxd
 apache_beam/metrics/execution.py
 apache_beam/metrics/execution_test.py
 apache_beam/metrics/metric.py
 apache_beam/metrics/metric_test.py
 apache_beam/metrics/metricbase.py
+apache_beam/metrics/monitoring_infos.py
 apache_beam/options/__init__.py
 apache_beam/options/pipeline_options.py
 apache_beam/options/pipeline_options_test.py
 apache_beam/options/pipeline_options_validator.py
 apache_beam/options/pipeline_options_validator_test.py
 apache_beam/options/value_provider.py
 apache_beam/options/value_provider_test.py
@@ -223,14 +231,15 @@
 apache_beam/portability/api/beam_runner_api_pb2.py
 apache_beam/portability/api/beam_runner_api_pb2_grpc.py
 apache_beam/portability/api/endpoints_pb2.py
 apache_beam/portability/api/endpoints_pb2_grpc.py
 apache_beam/portability/api/standard_window_fns_pb2.py
 apache_beam/portability/api/standard_window_fns_pb2_grpc.py
 apache_beam/runners/__init__.py
+apache_beam/runners/common.c
 apache_beam/runners/common.pxd
 apache_beam/runners/common.py
 apache_beam/runners/common_test.py
 apache_beam/runners/pipeline_context.py
 apache_beam/runners/pipeline_context_test.py
 apache_beam/runners/runner.py
 apache_beam/runners/runner_test.py
@@ -284,14 +293,16 @@
 apache_beam/runners/interactive/pipeline_analyzer.py
 apache_beam/runners/interactive/pipeline_analyzer_test.py
 apache_beam/runners/interactive/display/__init__.py
 apache_beam/runners/interactive/display/display_manager.py
 apache_beam/runners/interactive/display/interactive_pipeline_graph.py
 apache_beam/runners/interactive/display/pipeline_graph.py
 apache_beam/runners/interactive/display/pipeline_graph_renderer.py
+apache_beam/runners/internal/__init__.py
+apache_beam/runners/internal/names.py
 apache_beam/runners/job/__init__.py
 apache_beam/runners/job/manager.py
 apache_beam/runners/job/utils.py
 apache_beam/runners/portability/__init__.py
 apache_beam/runners/portability/flink_runner_test.py
 apache_beam/runners/portability/fn_api_runner.py
 apache_beam/runners/portability/fn_api_runner_test.py
@@ -307,29 +318,33 @@
 apache_beam/runners/test/__init__.py
 apache_beam/runners/worker/__init__.py
 apache_beam/runners/worker/bundle_processor.py
 apache_beam/runners/worker/data_plane.py
 apache_beam/runners/worker/data_plane_test.py
 apache_beam/runners/worker/log_handler.py
 apache_beam/runners/worker/log_handler_test.py
+apache_beam/runners/worker/logger.c
 apache_beam/runners/worker/logger.py
 apache_beam/runners/worker/logger_test.py
+apache_beam/runners/worker/opcounters.c
 apache_beam/runners/worker/opcounters.pxd
 apache_beam/runners/worker/opcounters.py
 apache_beam/runners/worker/opcounters_test.py
 apache_beam/runners/worker/operation_specs.py
+apache_beam/runners/worker/operations.c
 apache_beam/runners/worker/operations.pxd
 apache_beam/runners/worker/operations.py
 apache_beam/runners/worker/sdk_worker.py
 apache_beam/runners/worker/sdk_worker_main.py
 apache_beam/runners/worker/sdk_worker_main_test.py
 apache_beam/runners/worker/sdk_worker_test.py
 apache_beam/runners/worker/sideinputs.py
 apache_beam/runners/worker/sideinputs_test.py
 apache_beam/runners/worker/statesampler.py
+apache_beam/runners/worker/statesampler_fast.c
 apache_beam/runners/worker/statesampler_fast.pxd
 apache_beam/runners/worker/statesampler_fast.pyx
 apache_beam/runners/worker/statesampler_slow.py
 apache_beam/runners/worker/statesampler_test.py
 apache_beam/runners/worker/worker_id_interceptor.py
 apache_beam/runners/worker/worker_id_interceptor_test.py
 apache_beam/testing/__init__.py
@@ -353,29 +368,37 @@
 apache_beam/testing/benchmarks/nexmark/models/nexmark_model.py
 apache_beam/testing/benchmarks/nexmark/queries/__init__.py
 apache_beam/testing/benchmarks/nexmark/queries/query0.py
 apache_beam/testing/benchmarks/nexmark/queries/query1.py
 apache_beam/testing/benchmarks/nexmark/queries/query2.py
 apache_beam/testing/data/standard_coders.yaml
 apache_beam/testing/data/trigger_transcripts.yaml
+apache_beam/testing/load_tests/__init__.py
+apache_beam/testing/load_tests/co_group_by_key_test.py
+apache_beam/testing/load_tests/combine_test.py
+apache_beam/testing/load_tests/group_by_key_test.py
+apache_beam/testing/load_tests/load_test_metrics_utils.py
+apache_beam/testing/load_tests/pardo_test.py
 apache_beam/tools/__init__.py
 apache_beam/tools/coders_microbenchmark.py
 apache_beam/tools/distribution_counter_microbenchmark.py
 apache_beam/tools/map_fn_microbenchmark.py
 apache_beam/tools/microbenchmarks_test.py
 apache_beam/tools/sideinput_microbenchmark.py
 apache_beam/tools/utils.py
 apache_beam/transforms/__init__.py
 apache_beam/transforms/combiners.py
 apache_beam/transforms/combiners_test.py
 apache_beam/transforms/core.py
 apache_beam/transforms/create_source.py
 apache_beam/transforms/create_test.py
+apache_beam/transforms/cy_combiners.c
 apache_beam/transforms/cy_combiners.pxd
 apache_beam/transforms/cy_combiners.py
+apache_beam/transforms/cy_dataflow_distribution_counter.c
 apache_beam/transforms/cy_dataflow_distribution_counter.pxd
 apache_beam/transforms/cy_dataflow_distribution_counter.pyx
 apache_beam/transforms/dataflow_distribution_counter_test.py
 apache_beam/transforms/display.py
 apache_beam/transforms/display_test.py
 apache_beam/transforms/ptransform.py
 apache_beam/transforms/ptransform_test.py
@@ -402,23 +425,25 @@
 apache_beam/typehints/typecheck.py
 apache_beam/typehints/typed_pipeline_test.py
 apache_beam/typehints/typehints.py
 apache_beam/typehints/typehints_test.py
 apache_beam/utils/__init__.py
 apache_beam/utils/annotations.py
 apache_beam/utils/annotations_test.py
+apache_beam/utils/counters.c
 apache_beam/utils/counters.pxd
 apache_beam/utils/counters.py
 apache_beam/utils/counters_test.py
 apache_beam/utils/plugin.py
 apache_beam/utils/processes.py
 apache_beam/utils/processes_test.py
 apache_beam/utils/profiler.py
 apache_beam/utils/proto_utils.py
 apache_beam/utils/retry.py
 apache_beam/utils/retry_test.py
 apache_beam/utils/timestamp.py
 apache_beam/utils/timestamp_test.py
 apache_beam/utils/urns.py
+apache_beam/utils/windowed_value.c
 apache_beam/utils/windowed_value.pxd
 apache_beam/utils/windowed_value.py
 apache_beam/utils/windowed_value_test.py
```

## Comparing `apache-beam-2.8.0/apache_beam.egg-info/PKG-INFO` & `apache-beam-2.9.0/apache_beam.egg-info/PKG-INFO`

 * *Files 8% similar despite different names*

```diff
@@ -1,10 +1,10 @@
-Metadata-Version: 2.1
+Metadata-Version: 1.2
 Name: apache-beam
-Version: 2.8.0
+Version: 2.9.0
 Summary: Apache Beam SDK for Python
 Home-page: https://beam.apache.org
 Author: Apache Software Foundation
 Author-email: dev@beam.apache.org
 License: Apache License, Version 2.0
 Download-URL: https://pypi.python.org/pypi/apache-beam
 Description: 
@@ -18,10 +18,7 @@
 Classifier: Intended Audience :: End Users/Desktop
 Classifier: License :: OSI Approved :: Apache Software License
 Classifier: Operating System :: POSIX :: Linux
 Classifier: Programming Language :: Python :: 2.7
 Classifier: Topic :: Software Development :: Libraries
 Classifier: Topic :: Software Development :: Libraries :: Python Modules
 Requires-Python: >=2.7,<3.0
-Provides-Extra: test
-Provides-Extra: docs
-Provides-Extra: gcp
```

