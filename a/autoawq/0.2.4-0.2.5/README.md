# Comparing `tmp/autoawq-0.2.4-cp39-cp39-win_amd64.whl.zip` & `tmp/autoawq-0.2.5-cp39-cp39-win_amd64.whl.zip`

## zipinfo {}

```diff
@@ -1,61 +1,63 @@
-Zip file size: 81413 bytes, number of entries: 59
--rw-rw-rw-  2.0 fat       71 b- defN 24-Mar-24 11:28 awq/__init__.py
--rw-rw-rw-  2.0 fat      229 b- defN 24-Mar-24 11:28 awq/evaluation/__init__.py
--rw-rw-rw-  2.0 fat     5560 b- defN 24-Mar-24 11:28 awq/evaluation/eval_utils.py
--rw-rw-rw-  2.0 fat    13026 b- defN 24-Mar-24 11:28 awq/evaluation/humaneval_utils.py
--rw-rw-rw-  2.0 fat     5718 b- defN 24-Mar-24 11:28 awq/evaluation/kl_divergence.py
--rw-rw-rw-  2.0 fat      696 b- defN 24-Mar-24 11:28 awq/models/__init__.py
--rw-rw-rw-  2.0 fat     3883 b- defN 24-Mar-24 11:28 awq/models/_config.py
--rw-rw-rw-  2.0 fat     4783 b- defN 24-Mar-24 11:28 awq/models/aquila.py
--rw-rw-rw-  2.0 fat     3526 b- defN 24-Mar-24 11:28 awq/models/auto.py
--rw-rw-rw-  2.0 fat     4834 b- defN 24-Mar-24 11:28 awq/models/baichuan.py
--rw-rw-rw-  2.0 fat    20903 b- defN 24-Mar-24 11:28 awq/models/base.py
--rw-rw-rw-  2.0 fat     2356 b- defN 24-Mar-24 11:28 awq/models/bloom.py
--rw-rw-rw-  2.0 fat     4483 b- defN 24-Mar-24 11:28 awq/models/falcon.py
--rw-rw-rw-  2.0 fat     5346 b- defN 24-Mar-24 11:28 awq/models/gemma.py
--rw-rw-rw-  2.0 fat     1933 b- defN 24-Mar-24 11:28 awq/models/gpt_bigcode.py
--rw-rw-rw-  2.0 fat     2085 b- defN 24-Mar-24 11:28 awq/models/gpt_neox.py
--rw-rw-rw-  2.0 fat     1845 b- defN 24-Mar-24 11:28 awq/models/gptj.py
--rw-rw-rw-  2.0 fat     4811 b- defN 24-Mar-24 11:28 awq/models/llama.py
--rw-rw-rw-  2.0 fat     4959 b- defN 24-Mar-24 11:28 awq/models/llava.py
--rw-rw-rw-  2.0 fat     4805 b- defN 24-Mar-24 11:28 awq/models/mistral.py
--rw-rw-rw-  2.0 fat     6409 b- defN 24-Mar-24 11:28 awq/models/mixtral.py
--rw-rw-rw-  2.0 fat     3716 b- defN 24-Mar-24 11:28 awq/models/mpt.py
--rw-rw-rw-  2.0 fat     2078 b- defN 24-Mar-24 11:28 awq/models/opt.py
--rw-rw-rw-  2.0 fat     1486 b- defN 24-Mar-24 11:28 awq/models/qwen.py
--rw-rw-rw-  2.0 fat     4749 b- defN 24-Mar-24 11:28 awq/models/qwen2.py
--rw-rw-rw-  2.0 fat     4343 b- defN 24-Mar-24 11:28 awq/models/yi.py
--rw-rw-rw-  2.0 fat        0 b- defN 24-Mar-24 11:28 awq/modules/__init__.py
--rw-rw-rw-  2.0 fat      307 b- defN 24-Mar-24 11:28 awq/modules/act.py
--rw-rw-rw-  2.0 fat        0 b- defN 24-Mar-24 11:28 awq/modules/fused/__init__.py
--rw-rw-rw-  2.0 fat    10875 b- defN 24-Mar-24 11:28 awq/modules/fused/attn.py
--rw-rw-rw-  2.0 fat     8506 b- defN 24-Mar-24 11:28 awq/modules/fused/block.py
--rw-rw-rw-  2.0 fat     2533 b- defN 24-Mar-24 11:28 awq/modules/fused/cache.py
--rw-rw-rw-  2.0 fat     2474 b- defN 24-Mar-24 11:28 awq/modules/fused/mlp.py
--rw-rw-rw-  2.0 fat     6913 b- defN 24-Mar-24 11:28 awq/modules/fused/model.py
--rw-rw-rw-  2.0 fat     6073 b- defN 24-Mar-24 11:28 awq/modules/fused/moe.py
--rw-rw-rw-  2.0 fat      698 b- defN 24-Mar-24 11:28 awq/modules/fused/norm.py
--rw-rw-rw-  2.0 fat      285 b- defN 24-Mar-24 11:28 awq/modules/linear/__init__.py
--rw-rw-rw-  2.0 fat     4557 b- defN 24-Mar-24 11:28 awq/modules/linear/exllama.py
--rw-rw-rw-  2.0 fat     6855 b- defN 24-Mar-24 11:28 awq/modules/linear/exllamav2.py
--rw-rw-rw-  2.0 fat     8924 b- defN 24-Mar-24 11:28 awq/modules/linear/gemm.py
--rw-rw-rw-  2.0 fat     6836 b- defN 24-Mar-24 11:28 awq/modules/linear/gemv.py
--rw-rw-rw-  2.0 fat     6955 b- defN 24-Mar-24 11:28 awq/modules/linear/gemv_fast.py
--rw-rw-rw-  2.0 fat     7403 b- defN 24-Mar-24 11:28 awq/modules/linear/marlin.py
--rw-rw-rw-  2.0 fat        0 b- defN 24-Mar-24 11:28 awq/quantize/__init__.py
--rw-rw-rw-  2.0 fat    20580 b- defN 24-Mar-24 11:28 awq/quantize/quantizer.py
--rw-rw-rw-  2.0 fat     5344 b- defN 24-Mar-24 11:28 awq/quantize/scale.py
--rw-rw-rw-  2.0 fat        0 b- defN 24-Mar-24 11:28 awq/utils/__init__.py
--rw-rw-rw-  2.0 fat     2178 b- defN 24-Mar-24 11:28 awq/utils/calib_data.py
--rw-rw-rw-  2.0 fat     8708 b- defN 24-Mar-24 11:28 awq/utils/fused_utils.py
--rw-rw-rw-  2.0 fat     1771 b- defN 24-Mar-24 11:28 awq/utils/module.py
--rw-rw-rw-  2.0 fat     3271 b- defN 24-Mar-24 11:28 awq/utils/packing_utils.py
--rw-rw-rw-  2.0 fat      863 b- defN 24-Mar-24 11:28 awq/utils/parallel.py
--rw-rw-rw-  2.0 fat     5064 b- defN 24-Mar-24 11:28 awq/utils/quant_utils.py
--rw-rw-rw-  2.0 fat     3176 b- defN 24-Mar-24 11:28 awq/utils/utils.py
--rw-rw-rw-  2.0 fat     1089 b- defN 24-Mar-24 11:36 autoawq-0.2.4.dist-info/LICENSE
--rw-rw-rw-  2.0 fat    16531 b- defN 24-Mar-24 11:36 autoawq-0.2.4.dist-info/METADATA
--rw-rw-rw-  2.0 fat      100 b- defN 24-Mar-24 11:36 autoawq-0.2.4.dist-info/WHEEL
--rw-rw-rw-  2.0 fat       16 b- defN 24-Mar-24 11:36 autoawq-0.2.4.dist-info/top_level.txt
--rw-rw-r--  2.0 fat     4687 b- defN 24-Mar-24 11:36 autoawq-0.2.4.dist-info/RECORD
-59 files, 272205 bytes uncompressed, 74115 bytes compressed:  72.8%
+Zip file size: 84951 bytes, number of entries: 61
+-rw-rw-rw-  2.0 fat       71 b- defN 24-May-02 18:23 awq/__init__.py
+-rw-rw-rw-  2.0 fat      229 b- defN 24-May-02 18:23 awq/evaluation/__init__.py
+-rw-rw-rw-  2.0 fat     5560 b- defN 24-May-02 18:23 awq/evaluation/eval_utils.py
+-rw-rw-rw-  2.0 fat    13026 b- defN 24-May-02 18:23 awq/evaluation/humaneval_utils.py
+-rw-rw-rw-  2.0 fat     5718 b- defN 24-May-02 18:23 awq/evaluation/kl_divergence.py
+-rw-rw-rw-  2.0 fat      792 b- defN 24-May-02 18:23 awq/models/__init__.py
+-rw-rw-rw-  2.0 fat     3883 b- defN 24-May-02 18:23 awq/models/_config.py
+-rw-rw-rw-  2.0 fat     4783 b- defN 24-May-02 18:23 awq/models/aquila.py
+-rw-rw-rw-  2.0 fat     3828 b- defN 24-May-02 18:23 awq/models/auto.py
+-rw-rw-rw-  2.0 fat     4834 b- defN 24-May-02 18:23 awq/models/baichuan.py
+-rw-rw-rw-  2.0 fat    22547 b- defN 24-May-02 18:23 awq/models/base.py
+-rw-rw-rw-  2.0 fat     2356 b- defN 24-May-02 18:23 awq/models/bloom.py
+-rw-rw-rw-  2.0 fat     4483 b- defN 24-May-02 18:23 awq/models/falcon.py
+-rw-rw-rw-  2.0 fat     5346 b- defN 24-May-02 18:23 awq/models/gemma.py
+-rw-rw-rw-  2.0 fat     1933 b- defN 24-May-02 18:23 awq/models/gpt_bigcode.py
+-rw-rw-rw-  2.0 fat     2085 b- defN 24-May-02 18:23 awq/models/gpt_neox.py
+-rw-rw-rw-  2.0 fat     1845 b- defN 24-May-02 18:23 awq/models/gptj.py
+-rw-rw-rw-  2.0 fat     4811 b- defN 24-May-02 18:23 awq/models/llama.py
+-rw-rw-rw-  2.0 fat     4959 b- defN 24-May-02 18:23 awq/models/llava.py
+-rw-rw-rw-  2.0 fat     4805 b- defN 24-May-02 18:23 awq/models/mistral.py
+-rw-rw-rw-  2.0 fat     6409 b- defN 24-May-02 18:23 awq/models/mixtral.py
+-rw-rw-rw-  2.0 fat     3716 b- defN 24-May-02 18:23 awq/models/mpt.py
+-rw-rw-rw-  2.0 fat     2078 b- defN 24-May-02 18:23 awq/models/opt.py
+-rw-rw-rw-  2.0 fat     1486 b- defN 24-May-02 18:23 awq/models/qwen.py
+-rw-rw-rw-  2.0 fat     4749 b- defN 24-May-02 18:23 awq/models/qwen2.py
+-rw-rw-rw-  2.0 fat     4767 b- defN 24-May-02 18:23 awq/models/stablelm.py
+-rw-rw-rw-  2.0 fat     4707 b- defN 24-May-02 18:23 awq/models/starcoder2.py
+-rw-rw-rw-  2.0 fat     4343 b- defN 24-May-02 18:23 awq/models/yi.py
+-rw-rw-rw-  2.0 fat        0 b- defN 24-May-02 18:23 awq/modules/__init__.py
+-rw-rw-rw-  2.0 fat      307 b- defN 24-May-02 18:23 awq/modules/act.py
+-rw-rw-rw-  2.0 fat        0 b- defN 24-May-02 18:23 awq/modules/fused/__init__.py
+-rw-rw-rw-  2.0 fat    12560 b- defN 24-May-02 18:23 awq/modules/fused/attn.py
+-rw-rw-rw-  2.0 fat     8600 b- defN 24-May-02 18:23 awq/modules/fused/block.py
+-rw-rw-rw-  2.0 fat     2533 b- defN 24-May-02 18:23 awq/modules/fused/cache.py
+-rw-rw-rw-  2.0 fat     2474 b- defN 24-May-02 18:23 awq/modules/fused/mlp.py
+-rw-rw-rw-  2.0 fat     7070 b- defN 24-May-02 18:23 awq/modules/fused/model.py
+-rw-rw-rw-  2.0 fat     6073 b- defN 24-May-02 18:23 awq/modules/fused/moe.py
+-rw-rw-rw-  2.0 fat      698 b- defN 24-May-02 18:23 awq/modules/fused/norm.py
+-rw-rw-rw-  2.0 fat      285 b- defN 24-May-02 18:23 awq/modules/linear/__init__.py
+-rw-rw-rw-  2.0 fat     4557 b- defN 24-May-02 18:23 awq/modules/linear/exllama.py
+-rw-rw-rw-  2.0 fat     6855 b- defN 24-May-02 18:23 awq/modules/linear/exllamav2.py
+-rw-rw-rw-  2.0 fat     8926 b- defN 24-May-02 18:23 awq/modules/linear/gemm.py
+-rw-rw-rw-  2.0 fat     6836 b- defN 24-May-02 18:23 awq/modules/linear/gemv.py
+-rw-rw-rw-  2.0 fat     6998 b- defN 24-May-02 18:23 awq/modules/linear/gemv_fast.py
+-rw-rw-rw-  2.0 fat     7403 b- defN 24-May-02 18:23 awq/modules/linear/marlin.py
+-rw-rw-rw-  2.0 fat        0 b- defN 24-May-02 18:23 awq/quantize/__init__.py
+-rw-rw-rw-  2.0 fat    20705 b- defN 24-May-02 18:23 awq/quantize/quantizer.py
+-rw-rw-rw-  2.0 fat     5344 b- defN 24-May-02 18:23 awq/quantize/scale.py
+-rw-rw-rw-  2.0 fat        0 b- defN 24-May-02 18:23 awq/utils/__init__.py
+-rw-rw-rw-  2.0 fat     2178 b- defN 24-May-02 18:23 awq/utils/calib_data.py
+-rw-rw-rw-  2.0 fat     8708 b- defN 24-May-02 18:23 awq/utils/fused_utils.py
+-rw-rw-rw-  2.0 fat     1771 b- defN 24-May-02 18:23 awq/utils/module.py
+-rw-rw-rw-  2.0 fat     3271 b- defN 24-May-02 18:23 awq/utils/packing_utils.py
+-rw-rw-rw-  2.0 fat      863 b- defN 24-May-02 18:23 awq/utils/parallel.py
+-rw-rw-rw-  2.0 fat     5064 b- defN 24-May-02 18:23 awq/utils/quant_utils.py
+-rw-rw-rw-  2.0 fat     3176 b- defN 24-May-02 18:23 awq/utils/utils.py
+-rw-rw-rw-  2.0 fat     1089 b- defN 24-May-02 18:30 autoawq-0.2.5.dist-info/LICENSE
+-rw-rw-rw-  2.0 fat    16522 b- defN 24-May-02 18:30 autoawq-0.2.5.dist-info/METADATA
+-rw-rw-rw-  2.0 fat      100 b- defN 24-May-02 18:30 autoawq-0.2.5.dist-info/WHEEL
+-rw-rw-rw-  2.0 fat       16 b- defN 24-May-02 18:30 autoawq-0.2.5.dist-info/top_level.txt
+-rw-rw-r--  2.0 fat     4847 b- defN 24-May-02 18:30 autoawq-0.2.5.dist-info/RECORD
+61 files, 285978 bytes uncompressed, 77409 bytes compressed:  72.9%
```

## zipnote {}

```diff
@@ -69,14 +69,20 @@
 
 Filename: awq/models/qwen.py
 Comment: 
 
 Filename: awq/models/qwen2.py
 Comment: 
 
+Filename: awq/models/stablelm.py
+Comment: 
+
+Filename: awq/models/starcoder2.py
+Comment: 
+
 Filename: awq/models/yi.py
 Comment: 
 
 Filename: awq/modules/__init__.py
 Comment: 
 
 Filename: awq/modules/act.py
@@ -156,23 +162,23 @@
 
 Filename: awq/utils/quant_utils.py
 Comment: 
 
 Filename: awq/utils/utils.py
 Comment: 
 
-Filename: autoawq-0.2.4.dist-info/LICENSE
+Filename: autoawq-0.2.5.dist-info/LICENSE
 Comment: 
 
-Filename: autoawq-0.2.4.dist-info/METADATA
+Filename: autoawq-0.2.5.dist-info/METADATA
 Comment: 
 
-Filename: autoawq-0.2.4.dist-info/WHEEL
+Filename: autoawq-0.2.5.dist-info/WHEEL
 Comment: 
 
-Filename: autoawq-0.2.4.dist-info/top_level.txt
+Filename: autoawq-0.2.5.dist-info/top_level.txt
 Comment: 
 
-Filename: autoawq-0.2.4.dist-info/RECORD
+Filename: autoawq-0.2.5.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## awq/__init__.py

```diff
@@ -1,2 +1,2 @@
-__version__ = "0.2.4"
+__version__ = "0.2.5"
 from awq.models.auto import AutoAWQForCausalLM
```

## awq/models/__init__.py

```diff
@@ -11,7 +11,9 @@
 from .yi import YiAWQForCausalLM
 from .qwen import QwenAWQForCausalLM
 from .baichuan import BaichuanAWQForCausalLM
 from .llava import LlavaAWQForCausalLM
 from .mixtral import MixtralAWQForCausalLM
 from .qwen2 import Qwen2AWQForCausalLM
 from .gemma import GemmaAWQForCausalLM
+from .stablelm import StableLmAWQForCausalLM
+from .starcoder2 import Starcoder2AWQForCausalLM
```

## awq/models/auto.py

```diff
@@ -20,14 +20,16 @@
     "aquila": AquilaAWQForCausalLM,
     "Yi": YiAWQForCausalLM,
     "qwen": QwenAWQForCausalLM,
     "baichuan": BaichuanAWQForCausalLM,
     "llava": LlavaAWQForCausalLM,
     "qwen2": Qwen2AWQForCausalLM,
     "gemma": GemmaAWQForCausalLM,
+    "stablelm": StableLmAWQForCausalLM,
+    "starcoder2": Starcoder2AWQForCausalLM,
 }
 
 
 def check_and_get_model_type(model_dir, trust_remote_code=True, **model_init_kwargs):
     config = AutoConfig.from_pretrained(
         model_dir, trust_remote_code=trust_remote_code, **model_init_kwargs
     )
@@ -47,26 +49,28 @@
     @classmethod
     def from_pretrained(
         self,
         model_path,
         trust_remote_code=True,
         safetensors=True,
         device_map=None,
+        download_kwargs=None,
         **model_init_kwargs,
     ) -> BaseAWQForCausalLM:
         model_type = check_and_get_model_type(
             model_path, trust_remote_code, **model_init_kwargs
         )
 
         return AWQ_CAUSAL_LM_MODEL_MAP[model_type].from_pretrained(
             model_path,
             model_type,
             trust_remote_code=trust_remote_code,
             safetensors=safetensors,
             device_map=device_map,
+            download_kwargs=download_kwargs,
             **model_init_kwargs,
         )
 
     @classmethod
     def from_quantized(
         self,
         quant_path,
@@ -75,15 +79,17 @@
         trust_remote_code=True,
         fuse_layers=True,
         use_exllama=False,
         use_exllama_v2=False,
         batch_size=1,
         safetensors=True,
         device_map="balanced",
+        max_memory=None,
         offload_folder=None,
+        download_kwargs=None,
         **config_kwargs,
     ) -> BaseAWQForCausalLM:
         os.environ["AWQ_BATCH_SIZE"] = str(batch_size)
         model_type = check_and_get_model_type(quant_path, trust_remote_code)
 
         if config_kwargs.get("max_new_tokens") is not None:
             max_seq_len = config_kwargs["max_new_tokens"]
@@ -99,10 +105,12 @@
             max_seq_len,
             trust_remote_code=trust_remote_code,
             fuse_layers=fuse_layers,
             use_exllama=use_exllama,
             use_exllama_v2=use_exllama_v2,
             safetensors=safetensors,
             device_map=device_map,
+            max_memory=max_memory,
             offload_folder=offload_folder,
+            download_kwargs=download_kwargs,
             **config_kwargs,
         )
```

## awq/models/base.py

```diff
@@ -64,14 +64,16 @@
     "aquila": "AutoModelForCausalLM",
     "Yi": "AutoModelForCausalLM",
     "qwen": "AutoModelForCausalLM",
     "baichuan": "AutoModelForCausalLM",
     "llava": "AutoModelForVision2Seq",
     "qwen2": "AutoModelForCausalLM",
     "gemma": "AutoModelForCausalLM",
+    "stablelm": "AutoModelForCausalLM",
+    "starcoder2": "AutoModelForCausalLM",
 }
 
 
 class BaseAWQForCausalLM(nn.Module):
     def __init__(
         self,
         model: Annotated[PreTrainedModel, Doc("The pretrained or quantized model.")],
@@ -132,14 +134,20 @@
         ] = True,
         export_compatible: Annotated[
             bool,
             Doc(
                 "This argument avoids real quantization by only applying the scales without quantizing down to FP16."
             ),
         ] = False,
+        apply_clip: Annotated[
+            bool,
+            Doc(
+                "Whether to apply clipping to the model during quantization. Some models may perform better with this set to False."
+            ),
+        ] = True,
     ):
         """
         The main quantization function that you can use to quantize your model.
 
         Example:
 
         ```python
@@ -169,14 +177,15 @@
             self.quant_config.version,
             calib_data,
             split,
             text_column,
             duo_scaling,
             modules_to_not_convert=self.quant_config.modules_to_not_convert,
             export_compatible=export_compatible,
+            apply_clip=apply_clip,
         )
         self.quantizer.quantize()
 
         self.is_quantized = True
 
     @torch.no_grad()
     def pack(self):
@@ -286,25 +295,30 @@
         ] = True,
         device_map: Annotated[
             Union[str, Dict],
             Doc(
                 "A device map that will be passed onto the model loading method from transformers."
             ),
         ] = None,
+        download_kwargs: Annotated[
+            Dict, Doc("Used for configure download model"),
+        ] = None,
         **model_init_kwargs: Annotated[
             Dict,
             Doc(
                 "Additional kwargs that are passed to the model during initialization."
             ),
         ],
     ):
         """A method for initialization of pretrained models, usually in FP16."""
         # Get weights path and quant config
         model_weights_path, config, quant_config = self._load_config(
-            self, model_path, "", safetensors, trust_remote_code=trust_remote_code
+            self, model_path, "", safetensors,
+            trust_remote_code=trust_remote_code,
+            download_kwargs=download_kwargs
         )
 
         target_cls_name = TRANSFORMERS_AUTO_MAPPING_DICT[config.model_type]
         target_cls = getattr(transformers, target_cls_name)
 
         processor = None
         if target_cls_name == "AutoModelForVision2Seq":
@@ -375,18 +389,27 @@
         ] = False,
         device_map: Annotated[
             Union[str, Dict],
             Doc(
                 "A device map that will be passed onto the model loading method from transformers."
             ),
         ] = "balanced",
+        max_memory: Annotated[
+            Dict[Union[int, str], Union[int, str]], 
+            Doc(
+                'A dictionary device identifier to maximum memory which will be passed onto the model loading method from transformers. For example：{0: "4GB",1: "10GB"'
+            ),
+        ] = None,
         offload_folder: Annotated[
             str,
             Doc("The folder ot offload the model to."),
         ] = None,
+        download_kwargs: Annotated[
+            Dict, Doc("Used for configure download model"),
+        ] = None,
         **config_kwargs: Annotated[
             Dict,
             Doc(
                 "Additional kwargs that are passed to the config during initialization."
             ),
         ],
     ):
@@ -395,14 +418,15 @@
         model_weights_path, config, quant_config = self._load_config(
             self,
             model_path,
             model_filename,
             safetensors,
             trust_remote_code,
             max_seq_len=max_seq_len,
+            download_kwargs=download_kwargs,
             **config_kwargs,
         )
 
         target_cls_name = TRANSFORMERS_AUTO_MAPPING_DICT[config.model_type]
         target_cls = getattr(transformers, target_cls_name)
 
         # [STEP 3] Load model
@@ -427,14 +451,15 @@
 
         # loads the weights into modules and distributes
         # across available devices automatically
         load_checkpoint_and_dispatch(
             model,
             checkpoint=model_weights_path,
             device_map=device_map,
+            max_memory=max_memory,
             no_split_module_classes=[self.layer_type],
             offload_folder=offload_folder,
             dtype=torch_dtype,
         )
 
         # Dispath to devices
         if fuse_layers:
@@ -466,25 +491,37 @@
     def _load_config(
         self,
         model_path,
         model_filename,
         safetensors=True,
         trust_remote_code=True,
         max_seq_len=4096,
+        download_kwargs=None,
         **config_kwargs,
     ):
         # [STEP 1] Download model if path is not a directory
         if not os.path.isdir(model_path):
             ignore_patterns = ["*msgpack*", "*h5*", "optimizer.pt"]
             if safetensors:
                 ignore_patterns.extend(["*.pt*", "*.bin*", "consolidated*"])
             else:
                 ignore_patterns.append("*.safetensors*")
+            
+            if download_kwargs is None:
+                download_kwargs = {}
+            
+            if "ignore_patterns" in download_kwargs:
+                download_kwargs_ignore_patterns = download_kwargs.pop("ignore_patterns")
+
+                if isinstance(download_kwargs_ignore_patterns, str):
+                    ignore_patterns.append(download_kwargs_ignore_patterns)
+                elif isinstance(download_kwargs_ignore_patterns, list):
+                    ignore_patterns.extend(download_kwargs_ignore_patterns)
 
-            model_path = snapshot_download(model_path, ignore_patterns=ignore_patterns)
+            model_path = snapshot_download(model_path, ignore_patterns=ignore_patterns, **download_kwargs)
 
         if model_filename != "":
             model_weights_path = model_path + f"/{model_filename}"
         else:
             model_weights_path = model_path
 
         # [STEP 2] Load config and set sequence length
```

## awq/modules/fused/attn.py

```diff
@@ -25,17 +25,15 @@
 
 
 class RoPE(nn.Module):
     def __init__(self, head_dim, max_seq_len, device, rope_theta):
         super(RoPE, self).__init__()
 
         self.freqs_cis = nn.Parameter(
-            self.precompute_freqs_cis(
-                head_dim, max_seq_len * 2, rope_theta
-            ).to(device),
+            self.precompute_freqs_cis(head_dim, max_seq_len * 2, rope_theta).to(device),
             requires_grad=False,
         )
 
     @staticmethod
     def precompute_freqs_cis(dim: int, end: int, theta=10000.0):
         freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))
         t = torch.arange(end)
@@ -114,24 +112,25 @@
         qkv_layer,
         o_proj,
         dev,
         max_seq_len=2048,
         use_alibi=False,
         attention_shapes=None,
         rope_theta=10000,
+        partial_rotary_factor=1.0,
         head_dim=None,
         **kwargs
     ):
         super().__init__()
         self.hidden_size = hidden_size
         self.n_heads = n_heads
         self.n_kv_heads = n_kv_heads
         self.n_kv_groups = n_heads // n_kv_heads if n_kv_heads != 0 else 0
         self.head_dim = head_dim
-        
+
         if head_dim is None:
             self.head_dim = hidden_size // n_heads
 
         self.qkv_proj = qkv_layer
         self.o_proj = o_proj
         self.start_pos = 0
         self.use_alibi = use_alibi
@@ -163,16 +162,17 @@
 
         if use_alibi:
             self.alibi = ALiBi(n_heads, max_seq_len, dev)
             self.rotary_dim = 0
             self.is_neox = False
         else:
             self.alibi = None
-            self.rope = RoPE(self.head_dim, max_seq_len, dev, rope_theta)
-            self.rotary_dim = self.head_dim
+            self.partial_rotary_factor = partial_rotary_factor
+            self.rotary_dim = int(self.head_dim * self.partial_rotary_factor)
+            self.rope = RoPE(self.rotary_dim, max_seq_len, dev, rope_theta)
             self.is_neox = True
 
     def forward(
         self, hidden_states: torch.Tensor, attention_mask=None, *args, **kwargs
     ):
         bsz, seqlen, _ = hidden_states.shape
 
@@ -184,49 +184,68 @@
             elif bsz < self.cache_batch_size:
                 self.cache.decrease_batch_size(bsz)
                 self.cache_batch_size = bsz
 
             # Always reset to 0
             self.start_pos = 0
 
+        hf_is_generating = False
+        hf_is_first_forward = "past_key_value" in kwargs and kwargs["past_key_value"] is None
+        hf_is_new_cache_first_forward = "past_key_value" in kwargs and isinstance(kwargs["past_key_value"], DynamicCache) and kwargs["past_key_value"].get_seq_length() == 0
+
+        if self.is_hf_transformers and "use_cache" in kwargs:
+            hf_is_generating = kwargs["use_cache"]
+
+        # print(kwargs["past_key_value"].get_seq_length())
+
         # In case we re-generate, we need to refresh the starting position
         # to 0. We detect it by checking if `past_key_values` is set to None,
         # which indicates that we are on the first step of `generate()`.
         # This is only applicable for `transformers` integration
-        if (
-            self.is_hf_transformers
-            and "past_key_value" in kwargs
-            and kwargs["past_key_value"] is None
-        ):
+        if (self.is_hf_transformers and (hf_is_first_forward or hf_is_new_cache_first_forward)) or (self.is_hf_transformers and not hf_is_generating):
             self.start_pos = 0
+    
 
         xqkv = self.qkv_proj(hidden_states)
         xqkv = xqkv.view((bsz, seqlen) + self.attention_shapes["xqkv_view"])
 
         xq = self.attention_shapes["xq_slice"](xqkv)
         xk = self.attention_shapes["xk_slice"](xqkv)
         xv = self.attention_shapes["xv_slice"](xqkv)
 
-        if seqlen > 1 or not FT_INSTALLED:
+        if seqlen > 1 or self.partial_rotary_factor < 1 or not FT_INSTALLED:
             xq = xq.view((bsz, seqlen) + self.attention_shapes["xq_view"])
             xk = xk.view((bsz, seqlen) + self.attention_shapes["xk_view"])
             xv = xv.view((bsz, seqlen) + self.attention_shapes["xv_view"])
 
             if not self.use_alibi:
-                xq, xk = self.rope.forward(xq, xk, self.start_pos, seqlen)
-
-            self.cache.to(xq)
+                # Partial rotary embedding
+                if self.partial_rotary_factor < 1:
+                    xq_rot, xq_pass = (
+                        xq[..., : self.rotary_dim],
+                        xq[..., self.rotary_dim :],
+                    )
+                    xk_rot, xk_pass = (
+                        xk[..., : self.rotary_dim],
+                        xk[..., self.rotary_dim :],
+                    )
+                    xq_rot, xk_rot = self.rope.forward(xq_rot, xk_rot, self.start_pos, seqlen)
+                    xq = torch.cat((xq_rot, xq_pass), dim=-1)
+                    xk = torch.cat((xk_rot, xk_pass), dim=-1)
+                else:
+                    xq, xk = self.rope.forward(xq, xk, self.start_pos, seqlen)
 
             values_store = xv.transpose(2, 1)
             keys_store = (
                 xk.reshape((bsz, seqlen) + self.attention_shapes["xk_reshape"])
                 .permute(0, 2, 3, 1, 4)
                 .contiguous()
             )
 
+            self.cache.to(xq)
             self.cache.update_kv(values_store, keys_store, bsz, self.start_pos, seqlen)
 
             # Only necessary to retrieve from cache when we are not processing context
             if seqlen == 1:
                 xv, xk = self.cache.get_kv(bsz, self.start_pos, seqlen, self.head_dim)
 
             keys = xk
@@ -244,14 +263,19 @@
             scores = torch.matmul(xq, keys.transpose(2, 3)) / math.sqrt(self.head_dim)
 
             if self.use_alibi:
                 scores = self.alibi.forward(scores, seqlen)
 
             # When seqlen is 1, there is nothing else to attend to
             if attention_mask is not None and seqlen > 1:
+                # For llama-arch, the causal mask is preallocated with bsz x 1 x max_seq_len x max_seq_len, thus we 
+                # need to slice it
+                if attention_mask.shape[-1] != seqlen:
+                    attention_mask = attention_mask[:, :, :seqlen, :seqlen]
+
                 scores = (
                     scores + attention_mask
                 )  # (bs, n_local_heads, slen, cache_len + slen)
             scores = F.softmax(scores.float(), dim=-1).type_as(xq)
             output = torch.matmul(scores, values)  # (bs, n_local_heads, slen, head_dim)
             attention_weight = output.transpose(1, 2).contiguous().view(bsz, seqlen, -1)
         else:
@@ -274,18 +298,22 @@
                 self.is_neox,  # is neox
             )
             attention_weight = attention_weight.reshape(bsz, 1, -1)
 
         attn_output = self.o_proj(attention_weight)
         self.start_pos += seqlen
 
+        if self.is_hf_transformers and not hf_is_generating:
+            self.start_pos = 0
+
         # past_key_value is replaced with cache_v, cache_k, returning empty data
         # we pass a dummy past kv cache for transformers to be able to retrieve the correct info
         # about past key length
         past_key_value = [torch.zeros(1, 1, self.start_pos, 1)]
 
+
         if HF_NEW_CACHE_FORMAT and self.is_hf_transformers:
             new_cache = DynamicCache()
             new_cache.update(past_key_value[0], past_key_value[0], layer_idx=0)
             past_key_value = new_cache
 
         return attn_output, attention_weight, past_key_value
```

## awq/modules/fused/block.py

```diff
@@ -75,14 +75,15 @@
         o_proj,
         mlp,
         norm_1,
         norm_2,
         dev,
         max_seq_len,
         rope_theta=10000,
+        partial_rotary_factor=1.0,
         use_alibi=False,
         head_dim=None,
     ):
         super().__init__()
         self.n_heads = n_heads
         self.n_kv_heads = n_kv_heads
         self.head_dim = hidden_size // n_heads
@@ -99,14 +100,15 @@
             self.n_kv_heads,
             qkv_layer,
             o_proj,
             dev=dev,
             max_seq_len=max_seq_len,
             use_alibi=use_alibi,
             rope_theta=rope_theta,
+            partial_rotary_factor=partial_rotary_factor,
             head_dim=head_dim,
         ).to(dev)
         self.norm_2 = norm_2.to(dev)
         self.mlp = mlp.to(dev)
         self.device = dev
 
     def forward(
```

## awq/modules/fused/model.py

```diff
@@ -79,14 +79,22 @@
     def __init__(self, vocab_size, blocks, embedding, norm):
         super().__init__()
         self.vocab_size = vocab_size
         self.embedding = embedding
         self.blocks: List[LlamaLikeBlock] = nn.ModuleList(blocks)
         self.norm = norm
         self.last_forward_num_tokens = 0
+        
+    @property
+    def embed_tokens(self):
+        return self.embedding
+    
+    @property
+    def layers(self):
+        return self.blocks
 
     @torch.inference_mode()
     def forward(
         self,
         input_ids: torch.Tensor,
         attn_bias=None,
         attention_mask=None,
```

## awq/modules/linear/gemm.py

```diff
@@ -59,15 +59,15 @@
 
         return out
 
     @staticmethod
     def backward(ctx, grad_output):
         input, qweight, qzeros, scales, bias = ctx.saved_tensors
 
-        if awq_ext is None:
+        if not AWQ_INSTALLED:
             raise ValueError(
                 "auto-awq kernels is needed to be installed to use `.backward()`. Make sure to install the auto-awq kernels"
                 " by following the installation guides in https://github.com/casper-hansen/AutoAWQ_kernels"
             )
 
         # Cast to correct dtype for mixed precision training
         weights = awq_ext.dequantize_weights_cuda(
```

## awq/modules/linear/gemv_fast.py

```diff
@@ -185,15 +185,16 @@
         awq_linear.qzeros = qzeros.transpose(1, 0).contiguous()
 
         return awq_linear
 
     @torch.no_grad()
     def forward(self, x):
         inputs = x
-        if inputs.numel() / inputs.shape[-1] < 8:
+        batch_size, n_tokens, _ = inputs.shape
+        if batch_size < 8 and n_tokens == 1:
             out = awq_v2_ext.gemv_forward_cuda_decode(
                 inputs,
                 self.qweight,
                 self.scales,
                 self.qzeros,
                 inputs.numel() // inputs.shape[-1],
                 self.out_features,
```

## awq/quantize/quantizer.py

```diff
@@ -36,27 +36,29 @@
         version,
         calib_data,
         split,
         text_column,
         duo_scaling,
         modules_to_not_convert=None,
         export_compatible=False,
+        apply_clip=True,
     ) -> None:
         self.awq_model = awq_model
         self.model = model
         self.tokenizer = tokenizer
         self.w_bit = w_bit
         self.group_size = group_size
         self.zero_point = zero_point
         self.version = version
         self.calib_data = calib_data
         self.split = split
         self.text_column = text_column
         self.duo_scaling = duo_scaling
         self.export_compatible = export_compatible
+        self.apply_clip = apply_clip
         self.modules_to_not_convert = (
             modules_to_not_convert if modules_to_not_convert is not None else []
         )
         self.modules, self.module_kwargs, self.inps = self.init_quant()
 
     def pseudo_quantize_tensor(self, w: torch.Tensor):
         org_w_shape = w.shape
@@ -157,21 +159,22 @@
             ]
             apply_scale(self.modules[i], scales_list, input_feat_dict=input_feat)
             scales_list = append_str_prefix(
                 scales_list, get_op_name(self.model, self.modules[i]) + "."
             )
 
             # [STEP 3]: Compute and apply clipping list
-            clip_list = self._search_best_clip(
-                self.modules[i], named_linears, input_feat
-            )
-            apply_clip(self.modules[i], clip_list)
-            clip_list = append_str_prefix(
-                clip_list, get_op_name(self.model, self.modules[i]) + "."
-            )
+            if self.apply_clip:
+                clip_list = self._search_best_clip(
+                    self.modules[i], named_linears, input_feat
+                )
+                apply_clip(self.modules[i], clip_list)
+                clip_list = append_str_prefix(
+                    clip_list, get_op_name(self.model, self.modules[i]) + "."
+                )
 
             # [STEP 4]: Quantize weights
             if not self.export_compatible:
                 self._apply_quant(self.modules[i], named_linears)
 
             clear_memory()
```

## Comparing `autoawq-0.2.4.dist-info/LICENSE` & `autoawq-0.2.5.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `autoawq-0.2.4.dist-info/METADATA` & `autoawq-0.2.5.dist-info/METADATA`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: autoawq
-Version: 0.2.4
+Version: 0.2.5
 Summary: AutoAWQ implements the AWQ algorithm for 4-bit quantization with a 2x speedup during inference.
 Home-page: https://github.com/casper-hansen/AutoAWQ
 Author: Casper Hansen
 License: MIT
 Keywords: awq,autoawq,quantization,transformers
 Platform: linux
 Platform: windows
@@ -17,28 +17,28 @@
 Classifier: Programming Language :: Python :: 3.10
 Classifier: Programming Language :: Python :: 3.11
 Classifier: Programming Language :: C++
 Requires-Python: >=3.8.0
 Description-Content-Type: text/markdown
 License-File: LICENSE
 Requires-Dist: torch >=2.0.1
-Requires-Dist: transformers <=4.38.2,>=4.35.0
+Requires-Dist: transformers >=4.35.0
 Requires-Dist: tokenizers >=0.12.1
 Requires-Dist: typing-extensions >=4.8.0
 Requires-Dist: accelerate
 Requires-Dist: datasets
 Requires-Dist: zstandard
 Requires-Dist: autoawq-kernels
 Provides-Extra: dev
 Requires-Dist: black ; extra == 'dev'
 Requires-Dist: mkdocstrings-python ; extra == 'dev'
 Requires-Dist: mkdocs-material ; extra == 'dev'
 Requires-Dist: griffe-typingdoc ; extra == 'dev'
 Provides-Extra: eval
-Requires-Dist: lm-eval >=0.4.0 ; extra == 'eval'
+Requires-Dist: lm-eval ==0.4.1 ; extra == 'eval'
 Requires-Dist: tabulate ; extra == 'eval'
 Requires-Dist: protobuf ; extra == 'eval'
 Requires-Dist: evaluate ; extra == 'eval'
 Requires-Dist: scipy ; extra == 'eval'
 
 # AutoAWQ
```

### html2text {}

```diff
@@ -1,28 +1,28 @@
-Metadata-Version: 2.1 Name: autoawq Version: 0.2.4 Summary: AutoAWQ implements
+Metadata-Version: 2.1 Name: autoawq Version: 0.2.5 Summary: AutoAWQ implements
 the AWQ algorithm for 4-bit quantization with a 2x speedup during inference.
 Home-page: https://github.com/casper-hansen/AutoAWQ Author: Casper Hansen
 License: MIT Keywords: awq,autoawq,quantization,transformers Platform: linux
 Platform: windows Classifier: Environment :: GPU :: NVIDIA CUDA :: 11.8
 Classifier: Environment :: GPU :: NVIDIA CUDA :: 12 Classifier: License :: OSI
 Approved :: MIT License Classifier: Natural Language :: English Classifier:
 Programming Language :: Python :: 3.8 Classifier: Programming Language ::
 Python :: 3.9 Classifier: Programming Language :: Python :: 3.10 Classifier:
 Programming Language :: Python :: 3.11 Classifier: Programming Language :: C++
 Requires-Python: >=3.8.0 Description-Content-Type: text/markdown License-File:
-LICENSE Requires-Dist: torch >=2.0.1 Requires-Dist: transformers
-<=4.38.2,>=4.35.0 Requires-Dist: tokenizers >=0.12.1 Requires-Dist: typing-
-extensions >=4.8.0 Requires-Dist: accelerate Requires-Dist: datasets Requires-
-Dist: zstandard Requires-Dist: autoawq-kernels Provides-Extra: dev Requires-
-Dist: black ; extra == 'dev' Requires-Dist: mkdocstrings-python ; extra ==
-'dev' Requires-Dist: mkdocs-material ; extra == 'dev' Requires-Dist: griffe-
-typingdoc ; extra == 'dev' Provides-Extra: eval Requires-Dist: lm-eval >=0.4.0
-; extra == 'eval' Requires-Dist: tabulate ; extra == 'eval' Requires-Dist:
-protobuf ; extra == 'eval' Requires-Dist: evaluate ; extra == 'eval' Requires-
-Dist: scipy ; extra == 'eval' # AutoAWQ
+LICENSE Requires-Dist: torch >=2.0.1 Requires-Dist: transformers >=4.35.0
+Requires-Dist: tokenizers >=0.12.1 Requires-Dist: typing-extensions >=4.8.0
+Requires-Dist: accelerate Requires-Dist: datasets Requires-Dist: zstandard
+Requires-Dist: autoawq-kernels Provides-Extra: dev Requires-Dist: black ; extra
+== 'dev' Requires-Dist: mkdocstrings-python ; extra == 'dev' Requires-Dist:
+mkdocs-material ; extra == 'dev' Requires-Dist: griffe-typingdoc ; extra ==
+'dev' Provides-Extra: eval Requires-Dist: lm-eval ==0.4.1 ; extra == 'eval'
+Requires-Dist: tabulate ; extra == 'eval' Requires-Dist: protobuf ; extra ==
+'eval' Requires-Dist: evaluate ; extra == 'eval' Requires-Dist: scipy ; extra
+== 'eval' # AutoAWQ
                  | _RR_oo_aa_dd_mm_aa_pp | _EE_xx_aa_mm_pp_ll_ee_ss | _II_ss_ss_uu_ee_ss_::_ _HH_ee_ll_pp_ _WW_aa_nn_tt_ee_dd |
           _[_H_u_g_g_i_n_g_f_a_c_e_ _-_ _M_o_d_e_l_s_]_[_G_i_t_H_u_b_ _-_ _R_e_l_e_a_s_e_s_]_[_P_y_P_I_ _-_ _D_o_w_n_l_o_a_d_s_]
 AutoAWQ is an easy-to-use package for 4-bit quantized models. AutoAWQ speeds up
 models by 3x and reduces memory requirements by 3x compared to FP16. AutoAWQ
 implements the Activation-aware Weight Quantization (AWQ) algorithm for
 quantizing LLMs. AutoAWQ was created and improved upon from the [original work]
 (https://github.com/mit-han-lab/llm-awq) from MIT. *Latest News* ð¥ - [2023/
```

## Comparing `autoawq-0.2.4.dist-info/RECORD` & `autoawq-0.2.5.dist-info/RECORD`

 * *Files 16% similar despite different names*

```diff
@@ -1,59 +1,61 @@
-awq/__init__.py,sha256=sav4RRYsRE-Y2QVXNxrrw58LMvSA38Z4H4JQtO6-Q8A,71
+awq/__init__.py,sha256=48LEKhI_D-05Xr0nlwCzNMOZWgi2L1UpHS5TF2ZqBVw,71
 awq/evaluation/__init__.py,sha256=MlXavaJhTpTDVTrp71Urawj84McmPgNXA9ltXLto43k,229
 awq/evaluation/eval_utils.py,sha256=50Dyo117P7LYsN6_2FbaWA9l-UBWI_AtmBsKS5JTOyE,5560
 awq/evaluation/humaneval_utils.py,sha256=VtxlHmObpURQaBSVyJBWgZZQ6dHbfrHzkySNa8T0isk,13026
 awq/evaluation/kl_divergence.py,sha256=Dc1WVSCLjSEmSPL773-JOJOF-Lna3ZjPLeYxhSRYhGI,5718
-awq/models/__init__.py,sha256=NV8Aiyme089SM6c_5U-yTXae8t7iHPEGh1D6Uf_q9X4,696
+awq/models/__init__.py,sha256=Q5Klwcq2-wMLuc6jrteBjlEQBC1aHi-hIxws8e6iRtw,792
 awq/models/_config.py,sha256=Fm5BYlKL9J9glbqMUynZxXALuHS1lbR-gjClIYW4noc,3883
 awq/models/aquila.py,sha256=Mp2Jnlrzw9YhfGY-bLTa120O2FQbvrEIijKKEFahiYY,4783
-awq/models/auto.py,sha256=VF0pUGb5s1VgpJOEf63h-Tzg9a5J8SPDpdltNhXxY88,3526
+awq/models/auto.py,sha256=nzzXP9I2_ZRpVnnyH3K0n0OKd9QAOx6kThemWy1agEo,3828
 awq/models/baichuan.py,sha256=HhNwdWAJfRSnO4YULSOZt98Sh4a4AHbchYr2SIgwFuk,4834
-awq/models/base.py,sha256=A4zcyJ1u01x_ne33cs1Z42Dxp51fvxdnhslpSQ_caG8,20903
+awq/models/base.py,sha256=4hY-ZTU8yMT2jzJuTSNY9mdu8vZXMLVExr2cYI-oveg,22547
 awq/models/bloom.py,sha256=aD5s-GPrGeXCWu38sdM01uRtV-BidU6MKNzkKaEIkMQ,2356
 awq/models/falcon.py,sha256=-gr_QlqzhzrXvtm7juKBfx7NOS_wKkPvQUrq_gEkFjQ,4483
 awq/models/gemma.py,sha256=T_n_6cIZnRRWBKrEUJhWYjOI7zXNtb-uTLx9RN5yZIU,5346
 awq/models/gpt_bigcode.py,sha256=8n7NkFZ5jWOTrNeJqymOk1fF_nFA2xDHJZq9sUOtQQI,1933
 awq/models/gpt_neox.py,sha256=B0vCkldMnkxMIe3FRcpa2RSHEN2ycn-Hu2WgDoqDTXA,2085
 awq/models/gptj.py,sha256=qdtBr6FuqD3l92o8UrZYP2Ox7EWOqQYDXa4Myu1S7r4,1845
 awq/models/llama.py,sha256=ksx1Tq2gq14yag_SCcQeR6dQWF5e9MHsozjKdcqcKTo,4811
 awq/models/llava.py,sha256=4NemlHNCP0ZcPbHtN4hCPyDTZ7G-2m7BYWYLz_SM3Bw,4959
 awq/models/mistral.py,sha256=-sJYD733S7ngs_uLHbIvp1PG1TDQgQfv5OBFYQVvdqw,4805
 awq/models/mixtral.py,sha256=lM7P6I1MjPW8AbREm6gFBvRqlMYm1SUUdxmxruRd-As,6409
 awq/models/mpt.py,sha256=rxWdxAIoiRYaNFzgz2_uqB5cb0feRvOPzWR_wOxdv8o,3716
 awq/models/opt.py,sha256=EzjapS0z0IakTNzAz3gDUZggkYi6dGT5hL1kdOad8PQ,2078
 awq/models/qwen.py,sha256=HnfA79SC9MlvkmUEQ1vTgDW3nC9r0Extf6bEL-wIVyQ,1486
 awq/models/qwen2.py,sha256=GgBiUpcPG2DzPpynSNRD-xvF2ilJRdkxkx8x8LEQWbs,4749
+awq/models/stablelm.py,sha256=Jh3Fy6XXZOWqrufWEAO4iivn_eR3UMpJiTeKvU9Z5qE,4767
+awq/models/starcoder2.py,sha256=XliUg26W07DEHtwMGfur6gLPDohL2wwCBwvN67J_29E,4707
 awq/models/yi.py,sha256=WD_OUoxuOyvL6hn--GiTpwSxsJ3cA66HZamvCe1IpGg,4343
 awq/modules/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 awq/modules/act.py,sha256=mjD6bTt8MMo5cSGSncFBEt7kmPGbOMN0EfJ6SKXy__0,307
 awq/modules/fused/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-awq/modules/fused/attn.py,sha256=h1hs4oBPpLHrFI-yO6E7x-NdbUfTExcZ5u9If9e3ciQ,10875
-awq/modules/fused/block.py,sha256=hBuUIkYv69TPikMVnfjYGDOLaqKrzQHdqHZNMdkOMbQ,8506
+awq/modules/fused/attn.py,sha256=zfOgRtpCDiSZ5l7XpOEix53H0B4zD3cHfMWXeiVM5w4,12560
+awq/modules/fused/block.py,sha256=mB19t07x7Yx-niyi3vJUwKiMbCs8cp8_qmz5WzZxEAs,8600
 awq/modules/fused/cache.py,sha256=HZY6L7skuwSapzjn4CKXE8epBnONCzZrcUzHUuN3QEs,2533
 awq/modules/fused/mlp.py,sha256=nic2dxPgsqhtAIEssosLea4fgt-dqkybtVp5Zs72Dq4,2474
-awq/modules/fused/model.py,sha256=Rtxs5yrq8ijyRnRXkA5LjLSwja61_Qqz8SsYhGMajQk,6913
+awq/modules/fused/model.py,sha256=cmdgYi3PUlv0Kri0VK15CoTIJvna96SumarWq6mohC8,7070
 awq/modules/fused/moe.py,sha256=jIUSZAeM4AbDF_twHnCfYmWAfxLPEyZpoCCyX0GeD9s,6073
 awq/modules/fused/norm.py,sha256=zxHiBtLAqL0yJEEX4lRkpuRhhLlFPSgmAfhhdtF-m8g,698
 awq/modules/linear/__init__.py,sha256=zMMNH8F-I438LhfBFqxmKR7ZyGQ6BUmG3yrNp9iGzPY,285
 awq/modules/linear/exllama.py,sha256=RC6oJ4VcrAA0arqS0fvBfCDJ6qhSOrzByZEnl6ux_8E,4557
 awq/modules/linear/exllamav2.py,sha256=zW1Vzwy_zULq_alZwa4hWTES6r7KyROk-b2V8PwmP8g,6855
-awq/modules/linear/gemm.py,sha256=mNkXEbuh1CvFhY4WQh1NQRBofaIfT-MG0QvoxC0cpb8,8924
+awq/modules/linear/gemm.py,sha256=je3b-dWRnmMd0qnInZ-UPztenxmfOeGBiGnBC-uz_cw,8926
 awq/modules/linear/gemv.py,sha256=7w3v1BTKPmijUlipRqe3ooNZUnOeOk4nDePd-x9-9YU,6836
-awq/modules/linear/gemv_fast.py,sha256=712ihUfi-qdqs74mnMy7nqX5cjlO9jYKtrJQLSe4waM,6955
+awq/modules/linear/gemv_fast.py,sha256=DD2If5pGYPuRUbCMgf2S3IaixJVRXeq8xDDkKClipjk,6998
 awq/modules/linear/marlin.py,sha256=VAv0r5jtX9Ix449fFnMgtjCyIUo7tYsN0OPUYpVJYW8,7403
 awq/quantize/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-awq/quantize/quantizer.py,sha256=Qa3SRkzar5ufuYgrOchMUjwJorfVIVyuaYHgkSb3tBc,20580
+awq/quantize/quantizer.py,sha256=zHxM2vzDNDRpSmgxSpylFuHCLYWrtkr1BmM-i1Ae4DM,20705
 awq/quantize/scale.py,sha256=P_MjjQHI9itJOKnqDRG4UeJ5oX-o9P-eH9q8riqzEiI,5344
 awq/utils/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 awq/utils/calib_data.py,sha256=xw3PqVwXz8FoZUm7U7uuxztKcWLsJszrgJH7AgM-Mlw,2178
 awq/utils/fused_utils.py,sha256=mi4ScPWRgF0Ak7qRsZN9b_giyL8Nbww-AxRgPTMIxIo,8708
 awq/utils/module.py,sha256=B-DjhmGbIhYrgmf4aVjhZR8-Ffh3hPqAPASf-8iJnZY,1771
 awq/utils/packing_utils.py,sha256=FKp2fL06EJCgMhlD4NrAv7BSWGKIg656SXFou_eDmmE,3271
 awq/utils/parallel.py,sha256=Thrf08I6yHwloy7uA4d6waflLeROOEx3wiWcy0hGHTM,863
 awq/utils/quant_utils.py,sha256=2_N3Wz3Gjr8Ud0aphi41suqzSEIJ1BS3BtEzyMAsGQI,5064
 awq/utils/utils.py,sha256=ZTqyZaRMkxIbkTya-ZQHIH88pdP2FuVFpC_OwZuulfM,3176
-autoawq-0.2.4.dist-info/LICENSE,sha256=ZyOvqdsKHuBAn6udPNK7KM5WYk5prBydnkJKuC1iD4E,1089
-autoawq-0.2.4.dist-info/METADATA,sha256=zsRz50IbuF-6kis4XPFrkVXWPdO2xLwc3t5TB8PX5To,16531
-autoawq-0.2.4.dist-info/WHEEL,sha256=Z6c-bE0pUM47a70GvqO_SvH_XXU0lm62gEAKtoNJ08A,100
-autoawq-0.2.4.dist-info/top_level.txt,sha256=1ac2EWKyAP529fvezQOHpI34L36XqaymH-db8ncpOvY,16
-autoawq-0.2.4.dist-info/RECORD,,
+autoawq-0.2.5.dist-info/LICENSE,sha256=ZyOvqdsKHuBAn6udPNK7KM5WYk5prBydnkJKuC1iD4E,1089
+autoawq-0.2.5.dist-info/METADATA,sha256=-xNI_IkOGcMlwEuX1cqzX7H0UenWdupvxd7QgnVMnC0,16522
+autoawq-0.2.5.dist-info/WHEEL,sha256=Z6c-bE0pUM47a70GvqO_SvH_XXU0lm62gEAKtoNJ08A,100
+autoawq-0.2.5.dist-info/top_level.txt,sha256=1ac2EWKyAP529fvezQOHpI34L36XqaymH-db8ncpOvY,16
+autoawq-0.2.5.dist-info/RECORD,,
```

