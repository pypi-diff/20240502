# Comparing `tmp/OmniGenome-0.0.2a0-py3-none-any.whl.zip` & `tmp/OmniGenome-0.0.3a0-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,52 +1,52 @@
-Zip file size: 52114 bytes, number of entries: 50
--rw-rw-rw-  2.0 fat     3712 b- defN 24-Apr-29 14:11 omnigenome/__init__.py
+Zip file size: 53483 bytes, number of entries: 50
+-rw-rw-rw-  2.0 fat     3712 b- defN 24-May-02 13:19 omnigenome/__init__.py
 -rw-rw-rw-  2.0 fat        0 b- defN 24-Mar-16 16:24 omnigenome/bench/__init__.py
 -rw-rw-rw-  2.0 fat      342 b- defN 24-Apr-11 17:29 omnigenome/bench/auto_bench/__init__.py
--rw-rw-rw-  2.0 fat     7289 b- defN 24-Apr-29 14:58 omnigenome/bench/auto_bench/auto_bench.py
+-rw-rw-rw-  2.0 fat     7968 b- defN 24-May-02 13:20 omnigenome/bench/auto_bench/auto_bench.py
 -rw-rw-rw-  2.0 fat     7366 b- defN 24-Apr-29 14:11 omnigenome/bench/auto_bench/auto_bench_config.py
 -rw-rw-rw-  2.0 fat      971 b- defN 24-Apr-06 13:44 omnigenome/bench/auto_bench/config_check.py
 -rw-rw-rw-  2.0 fat      342 b- defN 24-Apr-11 17:29 omnigenome/bench/bench_hub/__init__.py
 -rw-rw-rw-  2.0 fat      439 b- defN 24-Apr-14 13:16 omnigenome/bench/bench_hub/bench_hub.py
 -rw-rw-rw-  2.0 fat      342 b- defN 24-Apr-27 14:21 omnigenome/src/__init__.py
 -rw-rw-rw-  2.0 fat      342 b- defN 24-Apr-06 13:21 omnigenome/src/abc/__init__.py
--rw-rw-rw-  2.0 fat    10343 b- defN 24-Apr-29 14:11 omnigenome/src/abc/abstract_dataset.py
+-rw-rw-rw-  2.0 fat    12137 b- defN 24-May-02 13:20 omnigenome/src/abc/abstract_dataset.py
 -rw-rw-rw-  2.0 fat     1773 b- defN 24-Apr-13 10:46 omnigenome/src/abc/abstract_metric.py
--rw-rw-rw-  2.0 fat    13841 b- defN 24-Apr-29 14:58 omnigenome/src/abc/abstract_model.py
--rw-rw-rw-  2.0 fat     3175 b- defN 24-Apr-29 14:58 omnigenome/src/abc/abstract_tokenizer.py
+-rw-rw-rw-  2.0 fat    13927 b- defN 24-May-01 20:17 omnigenome/src/abc/abstract_model.py
+-rw-rw-rw-  2.0 fat     3284 b- defN 24-May-02 13:20 omnigenome/src/abc/abstract_tokenizer.py
 -rw-rw-rw-  2.0 fat      634 b- defN 24-Apr-29 14:11 omnigenome/src/dataset/__init__.py
--rw-rw-rw-  2.0 fat     9110 b- defN 24-Apr-29 14:11 omnigenome/src/dataset/omnigenome_dataset.py
+-rw-rw-rw-  2.0 fat     8896 b- defN 24-Apr-30 20:16 omnigenome/src/dataset/omnigenome_dataset.py
 -rw-rw-rw-  2.0 fat      493 b- defN 24-Apr-29 14:11 omnigenome/src/metric/__init__.py
--rw-rw-rw-  2.0 fat     2660 b- defN 24-Apr-29 14:11 omnigenome/src/metric/classification_metric.py
--rw-rw-rw-  2.0 fat     2250 b- defN 24-Apr-29 14:11 omnigenome/src/metric/ranking_metric.py
--rw-rw-rw-  2.0 fat     2628 b- defN 24-Apr-29 14:11 omnigenome/src/metric/regression_metric.py
+-rw-rw-rw-  2.0 fat     2819 b- defN 24-Apr-30 02:44 omnigenome/src/metric/classification_metric.py
+-rw-rw-rw-  2.0 fat     2424 b- defN 24-Apr-30 02:45 omnigenome/src/metric/ranking_metric.py
+-rw-rw-rw-  2.0 fat     2785 b- defN 24-Apr-30 02:45 omnigenome/src/metric/regression_metric.py
 -rw-rw-rw-  2.0 fat        0 b- defN 24-Mar-16 16:34 omnigenome/src/misc/__init__.py
 -rw-rw-rw-  2.0 fat     6784 b- defN 24-Apr-29 14:11 omnigenome/src/misc/utils.py
 -rw-rw-rw-  2.0 fat      470 b- defN 24-Apr-29 14:11 omnigenome/src/model/__init__.py
 -rw-rw-rw-  2.0 fat      342 b- defN 24-Apr-08 20:27 omnigenome/src/model/classiifcation/__init__.py
--rw-rw-rw-  2.0 fat    11809 b- defN 24-Apr-29 12:13 omnigenome/src/model/classiifcation/model.py
+-rw-rw-rw-  2.0 fat    12353 b- defN 24-Apr-30 19:17 omnigenome/src/model/classiifcation/model.py
 -rw-rw-rw-  2.0 fat      342 b- defN 24-Apr-10 12:33 omnigenome/src/model/mlm/__init__.py
--rw-rw-rw-  2.0 fat     4521 b- defN 24-Apr-27 20:58 omnigenome/src/model/mlm/model.py
+-rw-rw-rw-  2.0 fat     4534 b- defN 24-May-01 20:25 omnigenome/src/model/mlm/model.py
 -rw-rw-rw-  2.0 fat      342 b- defN 24-Apr-08 20:27 omnigenome/src/model/regression/__init__.py
--rw-rw-rw-  2.0 fat    10948 b- defN 24-Apr-29 14:58 omnigenome/src/model/regression/model.py
+-rw-rw-rw-  2.0 fat    10956 b- defN 24-Apr-30 19:44 omnigenome/src/model/regression/model.py
 -rw-rw-rw-  2.0 fat      342 b- defN 24-Apr-08 21:23 omnigenome/src/model/seq2seq/__init__.py
 -rw-rw-rw-  2.0 fat      607 b- defN 24-Apr-27 14:47 omnigenome/src/model/seq2seq/model.py
 -rw-rw-rw-  2.0 fat      512 b- defN 24-Apr-29 14:11 omnigenome/src/tokenizer/__init__.py
 -rw-rw-rw-  2.0 fat     2996 b- defN 24-Apr-29 14:11 omnigenome/src/tokenizer/bpe_tokenizer.py
 -rw-rw-rw-  2.0 fat     4598 b- defN 24-Apr-29 14:11 omnigenome/src/tokenizer/kmers_tokenizer.py
--rw-rw-rw-  2.0 fat     3798 b- defN 24-Apr-29 14:11 omnigenome/src/tokenizer/single_nucleotide_tokenizer.py
+-rw-rw-rw-  2.0 fat     3868 b- defN 24-May-02 13:20 omnigenome/src/tokenizer/single_nucleotide_tokenizer.py
 -rw-rw-rw-  2.0 fat      409 b- defN 24-Apr-14 10:45 omnigenome/src/trainer/__init__.py
 -rw-rw-rw-  2.0 fat     1092 b- defN 24-Apr-29 14:11 omnigenome/src/trainer/hf_trainer.py
--rw-rw-rw-  2.0 fat    10163 b- defN 24-Apr-29 14:58 omnigenome/src/trainer/trainer.py
+-rw-rw-rw-  2.0 fat    10478 b- defN 24-May-02 13:20 omnigenome/src/trainer/trainer.py
 -rw-rw-rw-  2.0 fat        0 b- defN 24-Mar-16 16:25 omnigenome/utility/__init__.py
 -rw-rw-rw-  2.0 fat     9554 b- defN 24-Apr-24 20:40 omnigenome/utility/ensemble.py
 -rw-rw-rw-  2.0 fat    10629 b- defN 24-Apr-29 14:11 omnigenome/utility/hub_utils.py
 -rw-rw-rw-  2.0 fat      342 b- defN 24-Apr-11 17:29 omnigenome/utility/model_hub/__init__.py
 -rw-rw-rw-  2.0 fat     2993 b- defN 24-Apr-27 21:32 omnigenome/utility/model_hub/model_hub.py
 -rw-rw-rw-  2.0 fat      342 b- defN 24-Apr-06 13:21 omnigenome/utility/pipeline_hub/__init__.py
 -rw-rw-rw-  2.0 fat     5603 b- defN 24-Apr-29 14:11 omnigenome/utility/pipeline_hub/pipeline.py
 -rw-rw-rw-  2.0 fat      883 b- defN 24-Apr-14 15:43 omnigenome/utility/pipeline_hub/pipeline_hub.py
--rw-rw-rw-  2.0 fat      861 b- defN 24-Apr-29 14:59 OmniGenome-0.0.2a0.dist-info/METADATA
--rw-rw-rw-  2.0 fat       92 b- defN 24-Apr-29 14:59 OmniGenome-0.0.2a0.dist-info/WHEEL
--rw-rw-rw-  2.0 fat       11 b- defN 24-Apr-29 14:59 OmniGenome-0.0.2a0.dist-info/top_level.txt
-?rw-rw-r--  2.0 fat     4654 b- defN 24-Apr-29 14:59 OmniGenome-0.0.2a0.dist-info/RECORD
-50 files, 164091 bytes uncompressed, 44522 bytes compressed:  72.9%
+-rw-rw-rw-  2.0 fat     2272 b- defN 24-May-02 13:20 OmniGenome-0.0.3a0.dist-info/METADATA
+-rw-rw-rw-  2.0 fat       92 b- defN 24-May-02 13:20 OmniGenome-0.0.3a0.dist-info/WHEEL
+-rw-rw-rw-  2.0 fat       11 b- defN 24-May-02 13:20 OmniGenome-0.0.3a0.dist-info/top_level.txt
+?rw-rw-r--  2.0 fat     4655 b- defN 24-May-02 13:20 OmniGenome-0.0.3a0.dist-info/RECORD
+50 files, 169397 bytes uncompressed, 45891 bytes compressed:  72.9%
```

## zipnote {}

```diff
@@ -132,20 +132,20 @@
 
 Filename: omnigenome/utility/pipeline_hub/pipeline.py
 Comment: 
 
 Filename: omnigenome/utility/pipeline_hub/pipeline_hub.py
 Comment: 
 
-Filename: OmniGenome-0.0.2a0.dist-info/METADATA
+Filename: OmniGenome-0.0.3a0.dist-info/METADATA
 Comment: 
 
-Filename: OmniGenome-0.0.2a0.dist-info/WHEEL
+Filename: OmniGenome-0.0.3a0.dist-info/WHEEL
 Comment: 
 
-Filename: OmniGenome-0.0.2a0.dist-info/top_level.txt
+Filename: OmniGenome-0.0.3a0.dist-info/top_level.txt
 Comment: 
 
-Filename: OmniGenome-0.0.2a0.dist-info/RECORD
+Filename: OmniGenome-0.0.3a0.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## omnigenome/__init__.py

```diff
@@ -4,15 +4,15 @@
 # author: YANG, HENG <hy345@exeter.ac.uk> (杨恒)
 # github: https://github.com/yangheng95
 # huggingface: https://huggingface.co/yangheng
 # google scholar: https://scholar.google.com/citations?user=NPq5a_0AAAAJ&hl=en
 # Copyright (C) 2019-2024. All Rights Reserved.
 
 __name__ = "OmniGenome"
-__version__ = "0.0.2alpha"
+__version__ = "0.0.3alpha"
 __author__ = "YANG, HENG"
 __email__ = "yangheng2021@gmail.com"
 __license__ = "MIT"
 
 
 from .bench.auto_bench.auto_bench import AutoBench
 from .bench.auto_bench.auto_bench_config import AutoBenchConfig
```

## omnigenome/bench/auto_bench/auto_bench.py

```diff
@@ -5,14 +5,15 @@
 # github: https://github.com/yangheng95
 # huggingface: https://huggingface.co/yangheng
 # google scholar: https://scholar.google.com/citations?user=NPq5a_0AAAAJ&hl=en
 # Copyright (C) 2019-2024. All Rights Reserved.
 
 import importlib
 import os
+import warnings
 
 import autocuda
 import findfile
 import torch
 from metric_visualizer import MetricVisualizer
 
 from ...src.abc.abstract_tokenizer import OmniGenomeTokenizer
@@ -24,32 +25,31 @@
     def __init__(
         self, bench_root, model_name_or_path, tokenizer=None, device=None, **kwargs
     ):
         self.bench_root = bench_root.rstrip("/")
         self.model_name_or_path = model_name_or_path.rstrip("/")
         self.tokenizer = tokenizer.rstrip("/") if tokenizer else None
         self.device = device if device else autocuda.auto_cuda()
-
+        self.overwrite = kwargs.pop("overwrite", False)
         # Import benchmark list
         self.bench_metadata = importlib.import_module(f"{self.bench_root}.metadata")
         fprint("Loaded benchmarks: ", self.bench_metadata.bench_list)
 
         self.mv_path = f"{self.bench_root}-{self.model_name_or_path}.mv".replace(
             "/", "-"
         )
-        if os.path.exists(self.mv_path):
+        if os.path.exists(self.mv_path) and not self.overwrite:
             self.mv = MetricVisualizer.load(self.mv_path)
             self.mv.summary()
+        elif os.path.exists(self.mv_path) and self.overwrite:
+            os.remove(self.mv_path)
+            self.mv = MetricVisualizer(f"{self.bench_root}-{self.model_name_or_path}")
         else:
             self.mv = MetricVisualizer(f"{self.bench_root}-{self.model_name_or_path}")
 
-        for key, value in kwargs.items():
-            if key in self.bench_metadata:
-                self.bench_metadata[key] = value
-
         self.bench_info()
 
     def bench_info(self):
         info = f"Benchmark Root: {self.bench_root}\n"
         info += f"Benchmark List: {self.bench_metadata.bench_list}\n"
         info += f"Model Name or Path: {self.model_name_or_path}\n"
         info += f"Tokenizer: {self.tokenizer}\n"
@@ -58,15 +58,15 @@
         info += f"BenchConfig Details: {self.bench_metadata}\n"
         fprint(info)
         return info
 
     def run(self, **kwargs):
         """
 
-        :param kwargs: parameters in kwargs will be used to overwrite the default parameters in the benchmark config
+        :param kwargs: parameters in kwargs will be used to override the default parameters in the benchmark config
         :return:
         """
 
         # Import benchmark config
         for bench in self.bench_metadata.bench_list:
             bench_config_path = findfile.find_file(
                 self.bench_root, f"{self.bench_root}.{bench}.config".split(".")
@@ -74,27 +74,45 @@
             config = importlib.import_module(
                 bench_config_path.rstrip("/").replace(os.sep, ".").replace(".py", "")
             )
             bench_config = config.bench_config
 
             for key, value in kwargs.items():
                 if key in bench_config:
-                    bench_config[key] = value
+                    fprint(
+                        "Override", key, "with", value, "according to the input kwargs"
+                    )
+                    bench_config[key] = kwargs.pop(key)
+
+            if kwargs:
+                warnings.warn(f"Unused kwargs: {kwargs}")
+
+            fprint(
+                f"AutoBench Config for {bench}:",
+                "\n".join([f"{k}: {v}" for k, v in bench_config.items()]),
+            )
 
             # Init Tokenizer and Model
             if self.tokenizer:
                 tokenizer = OmniGenomeTokenizer.from_pretrained(
                     self.tokenizer, trust_remote_code=True
                 )
             else:
                 tokenizer = OmniGenomeTokenizer.from_pretrained(
                     self.model_name_or_path, trust_remote_code=True
                 )
             # Run Benchmarks
+            if not isinstance(bench_config["seeds"], list):
+                bench_config["seeds"] = [bench_config["seeds"]]
+
             for seed in bench_config["seeds"]:
+                batch_size = (
+                    bench_config["batch_size"] if "batch_size" in bench_config else 16
+                )
+
                 record_name = f"{self.bench_root}-{self.model_name_or_path}-{bench}"
                 # check if the record exists
                 if record_name in self.mv.transpose() and len(
                     list(self.mv.transpose()[record_name].values())[0]
                 ) >= len(bench_config["seeds"]):
                     continue
 
@@ -125,17 +143,14 @@
                 )
                 valid_set = dataset_cls(
                     data_source=bench_config["valid_file"],
                     tokenizer=tokenizer,
                     label2id=bench_config["label2id"],
                     max_length=bench_config["max_length"],
                 )
-                batch_size = (
-                    bench_config["batch_size"] if "batch_size" in bench_config else 8
-                )
 
                 train_loader = torch.utils.data.DataLoader(
                     train_set, batch_size=batch_size, shuffle=True
                 )
                 valid_loader = torch.utils.data.DataLoader(
                     valid_set, batch_size=batch_size
                 )
@@ -166,15 +181,17 @@
                     loss_fn=bench_config["loss_fn"]
                     if "loss_fn" in bench_config
                     else None,
                     compute_metrics=bench_config["compute_metrics"],
                     seed=seed,
                     device=self.device,
                 )
+
                 metrics = trainer.train()
+
                 for key, value in metrics["test"][-1].items():
                     self.mv.log(record_name, key, value)
                 fprint(metrics)
                 self.mv.summary(round=4)
                 self.mv.dump(self.mv_path)
                 del model, trainer, optimizer, train_loader, valid_loader, test_loader
                 torch.cuda.empty_cache()
```

## omnigenome/src/abc/abstract_dataset.py

```diff
@@ -50,16 +50,18 @@
         return self
 
 
 class OmniGenomeDataset(torch.utils.data.Dataset):
     def __init__(self, data_source, tokenizer, max_length=None, **kwargs):
         super(OmniGenomeDataset, self).__init__()
         self.metadata = env_meta_info()
-
         self.tokenizer = tokenizer
+        self.label2id = kwargs.get("label2id", None)
+        if self.label2id is not None:
+            self.id2label = {v: k for k, v in self.label2id.items()}
 
         if max_length is not None:
             fprint(
                 f"Detected max_length={max_length} in the dataset, using it as the max_length."
             )
             self.max_length = max_length
         elif (
@@ -76,22 +78,32 @@
         self.tokenizer.max_length = self.max_length
         self.examples = []
         self.data = []
 
         if data_source is not None and os.path.exists(data_source):
             fprint(f"Loading data from {data_source}...")
             self.load_data_source(data_source, **kwargs)
+            self._preprocessing()
 
             for example in tqdm.tqdm(self.examples):
+                self.max_length = min(
+                    self.max_length, max(self.max_length, len(example["sequence"]) - 4)
+                )
+                self.tokenizer.max_length = self.max_length
                 self.data.append(self.prepare_input(example))
 
+            self._postprocessing()
+
             if self.examples:
-                self._post_processing()
-                self._pad_and_truncate()
                 self.data = covert_input_to_tensor(self.data)
+                self._pad_and_truncate()
+                fprint(self.get_inputs_length())
+                fprint(f"Preview of the first two samples in the dataset:")
+                for sample in self.data[:2]:
+                    print(sample)
 
     def to(self, device):
         for data_item in self.data:
             for key, value in data_item.items():
                 if isinstance(value, torch.Tensor):
                     data_item[key] = value.to(device)
         return self
@@ -100,70 +112,85 @@
         if hasattr(self.tokenizer, "pad_token_id"):
             pad_token_id = self.tokenizer.pad_token_id
         else:
             pad_token_id = self.tokenizer.base_tokenizer.pad_token_id
         max_length = min(
             max(
                 max(
-                    torch.sum(data_item["input_ids"] != pad_token_id)
-                    for data_item in self.data
+                    [
+                        torch.sum(data_item["input_ids"] != pad_token_id)
+                        for data_item in self.data
+                    ]
+                ),
+                max(
+                    [
+                        data_item["labels"].shape[0]
+                        if data_item["labels"].shape
+                        else -1
+                        for data_item in self.data
+                    ]
                 ),
-                max(torch.sum(data_item["labels"] != -100) for data_item in self.data),
             ),
             self.max_length,
         )
+        label_padding_length = self._max_labels_length()
 
         for data_item in self.data:
             for key, value in data_item.items():
                 value = torch.tensor(np.array(value))
                 dtype = value.dtype
-                if isinstance(value, torch.Tensor) and value.dim() == 2:
+                if "label" in key:
+                    if value.dim() == 0:
+                        padding_length = 0
+                    else:
+                        padding_length = label_padding_length - value.size(0)
+                else:
                     padding_length = max_length - value.size(0)
+                if isinstance(value, torch.Tensor) and value.dim() == 2:
                     if padding_length > 0:
                         if key == "input_ids":
                             if hasattr(self.tokenizer, "pad_token_id"):
                                 _pad_value = self.tokenizer.pad_token_id * torch.ones(
                                     (padding_length, value.size(1))
                                 )
                             else:
                                 _pad_value = (
                                     self.tokenizer.base_tokenizer.pad_token_id
                                     * torch.ones((padding_length, value.size(1)))
                                 )
                         elif key == "attention_mask":
                             _pad_value = torch.zeros((padding_length, value.size(1)))
-                        elif "label" in key or "labels" in key:
+                        elif "label" in key:
                             _pad_value = -100 * torch.ones(
-                                (padding_length, value.size(1))
+                                (label_padding_length, value.size(1))
                             )
                         else:
                             _pad_value = pad_value * torch.ones(
                                 (padding_length, value.size(1))
                             )
                         data_item[key] = torch.cat([value, _pad_value], dim=0)
                     elif padding_length < 0:
                         data_item[key] = value[:max_length]
                     data_item[key] = data_item[key].to(dtype)
 
                 elif isinstance(value, torch.Tensor) and value.dim() == 1:
-                    padding_length = max_length - value.size(0)
                     if padding_length > 0:
                         if key == "input_ids":
                             if hasattr(self.tokenizer, "pad_token_id"):
                                 _pad_value = self.tokenizer.pad_token_id * torch.ones(
                                     (padding_length,)
                                 )
                             else:
                                 _pad_value = (
                                     self.tokenizer.base_tokenizer.pad_token_id
                                     * torch.ones((padding_length,))
                                 )
                         elif key == "attention_mask":
                             _pad_value = torch.zeros((padding_length,))
-                        elif "label" in key or "labels" in key:
+                        elif "label" in key:
                             _pad_value = -100 * torch.ones((padding_length,))
                         else:
                             _pad_value = pad_value * torch.ones((padding_length,))
                         data_item[key] = torch.cat([value, _pad_value], dim=0)
                     elif padding_length < 0:
                         data_item[key] = value[:max_length]
 
@@ -215,21 +242,31 @@
         return examples
 
     def prepare_input(self, instance, **kwargs):
         raise NotImplementedError(
             "The prepare_input() function should be implemented for your dataset."
         )
 
-    def _post_processing(self):
-        for data in self.data:
-            if "label" in data:
-                data["labels"] = data["label"]
-                del data["label"]
-
-        print(self.get_sequence_length())
+    def _preprocessing(self):
+        for idx, ex in enumerate(self.examples):
+            if "seq" in self.examples[idx]:
+                self.examples[idx]["sequence"] = self.examples[idx]["seq"]
+                del self.examples[idx]["seq"]
+            assert (
+                "sequence" in self.examples[idx]
+            ), "The 'sequence' field is required in the raw dataset."
+
+    def _postprocessing(self):
+        for idx, ex in enumerate(self.data):
+            if "label" in self.data[idx]:
+                self.data[idx]["labels"] = self.data[idx]["label"]
+                del self.data[idx]["label"]
+            assert (
+                "labels" in self.data[idx]
+            ), "The 'labels' field is required in the tokenized dataset."
 
     def __len__(self):
         return len(self.data)
 
     def __getitem__(self, idx):
         # convert the data item to a omnigenome dict
         return OmniGenomeDict(self.data[idx])
@@ -239,24 +276,30 @@
 
     def get_column(self, column_name):
         return [data_item[column_name] for data_item in self.data]
 
     def get_labels(self):
         return set(self.get_column("labels"))
 
-    def get_sequence_length(self):
+    def get_inputs_length(self):
         if hasattr(self.tokenizer, "pad_token_id"):
             pad_token_id = self.tokenizer.pad_token_id
         else:
             pad_token_id = self.tokenizer.base_tokenizer.pad_token_id
         length = {}
         all_seq_lengths = [
             torch.sum(data_item["input_ids"] != pad_token_id) for data_item in self.data
         ]
         length["avg"] = np.mean(all_seq_lengths)
         length["max"] = np.max(all_seq_lengths)
         length["min"] = np.min(all_seq_lengths)
         return length
 
+    def _max_labels_length(self):
+        if self.data[0]["labels"].dim() > 0:
+            return max([len(ex["labels"]) for ex in self.data])
+        else:
+            return 1
+
     def __iter__(self):
         for data_item in self.data:
             yield OmniGenomeDict(data_item)
```

## omnigenome/src/abc/abstract_model.py

```diff
@@ -58,14 +58,16 @@
         outputs = model(
             input_ids,
             attention_mask=attention_mask,
             output_hidden_states=True,
         )
 
     except Exception as e:
+        if "attention_mask" not in str(e):
+            raise e
         # For autoregressive models, the attention_mask is not required
         outputs = model(
             input_ids,
             output_hidden_states=True,
         )
 
     if not hasattr(outputs, "last_hidden_state"):
@@ -132,15 +134,17 @@
 class OmniGenomeModel(torch.nn.Module):
     def __init__(self, config_or_model_model, tokenizer, *args, **kwargs):
         self.loss_fn = None
 
         label2id = kwargs.pop("label2id", None)
         trust_remote_code = kwargs.pop("trust_remote_code", True)
         num_labels = kwargs.pop("num_labels", None)
-        num_labels = num_labels if num_labels is not None else len(label2id)
+
+        if label2id is not None and num_labels is None:
+            num_labels = len(label2id)
 
         # do not change the order of the following lines
         super().__init__(*args, **kwargs)
 
         if isinstance(config_or_model_model, str):
             config = AutoConfig.from_pretrained(
                 config_or_model_model,
```

## omnigenome/src/abc/abstract_tokenizer.py

```diff
@@ -23,17 +23,14 @@
         for key, value in kwargs.items():
             self.metadata[key] = value
 
         self.u2t = kwargs.get("u2t", False)
         self.t2u = kwargs.get("t2u", False)
         self.add_whitespace = kwargs.get("add_whitespace", False)
 
-        for key, value in base_tokenizer.__dict__.items():
-            self.key = value
-
     @staticmethod
     def from_pretrained(model_name_or_path, **kwargs):
         import importlib
 
         try:
             wrapper_name = (
                 f"{model_name_or_path.rstrip('/')}.omnigenome_wrapper".replace("/", ".")
@@ -84,7 +81,13 @@
         )
 
     def decode(self, sequence, **kwargs):
         raise NotImplementedError(
             "The decode() function should be adapted for different models,"
             " please implement it for your model."
         )
+
+    def __getattribute__(self, item):
+        try:
+            return super().__getattribute__(item)
+        except AttributeError:
+            return self.base_tokenizer.__getattribute__(item)
```

## omnigenome/src/dataset/omnigenome_dataset.py

```diff
@@ -12,18 +12,15 @@
 import torch
 
 from ..abc.abstract_dataset import OmniGenomeDataset
 from ... import __name__, __version__
 
 
 class OmniGenomeDatasetForTokenClassification(OmniGenomeDataset):
-    def __init__(self, data_source, tokenizer, label2id, max_length=None, **kwargs):
-        self.label2id = label2id
-        self.id2label = {v: k for k, v in label2id.items()}
-
+    def __init__(self, data_source, tokenizer, max_length=None, **kwargs):
         super(OmniGenomeDatasetForTokenClassification, self).__init__(
             data_source, tokenizer, max_length, **kwargs
         )
 
         self.metadata.update(
             {
                 "library_name": __name__,
@@ -71,18 +68,15 @@
                 [-100] + [self.label2id.get(str(l), -100) for l in labels] + [-100]
             )
             tokenized_inputs["labels"] = torch.tensor(tokenized_inputs["labels"])
         return tokenized_inputs
 
 
 class OmniGenomeDatasetForSequenceClassification(OmniGenomeDataset):
-    def __init__(self, data_source, tokenizer, label2id, max_length=None, **kwargs):
-        self.label2id = label2id
-        self.id2label = {v: k for k, v in label2id.items()}
-
+    def __init__(self, data_source, tokenizer, max_length=None, **kwargs):
         super(OmniGenomeDatasetForSequenceClassification, self).__init__(
             data_source, tokenizer, max_length, **kwargs
         )
 
         self.metadata.update(
             {
                 "library_name": __name__,
```

## omnigenome/src/metric/classification_metric.py

```diff
@@ -4,14 +4,15 @@
 # author: YANG, HENG <hy345@exeter.ac.uk> (杨恒)
 # github: https://github.com/yangheng95
 # huggingface: https://huggingface.co/yangheng
 # google scholar: https://scholar.google.com/citations?user=NPq5a_0AAAAJ&hl=en
 # Copyright (C) 2019-2024. All Rights Reserved.
 
 import types
+import warnings
 
 import numpy as np
 import sklearn.metrics as metrics
 
 from ..abc.abstract_metric import OmniGenomeMetric
 
 
@@ -36,18 +37,21 @@
                 """
                 Compute the metric, based on the true and predicted values.
                 :param y_true: the true values
                 :param y_pred: the predicted values
                 :param ignore_y: the value to ignore in the predictions and true values in corresponding positions
                 """
                 y_true, y_pred = ClassificationMetric.flatten(y_true, y_pred)
-                mask_idx = np.where(y_true != self.ignore_y)
+                y_true_mask_idx = np.where(y_true != self.ignore_y)
                 if self.ignore_y is not None:
-                    y_true = y_true[mask_idx]
-                    y_pred = y_pred[mask_idx]
+                    y_true = y_true[y_true_mask_idx]
+                    try:
+                        y_pred = y_pred[y_true_mask_idx]
+                    except Exception as e:
+                        warnings.warn(str(e))
 
                 kwargs.update(self.kwargs)
                 return {name: self.compute(y_true, y_pred, *args, **kwargs)}
 
             return wrapper
         else:
             return super().__getattribute__(name)
```

## omnigenome/src/metric/ranking_metric.py

```diff
@@ -5,14 +5,15 @@
 # github: https://github.com/yangheng95
 # huggingface: https://huggingface.co/yangheng
 # google scholar: https://scholar.google.com/citations?user=NPq5a_0AAAAJ&hl=en
 # Copyright (C) 2019-2024. All Rights Reserved.
 
 
 import types
+import warnings
 
 import numpy as np
 import sklearn.metrics as metrics
 
 from ..abc.abstract_metric import OmniGenomeMetric
 
 
@@ -32,20 +33,22 @@
             def wrapper(y_true, y_score, *args, **kwargs):
                 """
                 Compute the metric, based on the true and predicted values.
                 :param y_true: the true values
                 :param y_score: the predicted values
                 :param ignore_y: the value to ignore in the predictions and true values in corresponding positions
                 """
-                y_true = np.array(y_true)
-                y_score = np.array(y_score)
-
+                y_true, y_score = RankingMetric.flatten(y_true, y_score)
+                y_true_mask_idx = np.where(y_true != self.ignore_y)
                 if self.ignore_y is not None:
-                    y_true = y_true[y_true != self.ignore_y]
-                    y_score = y_score[y_score != self.ignore_y]
+                    y_true = y_true[y_true_mask_idx]
+                    try:
+                        y_score = y_score[y_true_mask_idx]
+                    except Exception as e:
+                        warnings.warn(str(e))
 
                 return {name: self.compute(y_true, y_score, *args, **kwargs)}
 
             return wrapper
         raise AttributeError(f"'CustomMetrics' object has no attribute '{name}'")
 
     def compute(self, y_true, y_score, *args, **kwargs):
```

## omnigenome/src/metric/regression_metric.py

```diff
@@ -5,14 +5,15 @@
 # github: https://github.com/yangheng95
 # huggingface: https://huggingface.co/yangheng
 # google scholar: https://scholar.google.com/citations?user=NPq5a_0AAAAJ&hl=en
 # Copyright (C) 2019-2024. All Rights Reserved.
 
 
 import types
+import warnings
 
 import numpy as np
 import sklearn.metrics as metrics
 
 from ..abc.abstract_metric import OmniGenomeMetric
 
 
@@ -36,19 +37,21 @@
                 """
                 Compute the metric, based on the true and predicted values.
                 :param y_true: the true values
                 :param y_score: the predicted values
                 :param ignore_y: the value to ignore in the predictions and true values in corresponding positions
                 """
                 y_true, y_score = RegressionMetric.flatten(y_true, y_score)
-                mask_idx = np.where(y_true != self.ignore_y)
+                y_true_mask_idx = np.where(y_true != self.ignore_y)
                 if self.ignore_y is not None:
-                    y_true = y_true[mask_idx]
-                    y_score = y_score[mask_idx]
-
+                    y_true = y_true[y_true_mask_idx]
+                    try:
+                        y_score = y_score[y_true_mask_idx]
+                    except Exception as e:
+                        warnings.warn(str(e))
                 kwargs.update(self.kwargs)
 
                 return {name: self.compute(y_true, y_score, *args, **kwargs)}
 
             return wrapper
         else:
             return super().__getattribute__(name)
```

## omnigenome/src/model/classiifcation/model.py

```diff
@@ -35,17 +35,25 @@
         outputs = {"logits": logits, "last_hidden_state": last_hidden_state}
         return outputs
 
     def predict(self, sequence_or_inputs, **kwargs):
         if not isinstance(sequence_or_inputs, BatchEncoding) and not isinstance(
             sequence_or_inputs, dict
         ):
-            inputs = self.tokenizer(sequence_or_inputs, return_tensors="pt", **kwargs)
+            inputs = self.tokenizer(
+                sequence_or_inputs,
+                padding=kwargs.pop("padding", "max_length"),
+                max_length=kwargs.pop("max_length", 512),
+                truncation=True,
+                return_tensors="pt",
+                **kwargs
+            )
         else:
             inputs = sequence_or_inputs
+
         inputs = inputs.to(self.model.device)
 
         with torch.no_grad():
             outputs = self(inputs)
         logits = outputs["logits"]
         last_hidden_state = outputs["last_hidden_state"]
 
@@ -58,15 +66,27 @@
             "logits": logits,
             "last_hidden_state": last_hidden_state,
         }
 
         return outputs
 
     def inference(self, sequence_or_inputs, **kwargs):
-        inputs = self.tokenizer(sequence_or_inputs, return_tensors="pt", **kwargs)
+        if not isinstance(sequence_or_inputs, BatchEncoding) and not isinstance(
+            sequence_or_inputs, dict
+        ):
+            inputs = self.tokenizer(
+                sequence_or_inputs,
+                padding=kwargs.pop("padding", "max_length"),
+                max_length=kwargs.pop("max_length", 512),
+                truncation=True,
+                return_tensors="pt",
+                **kwargs
+            )
+        else:
+            inputs = sequence_or_inputs
         inputs = inputs.to(self.model.device)
 
         with torch.no_grad():
             outputs = self(inputs)
         logits = outputs["logits"][:, 1:-1:, :]
         last_hidden_state = outputs["last_hidden_state"][:, 1:-1:, :]
 
@@ -124,15 +144,14 @@
             logits = logits[
                 torch.arange(inputs["input_ids"].size(0), device=logits.device),
                 sequence_lengths,
             ]
         else:
             last_hidden_state = self.pooler(last_hidden_state)
             logits = self.classifier(last_hidden_state)
-            logits = self.activation(logits)
             logits = self.softmax(logits)
 
         outputs = {"logits": logits, "last_hidden_state": last_hidden_state}
         return outputs
 
     def predict(self, sequence_or_inputs, **kwargs):
         if not isinstance(sequence_or_inputs, BatchEncoding) and not isinstance(
@@ -298,15 +317,14 @@
             pad_token_id = getattr(self.config, "pad_token_id", -100)
             sequence_lengths = inputs["input_ids"].ne(pad_token_id).sum(dim=1) - 1
             logits = logits[
                 torch.arange(inputs["input_ids"].size(0), device=logits.device),
                 sequence_lengths,
             ]
         else:
-            logits = self.activation(logits)
             logits = self.softmax(logits)
             logits = self.pooler(logits)
 
         outputs = {
             "logits": logits,
             "last_hidden_state": last_hidden_state,
             "ss_last_hidden_state": ss_last_hidden_state,
```

## omnigenome/src/model/mlm/model.py

```diff
@@ -13,15 +13,15 @@
 from ...abc.abstract_model import OmniGenomeModel
 
 
 class OmniGenomeEncoderModelForMLM(OmniGenomeModel):
     def __init__(self, config_or_model_model, tokenizer, *args, **kwargs):
         super().__init__(config_or_model_model, tokenizer, *args, **kwargs)
         self.metadata["model_name"] = self.__class__.__name__
-        if not hasattr(self.model, "lm_head"):
+        if "MaskedLM" not in self.model.__class__.__name__:
             raise ValueError(
                 "The model does not have a language model head, which is required for MLM."
                 "Please use a model that supports masked language modeling."
             )
         self.classifier = torch.nn.Linear(
             self.config.hidden_size, self.config.num_labels
         )
```

## omnigenome/src/model/regression/model.py

```diff
@@ -148,15 +148,15 @@
         with torch.no_grad():
             outputs = self(inputs)
         logits = outputs["logits"]
         last_hidden_state = outputs["last_hidden_state"]
 
         predictions = []
         for i in range(logits.shape[0]):
-            predictions.append(logits[i][0].item())
+            predictions.append(logits[i].cpu().numpy())
 
         outputs = {
             "predictions": predictions,
             "logits": logits,
             "last_hidden_state": last_hidden_state,
         }
 
@@ -169,15 +169,15 @@
         with torch.no_grad():
             outputs = self(inputs)
         logits = outputs["logits"]
         last_hidden_state = outputs["last_hidden_state"]
 
         predictions = []
         for i in range(logits.shape[0]):
-            predictions.append(logits[i][0].item())
+            predictions.append(logits[i].cpu().numpy())
 
         if not isinstance(sequence_or_inputs, list):
             outputs = {
                 "predictions": predictions[0],
                 "logits": logits[0],
                 "last_hidden_state": last_hidden_state[0],
             }
```

## omnigenome/src/tokenizer/single_nucleotide_tokenizer.py

```diff
@@ -47,21 +47,22 @@
             tokenized_inputs["input_ids"].append(
                 [bos_id] + self.base_tokenizer.convert_tokens_to_ids(tokens) + [eos_id]
             )
             tokenized_inputs["attention_mask"].append(
                 [1] * len(tokenized_inputs["input_ids"][-1])
             )
 
-        for i, ids in enumerate(tokenized_inputs["input_ids"]):
-            if ids.count(self.base_tokenizer.unk_token_id) / len(ids) > 0.1:
-                warnings.warn(
-                    f"Unknown tokens are more than "
-                    f"{ids.count(self.base_tokenizer.unk_token_id) / len(ids)}% in the {i}-th sequence, "
-                    f"please check the tokenization process."
-                )
+        if kwargs.get("warnings", True):
+            for i, ids in enumerate(tokenized_inputs["input_ids"]):
+                if ids.count(self.base_tokenizer.unk_token_id) / len(ids) > 0.1:
+                    warnings.warn(
+                        f"Unknown tokens are more than "
+                        f"{ids.count(self.base_tokenizer.unk_token_id) / len(ids)}% in the {i}-th sequence, "
+                        f"please check the tokenization process."
+                    )
         max_length = max(len(ids) for ids in tokenized_inputs["input_ids"])
         tokenized_inputs = self.base_tokenizer.pad(
             tokenized_inputs,
             padding=kwargs.get("padding", "max_length"),
             max_length=min(max_length, kwargs.get("max_length", 512)),
             return_attention_mask=kwargs.get("return_attention_mask", True),
             return_tensors="pt",
```

## omnigenome/src/trainer/trainer.py

```diff
@@ -11,35 +11,49 @@
 import autocuda
 import numpy as np
 import torch
 from tqdm import tqdm
 
 from ..misc.utils import env_meta_info, fprint, seed_everything
 
+import sklearn.metrics
 
-def _infer_optimization_direction(metrics, prev_metrics):
-    is_prev_increasing = np.mean(list(prev_metrics[0].values())[0]) < np.mean(
-        list(prev_metrics[-1].values())[0]
-    )
-    is_still_increasing = np.mean(list(prev_metrics[1].values())[0]) < np.mean(
-        list(metrics.values())[0]
-    )
 
-    if is_prev_increasing and is_still_increasing:
+def _infer_optimization_direction(metrics, prev_metrics):
+    if "score" in list(prev_metrics[0].keys())[0]:
         return "larger_is_better"
+    elif "precision" in list(prev_metrics[0].keys())[0]:
+        return "larger_is_better"
+    elif "loss" in list(prev_metrics[0].keys())[0]:
+        return "smaller_is_better"
+    elif "error" in list(prev_metrics[0].keys())[0]:
+        return "smaller_is_better"
+    else:
+        fprint(
+            "Cannot determine the optimization direction. Trying to infer from the metrics."
+        )
+        is_prev_increasing = np.mean(list(prev_metrics[0].values())[0]) < np.mean(
+            list(prev_metrics[-1].values())[0]
+        )
+        is_still_increasing = np.mean(list(prev_metrics[1].values())[0]) < np.mean(
+            list(metrics.values())[0]
+        )
 
-    is_prev_decreasing = np.mean(list(prev_metrics[0].values())[0]) > np.mean(
-        list(prev_metrics[-1].values())[0]
-    )
-    is_still_decreasing = np.mean(list(prev_metrics[1].values())[0]) > np.mean(
-        list(metrics.values())
-    )
+        if is_prev_increasing and is_still_increasing:
+            return "larger_is_better"
 
-    if is_prev_decreasing and is_still_decreasing:
-        return "smaller_is_better"
+        is_prev_decreasing = np.mean(list(prev_metrics[0].values())[0]) > np.mean(
+            list(prev_metrics[-1].values())[0]
+        )
+        is_still_decreasing = np.mean(list(prev_metrics[1].values())[0]) > np.mean(
+            list(metrics.values())
+        )
+
+        if is_prev_decreasing and is_still_decreasing:
+            return "smaller_is_better"
 
 
 class Trainer:
     def __init__(
         self,
         model,
         train_loader: torch.utils.data.DataLoader = None,
@@ -103,27 +117,29 @@
             self.metrics.update({"best_valid": metrics})
 
         if _infer_optimization_direction(metrics, prev_metrics) == "larger_is_better":
             if np.mean(list(metrics.values())[0]) > np.mean(
                 list(self.metrics["best_valid"].values())[0]
             ):
                 self.metrics.update({"best_valid": metrics})
+                return True
+            else:
+                return False
         elif (
             _infer_optimization_direction(metrics, prev_metrics) == "smaller_is_better"
         ):
             if np.mean(list(metrics.values())[0]) < np.mean(
                 list(self.metrics["best_valid"].values())[0]
             ):
                 self.metrics.update({"best_valid": metrics})
-        else:
-            return False
-        if self.metrics["best_valid"] == metrics:
-            return True
-        else:
-            return False
+                return True
+            else:
+                return False
+
+        return False
 
     def train(self, path_to_save=None, autocast=False, **kwargs):
         seed_everything(self.seed)
         patience = 0
 
         if self.eval_loader is not None and len(self.eval_loader) > 0:
             valid_metrics = self.evaluate()
@@ -186,14 +202,16 @@
             _path_to_save = path_to_save + "_final"
             if self.metrics["test_metrics"]:
                 for key, value in self.metrics["test_metrics"][-1].items():
                     _path_to_save += f"_seed_{self.seed}_{key}_{value:.4f}"
 
             self.save_model(path_to_save, **kwargs)
 
+        self._remove_state_dict()
+
         return self.metrics
 
     def evaluate(self):
         valid_metrics = {}
         with torch.no_grad():
             self.model.eval()
             val_truth = []
@@ -244,31 +262,27 @@
     def save_model(self, path, overwrite=False, **kwargs):
         self.model.save(path, overwrite, **kwargs)
 
     def _load_state_dict(self):
         model_state_dict_path = (
             self.model.model.__class__.__name__ + "_init_model_state_dict.pt"
         )
-        optimizer_state_dict_path = (
-            self.model.model.__class__.__name__ + "_init_optimizer_state_dict.pt"
-        )
-        if os.path.exists(optimizer_state_dict_path):
-            self.optimizer.load_state_dict(torch.load(optimizer_state_dict_path))
         if os.path.exists(model_state_dict_path):
             self.model.load_state_dict(torch.load(model_state_dict_path))
         self.model.to(self.device)
 
     def _save_state_dict(self):
         model_state_dict_path = (
             self.model.model.__class__.__name__ + "_init_model_state_dict.pt"
         )
-        optimizer_state_dict_path = (
-            self.model.model.__class__.__name__ + "_init_optimizer_state_dict.pt"
-        )
         if os.path.exists(model_state_dict_path):
             os.remove(model_state_dict_path)
-        if os.path.exists(optimizer_state_dict_path):
-            os.remove(optimizer_state_dict_path)
         self.model.to("cpu")
-        torch.save(self.optimizer.state_dict(), optimizer_state_dict_path)
         torch.save(self.model.state_dict(), model_state_dict_path)
         self.model.to(self.device)
+
+    def _remove_state_dict(self):
+        model_state_dict_path = (
+            self.model.model.__class__.__name__ + "_init_model_state_dict.pt"
+        )
+        if os.path.exists(model_state_dict_path):
+            os.remove(model_state_dict_path)
```

## Comparing `OmniGenome-0.0.2a0.dist-info/RECORD` & `OmniGenome-0.0.3a0.dist-info/RECORD`

 * *Files 7% similar despite different names*

```diff
@@ -1,50 +1,50 @@
-omnigenome/__init__.py,sha256=BY-2xpD7tc45zUwGz08AGSuHlSh1g2tvUUyICDT0LFc,3712
+omnigenome/__init__.py,sha256=nVHH2iXy0Ytqx-g_tv639QIzx3MZHiVaUHX4um5uRac,3712
 omnigenome/bench/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 omnigenome/bench/auto_bench/__init__.py,sha256=JEdZznP93R9CQ2zZCXPOIxN0OizNa2RNa30EDkRCz3o,342
-omnigenome/bench/auto_bench/auto_bench.py,sha256=myB7g-Z59ZpKeJYr9BMZa_kq_WQ7eaxzUExzc7voh9o,7289
+omnigenome/bench/auto_bench/auto_bench.py,sha256=ktjoa13nwNmwbQc52OAhmazpnxKIlR3HFSN3IKx8kcY,7968
 omnigenome/bench/auto_bench/auto_bench_config.py,sha256=PN1B2EBJA43N5TApj-kvMPMuI02UDIH8oUKwDtDiWI4,7366
 omnigenome/bench/auto_bench/config_check.py,sha256=PWusimXDSLJyOivrVRMnMFolMI16ttAmdDSzb5_VrTE,971
 omnigenome/bench/bench_hub/__init__.py,sha256=JEdZznP93R9CQ2zZCXPOIxN0OizNa2RNa30EDkRCz3o,342
 omnigenome/bench/bench_hub/bench_hub.py,sha256=Up4JjqGgKH1jtYPRwvb4w1UpxE6W3FxlUEuUcEelyKI,439
 omnigenome/src/__init__.py,sha256=P_xAnv8ydAA49O2NeRBf7TEU78OZwGE5xEAT8V2FdoQ,342
 omnigenome/src/abc/__init__.py,sha256=ir0dYJyibYfFlZaRG4PzPgqNmhHReqrvFRg8KzmFc4w,342
-omnigenome/src/abc/abstract_dataset.py,sha256=XKos3Smt0GNFukOODPa6rnOw75DQqWW3KtmR_jbOrIY,10343
+omnigenome/src/abc/abstract_dataset.py,sha256=yA2HX1nDrfKR6p9_B4DFBSC_BfbX6yywmSc3n7h1PTE,12137
 omnigenome/src/abc/abstract_metric.py,sha256=xGHNFEcnSwZcnanEAUs9__cNLPzgtN5r26xc6x2ehWU,1773
-omnigenome/src/abc/abstract_model.py,sha256=lYrNSnwkJTtZoK3Lbchz2YRGV4TpRNGCgO3fqgv_vHM,13841
-omnigenome/src/abc/abstract_tokenizer.py,sha256=DnUDfRI7Xy0BMWloV2MtF84sxgoQ1zhKcQDStsPWjeM,3175
+omnigenome/src/abc/abstract_model.py,sha256=JaU-c4lvZ-WuAfVwMgHlmSIgC8b_NiIuejiWeO10xSM,13927
+omnigenome/src/abc/abstract_tokenizer.py,sha256=IRMZwy7og5aKd8M7XPEFc5CQyHnCIYj94tO6Vz-lPj8,3284
 omnigenome/src/dataset/__init__.py,sha256=GI5cNvFQyAGjlPtytIt3--nIcz-ZBQQZRAcf4tsHLkw,634
-omnigenome/src/dataset/omnigenome_dataset.py,sha256=bu6u_6P7wHivUcGfSpoiZlUOkVydTOQzdFODh_ldZWU,9110
+omnigenome/src/dataset/omnigenome_dataset.py,sha256=PQG_t89atEAZQWDQt4022UdrTaqU9g9Kri9HTM-7h2M,8896
 omnigenome/src/metric/__init__.py,sha256=9xXOdKCh1TtV0uwPW759-7pWwyT_pDU7GeSrKuztpYw,493
-omnigenome/src/metric/classification_metric.py,sha256=Xye7S-liP0MZhlkR-Ucn7L4j6Rzhl6cx0J2quCSeXeM,2660
-omnigenome/src/metric/ranking_metric.py,sha256=kWO6D5FOo1LSMFlOP8D4box36GUx2urHzevLIPHkuqo,2250
-omnigenome/src/metric/regression_metric.py,sha256=gvXEpCJLDxVFpHej8wdz5LrfokvZ0a28UrxTdY-Lk7c,2628
+omnigenome/src/metric/classification_metric.py,sha256=djDoVY7-QqyC0lai9ATpvysjB7LF1rqFhLr5DMrDeX8,2819
+omnigenome/src/metric/ranking_metric.py,sha256=FBW7ojH027ODbsd6a-K_4a5JMJVB0mWVrhcXgvSfZu4,2424
+omnigenome/src/metric/regression_metric.py,sha256=fm9U5_K6CXRSIMeNp5Z0gXkN1xpcOhu_jeDw15uxtgQ,2785
 omnigenome/src/misc/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 omnigenome/src/misc/utils.py,sha256=qyL9-nO9tgTHAcuawhdo7CLVPX1O2xF7TI2MPozMBFk,6784
 omnigenome/src/model/__init__.py,sha256=1eubHrpfpufX_njO9IUv3ofPbAdHT-XBRkItkzKi3CM,470
 omnigenome/src/model/classiifcation/__init__.py,sha256=dPugrswvrM58nVxS5FlCBnx2meuTPY9SzOA4YLrP-l4,342
-omnigenome/src/model/classiifcation/model.py,sha256=WCgB_LQ2rpjJgg2dthN6Cvovxc0AxhzgQ7ygUmoEaps,11809
+omnigenome/src/model/classiifcation/model.py,sha256=AgS3-rKrxj59KVjySjrSW_RcHKVoKXEOJS9geUWTGg8,12353
 omnigenome/src/model/mlm/__init__.py,sha256=0-S-GmzGIx7qB7Aixh_KovHxAcvKHgCZJcxYHifjZ-A,342
-omnigenome/src/model/mlm/model.py,sha256=Hw6ywJdN0lhonKAaKa3f86-vHBYfuihDgySo7hheGTg,4521
+omnigenome/src/model/mlm/model.py,sha256=JxtcLcP50eKny7fSgj_q0WbCM8t9OyIedcV2LqpM67o,4534
 omnigenome/src/model/regression/__init__.py,sha256=P_xAnv8ydAA49O2NeRBf7TEU78OZwGE5xEAT8V2FdoQ,342
-omnigenome/src/model/regression/model.py,sha256=phI-f4FXB3cDcrPKZsyEnx5cJCcNAILUHX8duMn9Ges,10948
+omnigenome/src/model/regression/model.py,sha256=IE8iaQzrazP7hUTrjPEvMidRNQJ7febgYDOt8lu8520,10956
 omnigenome/src/model/seq2seq/__init__.py,sha256=sP2vP_2XzMI80eWAbTT5cboKY9lxjPxe-Mtfiu_vK-A,342
 omnigenome/src/model/seq2seq/model.py,sha256=gr9ve3zn_Q8oveTZkjqsG2erow05dIRer1CMsfpAFCc,607
 omnigenome/src/tokenizer/__init__.py,sha256=uugoAoUSB-C-V4XJIoYLgAzB1nLKcTxGGb1bCBm4_-U,512
 omnigenome/src/tokenizer/bpe_tokenizer.py,sha256=4KZzG-IeXfeW6vZqiAe38o2qvBm1H4inSTAqfqENzEI,2996
 omnigenome/src/tokenizer/kmers_tokenizer.py,sha256=YlFWzOaNj-lxToAtkw-PyIyO7TrxcTZy_igysZhWwYU,4598
-omnigenome/src/tokenizer/single_nucleotide_tokenizer.py,sha256=Al_5tAXBr4b3m3_Wgz8UtzeJ_-CAVk6cZ2mp-yMumkE,3798
+omnigenome/src/tokenizer/single_nucleotide_tokenizer.py,sha256=ZeLEIz3fMs9J19mNvHZXcEYSuPpX4jFddd6k__1HVcI,3868
 omnigenome/src/trainer/__init__.py,sha256=oM-Jh4_-RDi0j2omG-7Vxwx3LAR_LAgc2gAERCWMi4Q,409
 omnigenome/src/trainer/hf_trainer.py,sha256=ZQipIZj0WXwHpgf8nsss2KRq9bCH4LI32Uo_iQKT5Bg,1092
-omnigenome/src/trainer/trainer.py,sha256=A_7T5illhGBIVtesQNwEovBD8f19HUQBUnULCTwpew8,10163
+omnigenome/src/trainer/trainer.py,sha256=CDpSexoW7jQhf-FphJ2JNyTO0bAgjMQznweej6kSKNU,10478
 omnigenome/utility/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 omnigenome/utility/ensemble.py,sha256=kpX-R_FpAYIkOX24iZle4Q_V7dcSDpim1jx2XJczra4,9554
 omnigenome/utility/hub_utils.py,sha256=8JEUdcs2BmicvwgjLjmBjmxkW7KmYC4Fg-3rYIkjOeY,10629
 omnigenome/utility/model_hub/__init__.py,sha256=p_rxaidCUZTW_Ws4q8B36RMiqK_wnRRsjtxA4CFp6lc,342
 omnigenome/utility/model_hub/model_hub.py,sha256=48U1qykhrFkaxvfc9LZmlwofmToJ6nRfUo0MnfLh28w,2993
 omnigenome/utility/pipeline_hub/__init__.py,sha256=5EQKxssQeyl195CeJG31QuwjTt8vrUWbBjim0vzzF1I,342
 omnigenome/utility/pipeline_hub/pipeline.py,sha256=ETxIVyAPISSszuCuDjrCmVdEWkwGITu3tSG6iBfRJ1c,5603
 omnigenome/utility/pipeline_hub/pipeline_hub.py,sha256=XJOHduSM-T7olGiqdTp-fg_QKQ9-y40IGg-IcoX24-0,883
-OmniGenome-0.0.2a0.dist-info/METADATA,sha256=GTFxWM6xfKQNKPVqGMvMajis2BxHJhADeQgyidzEZqg,861
-OmniGenome-0.0.2a0.dist-info/WHEEL,sha256=2wepM1nk4DS4eFpYrW1TTqPcoGNfHhhO_i5m4cOimbo,92
-OmniGenome-0.0.2a0.dist-info/top_level.txt,sha256=LVFxm_WPaxjj9KnAqdW94W4D4lbOk30gdsaKlJiSzTo,11
-OmniGenome-0.0.2a0.dist-info/RECORD,,
+OmniGenome-0.0.3a0.dist-info/METADATA,sha256=RjmiIiy_7HvoHP4urkEGgsetZWYd7anLhcr1VW_TPoo,2272
+OmniGenome-0.0.3a0.dist-info/WHEEL,sha256=2wepM1nk4DS4eFpYrW1TTqPcoGNfHhhO_i5m4cOimbo,92
+OmniGenome-0.0.3a0.dist-info/top_level.txt,sha256=LVFxm_WPaxjj9KnAqdW94W4D4lbOk30gdsaKlJiSzTo,11
+OmniGenome-0.0.3a0.dist-info/RECORD,,
```

